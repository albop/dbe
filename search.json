[
  {
    "objectID": "pushups/pushups_1_correction.html",
    "href": "pushups/pushups_1_correction.html",
    "title": "Basics and Collections",
    "section": "",
    "text": "See “Check Your Understanding” from Basics and Collections\n\n\nBelow this cell, add\n\nA Markdown cell with\n\n\n\ntwo levels of headings;\n\na numbered list (We ask for a list in Markdown, not a Python list object);\n\nan unnumbered list (again not a Python list object);\n\ntext with a * and a - sign (hint: look at this cell and escape characters)\n\nbackticked code (see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n\n\n\nA Markdown cell with\n\n\n\nthe quadratic formula embedded in the cell using LaTeX\n\nresponse:",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-1",
    "href": "pushups/pushups_1_correction.html#question-1",
    "title": "Basics and Collections",
    "section": "",
    "text": "Below this cell, add\n\nA Markdown cell with\n\n\n\ntwo levels of headings;\n\na numbered list (We ask for a list in Markdown, not a Python list object);\n\nan unnumbered list (again not a Python list object);\n\ntext with a * and a - sign (hint: look at this cell and escape characters)\n\nbackticked code (see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n\n\n\nA Markdown cell with\n\n\n\nthe quadratic formula embedded in the cell using LaTeX\n\nresponse:",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#is-a-rose",
    "href": "pushups/pushups_1_correction.html#is-a-rose",
    "title": "Basics and Collections",
    "section": "is a rose",
    "text": "is a rose\n\nis a rose\n\nbrexit\nis brexit\nis brexit\n\nHere is nice quadratic formula: \\[\nx=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\n\\]",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-2",
    "href": "pushups/pushups_1_correction.html#question-2",
    "title": "Basics and Collections",
    "section": "Question 2",
    "text": "Question 2\nComplete the following code, which sets up variables a, b, and c, to find the roots using the quadratic formula.\n\\[\nx=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\n\\]\nNote: because there are two roots, you will need to calculate two values of x\n\na = 1.0\nb = 2.0\nc = 1.0\n# we need the square root function\nfrom math import sqrt\n\nx1 = -b+sqrt(b**2-4*a*c)/(2*a)\nx2 = -b-sqrt(b**2-4*a*c)/(2*a)\n\nprint(f\"The two roots are {x1} and {x2}\")\n\nThe two roots are -2.0 and -2.0",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-3",
    "href": "pushups/pushups_1_correction.html#question-3",
    "title": "Basics and Collections",
    "section": "Question 3",
    "text": "Question 3\nIn the cell below, use tab completion to find a function from the time module that displays the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the name of the function you found) to see information about that function, then call the function. (Hint: look for something involving the word local).\n\nimport time\n# Your code goes here\n# time. # uncomment and hit &lt;TAB&gt; to see functions\n\ntime.localtime()\n\ntime.struct_time(tm_year=2023, tm_mon=2, tm_mday=7, tm_hour=23, tm_min=15, tm_sec=33, tm_wday=1, tm_yday=38, tm_isdst=0)\n\n\nHint: if you are using an online jupyter server, the time will be based on the server settings. If it doesn’t match your location, don’t worry about it.",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-4",
    "href": "pushups/pushups_1_correction.html#question-4",
    "title": "Basics and Collections",
    "section": "Question 4",
    "text": "Question 4\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with the value 0.025\n\nT: An integer with the value 30\n\nCompute the present discounted value of a payment (D) made in T years, assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nHint: The formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# Your code goes here\nD = 10.0\nr = 0.025\nT = 30\n\nPDV = D/(1+r)**T\n\nf\"Present discount factor is: {PDV: .2f}\"   # the .2f is an instruction to limit the number of float numbers\n\n'Present discount factor is:  4.77'",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-5",
    "href": "pushups/pushups_1_correction.html#question-5",
    "title": "Basics and Collections",
    "section": "Question 5",
    "text": "Question 5\nHow could you use the variables x and y to create the sentence Hello World ?\nHint: Think about how to represent a space as a string.\n\nx = \"Hello\"\ny = \"World\"\n# Your code goes here\n\nx+\" \"+y\n\n'Hello World'",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-6",
    "href": "pushups/pushups_1_correction.html#question-6",
    "title": "Basics and Collections",
    "section": "Question 6",
    "text": "Question 6\nSuppose you are working with price data and come across the value \"€6.50\".\nWhen Python tries to interpret this value, it sees the value as the string \"€6.50\" instead of the number 6.50. (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nHint: Once the string is in a suitable format, you can call float(clean_price) to make it a number.\n\nprice = \"€6.50\"\n# Your code goes here\n\n# remove the euro symbol\nprice[1:]\ns = price.strip(\"€\")\n\nfloat(s)\n\n6.5",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-7",
    "href": "pushups/pushups_1_correction.html#question-7",
    "title": "Basics and Collections",
    "section": "Question 7",
    "text": "Question 7\nUse Python formatting (e.g. print(f\"text {somecode}\") where somecode is a valid expression or variable name) to produce the following output.\nThe 1st quarter revenue was $110M\nThe 2nd quarter revenue was $95M\nThe 3rd quarter revenue was $100M\nThe 4th quarter revenue was $130M\n\nns = [110, 95, 100, 130]\nqs = [\"1st\", \"2nd\", \"3rd\", \"4th\"]\n\n\nfor i in [0, 1, 2, 3]:\n    q = qs[i]\n    n = ns[i]\n    s = f\"The {q} quarter revenue was {n}M\"\n    print(s)\n\nThe 1st quarter revenue was 110M\nThe 2nd quarter revenue was 95M\nThe 3rd quarter revenue was 100M\nThe 4th quarter revenue was 130M",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1_correction.html#question-8",
    "href": "pushups/pushups_1_correction.html#question-8",
    "title": "Basics and Collections",
    "section": "Question 8",
    "text": "Question 8\nDefine two lists y and z.\nThey can contain anything you want.\nCheck what happens when you do y + z. When you have finished that, try 2 * x and x * 2 where x represents the object you created from y + z.\nBriefly explain.\n\ny = [1,2,3,4,5] # fill me in!\nz = [\"once\", \"I\", \"caught\", \"a\", \"fish\", \"alive\"] # fill me in!\n# Your code goes here\n\n[1, 2, 3, 4, 5, 'once', 'I', 'caught', 'a', 'fish', 'alive']\n\n\n\nx = y+z # concatenates the two strings\nx\n\n[1, 2, 3, 4, 5, 'once', 'I', 'caught', 'a', 'fish', 'alive']\n\n\n\n2*x # repeats the list twice\n\n[1,\n 2,\n 3,\n 4,\n 5,\n 'once',\n 'I',\n 'caught',\n 'a',\n 'fish',\n 'alive',\n 1,\n 2,\n 3,\n 4,\n 5,\n 'once',\n 'I',\n 'caught',\n 'a',\n 'fish',\n 'alive']",
    "crumbs": [
      "pushups",
      "Basics and Collections"
    ]
  },
  {
    "objectID": "pushups/pushups_1.html",
    "href": "pushups/pushups_1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "See “Check Your Understanding” from Basics and Collections\n\n\nBelow this cell, add\n\nA Markdown cell with\n\n\n\ntwo levels of headings;\n\na numbered list (We ask for a list in Markdown, not a Python list object);\n\nan unnumbered list (again not a Python list object);\n\ntext with a * and a - sign (hint: look at this cell and escape characters)\n\nbackticked code (see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n\n\n\nA Markdown cell with\n\n\n\nthe quadratic formula embedded in the cell using LaTeX\n\n\n\n\nComplete the following code, which sets up variables a, b, and c, to find the roots using the quadratic formula.\n\\[\nx=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\n\\]\nNote: because there are two roots, you will need to calculate two values of x\n\na = 1.0\nb = 2.0\nc = 1.0\n# Your code goes here\n\n\n\n\nIn the cell below, use tab completion to find a function from the time module that displays the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the name of the function you found) to see information about that function, then call the function. (Hint: look for something involving the word local).\n\nimport time\n# Your code goes here\n# time. # uncomment and hit &lt;TAB&gt; to see functions\n\nHint: if you are using an online jupyter server, the time will be based on the server settings. If it doesn’t match your location, don’t worry about it.\n\n\n\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with the value 0.025\n\nT: An integer with the value 30\n\nCompute the present discounted value of a payment (D) made in T years, assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nHint: The formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# Your code goes here\n\n\n\n\nHow could you use the variables x and y to create the sentence Hello World ?\nHint: Think about how to represent a space as a string.\n\nx = \"Hello\"\ny = \"World\"\n# Your code goes here\n\n\n\n\nSuppose you are working with price data and come across the value \"€6.50\".\nWhen Python tries to interpret this value, it sees the value as the string \"€6.50\" instead of the number 6.50. (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nHint: Once the string is in a suitable format, you can call float(clean_price) to make it a number.\n\nprice = \"€6.50\"\n# Your code goes here\n\n\n\n\nUse Python formatting (e.g. print(f\"text {somecode}\") where somecode is a valid expression or variable name) to produce the following output.\nThe 1st quarter revenue was $110M\nThe 2nd quarter revenue was $95M\nThe 3rd quarter revenue was $100M\nThe 4th quarter revenue was $130M\n\n# Your code goes here\n\n\n\n\nDefine two lists y and z.\nThey can contain anything you want.\nCheck what happens when you do y + z. When you have finished that, try 2 * x and x * 2 where x represents the object you created from y + z.\nBriefly explain.\n\ny = [] # fill me in!\nz = [] # fill me in!\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-1",
    "href": "pushups/pushups_1.html#question-1",
    "title": "Problem Set 1",
    "section": "",
    "text": "Below this cell, add\n\nA Markdown cell with\n\n\n\ntwo levels of headings;\n\na numbered list (We ask for a list in Markdown, not a Python list object);\n\nan unnumbered list (again not a Python list object);\n\ntext with a * and a - sign (hint: look at this cell and escape characters)\n\nbackticked code (see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n\n\n\nA Markdown cell with\n\n\n\nthe quadratic formula embedded in the cell using LaTeX"
  },
  {
    "objectID": "pushups/pushups_1.html#question-2",
    "href": "pushups/pushups_1.html#question-2",
    "title": "Problem Set 1",
    "section": "",
    "text": "Complete the following code, which sets up variables a, b, and c, to find the roots using the quadratic formula.\n\\[\nx=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\n\\]\nNote: because there are two roots, you will need to calculate two values of x\n\na = 1.0\nb = 2.0\nc = 1.0\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-3",
    "href": "pushups/pushups_1.html#question-3",
    "title": "Problem Set 1",
    "section": "",
    "text": "In the cell below, use tab completion to find a function from the time module that displays the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the name of the function you found) to see information about that function, then call the function. (Hint: look for something involving the word local).\n\nimport time\n# Your code goes here\n# time. # uncomment and hit &lt;TAB&gt; to see functions\n\nHint: if you are using an online jupyter server, the time will be based on the server settings. If it doesn’t match your location, don’t worry about it."
  },
  {
    "objectID": "pushups/pushups_1.html#question-4",
    "href": "pushups/pushups_1.html#question-4",
    "title": "Problem Set 1",
    "section": "",
    "text": "Create the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with the value 0.025\n\nT: An integer with the value 30\n\nCompute the present discounted value of a payment (D) made in T years, assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nHint: The formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-5",
    "href": "pushups/pushups_1.html#question-5",
    "title": "Problem Set 1",
    "section": "",
    "text": "How could you use the variables x and y to create the sentence Hello World ?\nHint: Think about how to represent a space as a string.\n\nx = \"Hello\"\ny = \"World\"\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-6",
    "href": "pushups/pushups_1.html#question-6",
    "title": "Problem Set 1",
    "section": "",
    "text": "Suppose you are working with price data and come across the value \"€6.50\".\nWhen Python tries to interpret this value, it sees the value as the string \"€6.50\" instead of the number 6.50. (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nHint: Once the string is in a suitable format, you can call float(clean_price) to make it a number.\n\nprice = \"€6.50\"\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-7",
    "href": "pushups/pushups_1.html#question-7",
    "title": "Problem Set 1",
    "section": "",
    "text": "Use Python formatting (e.g. print(f\"text {somecode}\") where somecode is a valid expression or variable name) to produce the following output.\nThe 1st quarter revenue was $110M\nThe 2nd quarter revenue was $95M\nThe 3rd quarter revenue was $100M\nThe 4th quarter revenue was $130M\n\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_1.html#question-8",
    "href": "pushups/pushups_1.html#question-8",
    "title": "Problem Set 1",
    "section": "",
    "text": "Define two lists y and z.\nThey can contain anything you want.\nCheck what happens when you do y + z. When you have finished that, try 2 * x and x * 2 where x represents the object you created from y + z.\nBriefly explain.\n\ny = [] # fill me in!\nz = [] # fill me in!\n# Your code goes here"
  },
  {
    "objectID": "pushups/pushups_2_0.html",
    "href": "pushups/pushups_2_0.html",
    "title": "Pushups 2",
    "section": "",
    "text": "Verify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\nt = (2,2,4)\n\n\n# change first element of t\nt[0] = 100\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n# appending to t\nt.append(3)\n\nAttributeError: 'tuple' object has no attribute 'append'\n\n\n\n# sorting t\nt.sort()\n\nAttributeError: 'tuple' object has no attribute 'sort'\n\n\n\n# this works and returns a new list\nsorted(t)\n\n[2, 2, 4]\n\n\n\n# reversing t\nt.reverse()\n\nAttributeError: 'tuple' object has no attribute 'reverse'\n\n\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n{\n    \"AAPL\": 175.96,\n    \"GOOGL\": 1047.43,\n    \"TVIX\": 8.38\n}\n\n{'AAPL': 175.96, 'GOOGL': 1047.43, 'TVIX': 8.38}\n\n\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\naustralia_data = {\n    \"coast_line\": 253760, #int\n    \"languages\": { # dict\n        \"English\": 72,\n        \"Mandarin\": 2.7,\n        \"Arabic\": 1.4,\n        \"Vietnamese\": 1.3,\n        \"Cantonese\": 1.2,\n        \"other\": 15.7,\n        \"unspecified\": 5.7\n    },\n    \"irrigated_land\": 15.210, #float\n    \"aquifers\": [\"Great Artesian Basin\", \"Canning Basin\"], # list\n    # string\n    \"population_distribution\": \"\"\"population is primarily located on the periphery, with the highest concentration of people residing in the east and southeast; a secondary population center is located in and around Perth in the west; of the States and Territories, New South Wales has, by far, the largest population; the interior, or \"outback\", has a very sparse population\"\"\"\n}\n\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\naustralia_data.pop(\"irrigated_land\")\n\n15.21\n\n\n\naustralia_data\n\n{'coast_line': 253760,\n 'languages': {'English': 72,\n  'Mandarin': 2.7,\n  'Arabic': 1.4,\n  'Vietnamese': 1.3,\n  'Cantonese': 1.2,\n  'other': 15.7,\n  'unspecified': 5.7},\n 'aquifers': ['Great Artesian Basin', 'Canning Basin'],\n 'population_distribution': 'population is primarily located on the periphery, with the highest concentration of people residing in the east and southeast; a secondary population center is located in and around Perth in the west; of the States and Territories, New South Wales has, by far, the largest population; the interior, or \"outback\", has a very sparse population'}\n\n\n\n\n\nExplain what happens to the value you popped.\n\nIt returns the value, and removes it from the dictionary object.\n\n\n'irrigated_land' in australia_data\n\nFalse\n\n\nExperiment with calling pop twice.\n\naustralia_data.pop(\"irrigated_land\")\n## errors because the key is not in the dictionary any more\n\nKeyError: 'irrigated_land'\n\n\n\n\n\n\nNote, if dbnomics is not installed on your machine, install it by running the following cell:\n\n!pip install dbnomics\n\nChoose any OECD country you like and download historical, yearly data for inflation and gdp growth from the dbnomics website.\nCheck the maximum common availability period for both series.\nCompute average and standard deviations for both series.\nBonus: try to get the relevant data as one single dataframe."
  },
  {
    "objectID": "pushups/pushups_2_0.html#tuples-and-dictionaries",
    "href": "pushups/pushups_2_0.html#tuples-and-dictionaries",
    "title": "Pushups 2",
    "section": "",
    "text": "Verify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\nt = (2,2,4)\n\n\n# change first element of t\nt[0] = 100\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n# appending to t\nt.append(3)\n\nAttributeError: 'tuple' object has no attribute 'append'\n\n\n\n# sorting t\nt.sort()\n\nAttributeError: 'tuple' object has no attribute 'sort'\n\n\n\n# this works and returns a new list\nsorted(t)\n\n[2, 2, 4]\n\n\n\n# reversing t\nt.reverse()\n\nAttributeError: 'tuple' object has no attribute 'reverse'\n\n\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n{\n    \"AAPL\": 175.96,\n    \"GOOGL\": 1047.43,\n    \"TVIX\": 8.38\n}\n\n{'AAPL': 175.96, 'GOOGL': 1047.43, 'TVIX': 8.38}\n\n\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\naustralia_data = {\n    \"coast_line\": 253760, #int\n    \"languages\": { # dict\n        \"English\": 72,\n        \"Mandarin\": 2.7,\n        \"Arabic\": 1.4,\n        \"Vietnamese\": 1.3,\n        \"Cantonese\": 1.2,\n        \"other\": 15.7,\n        \"unspecified\": 5.7\n    },\n    \"irrigated_land\": 15.210, #float\n    \"aquifers\": [\"Great Artesian Basin\", \"Canning Basin\"], # list\n    # string\n    \"population_distribution\": \"\"\"population is primarily located on the periphery, with the highest concentration of people residing in the east and southeast; a secondary population center is located in and around Perth in the west; of the States and Territories, New South Wales has, by far, the largest population; the interior, or \"outback\", has a very sparse population\"\"\"\n}\n\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\naustralia_data.pop(\"irrigated_land\")\n\n15.21\n\n\n\naustralia_data\n\n{'coast_line': 253760,\n 'languages': {'English': 72,\n  'Mandarin': 2.7,\n  'Arabic': 1.4,\n  'Vietnamese': 1.3,\n  'Cantonese': 1.2,\n  'other': 15.7,\n  'unspecified': 5.7},\n 'aquifers': ['Great Artesian Basin', 'Canning Basin'],\n 'population_distribution': 'population is primarily located on the periphery, with the highest concentration of people residing in the east and southeast; a secondary population center is located in and around Perth in the west; of the States and Territories, New South Wales has, by far, the largest population; the interior, or \"outback\", has a very sparse population'}\n\n\n\n\n\nExplain what happens to the value you popped.\n\nIt returns the value, and removes it from the dictionary object.\n\n\n'irrigated_land' in australia_data\n\nFalse\n\n\nExperiment with calling pop twice.\n\naustralia_data.pop(\"irrigated_land\")\n## errors because the key is not in the dictionary any more\n\nKeyError: 'irrigated_land'"
  },
  {
    "objectID": "pushups/pushups_2_0.html#prep-work-for-the-philips-curve",
    "href": "pushups/pushups_2_0.html#prep-work-for-the-philips-curve",
    "title": "Pushups 2",
    "section": "",
    "text": "Note, if dbnomics is not installed on your machine, install it by running the following cell:\n\n!pip install dbnomics\n\nChoose any OECD country you like and download historical, yearly data for inflation and gdp growth from the dbnomics website.\nCheck the maximum common availability period for both series.\nCompute average and standard deviations for both series.\nBonus: try to get the relevant data as one single dataframe."
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions.html",
    "href": "tutorials/session_6/machine_learning_regressions.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:\n\ncreate a training set and a validation set\ntrain a model with sklearn\nperform a validation test\n\n\n\nImport the diabetes dataset from sklearn. Describe it.\nSplit the dataset into a training set (70%) and a test set (30%)\nTrain a linear model (with intercept) on the training set\nCompute the fitting score on the test set.\nShould we adjust the size of the test set? What would be the problem?\nImplement \\(k\\)-fold model with \\(k=3\\).\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\n\n\nImport the California Housing Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?"
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "href": "tutorials/session_6/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\nSplit the dataset into a training set (70%) and a test set (30%)\nTrain a linear model (with intercept) on the training set\nCompute the fitting score on the test set.\nShould we adjust the size of the test set? What would be the problem?\nImplement \\(k\\)-fold model with \\(k=3\\).\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?"
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "tutorials/session_6/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the California Housing Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html",
    "href": "tutorials/session_1/qe_control_answers.html",
    "title": "Control Flow",
    "section": "",
    "text": "Prerequisites\n\nBooleans section in Basics\n\nCollections\n\nOutcomes\n\nAsset pricing and NPV\n\nUnderstand basic principles of pricing assets with deterministic payoffs\n\nApply programming with iteration and conditionals to asset pricing examples\n\n\nConditionals\n\nUnderstand what a conditional is\n\nBe able to construct if/elif/else conditional blocks\n\nUnderstand how conditionals can be used to selectively execute blocks of code\n\n\nIteration\n\nUnderstand what an iterable is\n\nBe able to write for and while loops\n\nUnderstand the keywords break and continue\n\n\n\n\n\nIn this lecture, we’ll introduce two related topics from economics:\n\nNet present valuations\n\nAsset pricing\n\nThese topics will motivate some of the programming we do in this course.\nIn economics and finance, “assets” provide a stream of payoffs.\nThese “assets” can be concrete or abstract: a stock pays dividends over time, a bond pays interest, an apple tree provides apples, a job pays wages, and an education provides possible jobs (which, in turn, pay wages).\nWhen deciding the price to pay for an asset or how to choose between different alternatives, we need to take into account that most people would prefer to receive 1 today vs. 1 next year.\nThis reflection on consumer preferences leads to the notion of a discount rate. If you are indifferent between receiving 1.00 today and 1.10 next year, then the discount rate over the next year is $ r = 0.10 $.\nIf we assume that an individuals preferences are consistent over time, then we can apply that same discount rate to valuing assets further into the future.\nFor example, we would expect that the consumer would be indifferent between consuming 1.00 today and $ (1+r)(1+r) = 1.21 $ dollars two years from now (i.e. discount twice).\nInverting this formula, 1 delivered two years from now is equivalent to $ $ today.\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\nIf an asset pays a stream of payoffs over multiple time periods, then we can use a discount rate to calculate the value to the consumer of a entire sequence of payoffs.\nMost generally, we enumerate each discrete time period (e.g. year, month, day) by the index $ t $ where today is $ t=0 $ and the asset lives for $ T $ periods.\nList the payoff at each time period as $ y_t $, which we will assume, for now, is known in advance.\nThen if the discount factor is $ r $, the consumer “values” the payoff $ y_t $ delivered at time $ t $ as $ y_t $ where we note that if $ t=0 \\(, the value is just the current payoff\\) y_0 $.\nUsing this logic, we can write an expression for the value of the entire sequence of payoffs with a sum.\n \\[\nP_0 = \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t y_t \\tag{1}\n\\]\nIf $ y_t $ is a constant, then we can compute this sum with a simple formula!\nBelow, we present some useful formulas that come from infinite series that we will use to get our net present value formula.\nFor any constant $ 0 &lt; &lt; 1 $ and integer value $ &gt; 0 $,\n \\[\n\\begin{aligned}\n\\sum_{t=0}^{\\infty} \\beta^t & = \\frac{1}{1-\\beta}\\\\\n\\sum_{t=0}^{\\tau} \\beta^t &= \\frac{1- \\beta^{\\tau+1}}{1-\\beta}\\\\\n\\sum_{t=\\tau}^{\\infty} \\beta^t &=  \\frac{\\beta^{\\tau}}{1-\\beta}\n\\end{aligned} \\tag{2}\n\\]\nIn the case of an asset which pays one dollar until time $ T $, we can use these formulas, taking $ = $ and $ T = $, to find\n\\[\n\\begin{aligned}\nP_0 &= \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t = \\frac{1- (\\frac{1}{1+r})^{\\tau+1}}{1-\\frac{1}{1+r}}\\\\\n&= \\frac{1 + r}{r} - \\frac{1}{r}\\left(\\frac{1}{1+r} \\right)^\\tau\n\\end{aligned}\n\\]\nNote that we can also consider an asset that lives and pays forever if $ T= $, and from (2), the value of an asset which pays 1 forever is $ $.\n\n\n\n\nSometimes, we will only want to execute some piece of code if a certain condition is met.\nThese conditions can be anything.\nFor example, we might add to total sales if the transaction value is positive, but add to total returns if the value is negative.\nOr, we might want to add up all incurred costs, only if the transaction happened before a certain date.\nWe use conditionals to run particular pieces of code when certain criterion are met.\nConditionals are closely tied to booleans, so if you don’t remember what those are, go back to the basics lecture for a refresher.\nThe basic syntax for conditionals is\n\nif condition:\n    # code to run when condition is True\nelse:\n    # code to run if no conditions above are True\n\nNote that immediately following the condition, there is a colon and that the next line begins with blank spaces.\nUsing 4 spaces is a very strong convention, so that is what we do — we recommend that you do the same.\nAlso note that the else clause is optional.\nLet’s see some simple examples.\n\nif True:\n    print(\"This is where `True` code is run\")\n\nAlternatively, you could have a test which returns a booleans\n\nif 1 &lt; 2:\n     print(\"This is where `True` code is run\")\n\nThis example is equivalent to just typing the print statement, but the example below isn’t…\n\nif False:\n    print(\"This is where `True` code is run\")\n\nOr\n\nif 1 &gt; 2:\n     print(\"This is where `True` code is run\")\n\nNotice that when you run the cells above nothing is printed.\nThat is because the condition for the if statement was not true, so the code inside the indented block was never run.\nThis also allows us to demonstrate the role of indentation in determining the “block” of code.\n\nval = False\n\nif val is True: # check an expression\n    print(\"This is where `True` code is run\")\n    print(\"More code in the if block\")\nprint(\"Code runs after 'if' block, regardless of val\")\n\n\n\n\nSee exercise 2 in the exercise list.\nThe next example shows us how else works.\n\nval = (2 == 4)  # returns False\nif val is True:\n    print(\"This is where `True` code is run\")\nelse:\n    print(\"This is where `False` code is run\")\n    print(\"More else code\")\nprint(\"Code runs after 'if' block, regardless of val\")\n\nThe if False: ... part of this example is the same as the example before, but now, we added an else: clause.\nIn this case, because the conditional for the if statement was not True, the if code block was not executed, but the else block was.\nFinally, the Condition is True is assumed in the if statement, and is often left out. For example, the following are identical\n\nif (1 &lt; 2) is True:\n    print(\"1 &lt; 2\")\n\nif 1 &lt; 2:\n    print(\"1 &lt; 2\")\n\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nSee exercise 4 in the exercise list.\n\n\nSometimes, you have more than one condition you want to check.\nFor example, you might want to run a different set of code based on which quarter a particular transaction took place in.\nIn this case you could check whether the date is in Q1, or in Q2, or in Q3, or if not any of these it must be in Q4.\nThe way to express this type of conditional is to use one or more elif clause in addition to the if and the else.\nThe syntax is\n\nif condition1:\n    # code to run when condition1 is True\nelif condition2:\n    # code to run when condition2 is True\nelif condition3:\n    # code to run when condition3 is True\nelse:\n    # code to run when none of the above are true\n\nYou can include as many elif clauses as you want.\nAs before, the else part is optional.\nHere’s how we might express the quarter example referred to above.\n\nimport datetime\nhalloween = datetime.date(2017, 10, 31)\n\nif halloween.month &gt; 9:\n    print(\"Halloween is in Q4\")\nelif halloween.month &gt; 6:\n    print(\"Halloween is in Q3\")\nelif halloween.month &gt; 3:\n    print(\"Halloween is in Q2\")\nelse:\n    print(\"Halloween is in Q1\")\n\nNote that when there are multiple if or elif conditions, only the code corresponding to the first true clause is run.\nWe saw this in action above.\nWe know that when halloween.month &gt; 9 is true, then halloween.month &gt; 6 and halloween.month &gt; 3 must also be true, but only the code block associated with halloween.month &gt; 9 was printed.\n\n\n\n\nWhen doing computations or analyzing data, we often need to repeat certain operations a finite number of times or until some condition is met.\nExamples include processing all data files in a directory (folder), aggregating revenues and costs for every period in a year, or computing the net present value of certain assets. (In fact, later in this section, we will verify the equations that we wrote down above.)\nThese are all examples of a programming concept called iteration.\nWe feel the concept is best understood through example, so we will present a contrived example and then discuss the details behind doing iteration in Python.\n\n\nSuppose we wanted to print out the first 10 integers and their squares.\nWe could do something like this.\n\nprint(f\"1**2 = {1**2}\")\nprint(f\"2**2 = {2**2}\")\nprint(f\"3**2 = {3**2}\")\nprint(f\"4**2 = {4**2}\")\n# .. and so on until 10\n\nAs you can see, the code above is repetitive.\nFor each integer, the code is exactly the same except for the two places where the “current” integer appears.\nSuppose that I asked you to write the same print statement for an int stored in a variable named i.\nYou might write the following code:\n\nprint(f\"{i}**2 = {i**2}\")\n\nThis more general version of the operation suggests a strategy for achieving our goal with less repetition: have a variable i take on the values 1 through 10 (Quiz: How can we use range to create the numbers 1 to 10?) and run the line of code above for each new value of i.\nThis can be accomplished with a for loop!\n\nfor i in range(1, 11):\n     print(f\"{i}**2 = {i**2}\")\n\nWhoa, what just happened?\nThe integer i took on the values in range(1, 11) one by one and for each new value it did the operations in the indented block (here just one line that called the print function).\n\n\n\nThe general structure of a standard for loop is as follows.\n\nfor item in iterable:\n   # operation 1 with item\n   # operation 2 with item\n   # ...\n   # operation N with item\n\nwhere iterable is anything capable of producing one item at a time (see here for official definition from the Python team).\nWe’ve actually already seen some of the most common iterables!\nLists, tuples, dicts, and range/zip/enumerate objects are all iterables.\nNote that we can have as many operations as we want inside the indented block.\nWe will refer to the indented block as the “body” of the loop.\nWhen the for loop is executed, item will take on one value from iterable at a time and execute the loop body for each value.\n\n\n\n\nSee exercise 5 in the exercise list.\nWhen iterating, each item in iterable might actually contain more than one value.\nRecall that tuples (and lists) can be unpacked directly into variables.\n\ntup = (4, \"test\")\ni, x = tup\nprint(f\"i = {i}, x = {x}, tup = {tup}\")\n\nAlso, recall that the value of a enumerate(iterable) is a tuple of the form (i, x) where iterable[i] == x.\nWhen we use enumerate in a for loop, we can “unpack” both values at the same time as follows:\n\n# revenue by quarter\ncompany_revenue = [5.12, 5.20, 5.50, 6.50]\n\nfor index, value in enumerate(company_revenue):\n    print(f\"quarter {index} revenue is ${value} million\")\n\nSimilarly, the index can be used to access another vector.\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\nfor index, city in enumerate(cities):\n    state = states[index]\n    print(f\"{city} is in {state}\")\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nA related but slightly different form of iteration is to repeat something until some condition is met.\nThis is typically achieved using a while loop.\nThe structure of a while loop is\n\nwhile True_condition:\n    # repeat these steps\n\nwhere True_condition is some conditional statement that should evaluate to True when iterations should continue and False when Python should stop iterating.\nFor example, suppose we wanted to know the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nWe figure this out using a while loop as follows.\n\ntotal = 0\ni = 0\nwhile total &lt;= 1000:\n    i = i + 1\n    total = total + i\n\nprint(\"The answer is\", i)\n\nLet’s check our work.\n\n# Should be just less than 1000 because range(45) goes from 0 to 44\nsum(range(45))\n\n\n# should be between 990 + 45 = 1035\nsum(range(46))\n\nA warning: one common programming error with while loops is to forget to set the variable you use in the condition prior to executing. For example, take the following code which correctly sets a counter\n\ni = 0\n\nAnd then executes a while loop\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\nNo problems. But if you were to execute the above cell again, or another cell, the i=3 remains, and code is never executed (since i &lt; 3 begins as False).\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\n\n\nSometimes we want to stop a loop early if some condition is met.\nLet’s revisit the example of finding the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nClearly N must be less than 1000, so we know we will find the answer if we start with a for loop over all items in range(1001).\nThen, we can keep a running total as we proceed and tell Python to stop iterating through our range once total goes above 1000.\n\ntotal = 0\nfor i in range(1001):\n    total = total + i\n    if total &gt; 1000:\n        break\n\nprint(\"The answer is\", i)\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\nSometimes we might want to stop the body of a loop early if a condition is met.\nTo do this we can use the continue keyword.\nThe basic syntax for doing this is:\n\nfor item in iterable:\n    # always do these operations\n    if condition:\n        continue\n\n    # only do these operations if condition is False\n\nInside the loop body, Python will stop that loop iteration of the loop and continue directly to the next iteration when it encounters the continue statement.\nFor example, suppose I ask you to loop over the numbers 1 to 10 and print out the message “{i} An odd number!” whenever the number i is odd, and do nothing otherwise.\nYou can use continue to do this as follows:\n\nfor i in range(1, 11):\n    if i % 2 == 0:  # an even number... This is modulus division\n        continue\n\n    print(i, \"is an odd number!\")\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\n\n\nOften, we will want to perform a very simple operation for every element of some iterable and create a new iterable with these values.\nThis could be done by writing a for loop and saving each value, but often using what is called a comprehension is more readable.\nLike many Python concepts, a comprehension is easiest to understand through example.\nImagine that we have a list x with a list of numbers. We would like to create a list x2 which has the squared values of x.\n\nx = list(range(4))\n\n# Create squared values with a loop\nx2_loop = []\nfor x_val in x:\n    x2_loop.append(x_val**2)\n\n# Create squared values with a comprehension\nx2_comp = [x_val**2 for x_val in x]\n\nprint(x2_loop)\nprint(x2_comp)\n\nNotice that much of the same text appears when we do the operation in the loop and when we do the operation with the comprehension.\n\nWe need to specify what we are iterating over – in both cases, this is for x_val in x.\n\nWe need to square each element x_val**2.\n\nIt needs to be stored somewhere – in x2_loop, this is done by appending each element to a list, and in x2_comp, this is done automatically because the operation is enclosed in a list.\n\nWe can do comprehension with many different types of iterables, so we demonstrate a few more below.\n\n# Create a dictionary from lists\ntickers = [\"AAPL\", \"GOOGL\", \"TVIX\"]\nprices = [175.96, 1047.43, 8.38]\nd = {key: value for key, value in zip(tickers, prices)}\nd\n\n\n# Create a list from a dictionary\nd = {\"AMZN\": \"Seattle\", \"TVIX\": \"Zurich\", \"AAPL\": \"Cupertino\"}\n\nhq_cities = [d[ticker] for ticker in d.keys()]\nhq_cities\n\n\nimport math\n\n# List from list\nx = range(10)\n\nsin_x = [math.sin(x_val) for x_val in x]\nsin_x\n\n\n\n\nSee exercise 10 in the exercise list.\nFinally, we can use this approach to build complicated nested dictionaries.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nexports = [ {\"manufacturing\": 2.4, \"agriculture\": 1.5, \"services\": 0.5},\n            {\"manufacturing\": 2.5, \"agriculture\": 1.4, \"services\": 0.9},\n            {\"manufacturing\": 2.7, \"agriculture\": 1.4, \"services\": 1.5}]\ndata = zip(years, gdp_data,exports)\ndata_dict = {year : {\"gdp\" : gdp, \"exports\": exports} for year, gdp, exports in data}\nprint(data_dict)\n\n# total exports by year\n[data_dict[year][\"exports\"][\"services\"] for year in data_dict.keys()]\n\n\n\n\n\n\n\nGovernment bonds are often issued as zero-coupon bonds meaning that they make no payments throughout the entire time that they are held, but, rather make a single payment at the time of maturity.\nHow much should you be willing to pay for a zero-coupon bond that paid 100 in 10 years with an interest rate of 5%?\n\n# your code here\n\n(back to text)\n\n\n\nRun the following two variations on the code with only a single change in the indentation.\nAfter, modify the x to print 3 and then 2, 3 instead.\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\n    print(\"2\")\nprint(\"3\")\n\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\nprint(\"2\") # changed the indentation\nprint(\"3\")\n\n(back to text)\n\n\n\nUsing the code cell below as a start, print \"Good afternoon\" if the current_time is past noon.\nOtherwise, do nothing.\nWrite some conditional based on current_time.hour.\n\nimport datetime\ncurrent_time = datetime.datetime.now()\n\n## your code here\n\nmore text after\n(back to text)\n\n\n\nIn this example, you will generate a random number between 0 and 1 and then display “x &gt; 0.5” or “x &lt; 0.5” depending on the value of the number.\nThis also introduces a new package numpy.random for drawing random numbers (more in the randomness lecture).\n\nimport numpy as np\nx = np.random.random()\nprint(f\"x = {x}\")\n\n## your code here\n\n(back to text)\n\n\n\nIn economics, when an individual has some knowledge, skills, or education which provides them with a source of future income, we call it human capital.\nWhen a student graduating from high school is considering whether to continue with post-secondary education, they may consider that it gives them higher paying jobs in the future, but requires that they don’t begin working until after graduation.\nConsider the simplified example where a student has perfectly forecastable employment and is given two choices:\n\nBegin working immediately and make 40,000 a year until they retire 40 years later.\n\nPay 5,000 a year for the next 4 years to attend university, then get a job paying 50,000 a year until they retire 40 years after making the college attendance decision.\n\nShould the student enroll in school if the discount rate is r = 0.05?\n\n# Discount rate\nr = 0.05\n\n# High school wage\nw_hs = 40_000\n\n# College wage and cost of college\nc_college = 5_000\nw_college = 50_000\n\n# Compute npv of being a hs worker\n\n# Compute npv of attending college\n\n# Compute npv of being a college worker\n\n# Is npv_collegeworker - npv_collegecost &gt; npv_hsworker\n\n(back to text)\n\n\n\nInstead of the above, write a for loop that uses the lists of cities and states below to print the same “{city} is in {state}” using a zip instead of an enumerate.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# Your code here\n\n(back to text)\n\n\n\nCompanies often invest in training their employees to raise their productivity. Economists sometimes wonder why companies spend this money when this incentivizes other companies to hire their employees away with higher salaries since employees gain human capital from training?\nLet’s say that it costs a company 25,000 dollars to teach their employees Python, but it raises their output by 2,500 per month. How many months would an employee need to stay for the company to find it profitable to pay for their employees to learn Python if their discount rate is r = 0.01?\n\n# Define cost of teaching python\ncost = 25_000\nr = 0.01\n\n# Per month value\nadded_value = 2500\n\nn_months = 0\ntotal_npv = 0.0\n\n# Put condition below here\nwhile False: # (replace False with your condition here)\n    n_months = n_months + 1  # Increment how many months they've worked\n\n    # Increase total_npv\n\n(back to text)\n\n\n\nTry to find the index of the first value in x that is greater than 0.999 using a for loop and break.\ntry iterating over range(len(x)).\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nWrite a for loop that adds up all values in x that are greater than or equal to 0.5.\nUse the continue word to end the body of the loop early for all values of x that are less than 0.5.\nTry starting your loop with for value in x: instead of iterating over the indices of x.\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nReturning to our previous example: print “{city} is in {state}” for each combination using a zip and a comprehension.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# your code here\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#net-present-values-and-asset-pricing",
    "href": "tutorials/session_1/qe_control_answers.html#net-present-values-and-asset-pricing",
    "title": "Control Flow",
    "section": "",
    "text": "In this lecture, we’ll introduce two related topics from economics:\n\nNet present valuations\n\nAsset pricing\n\nThese topics will motivate some of the programming we do in this course.\nIn economics and finance, “assets” provide a stream of payoffs.\nThese “assets” can be concrete or abstract: a stock pays dividends over time, a bond pays interest, an apple tree provides apples, a job pays wages, and an education provides possible jobs (which, in turn, pay wages).\nWhen deciding the price to pay for an asset or how to choose between different alternatives, we need to take into account that most people would prefer to receive 1 today vs. 1 next year.\nThis reflection on consumer preferences leads to the notion of a discount rate. If you are indifferent between receiving 1.00 today and 1.10 next year, then the discount rate over the next year is $ r = 0.10 $.\nIf we assume that an individuals preferences are consistent over time, then we can apply that same discount rate to valuing assets further into the future.\nFor example, we would expect that the consumer would be indifferent between consuming 1.00 today and $ (1+r)(1+r) = 1.21 $ dollars two years from now (i.e. discount twice).\nInverting this formula, 1 delivered two years from now is equivalent to $ $ today."
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercise",
    "href": "tutorials/session_1/qe_control_answers.html#exercise",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 1 in the exercise list.\n\n\n\nIf an asset pays a stream of payoffs over multiple time periods, then we can use a discount rate to calculate the value to the consumer of a entire sequence of payoffs.\nMost generally, we enumerate each discrete time period (e.g. year, month, day) by the index $ t $ where today is $ t=0 $ and the asset lives for $ T $ periods.\nList the payoff at each time period as $ y_t $, which we will assume, for now, is known in advance.\nThen if the discount factor is $ r $, the consumer “values” the payoff $ y_t $ delivered at time $ t $ as $ y_t $ where we note that if $ t=0 \\(, the value is just the current payoff\\) y_0 $.\nUsing this logic, we can write an expression for the value of the entire sequence of payoffs with a sum.\n \\[\nP_0 = \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t y_t \\tag{1}\n\\]\nIf $ y_t $ is a constant, then we can compute this sum with a simple formula!\nBelow, we present some useful formulas that come from infinite series that we will use to get our net present value formula.\nFor any constant $ 0 &lt; &lt; 1 $ and integer value $ &gt; 0 $,\n \\[\n\\begin{aligned}\n\\sum_{t=0}^{\\infty} \\beta^t & = \\frac{1}{1-\\beta}\\\\\n\\sum_{t=0}^{\\tau} \\beta^t &= \\frac{1- \\beta^{\\tau+1}}{1-\\beta}\\\\\n\\sum_{t=\\tau}^{\\infty} \\beta^t &=  \\frac{\\beta^{\\tau}}{1-\\beta}\n\\end{aligned} \\tag{2}\n\\]\nIn the case of an asset which pays one dollar until time $ T $, we can use these formulas, taking $ = $ and $ T = $, to find\n\\[\n\\begin{aligned}\nP_0 &= \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t = \\frac{1- (\\frac{1}{1+r})^{\\tau+1}}{1-\\frac{1}{1+r}}\\\\\n&= \\frac{1 + r}{r} - \\frac{1}{r}\\left(\\frac{1}{1+r} \\right)^\\tau\n\\end{aligned}\n\\]\nNote that we can also consider an asset that lives and pays forever if $ T= $, and from (2), the value of an asset which pays 1 forever is $ $."
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#conditional-statements-and-blocks",
    "href": "tutorials/session_1/qe_control_answers.html#conditional-statements-and-blocks",
    "title": "Control Flow",
    "section": "",
    "text": "Sometimes, we will only want to execute some piece of code if a certain condition is met.\nThese conditions can be anything.\nFor example, we might add to total sales if the transaction value is positive, but add to total returns if the value is negative.\nOr, we might want to add up all incurred costs, only if the transaction happened before a certain date.\nWe use conditionals to run particular pieces of code when certain criterion are met.\nConditionals are closely tied to booleans, so if you don’t remember what those are, go back to the basics lecture for a refresher.\nThe basic syntax for conditionals is\n\nif condition:\n    # code to run when condition is True\nelse:\n    # code to run if no conditions above are True\n\nNote that immediately following the condition, there is a colon and that the next line begins with blank spaces.\nUsing 4 spaces is a very strong convention, so that is what we do — we recommend that you do the same.\nAlso note that the else clause is optional.\nLet’s see some simple examples.\n\nif True:\n    print(\"This is where `True` code is run\")\n\nAlternatively, you could have a test which returns a booleans\n\nif 1 &lt; 2:\n     print(\"This is where `True` code is run\")\n\nThis example is equivalent to just typing the print statement, but the example below isn’t…\n\nif False:\n    print(\"This is where `True` code is run\")\n\nOr\n\nif 1 &gt; 2:\n     print(\"This is where `True` code is run\")\n\nNotice that when you run the cells above nothing is printed.\nThat is because the condition for the if statement was not true, so the code inside the indented block was never run.\nThis also allows us to demonstrate the role of indentation in determining the “block” of code.\n\nval = False\n\nif val is True: # check an expression\n    print(\"This is where `True` code is run\")\n    print(\"More code in the if block\")\nprint(\"Code runs after 'if' block, regardless of val\")"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercise-1",
    "href": "tutorials/session_1/qe_control_answers.html#exercise-1",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 2 in the exercise list.\nThe next example shows us how else works.\n\nval = (2 == 4)  # returns False\nif val is True:\n    print(\"This is where `True` code is run\")\nelse:\n    print(\"This is where `False` code is run\")\n    print(\"More else code\")\nprint(\"Code runs after 'if' block, regardless of val\")\n\nThe if False: ... part of this example is the same as the example before, but now, we added an else: clause.\nIn this case, because the conditional for the if statement was not True, the if code block was not executed, but the else block was.\nFinally, the Condition is True is assumed in the if statement, and is often left out. For example, the following are identical\n\nif (1 &lt; 2) is True:\n    print(\"1 &lt; 2\")\n\nif 1 &lt; 2:\n    print(\"1 &lt; 2\")"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercise-2",
    "href": "tutorials/session_1/qe_control_answers.html#exercise-2",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 3 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercise-3",
    "href": "tutorials/session_1/qe_control_answers.html#exercise-3",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 4 in the exercise list.\n\n\nSometimes, you have more than one condition you want to check.\nFor example, you might want to run a different set of code based on which quarter a particular transaction took place in.\nIn this case you could check whether the date is in Q1, or in Q2, or in Q3, or if not any of these it must be in Q4.\nThe way to express this type of conditional is to use one or more elif clause in addition to the if and the else.\nThe syntax is\n\nif condition1:\n    # code to run when condition1 is True\nelif condition2:\n    # code to run when condition2 is True\nelif condition3:\n    # code to run when condition3 is True\nelse:\n    # code to run when none of the above are true\n\nYou can include as many elif clauses as you want.\nAs before, the else part is optional.\nHere’s how we might express the quarter example referred to above.\n\nimport datetime\nhalloween = datetime.date(2017, 10, 31)\n\nif halloween.month &gt; 9:\n    print(\"Halloween is in Q4\")\nelif halloween.month &gt; 6:\n    print(\"Halloween is in Q3\")\nelif halloween.month &gt; 3:\n    print(\"Halloween is in Q2\")\nelse:\n    print(\"Halloween is in Q1\")\n\nNote that when there are multiple if or elif conditions, only the code corresponding to the first true clause is run.\nWe saw this in action above.\nWe know that when halloween.month &gt; 9 is true, then halloween.month &gt; 6 and halloween.month &gt; 3 must also be true, but only the code block associated with halloween.month &gt; 9 was printed."
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#iteration",
    "href": "tutorials/session_1/qe_control_answers.html#iteration",
    "title": "Control Flow",
    "section": "",
    "text": "When doing computations or analyzing data, we often need to repeat certain operations a finite number of times or until some condition is met.\nExamples include processing all data files in a directory (folder), aggregating revenues and costs for every period in a year, or computing the net present value of certain assets. (In fact, later in this section, we will verify the equations that we wrote down above.)\nThese are all examples of a programming concept called iteration.\nWe feel the concept is best understood through example, so we will present a contrived example and then discuss the details behind doing iteration in Python.\n\n\nSuppose we wanted to print out the first 10 integers and their squares.\nWe could do something like this.\n\nprint(f\"1**2 = {1**2}\")\nprint(f\"2**2 = {2**2}\")\nprint(f\"3**2 = {3**2}\")\nprint(f\"4**2 = {4**2}\")\n# .. and so on until 10\n\nAs you can see, the code above is repetitive.\nFor each integer, the code is exactly the same except for the two places where the “current” integer appears.\nSuppose that I asked you to write the same print statement for an int stored in a variable named i.\nYou might write the following code:\n\nprint(f\"{i}**2 = {i**2}\")\n\nThis more general version of the operation suggests a strategy for achieving our goal with less repetition: have a variable i take on the values 1 through 10 (Quiz: How can we use range to create the numbers 1 to 10?) and run the line of code above for each new value of i.\nThis can be accomplished with a for loop!\n\nfor i in range(1, 11):\n     print(f\"{i}**2 = {i**2}\")\n\nWhoa, what just happened?\nThe integer i took on the values in range(1, 11) one by one and for each new value it did the operations in the indented block (here just one line that called the print function).\n\n\n\nThe general structure of a standard for loop is as follows.\n\nfor item in iterable:\n   # operation 1 with item\n   # operation 2 with item\n   # ...\n   # operation N with item\n\nwhere iterable is anything capable of producing one item at a time (see here for official definition from the Python team).\nWe’ve actually already seen some of the most common iterables!\nLists, tuples, dicts, and range/zip/enumerate objects are all iterables.\nNote that we can have as many operations as we want inside the indented block.\nWe will refer to the indented block as the “body” of the loop.\nWhen the for loop is executed, item will take on one value from iterable at a time and execute the loop body for each value.\n\n\n\n\nSee exercise 5 in the exercise list.\nWhen iterating, each item in iterable might actually contain more than one value.\nRecall that tuples (and lists) can be unpacked directly into variables.\n\ntup = (4, \"test\")\ni, x = tup\nprint(f\"i = {i}, x = {x}, tup = {tup}\")\n\nAlso, recall that the value of a enumerate(iterable) is a tuple of the form (i, x) where iterable[i] == x.\nWhen we use enumerate in a for loop, we can “unpack” both values at the same time as follows:\n\n# revenue by quarter\ncompany_revenue = [5.12, 5.20, 5.50, 6.50]\n\nfor index, value in enumerate(company_revenue):\n    print(f\"quarter {index} revenue is ${value} million\")\n\nSimilarly, the index can be used to access another vector.\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\nfor index, city in enumerate(cities):\n    state = states[index]\n    print(f\"{city} is in {state}\")\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nA related but slightly different form of iteration is to repeat something until some condition is met.\nThis is typically achieved using a while loop.\nThe structure of a while loop is\n\nwhile True_condition:\n    # repeat these steps\n\nwhere True_condition is some conditional statement that should evaluate to True when iterations should continue and False when Python should stop iterating.\nFor example, suppose we wanted to know the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nWe figure this out using a while loop as follows.\n\ntotal = 0\ni = 0\nwhile total &lt;= 1000:\n    i = i + 1\n    total = total + i\n\nprint(\"The answer is\", i)\n\nLet’s check our work.\n\n# Should be just less than 1000 because range(45) goes from 0 to 44\nsum(range(45))\n\n\n# should be between 990 + 45 = 1035\nsum(range(46))\n\nA warning: one common programming error with while loops is to forget to set the variable you use in the condition prior to executing. For example, take the following code which correctly sets a counter\n\ni = 0\n\nAnd then executes a while loop\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\nNo problems. But if you were to execute the above cell again, or another cell, the i=3 remains, and code is never executed (since i &lt; 3 begins as False).\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\n\n\nSometimes we want to stop a loop early if some condition is met.\nLet’s revisit the example of finding the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nClearly N must be less than 1000, so we know we will find the answer if we start with a for loop over all items in range(1001).\nThen, we can keep a running total as we proceed and tell Python to stop iterating through our range once total goes above 1000.\n\ntotal = 0\nfor i in range(1001):\n    total = total + i\n    if total &gt; 1000:\n        break\n\nprint(\"The answer is\", i)\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\nSometimes we might want to stop the body of a loop early if a condition is met.\nTo do this we can use the continue keyword.\nThe basic syntax for doing this is:\n\nfor item in iterable:\n    # always do these operations\n    if condition:\n        continue\n\n    # only do these operations if condition is False\n\nInside the loop body, Python will stop that loop iteration of the loop and continue directly to the next iteration when it encounters the continue statement.\nFor example, suppose I ask you to loop over the numbers 1 to 10 and print out the message “{i} An odd number!” whenever the number i is odd, and do nothing otherwise.\nYou can use continue to do this as follows:\n\nfor i in range(1, 11):\n    if i % 2 == 0:  # an even number... This is modulus division\n        continue\n\n    print(i, \"is an odd number!\")\n\n\n\n\nSee exercise 9 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#comprehension",
    "href": "tutorials/session_1/qe_control_answers.html#comprehension",
    "title": "Control Flow",
    "section": "",
    "text": "Often, we will want to perform a very simple operation for every element of some iterable and create a new iterable with these values.\nThis could be done by writing a for loop and saving each value, but often using what is called a comprehension is more readable.\nLike many Python concepts, a comprehension is easiest to understand through example.\nImagine that we have a list x with a list of numbers. We would like to create a list x2 which has the squared values of x.\n\nx = list(range(4))\n\n# Create squared values with a loop\nx2_loop = []\nfor x_val in x:\n    x2_loop.append(x_val**2)\n\n# Create squared values with a comprehension\nx2_comp = [x_val**2 for x_val in x]\n\nprint(x2_loop)\nprint(x2_comp)\n\nNotice that much of the same text appears when we do the operation in the loop and when we do the operation with the comprehension.\n\nWe need to specify what we are iterating over – in both cases, this is for x_val in x.\n\nWe need to square each element x_val**2.\n\nIt needs to be stored somewhere – in x2_loop, this is done by appending each element to a list, and in x2_comp, this is done automatically because the operation is enclosed in a list.\n\nWe can do comprehension with many different types of iterables, so we demonstrate a few more below.\n\n# Create a dictionary from lists\ntickers = [\"AAPL\", \"GOOGL\", \"TVIX\"]\nprices = [175.96, 1047.43, 8.38]\nd = {key: value for key, value in zip(tickers, prices)}\nd\n\n\n# Create a list from a dictionary\nd = {\"AMZN\": \"Seattle\", \"TVIX\": \"Zurich\", \"AAPL\": \"Cupertino\"}\n\nhq_cities = [d[ticker] for ticker in d.keys()]\nhq_cities\n\n\nimport math\n\n# List from list\nx = range(10)\n\nsin_x = [math.sin(x_val) for x_val in x]\nsin_x"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercise-9",
    "href": "tutorials/session_1/qe_control_answers.html#exercise-9",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 10 in the exercise list.\nFinally, we can use this approach to build complicated nested dictionaries.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nexports = [ {\"manufacturing\": 2.4, \"agriculture\": 1.5, \"services\": 0.5},\n            {\"manufacturing\": 2.5, \"agriculture\": 1.4, \"services\": 0.9},\n            {\"manufacturing\": 2.7, \"agriculture\": 1.4, \"services\": 1.5}]\ndata = zip(years, gdp_data,exports)\ndata_dict = {year : {\"gdp\" : gdp, \"exports\": exports} for year, gdp, exports in data}\nprint(data_dict)\n\n# total exports by year\n[data_dict[year][\"exports\"][\"services\"] for year in data_dict.keys()]"
  },
  {
    "objectID": "tutorials/session_1/qe_control_answers.html#exercises",
    "href": "tutorials/session_1/qe_control_answers.html#exercises",
    "title": "Control Flow",
    "section": "",
    "text": "Government bonds are often issued as zero-coupon bonds meaning that they make no payments throughout the entire time that they are held, but, rather make a single payment at the time of maturity.\nHow much should you be willing to pay for a zero-coupon bond that paid 100 in 10 years with an interest rate of 5%?\n\n# your code here\n\n(back to text)\n\n\n\nRun the following two variations on the code with only a single change in the indentation.\nAfter, modify the x to print 3 and then 2, 3 instead.\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\n    print(\"2\")\nprint(\"3\")\n\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\nprint(\"2\") # changed the indentation\nprint(\"3\")\n\n(back to text)\n\n\n\nUsing the code cell below as a start, print \"Good afternoon\" if the current_time is past noon.\nOtherwise, do nothing.\nWrite some conditional based on current_time.hour.\n\nimport datetime\ncurrent_time = datetime.datetime.now()\n\n## your code here\n\nmore text after\n(back to text)\n\n\n\nIn this example, you will generate a random number between 0 and 1 and then display “x &gt; 0.5” or “x &lt; 0.5” depending on the value of the number.\nThis also introduces a new package numpy.random for drawing random numbers (more in the randomness lecture).\n\nimport numpy as np\nx = np.random.random()\nprint(f\"x = {x}\")\n\n## your code here\n\n(back to text)\n\n\n\nIn economics, when an individual has some knowledge, skills, or education which provides them with a source of future income, we call it human capital.\nWhen a student graduating from high school is considering whether to continue with post-secondary education, they may consider that it gives them higher paying jobs in the future, but requires that they don’t begin working until after graduation.\nConsider the simplified example where a student has perfectly forecastable employment and is given two choices:\n\nBegin working immediately and make 40,000 a year until they retire 40 years later.\n\nPay 5,000 a year for the next 4 years to attend university, then get a job paying 50,000 a year until they retire 40 years after making the college attendance decision.\n\nShould the student enroll in school if the discount rate is r = 0.05?\n\n# Discount rate\nr = 0.05\n\n# High school wage\nw_hs = 40_000\n\n# College wage and cost of college\nc_college = 5_000\nw_college = 50_000\n\n# Compute npv of being a hs worker\n\n# Compute npv of attending college\n\n# Compute npv of being a college worker\n\n# Is npv_collegeworker - npv_collegecost &gt; npv_hsworker\n\n(back to text)\n\n\n\nInstead of the above, write a for loop that uses the lists of cities and states below to print the same “{city} is in {state}” using a zip instead of an enumerate.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# Your code here\n\n(back to text)\n\n\n\nCompanies often invest in training their employees to raise their productivity. Economists sometimes wonder why companies spend this money when this incentivizes other companies to hire their employees away with higher salaries since employees gain human capital from training?\nLet’s say that it costs a company 25,000 dollars to teach their employees Python, but it raises their output by 2,500 per month. How many months would an employee need to stay for the company to find it profitable to pay for their employees to learn Python if their discount rate is r = 0.01?\n\n# Define cost of teaching python\ncost = 25_000\nr = 0.01\n\n# Per month value\nadded_value = 2500\n\nn_months = 0\ntotal_npv = 0.0\n\n# Put condition below here\nwhile False: # (replace False with your condition here)\n    n_months = n_months + 1  # Increment how many months they've worked\n\n    # Increase total_npv\n\n(back to text)\n\n\n\nTry to find the index of the first value in x that is greater than 0.999 using a for loop and break.\ntry iterating over range(len(x)).\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nWrite a for loop that adds up all values in x that are greater than or equal to 0.5.\nUse the continue word to end the body of the loop early for all values of x that are less than 0.5.\nTry starting your loop with for value in x: instead of iterating over the indices of x.\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nReturning to our previous example: print “{city} is in {state}” for each combination using a zip and a comprehension.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# your code here\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html",
    "href": "tutorials/session_1/qe_basics.html",
    "title": "Basics",
    "section": "",
    "text": "Outcomes\n\nProgramming concepts\n\nUnderstand variable assignment\n\nKnow what a function is and how to figure out what it does\n\nBe able to use tab completion\n\n\nNumbers in Python\n\nUnderstand how Python represents numbers\n\nKnow the distinction between int and float\n\nBe familiar with various binary operators for numbers\n\nIntroduction to the math library\n\n\nText (strings) in Python\n\nUnderstand what a string is and when it is useful\n\nLearn some of the methods associated with strings\n\nCombining strings and output\n\n\nTrue and False (booleans) in Python\n\nUnderstand what a boolean is\n\nBecome familiar with all binary operators that return booleans\n\n\n\n\nWe are ready to begin writing code!\nIn this section, we will teach you some basic concepts of programming and where to search for help.\n\n\nThe first thing we will learn is the idea of variable assignment.\nVariable assignment associates a value to a variable.\nBelow, we assign the value “Hello World” to the variable x\n\nx = \"Hello World\"\n\n\nx\n\n'Hello World'\n\n\nOnce we have assigned a value to a variable, Python will remember this variable as long as the current session of Python is still running.\nNotice how writing x into the prompt below outputs the value “Hello World”.\n\nx\n\n'Hello World'\n\n\nHowever, Python returns an error if we ask it about variables that have not yet been created.\n\n# uncomment (delete the # and the space) the line below and run\ny\n\nNameError: name 'y' is not defined\n\n\nIt is also useful to understand the order in which operations happen.\nFirst, the right side of the equal sign is computed.\nThen, that computed value is stored as the variable to the left of the equal sign.\n\n\n\nSee exercise 1 in the exercise list.\nKeep in mind that the variable binds a name to something stored in memory.\nThe name can even be bound to a value of a completely different type.\n\nx = 2\nprint(x)\nx = \"something else\"\nprint(x)\n\n\n\n\nComments are short notes that you leave for yourself and for others who read your code.\nThey should be used to explain what the code does.\nA comment is made with the #. Python ignores everything in a line that follows a #.\nLet’s practice making some comments.\n\ni = 1  # Assign the value 1 to variable i\nj = 2  # Assign the value 2 to variable j\n\n# We add i and j below this line\ni + j\n\n3\n\n\n\n\n\n\nFunctions are processes that take an input (or inputs) and produce an output.\nIf we had a function called f that took two arguments x and y, we would write f(x, y) to use the function.\nFor example, the function print simply prints whatever it is given. Recall the variable we created called x.\n\nprint(x)\n\nHello World\n\n\n\n\nWe can figure out what a function does by asking for help.\nIn Jupyter notebooks, this is done by placing a ? after the function name (without using parenthesis) and evaluating the cell.\nFor example, we can ask for help on the print function by writing print?.\nDepending on how you launched Jupyter, this will either launch\n\nJupyterLab: display the help in text below the cell.\n\nClassic Jupyter Notebooks: display a new panel at the bottom of your screen. You can exit this panel by hitting the escape key or clicking the x at the top right of the panel.\n\n\nprint?\n\n\nDocstring:\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\n# print? # remove the comment and &lt;Shift-Enter&gt;\n\n\n\n\nSee exercise 2 in the exercise list.\nJupyterLab also has a “Contextual Help” (previously called “Inspector”) window. To use,\n\nGo to the Commands and choose Contextual Help (or Inspector), or select &lt;Ctrl-I&gt; (&lt;Cmd-I&gt; for OSX users).\n\nDrag the new inspector pain to dock in the screen next to your code.\n\nThen, type print or any other function into a cell and see the help.\n\n\n# len? # remove the comment and &lt;Shift-Enter&gt;\n\nWe will learn much more about functions, including how to write our own, in a future lecture.\n\n\n\n\nEverything in Python is an object.\nObjects are “things” that contain 1) data and 2) functions that can operate on the data.\nSometimes we refer to the functions inside an object as methods.\nWe can investigate what data is inside an object and which methods it supports by typing . after that particular variable, then hitting TAB.\nIt should then list data and method names to the right of the variable name like this:\n\n\n\nhttps://datascience.quantecon.org/_static/introspection.png\n\n\nYou can scroll through this list by using the up and down arrows.\nWe often refer to this as “tab completion” or “introspection”.\nLet’s do this together below. Keep going down until you find the method split.\n\n# Type a period after `x` and then press TAB.\nx\n\nOnce you have found the method split, you can use the method by adding parenthesis after it.\nLet’s call the split method, which doesn’t have any other required parameters. (Quiz: how would we check that?)\n\nx.split()\n\nWe often want to identify what kind of object some value is– called its “type”.\nA “type” is an abstraction which defines a set of behavior for any “instance” of that type i.e. 2.0 and 3.0 are instances of float, where float has a set of particular common behaviors.\nIn particular, the type determines:\n\nthe available data for any “instance” of the type (where each instance may have different values of the data).\n\nthe methods that can be applied on the object and its data.\n\nWe can figure this out by using the type function.\nThe type function takes a single argument and outputs the type of that argument.\n\ntype(3)\n\n\ntype(\"Hello World\")\n\n\ntype([1, 2, 3])\n\nWe will learn more about each of these types (and others!) and how to use them soon, so stay tuned!\n\n\n\n\nPython takes a modular approach to tools.\nBy this we mean that sets of related tools are bundled together into packages. (You may also hear the term modules to describe the same thing.)\nFor example:\n\npandas is a package that implements the tools necessary to do scalable data analysis.\n\nmatplotlib is a package that implements visualization tools.\n\nrequests and urllib are packages that allow Python to interface with the internet.\n\nAs we move further into the class, being able to access these packages will become very important.\nWe can bring a package’s functionality into our current Python session by writing\n\nimport package\n\nOnce we have done this, any function or object from that package can be accessed by using package.name.\nHere’s an example.\n\nimport sys   # for dealing with your computer's system\nsys.version  # information about the Python version in use\n\n\n\n\nSee exercise 3 in the exercise list.\n\n\nSome packages have long names (see matplotlib, for example) which makes accessing the package functionality somewhat inconvenient.\nTo ease this burden, Python allows us to give aliases or “nicknames” to packages.\nFor example we can write:\n\nimport package as p\n\nThis statement allows us to access the packages functionality as p.function_name rather than package.function_name.\nSome common aliases for packages are\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib as mpl\n\nimport datetime as dt\n\nWhile you can choose any name for an alias, we suggest that you stick to the common ones.\nYou will learn what these common ones are over time.\n\n\n\nSee exercise 4 in the exercise list.\n\n\n\n\nA common saying in the software engineering world is:\n\nAlways code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability.\n\nThis might be a dramatic take, but the most important feature of your code after correctness is readability.\nWe encourage you to do everything in your power to make your code as readable as possible.\nHere are some suggestions for how to do so:\n\nComment frequently. Leaving short notes not only will help others who use your code, but will also help you interpret your code after some time has passed.\n\nAnytime you use a comma, place a space immediately afterwards.\n\nWhitespace is your friend. Don’t write line after line of code – use blank lines to break it up.\n\nDon’t let your lines run too long. Some people reading your code will be on a laptop, so you want to ensure that they don’t need to scroll horizontally and right to read your code. We recommend no more than 80 characters per line.\n\n\n\n\nPython has two types of numbers.\n\nInteger (int): These can only take the values of the integers i.e. $ {, -2, -1, 0, 1, 2, } $\n\nFloating Point Number (float): Think of these as any real number such as $ 1.0 $, $ 3.1415 $, or $ -100.022358923223 $…\n\nThe easiest way to differentiate these types of numbers is to find a decimal place after the number.\nA float will have a decimal place, but an integer will not.\nBelow, we assign integers to the variables xi and zi and assign floating point numbers to the variables xf and zf.\n\nxi = 1\nxf = 1.0\nzi = 123\nzf = 1230.5  # Notice -- There are no commas!\nzf2 = 1_230.5  # If needed, we use `_` to separate numbers for readability\n\n\n\n\nSee exercise 5 in the exercise list.\n\n\nYou can use Python to perform mathematical calculations.\n\na = 4\nb = 2\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\nprint(\"a ^ b is\", a^b)\n\nYou likely could have guessed all except the last two.\nPython uses **, not ^, for exponentiation (raising a number to a power)!\nNotice also that above +, - and ** all returned an integer type, but / converted the result to a float.\nWhen possible, operations between integers return an integer type.\nAll operations involving a float will result in a float.\n\na = 4\nb = 2.0\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\n\nWe can also chain together operations.\nWhen doing this, Python follows the standard order of operations — parenthesis, exponents, multiplication and division, followed by addition and subtraction.\nFor example,\n\nx = 2.0\ny = 3.0\nz1 = x + y * x\nz2 = (x + y) * x\n\nWhat do you think z1 is?\nHow about z2?\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe often want to use other math functions on our numbers. Let’s try to calculate sin(2.5).\n\nsin(2.5)\n\nAs seen above, Python complains that sin isn’t defined.\nThe problem here is that the sin function – as well as many other standard math functions – are contained in the math package.\nWe must begin by importing the math package.\n\nimport math\n\nNow, we can use math.[TAB] to see what functions are available to us.\n\n# uncomment, add a period (`.`) and pres TAB\n# math\n\n\n# found math.sin!\nmath.sin(2.5)\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\nYou are less likely to run into the following operators, but understanding that they exist is useful.\nFor two numbers assigned to the variables x and y,\n\nFloor division: x // y\n\nModulus division: x % y\n\nRemember when you first learned how to do division and you were asked to talk about the quotient and the remainder?\nThat’s what these operators correspond to…\nFloor division returns the number of times the divisor goes into the dividend (the quotient) and modulus division returns the remainder.\nAn example would be 37 divided by 7:\n\nFloor division would return 5 (7 * 5 = 35)\n\nModulus division would return 2 (2 + 35 = 37)\n\nTry it!\n\n37 // 7\n\n\n37 % 7\n\n\n\n\n\n\nTextual information is stored in a data type called a string.\nTo denote that you would like something to be stored as a string, you place it inside of quotation marks.\nFor example,\n\n\"this is a string\"  # Notice the quotation marks\n'this is a string'  # Notice the quotation marks\nthis is not a string  # No quotation marks\n\nYou can use either \" or ' to create a string. Just make sure that you start and end the string with the same one!\nNotice that if we ask Python to tell us the type of a string, it abbreviates its answer to str.\n\ntype(\"this is a string\")\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\nSome of the arithmetic operators we saw in the numbers lecture also work on strings:\n\nPut two strings together: x + y.\n\nRepeat the string x a total of n times: n * x (or x * n).\n\n\nx = \"Hello\"\ny = \"World\"\n\n\nx + y\n\n\n3 * x\n\nWhat happens if we try * with two strings, or - or /?\nThe best way to find out is to try it!\n\na = \"1\"\nb = \"2\"\na * b\n\n\na - b\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\nWe can use many methods to manipulate strings.\nWe will not be able to cover all of them here, but let’s take a look at some of the most useful ones.\n\nx\n\n\nx.lower()  # Makes all letters lower case\n\n\nx.upper()  # Makes all letters upper case\n\n\nx.count(\"l\")  # Counts number of a particular string\n\n\nx.count(\"ll\")\n\n\n\n\nSee exercise 10 in the exercise list.\n\n\n\nSee exercise 11 in the exercise list.\n\n\n\nSometimes we’d like to reuse some portion of a string repeatedly, but still make some relatively small changes at each usage.\nWe can do this with string formatting, which done by using {} as a placeholder where we’d like to change the string, with a variable name or expression.\nLet’s look at an example.\n\ncountry = \"Vietnam\"\nGDP = 223.9\nyear = 2017\nmy_string = f\"{country} had ${GDP} billion GDP in {year}\"\nprint(my_string)\n\nRather than just substituting a variable name, you can use a calculation or expression.\n\nprint(f\"{5}**2 = {5**2}\")\n\nOr, using our previous example\n\nmy_string = f\"{country} had ${GDP * 1_000_000} GDP in {year}\"\nprint(my_string)\n\nIn these cases, the f in front of the string causes Python interpolate any valid expression within the {} braces.\n\n\n\nSee exercise 12 in the exercise list.\nAlternatively, to reuse a formatted string, you can call the format method (noting that you do not put f in front).\n\ngdp_string = \"{country} had ${GDP} billion in {year}\"\n\ngdp_string.format(country = \"Vietnam\", GDP = 223.9, year = 2017)\n\n\n\n\nSee exercise 13 in the exercise list.\n\n\n\nSee exercise 14 in the exercise list.\nFor more information on what you can do with string formatting (there is a lot that can be done…), see the official Python documentation on the subject.\n\n\n\n\nA boolean is a type that denotes true or false.\nAs you will soon see in the control flow chapter, using boolean values allows you to perform or skip operations depending on whether or not a condition is met.\nLet’s start by creating some booleans and looking at them.\n\nx = True\ny = False\n\ntype(x)\n\n\nx\n\n\ny\n\n\n\nRather than directly write True or False, you will usually create booleans by making a comparison.\nFor example, you might want to evaluate whether the price of a particular asset is greater than or less than some price.\nFor two variables x and y, we can do the following comparisons:\n\nGreater than: x &gt; y\n\nLess than: x &lt; y\n\nEqual to: ==\n\nGreater than or equal to: x &gt;= y\n\nLess than or equal to: x &lt;= y\n\nWe demonstrate these below.\n\na = 4\nb = 2\n\nprint(\"a &gt; b\", \"is\", a &gt; b)\nprint(\"a &lt; b\", \"is\", a &lt; b)\nprint(\"a == b\", \"is\", a == b)\nprint(\"a &gt;= b\", \"is\", a &gt;= b)\nprint(\"a &lt;= b\", \"is\", a &lt;= b)\n\n\n\n\nOccasionally, determining whether a statement is “not true” or “not false” is more convenient than simply “true” or “false”.\nThis is known as negating a statement.\nIn Python, we can negate a boolean using the word not.\n\nnot False\n\n\nnot True\n\n\n\n\nSometimes we need to evaluate multiple comparisons at once.\nThis is done by using the words and and or.\nHowever, these are the “mathematical” ands and ors – so they don’t carry the same meaning as you’d use them in colloquial English.\n\na and b is true only when both a and b are true.\n\na or b is true whenever at least one of a or b is true.\n\nFor example\n\nThe statement “I will accept the new job if the salary is higher and I receive more vacation days” means that you would only accept the new job if you both receive a higher salary and are given more vacation days.\n\nThe statement “I will accept the new job if the salary is higher or I receive more vacation days” means that you would accept the job if\n\nthey raised your salary, (2) you are given more vacation days, or\nthey raise your salary and give you more vacation days.\n\n\nLet’s see some examples.\n\nTrue and False\n\n\nTrue and True\n\n\nTrue or False\n\n\nFalse or False\n\n\n# Can chain multiple comparisons together.\nTrue and (False or True)\n\n\n\n\nSee exercise 15 in the exercise list.\n\n\n\nWe have seen how we can use the words and and or to process two booleans at a time.\nThe functions all and any allow us to process an unlimited number of booleans at once.\nall(bools) will return True if and only if all the booleans in bools is True and returns False otherwise.\nany(bools) returns True whenever one or more of bools is True.\nThe exercise below will give you a chance to practice.\n\n\n\nSee exercise 16 in the exercise list.\n\n\n\n\n\n\n\nWhat do you think the value of z is after running the code below?\n\nz = 3\nz = z + 4\nprint(\"z is\", z)\n\n(back to text)\n\n\n\nRead about out what the len function does (by writing len?).\nWhat will it produce if we give it the variable x?\nCheck whether you were right by running the code len(x).\n(back to text)\n\n\n\nWe can use our introspection skills to investigate a package’s contents.\nIn the cell below, use tab completion to find a function from the time module that will display the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the function you found) to see information about that function and then call the function.\nLook for something to do with the word local\n\nimport time\n# your code here -- notice the comment!\n\n(back to text)\n\n\n\nTry running import time as t in the cell below, then see if you can call the function you identified above.\nDoes it work?\n(back to text)\n\n\n\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with value 0.025\n\nT: An integer with value 30\n\nWe will use them in a later exercise.\n\n# your code here!\n\n(back to text)\n\n\n\nRemember the variables we created earlier?\nLet’s compute the present discounted value of a payment ($ D $) made in $ T $ years assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nThe formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# your code here\n\n(back to text)\n\n\n\nVerify the “trick” where the percent difference ($ \\()\nbetween two numbers close to 1 can be well approximated by the difference\nbetween the log of the two numbers (\\) (x) - (y) $).\nUse the numbers x and y below.\nyou will want to use the math.log function\n\n# your code here\n\n(back to text)\n\n\n\nThe code below is invalid Python code\n\nx = 'What's wrong with this string'\n\nCan you fix it?\nTry creating a code cell below and testing things out until you find a solution.\n(back to text)\n\n\n\nUsing the variables x and y, how could you create the sentence Hello World?\nThink about how to represent a space as a string.\n(back to text)\n\n\n\nOne of our favorite (and most frequently used) string methods is replace.\nIt substitutes all occurrences of a particular pattern with a different pattern.\nFor the variable test below, use the replace method to change the c to a d.\nType test.replace? to get some help for how to use the method replace.\n\ntest = \"abc\"\n\n(back to text)\n\n\n\nSuppose you are working with price data and encounter the value \"\\$6.50\".\nWe recognize this as being a number representing the quantity “six dollars and fifty cents.”\nHowever, Python interprets the value as the string \"\\$6.50\". (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nOnce the string is in a suitable format, you can call write float(clean_price) to make it a number.\n\nprice = \"$6.50\"\n\n(back to text)\n\n\n\nLookup a country in World Bank database, and format a string showing the growth rate of GDP over the last 2 years.\n(back to text)\n\n\n\nInstead of hard-coding the values above, try to use the country, GDP and year variables you previously defined.\n(back to text)\n\n\n\nCreate a new string and use formatting to produce each of the following statements\n\n“The 1st quarter revenue was 110M”\n\n“The 2nd quarter revenue was 95M”\n\n“The 3rd quarter revenue was 100M”\n\n“The 4th quarter revenue was 130M”\n\n(back to text)\n\n\n\nWithout typing the commands, determine whether the following statements are true or false.\nOnce you have evaluated whether the command is True or False, run the code in Python.\n\nx = 2\ny = 2\nz = 4\n\n# Statement 1\nx &gt; z\n\n# Statement 1\nx == y\n\n# Statement 3\n(x &lt; y) and (x &gt; y)\n\n# Statement 4\n(x &lt; y) or (x &gt; y)\n\n# Statement 5\n(x &lt;= y) and (x &gt;= y)\n\n# Statement 6\nTrue and ((x &lt; z) or (x &lt; y))\n\n\n# code here!\n\n(back to text)\n\n\n\nFor each of the code cells below, think carefully about what you expect to be returned before evaluating the cell.\nThen evaluate the cell to check your intuitions.\nNOTE: For now, do not worry about what the [ and ] mean – they allow us to create lists which we will learn about in an upcoming lecture.\n\nall([True, True, True])\n\n\nall([False, True, False])\n\n\nall([False, False, False])\n\n\nany([True, True, True])\n\n\nany([False, True, False])\n\n\nany([False, False, False])\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#first-steps",
    "href": "tutorials/session_1/qe_basics.html#first-steps",
    "title": "Basics",
    "section": "",
    "text": "We are ready to begin writing code!\nIn this section, we will teach you some basic concepts of programming and where to search for help.\n\n\nThe first thing we will learn is the idea of variable assignment.\nVariable assignment associates a value to a variable.\nBelow, we assign the value “Hello World” to the variable x\n\nx = \"Hello World\"\n\n\nx\n\n'Hello World'\n\n\nOnce we have assigned a value to a variable, Python will remember this variable as long as the current session of Python is still running.\nNotice how writing x into the prompt below outputs the value “Hello World”.\n\nx\n\n'Hello World'\n\n\nHowever, Python returns an error if we ask it about variables that have not yet been created.\n\n# uncomment (delete the # and the space) the line below and run\ny\n\nNameError: name 'y' is not defined\n\n\nIt is also useful to understand the order in which operations happen.\nFirst, the right side of the equal sign is computed.\nThen, that computed value is stored as the variable to the left of the equal sign.\n\n\n\nSee exercise 1 in the exercise list.\nKeep in mind that the variable binds a name to something stored in memory.\nThe name can even be bound to a value of a completely different type.\n\nx = 2\nprint(x)\nx = \"something else\"\nprint(x)\n\n\n\n\nComments are short notes that you leave for yourself and for others who read your code.\nThey should be used to explain what the code does.\nA comment is made with the #. Python ignores everything in a line that follows a #.\nLet’s practice making some comments.\n\ni = 1  # Assign the value 1 to variable i\nj = 2  # Assign the value 2 to variable j\n\n# We add i and j below this line\ni + j\n\n3"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#functions",
    "href": "tutorials/session_1/qe_basics.html#functions",
    "title": "Basics",
    "section": "",
    "text": "Functions are processes that take an input (or inputs) and produce an output.\nIf we had a function called f that took two arguments x and y, we would write f(x, y) to use the function.\nFor example, the function print simply prints whatever it is given. Recall the variable we created called x.\n\nprint(x)\n\nHello World\n\n\n\n\nWe can figure out what a function does by asking for help.\nIn Jupyter notebooks, this is done by placing a ? after the function name (without using parenthesis) and evaluating the cell.\nFor example, we can ask for help on the print function by writing print?.\nDepending on how you launched Jupyter, this will either launch\n\nJupyterLab: display the help in text below the cell.\n\nClassic Jupyter Notebooks: display a new panel at the bottom of your screen. You can exit this panel by hitting the escape key or clicking the x at the top right of the panel.\n\n\nprint?\n\n\nDocstring:\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\n# print? # remove the comment and &lt;Shift-Enter&gt;\n\n\n\n\nSee exercise 2 in the exercise list.\nJupyterLab also has a “Contextual Help” (previously called “Inspector”) window. To use,\n\nGo to the Commands and choose Contextual Help (or Inspector), or select &lt;Ctrl-I&gt; (&lt;Cmd-I&gt; for OSX users).\n\nDrag the new inspector pain to dock in the screen next to your code.\n\nThen, type print or any other function into a cell and see the help.\n\n\n# len? # remove the comment and &lt;Shift-Enter&gt;\n\nWe will learn much more about functions, including how to write our own, in a future lecture."
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#objects-and-types",
    "href": "tutorials/session_1/qe_basics.html#objects-and-types",
    "title": "Basics",
    "section": "",
    "text": "Everything in Python is an object.\nObjects are “things” that contain 1) data and 2) functions that can operate on the data.\nSometimes we refer to the functions inside an object as methods.\nWe can investigate what data is inside an object and which methods it supports by typing . after that particular variable, then hitting TAB.\nIt should then list data and method names to the right of the variable name like this:\n\n\n\nhttps://datascience.quantecon.org/_static/introspection.png\n\n\nYou can scroll through this list by using the up and down arrows.\nWe often refer to this as “tab completion” or “introspection”.\nLet’s do this together below. Keep going down until you find the method split.\n\n# Type a period after `x` and then press TAB.\nx\n\nOnce you have found the method split, you can use the method by adding parenthesis after it.\nLet’s call the split method, which doesn’t have any other required parameters. (Quiz: how would we check that?)\n\nx.split()\n\nWe often want to identify what kind of object some value is– called its “type”.\nA “type” is an abstraction which defines a set of behavior for any “instance” of that type i.e. 2.0 and 3.0 are instances of float, where float has a set of particular common behaviors.\nIn particular, the type determines:\n\nthe available data for any “instance” of the type (where each instance may have different values of the data).\n\nthe methods that can be applied on the object and its data.\n\nWe can figure this out by using the type function.\nThe type function takes a single argument and outputs the type of that argument.\n\ntype(3)\n\n\ntype(\"Hello World\")\n\n\ntype([1, 2, 3])\n\nWe will learn more about each of these types (and others!) and how to use them soon, so stay tuned!"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#modules",
    "href": "tutorials/session_1/qe_basics.html#modules",
    "title": "Basics",
    "section": "",
    "text": "Python takes a modular approach to tools.\nBy this we mean that sets of related tools are bundled together into packages. (You may also hear the term modules to describe the same thing.)\nFor example:\n\npandas is a package that implements the tools necessary to do scalable data analysis.\n\nmatplotlib is a package that implements visualization tools.\n\nrequests and urllib are packages that allow Python to interface with the internet.\n\nAs we move further into the class, being able to access these packages will become very important.\nWe can bring a package’s functionality into our current Python session by writing\n\nimport package\n\nOnce we have done this, any function or object from that package can be accessed by using package.name.\nHere’s an example.\n\nimport sys   # for dealing with your computer's system\nsys.version  # information about the Python version in use"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#exercise-2",
    "href": "tutorials/session_1/qe_basics.html#exercise-2",
    "title": "Basics",
    "section": "",
    "text": "See exercise 3 in the exercise list.\n\n\nSome packages have long names (see matplotlib, for example) which makes accessing the package functionality somewhat inconvenient.\nTo ease this burden, Python allows us to give aliases or “nicknames” to packages.\nFor example we can write:\n\nimport package as p\n\nThis statement allows us to access the packages functionality as p.function_name rather than package.function_name.\nSome common aliases for packages are\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib as mpl\n\nimport datetime as dt\n\nWhile you can choose any name for an alias, we suggest that you stick to the common ones.\nYou will learn what these common ones are over time.\n\n\n\nSee exercise 4 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#good-code-habits",
    "href": "tutorials/session_1/qe_basics.html#good-code-habits",
    "title": "Basics",
    "section": "",
    "text": "A common saying in the software engineering world is:\n\nAlways code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability.\n\nThis might be a dramatic take, but the most important feature of your code after correctness is readability.\nWe encourage you to do everything in your power to make your code as readable as possible.\nHere are some suggestions for how to do so:\n\nComment frequently. Leaving short notes not only will help others who use your code, but will also help you interpret your code after some time has passed.\n\nAnytime you use a comma, place a space immediately afterwards.\n\nWhitespace is your friend. Don’t write line after line of code – use blank lines to break it up.\n\nDon’t let your lines run too long. Some people reading your code will be on a laptop, so you want to ensure that they don’t need to scroll horizontally and right to read your code. We recommend no more than 80 characters per line."
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#numbers",
    "href": "tutorials/session_1/qe_basics.html#numbers",
    "title": "Basics",
    "section": "",
    "text": "Python has two types of numbers.\n\nInteger (int): These can only take the values of the integers i.e. $ {, -2, -1, 0, 1, 2, } $\n\nFloating Point Number (float): Think of these as any real number such as $ 1.0 $, $ 3.1415 $, or $ -100.022358923223 $…\n\nThe easiest way to differentiate these types of numbers is to find a decimal place after the number.\nA float will have a decimal place, but an integer will not.\nBelow, we assign integers to the variables xi and zi and assign floating point numbers to the variables xf and zf.\n\nxi = 1\nxf = 1.0\nzi = 123\nzf = 1230.5  # Notice -- There are no commas!\nzf2 = 1_230.5  # If needed, we use `_` to separate numbers for readability"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#exercise-4",
    "href": "tutorials/session_1/qe_basics.html#exercise-4",
    "title": "Basics",
    "section": "",
    "text": "See exercise 5 in the exercise list.\n\n\nYou can use Python to perform mathematical calculations.\n\na = 4\nb = 2\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\nprint(\"a ^ b is\", a^b)\n\nYou likely could have guessed all except the last two.\nPython uses **, not ^, for exponentiation (raising a number to a power)!\nNotice also that above +, - and ** all returned an integer type, but / converted the result to a float.\nWhen possible, operations between integers return an integer type.\nAll operations involving a float will result in a float.\n\na = 4\nb = 2.0\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\n\nWe can also chain together operations.\nWhen doing this, Python follows the standard order of operations — parenthesis, exponents, multiplication and division, followed by addition and subtraction.\nFor example,\n\nx = 2.0\ny = 3.0\nz1 = x + y * x\nz2 = (x + y) * x\n\nWhat do you think z1 is?\nHow about z2?\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe often want to use other math functions on our numbers. Let’s try to calculate sin(2.5).\n\nsin(2.5)\n\nAs seen above, Python complains that sin isn’t defined.\nThe problem here is that the sin function – as well as many other standard math functions – are contained in the math package.\nWe must begin by importing the math package.\n\nimport math\n\nNow, we can use math.[TAB] to see what functions are available to us.\n\n# uncomment, add a period (`.`) and pres TAB\n# math\n\n\n# found math.sin!\nmath.sin(2.5)\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\nYou are less likely to run into the following operators, but understanding that they exist is useful.\nFor two numbers assigned to the variables x and y,\n\nFloor division: x // y\n\nModulus division: x % y\n\nRemember when you first learned how to do division and you were asked to talk about the quotient and the remainder?\nThat’s what these operators correspond to…\nFloor division returns the number of times the divisor goes into the dividend (the quotient) and modulus division returns the remainder.\nAn example would be 37 divided by 7:\n\nFloor division would return 5 (7 * 5 = 35)\n\nModulus division would return 2 (2 + 35 = 37)\n\nTry it!\n\n37 // 7\n\n\n37 % 7"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#strings",
    "href": "tutorials/session_1/qe_basics.html#strings",
    "title": "Basics",
    "section": "",
    "text": "Textual information is stored in a data type called a string.\nTo denote that you would like something to be stored as a string, you place it inside of quotation marks.\nFor example,\n\n\"this is a string\"  # Notice the quotation marks\n'this is a string'  # Notice the quotation marks\nthis is not a string  # No quotation marks\n\nYou can use either \" or ' to create a string. Just make sure that you start and end the string with the same one!\nNotice that if we ask Python to tell us the type of a string, it abbreviates its answer to str.\n\ntype(\"this is a string\")"
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#exercise-7",
    "href": "tutorials/session_1/qe_basics.html#exercise-7",
    "title": "Basics",
    "section": "",
    "text": "See exercise 8 in the exercise list.\n\n\nSome of the arithmetic operators we saw in the numbers lecture also work on strings:\n\nPut two strings together: x + y.\n\nRepeat the string x a total of n times: n * x (or x * n).\n\n\nx = \"Hello\"\ny = \"World\"\n\n\nx + y\n\n\n3 * x\n\nWhat happens if we try * with two strings, or - or /?\nThe best way to find out is to try it!\n\na = \"1\"\nb = \"2\"\na * b\n\n\na - b\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\nWe can use many methods to manipulate strings.\nWe will not be able to cover all of them here, but let’s take a look at some of the most useful ones.\n\nx\n\n\nx.lower()  # Makes all letters lower case\n\n\nx.upper()  # Makes all letters upper case\n\n\nx.count(\"l\")  # Counts number of a particular string\n\n\nx.count(\"ll\")\n\n\n\n\nSee exercise 10 in the exercise list.\n\n\n\nSee exercise 11 in the exercise list.\n\n\n\nSometimes we’d like to reuse some portion of a string repeatedly, but still make some relatively small changes at each usage.\nWe can do this with string formatting, which done by using {} as a placeholder where we’d like to change the string, with a variable name or expression.\nLet’s look at an example.\n\ncountry = \"Vietnam\"\nGDP = 223.9\nyear = 2017\nmy_string = f\"{country} had ${GDP} billion GDP in {year}\"\nprint(my_string)\n\nRather than just substituting a variable name, you can use a calculation or expression.\n\nprint(f\"{5}**2 = {5**2}\")\n\nOr, using our previous example\n\nmy_string = f\"{country} had ${GDP * 1_000_000} GDP in {year}\"\nprint(my_string)\n\nIn these cases, the f in front of the string causes Python interpolate any valid expression within the {} braces.\n\n\n\nSee exercise 12 in the exercise list.\nAlternatively, to reuse a formatted string, you can call the format method (noting that you do not put f in front).\n\ngdp_string = \"{country} had ${GDP} billion in {year}\"\n\ngdp_string.format(country = \"Vietnam\", GDP = 223.9, year = 2017)\n\n\n\n\nSee exercise 13 in the exercise list.\n\n\n\nSee exercise 14 in the exercise list.\nFor more information on what you can do with string formatting (there is a lot that can be done…), see the official Python documentation on the subject."
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#booleans",
    "href": "tutorials/session_1/qe_basics.html#booleans",
    "title": "Basics",
    "section": "",
    "text": "A boolean is a type that denotes true or false.\nAs you will soon see in the control flow chapter, using boolean values allows you to perform or skip operations depending on whether or not a condition is met.\nLet’s start by creating some booleans and looking at them.\n\nx = True\ny = False\n\ntype(x)\n\n\nx\n\n\ny\n\n\n\nRather than directly write True or False, you will usually create booleans by making a comparison.\nFor example, you might want to evaluate whether the price of a particular asset is greater than or less than some price.\nFor two variables x and y, we can do the following comparisons:\n\nGreater than: x &gt; y\n\nLess than: x &lt; y\n\nEqual to: ==\n\nGreater than or equal to: x &gt;= y\n\nLess than or equal to: x &lt;= y\n\nWe demonstrate these below.\n\na = 4\nb = 2\n\nprint(\"a &gt; b\", \"is\", a &gt; b)\nprint(\"a &lt; b\", \"is\", a &lt; b)\nprint(\"a == b\", \"is\", a == b)\nprint(\"a &gt;= b\", \"is\", a &gt;= b)\nprint(\"a &lt;= b\", \"is\", a &lt;= b)\n\n\n\n\nOccasionally, determining whether a statement is “not true” or “not false” is more convenient than simply “true” or “false”.\nThis is known as negating a statement.\nIn Python, we can negate a boolean using the word not.\n\nnot False\n\n\nnot True\n\n\n\n\nSometimes we need to evaluate multiple comparisons at once.\nThis is done by using the words and and or.\nHowever, these are the “mathematical” ands and ors – so they don’t carry the same meaning as you’d use them in colloquial English.\n\na and b is true only when both a and b are true.\n\na or b is true whenever at least one of a or b is true.\n\nFor example\n\nThe statement “I will accept the new job if the salary is higher and I receive more vacation days” means that you would only accept the new job if you both receive a higher salary and are given more vacation days.\n\nThe statement “I will accept the new job if the salary is higher or I receive more vacation days” means that you would accept the job if\n\nthey raised your salary, (2) you are given more vacation days, or\nthey raise your salary and give you more vacation days.\n\n\nLet’s see some examples.\n\nTrue and False\n\n\nTrue and True\n\n\nTrue or False\n\n\nFalse or False\n\n\n# Can chain multiple comparisons together.\nTrue and (False or True)\n\n\n\n\nSee exercise 15 in the exercise list.\n\n\n\nWe have seen how we can use the words and and or to process two booleans at a time.\nThe functions all and any allow us to process an unlimited number of booleans at once.\nall(bools) will return True if and only if all the booleans in bools is True and returns False otherwise.\nany(bools) returns True whenever one or more of bools is True.\nThe exercise below will give you a chance to practice.\n\n\n\nSee exercise 16 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_basics.html#exercises",
    "href": "tutorials/session_1/qe_basics.html#exercises",
    "title": "Basics",
    "section": "",
    "text": "What do you think the value of z is after running the code below?\n\nz = 3\nz = z + 4\nprint(\"z is\", z)\n\n(back to text)\n\n\n\nRead about out what the len function does (by writing len?).\nWhat will it produce if we give it the variable x?\nCheck whether you were right by running the code len(x).\n(back to text)\n\n\n\nWe can use our introspection skills to investigate a package’s contents.\nIn the cell below, use tab completion to find a function from the time module that will display the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the function you found) to see information about that function and then call the function.\nLook for something to do with the word local\n\nimport time\n# your code here -- notice the comment!\n\n(back to text)\n\n\n\nTry running import time as t in the cell below, then see if you can call the function you identified above.\nDoes it work?\n(back to text)\n\n\n\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with value 0.025\n\nT: An integer with value 30\n\nWe will use them in a later exercise.\n\n# your code here!\n\n(back to text)\n\n\n\nRemember the variables we created earlier?\nLet’s compute the present discounted value of a payment ($ D $) made in $ T $ years assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nThe formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# your code here\n\n(back to text)\n\n\n\nVerify the “trick” where the percent difference ($ \\()\nbetween two numbers close to 1 can be well approximated by the difference\nbetween the log of the two numbers (\\) (x) - (y) $).\nUse the numbers x and y below.\nyou will want to use the math.log function\n\n# your code here\n\n(back to text)\n\n\n\nThe code below is invalid Python code\n\nx = 'What's wrong with this string'\n\nCan you fix it?\nTry creating a code cell below and testing things out until you find a solution.\n(back to text)\n\n\n\nUsing the variables x and y, how could you create the sentence Hello World?\nThink about how to represent a space as a string.\n(back to text)\n\n\n\nOne of our favorite (and most frequently used) string methods is replace.\nIt substitutes all occurrences of a particular pattern with a different pattern.\nFor the variable test below, use the replace method to change the c to a d.\nType test.replace? to get some help for how to use the method replace.\n\ntest = \"abc\"\n\n(back to text)\n\n\n\nSuppose you are working with price data and encounter the value \"\\$6.50\".\nWe recognize this as being a number representing the quantity “six dollars and fifty cents.”\nHowever, Python interprets the value as the string \"\\$6.50\". (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nOnce the string is in a suitable format, you can call write float(clean_price) to make it a number.\n\nprice = \"$6.50\"\n\n(back to text)\n\n\n\nLookup a country in World Bank database, and format a string showing the growth rate of GDP over the last 2 years.\n(back to text)\n\n\n\nInstead of hard-coding the values above, try to use the country, GDP and year variables you previously defined.\n(back to text)\n\n\n\nCreate a new string and use formatting to produce each of the following statements\n\n“The 1st quarter revenue was 110M”\n\n“The 2nd quarter revenue was 95M”\n\n“The 3rd quarter revenue was 100M”\n\n“The 4th quarter revenue was 130M”\n\n(back to text)\n\n\n\nWithout typing the commands, determine whether the following statements are true or false.\nOnce you have evaluated whether the command is True or False, run the code in Python.\n\nx = 2\ny = 2\nz = 4\n\n# Statement 1\nx &gt; z\n\n# Statement 1\nx == y\n\n# Statement 3\n(x &lt; y) and (x &gt; y)\n\n# Statement 4\n(x &lt; y) or (x &gt; y)\n\n# Statement 5\n(x &lt;= y) and (x &gt;= y)\n\n# Statement 6\nTrue and ((x &lt; z) or (x &lt; y))\n\n\n# code here!\n\n(back to text)\n\n\n\nFor each of the code cells below, think carefully about what you expect to be returned before evaluating the cell.\nThen evaluate the cell to check your intuitions.\nNOTE: For now, do not worry about what the [ and ] mean – they allow us to create lists which we will learn about in an upcoming lecture.\n\nall([True, True, True])\n\n\nall([False, True, False])\n\n\nall([False, False, False])\n\n\nany([True, True, True])\n\n\nany([False, True, False])\n\n\nany([False, False, False])\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html",
    "href": "tutorials/session_1/qe_basics_answers.html",
    "title": "Basics",
    "section": "",
    "text": "Outcomes\n\nProgramming concepts\n\nUnderstand variable assignment\n\nKnow what a function is and how to figure out what it does\n\nBe able to use tab completion\n\n\nNumbers in Python\n\nUnderstand how Python represents numbers\n\nKnow the distinction between int and float\n\nBe familiar with various binary operators for numbers\n\nIntroduction to the math library\n\n\nText (strings) in Python\n\nUnderstand what a string is and when it is useful\n\nLearn some of the methods associated with strings\n\nCombining strings and output\n\n\nTrue and False (booleans) in Python\n\nUnderstand what a boolean is\n\nBecome familiar with all binary operators that return booleans\n\n\n\n\nWe are ready to begin writing code!\nIn this section, we will teach you some basic concepts of programming and where to search for help.\n\n\nThe first thing we will learn is the idea of variable assignment.\nVariable assignment associates a value to a variable.\nBelow, we assign the value “Hello World” to the variable x\n\nx = \"Hello World\"\n\n\nx\n\n'Hello World'\n\n\nOnce we have assigned a value to a variable, Python will remember this variable as long as the current session of Python is still running.\nNotice how writing x into the prompt below outputs the value “Hello World”.\n\nx\n\n'Hello World'\n\n\nHowever, Python returns an error if we ask it about variables that have not yet been created.\n\n# uncomment (delete the # and the space) the line below and run\ny\n\nNameError: name 'y' is not defined\n\n\nIt is also useful to understand the order in which operations happen.\nFirst, the right side of the equal sign is computed.\nThen, that computed value is stored as the variable to the left of the equal sign.\n\n\n\nSee exercise 1 in the exercise list.\nKeep in mind that the variable binds a name to something stored in memory.\nThe name can even be bound to a value of a completely different type.\n\nx = 2\nprint(x)\nx = \"something else\"\nprint(x)\n\n\n\n\nComments are short notes that you leave for yourself and for others who read your code.\nThey should be used to explain what the code does.\nA comment is made with the #. Python ignores everything in a line that follows a #.\nLet’s practice making some comments.\n\ni = 1  # Assign the value 1 to variable i\nj = 2  # Assign the value 2 to variable j\n\n# We add i and j below this line\ni + j\n\n3\n\n\n\n\n\n\nFunctions are processes that take an input (or inputs) and produce an output.\nIf we had a function called f that took two arguments x and y, we would write f(x, y) to use the function.\nFor example, the function print simply prints whatever it is given. Recall the variable we created called x.\n\nprint(x)\n\nHello World\n\n\n\n\nWe can figure out what a function does by asking for help.\nIn Jupyter notebooks, this is done by placing a ? after the function name (without using parenthesis) and evaluating the cell.\nFor example, we can ask for help on the print function by writing print?.\nDepending on how you launched Jupyter, this will either launch\n\nJupyterLab: display the help in text below the cell.\n\nClassic Jupyter Notebooks: display a new panel at the bottom of your screen. You can exit this panel by hitting the escape key or clicking the x at the top right of the panel.\n\n\nprint?\n\n\nDocstring:\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\n# print? # remove the comment and &lt;Shift-Enter&gt;\n\n\n\n\nSee exercise 2 in the exercise list.\nJupyterLab also has a “Contextual Help” (previously called “Inspector”) window. To use,\n\nGo to the Commands and choose Contextual Help (or Inspector), or select &lt;Ctrl-I&gt; (&lt;Cmd-I&gt; for OSX users).\n\nDrag the new inspector pain to dock in the screen next to your code.\n\nThen, type print or any other function into a cell and see the help.\n\n\n# len? # remove the comment and &lt;Shift-Enter&gt;\n\nWe will learn much more about functions, including how to write our own, in a future lecture.\n\n\n\n\nEverything in Python is an object.\nObjects are “things” that contain 1) data and 2) functions that can operate on the data.\nSometimes we refer to the functions inside an object as methods.\nWe can investigate what data is inside an object and which methods it supports by typing . after that particular variable, then hitting TAB.\nIt should then list data and method names to the right of the variable name like this:\n\n\n\nhttps://datascience.quantecon.org/_static/introspection.png\n\n\nYou can scroll through this list by using the up and down arrows.\nWe often refer to this as “tab completion” or “introspection”.\nLet’s do this together below. Keep going down until you find the method split.\n\n# Type a period after `x` and then press TAB.\nx\n\nOnce you have found the method split, you can use the method by adding parenthesis after it.\nLet’s call the split method, which doesn’t have any other required parameters. (Quiz: how would we check that?)\n\nx.split()\n\nWe often want to identify what kind of object some value is– called its “type”.\nA “type” is an abstraction which defines a set of behavior for any “instance” of that type i.e. 2.0 and 3.0 are instances of float, where float has a set of particular common behaviors.\nIn particular, the type determines:\n\nthe available data for any “instance” of the type (where each instance may have different values of the data).\n\nthe methods that can be applied on the object and its data.\n\nWe can figure this out by using the type function.\nThe type function takes a single argument and outputs the type of that argument.\n\ntype(3)\n\n\ntype(\"Hello World\")\n\n\ntype([1, 2, 3])\n\nWe will learn more about each of these types (and others!) and how to use them soon, so stay tuned!\n\n\n\n\nPython takes a modular approach to tools.\nBy this we mean that sets of related tools are bundled together into packages. (You may also hear the term modules to describe the same thing.)\nFor example:\n\npandas is a package that implements the tools necessary to do scalable data analysis.\n\nmatplotlib is a package that implements visualization tools.\n\nrequests and urllib are packages that allow Python to interface with the internet.\n\nAs we move further into the class, being able to access these packages will become very important.\nWe can bring a package’s functionality into our current Python session by writing\n\nimport package\n\nOnce we have done this, any function or object from that package can be accessed by using package.name.\nHere’s an example.\n\nimport sys   # for dealing with your computer's system\nsys.version  # information about the Python version in use\n\n\n\n\nSee exercise 3 in the exercise list.\n\n\nSome packages have long names (see matplotlib, for example) which makes accessing the package functionality somewhat inconvenient.\nTo ease this burden, Python allows us to give aliases or “nicknames” to packages.\nFor example we can write:\n\nimport package as p\n\nThis statement allows us to access the packages functionality as p.function_name rather than package.function_name.\nSome common aliases for packages are\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib as mpl\n\nimport datetime as dt\n\nWhile you can choose any name for an alias, we suggest that you stick to the common ones.\nYou will learn what these common ones are over time.\n\n\n\nSee exercise 4 in the exercise list.\n\n\n\n\nA common saying in the software engineering world is:\n\nAlways code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability.\n\nThis might be a dramatic take, but the most important feature of your code after correctness is readability.\nWe encourage you to do everything in your power to make your code as readable as possible.\nHere are some suggestions for how to do so:\n\nComment frequently. Leaving short notes not only will help others who use your code, but will also help you interpret your code after some time has passed.\n\nAnytime you use a comma, place a space immediately afterwards.\n\nWhitespace is your friend. Don’t write line after line of code – use blank lines to break it up.\n\nDon’t let your lines run too long. Some people reading your code will be on a laptop, so you want to ensure that they don’t need to scroll horizontally and right to read your code. We recommend no more than 80 characters per line.\n\n\n\n\nPython has two types of numbers.\n\nInteger (int): These can only take the values of the integers i.e. $ {, -2, -1, 0, 1, 2, } $\n\nFloating Point Number (float): Think of these as any real number such as $ 1.0 $, $ 3.1415 $, or $ -100.022358923223 $…\n\nThe easiest way to differentiate these types of numbers is to find a decimal place after the number.\nA float will have a decimal place, but an integer will not.\nBelow, we assign integers to the variables xi and zi and assign floating point numbers to the variables xf and zf.\n\nxi = 1\nxf = 1.0\nzi = 123\nzf = 1230.5  # Notice -- There are no commas!\nzf2 = 1_230.5  # If needed, we use `_` to separate numbers for readability\n\n\n\n\nSee exercise 5 in the exercise list.\n\n\nYou can use Python to perform mathematical calculations.\n\na = 4\nb = 2\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\nprint(\"a ^ b is\", a^b)\n\nYou likely could have guessed all except the last two.\nPython uses **, not ^, for exponentiation (raising a number to a power)!\nNotice also that above +, - and ** all returned an integer type, but / converted the result to a float.\nWhen possible, operations between integers return an integer type.\nAll operations involving a float will result in a float.\n\na = 4\nb = 2.0\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\n\nWe can also chain together operations.\nWhen doing this, Python follows the standard order of operations — parenthesis, exponents, multiplication and division, followed by addition and subtraction.\nFor example,\n\nx = 2.0\ny = 3.0\nz1 = x + y * x\nz2 = (x + y) * x\n\nWhat do you think z1 is?\nHow about z2?\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe often want to use other math functions on our numbers. Let’s try to calculate sin(2.5).\n\nsin(2.5)\n\nAs seen above, Python complains that sin isn’t defined.\nThe problem here is that the sin function – as well as many other standard math functions – are contained in the math package.\nWe must begin by importing the math package.\n\nimport math\n\nNow, we can use math.[TAB] to see what functions are available to us.\n\n# uncomment, add a period (`.`) and pres TAB\n# math\n\n\n# found math.sin!\nmath.sin(2.5)\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\nYou are less likely to run into the following operators, but understanding that they exist is useful.\nFor two numbers assigned to the variables x and y,\n\nFloor division: x // y\n\nModulus division: x % y\n\nRemember when you first learned how to do division and you were asked to talk about the quotient and the remainder?\nThat’s what these operators correspond to…\nFloor division returns the number of times the divisor goes into the dividend (the quotient) and modulus division returns the remainder.\nAn example would be 37 divided by 7:\n\nFloor division would return 5 (7 * 5 = 35)\n\nModulus division would return 2 (2 + 35 = 37)\n\nTry it!\n\n37 // 7\n\n\n37 % 7\n\n\n\n\n\n\nTextual information is stored in a data type called a string.\nTo denote that you would like something to be stored as a string, you place it inside of quotation marks.\nFor example,\n\n\"this is a string\"  # Notice the quotation marks\n'this is a string'  # Notice the quotation marks\nthis is not a string  # No quotation marks\n\nYou can use either \" or ' to create a string. Just make sure that you start and end the string with the same one!\nNotice that if we ask Python to tell us the type of a string, it abbreviates its answer to str.\n\ntype(\"this is a string\")\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\nSome of the arithmetic operators we saw in the numbers lecture also work on strings:\n\nPut two strings together: x + y.\n\nRepeat the string x a total of n times: n * x (or x * n).\n\n\nx = \"Hello\"\ny = \"World\"\n\n\nx + y\n\n\n3 * x\n\nWhat happens if we try * with two strings, or - or /?\nThe best way to find out is to try it!\n\na = \"1\"\nb = \"2\"\na * b\n\n\na - b\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\nWe can use many methods to manipulate strings.\nWe will not be able to cover all of them here, but let’s take a look at some of the most useful ones.\n\nx\n\n\nx.lower()  # Makes all letters lower case\n\n\nx.upper()  # Makes all letters upper case\n\n\nx.count(\"l\")  # Counts number of a particular string\n\n\nx.count(\"ll\")\n\n\n\n\nSee exercise 10 in the exercise list.\n\n\n\nSee exercise 11 in the exercise list.\n\n\n\nSometimes we’d like to reuse some portion of a string repeatedly, but still make some relatively small changes at each usage.\nWe can do this with string formatting, which done by using {} as a placeholder where we’d like to change the string, with a variable name or expression.\nLet’s look at an example.\n\ncountry = \"Vietnam\"\nGDP = 223.9\nyear = 2017\nmy_string = f\"{country} had ${GDP} billion GDP in {year}\"\nprint(my_string)\n\nRather than just substituting a variable name, you can use a calculation or expression.\n\nprint(f\"{5}**2 = {5**2}\")\n\nOr, using our previous example\n\nmy_string = f\"{country} had ${GDP * 1_000_000} GDP in {year}\"\nprint(my_string)\n\nIn these cases, the f in front of the string causes Python interpolate any valid expression within the {} braces.\n\n\n\nSee exercise 12 in the exercise list.\nAlternatively, to reuse a formatted string, you can call the format method (noting that you do not put f in front).\n\ngdp_string = \"{country} had ${GDP} billion in {year}\"\n\ngdp_string.format(country = \"Vietnam\", GDP = 223.9, year = 2017)\n\n\n\n\nSee exercise 13 in the exercise list.\n\n\n\nSee exercise 14 in the exercise list.\nFor more information on what you can do with string formatting (there is a lot that can be done…), see the official Python documentation on the subject.\n\n\n\n\nA boolean is a type that denotes true or false.\nAs you will soon see in the control flow chapter, using boolean values allows you to perform or skip operations depending on whether or not a condition is met.\nLet’s start by creating some booleans and looking at them.\n\nx = True\ny = False\n\ntype(x)\n\n\nx\n\n\ny\n\n\n\nRather than directly write True or False, you will usually create booleans by making a comparison.\nFor example, you might want to evaluate whether the price of a particular asset is greater than or less than some price.\nFor two variables x and y, we can do the following comparisons:\n\nGreater than: x &gt; y\n\nLess than: x &lt; y\n\nEqual to: ==\n\nGreater than or equal to: x &gt;= y\n\nLess than or equal to: x &lt;= y\n\nWe demonstrate these below.\n\na = 4\nb = 2\n\nprint(\"a &gt; b\", \"is\", a &gt; b)\nprint(\"a &lt; b\", \"is\", a &lt; b)\nprint(\"a == b\", \"is\", a == b)\nprint(\"a &gt;= b\", \"is\", a &gt;= b)\nprint(\"a &lt;= b\", \"is\", a &lt;= b)\n\n\n\n\nOccasionally, determining whether a statement is “not true” or “not false” is more convenient than simply “true” or “false”.\nThis is known as negating a statement.\nIn Python, we can negate a boolean using the word not.\n\nnot False\n\n\nnot True\n\n\n\n\nSometimes we need to evaluate multiple comparisons at once.\nThis is done by using the words and and or.\nHowever, these are the “mathematical” ands and ors – so they don’t carry the same meaning as you’d use them in colloquial English.\n\na and b is true only when both a and b are true.\n\na or b is true whenever at least one of a or b is true.\n\nFor example\n\nThe statement “I will accept the new job if the salary is higher and I receive more vacation days” means that you would only accept the new job if you both receive a higher salary and are given more vacation days.\n\nThe statement “I will accept the new job if the salary is higher or I receive more vacation days” means that you would accept the job if\n\nthey raised your salary, (2) you are given more vacation days, or\nthey raise your salary and give you more vacation days.\n\n\nLet’s see some examples.\n\nTrue and False\n\n\nTrue and True\n\n\nTrue or False\n\n\nFalse or False\n\n\n# Can chain multiple comparisons together.\nTrue and (False or True)\n\n\n\n\nSee exercise 15 in the exercise list.\n\n\n\nWe have seen how we can use the words and and or to process two booleans at a time.\nThe functions all and any allow us to process an unlimited number of booleans at once.\nall(bools) will return True if and only if all the booleans in bools is True and returns False otherwise.\nany(bools) returns True whenever one or more of bools is True.\nThe exercise below will give you a chance to practice.\n\n\n\nSee exercise 16 in the exercise list.\n\n\n\n\n\n\n\nWhat do you think the value of z is after running the code below?\n\nz = 3\nz = z + 4\nprint(\"z is\", z)\n\nz is 7\n\n\nI think the answer is: 7\n(back to text)\n\n\n\nRead about out what the len function does (by writing len?).\nWhat will it produce if we give it the variable x?\nCheck whether you were right by running the code len(x).\n(back to text)\nlen counts the number of objects in a collection and returns an integer. The string x has 11 elements, which should be the answer.\n\nlen?\n\n\nSignature: len(obj, /)\nDocstring: Return the number of items in a container.\nType:      builtin_function_or_method\n\n\n\n\nlen(x)\n\n11\n\n\n\n\n\nWe can use our introspection skills to investigate a package’s contents.\nIn the cell below, use tab completion to find a function from the time module that will display the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the function you found) to see information about that function and then call the function.\nLook for something to do with the word local\n\nimport time\n# your code here -- notice the comment!\n\n(back to text)\n\n\n\nTry running import time as t in the cell below, then see if you can call the function you identified above.\nDoes it work?\n(back to text)\n\n\n\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with value 0.025\n\nT: An integer with value 30\n\nWe will use them in a later exercise.\n\n# your code here!\n\n(back to text)\n\n\n\nRemember the variables we created earlier?\nLet’s compute the present discounted value of a payment ($ D $) made in $ T $ years assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nThe formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# your code here\n\n(back to text)\n\n\n\nVerify the “trick” where the percent difference ($ \\()\nbetween two numbers close to 1 can be well approximated by the difference\nbetween the log of the two numbers (\\) (x) - (y) $).\nUse the numbers x and y below.\nyou will want to use the math.log function\n\nx = 1.05\ny = 1.02\n\n(back to text)\n\n\n\nThe code below is invalid Python code\n\nx = 'What's wrong with this string'\n\nCan you fix it?\nTry creating a code cell below and testing things out until you find a solution.\n(back to text)\n\n\n\nUsing the variables x and y, how could you create the sentence Hello World?\nThink about how to represent a space as a string.\n(back to text)\n\n\n\nOne of our favorite (and most frequently used) string methods is replace.\nIt substitutes all occurrences of a particular pattern with a different pattern.\nFor the variable test below, use the replace method to change the c to a d.\nType test.replace? to get some help for how to use the method replace.\n\ntest = \"abc\"\n\n(back to text)\n\n\n\nSuppose you are working with price data and encounter the value \"\\$6.50\".\nWe recognize this as being a number representing the quantity “six dollars and fifty cents.”\nHowever, Python interprets the value as the string \"\\$6.50\". (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nOnce the string is in a suitable format, you can call write float(clean_price) to make it a number.\n\nprice = \"$6.50\"\n\n(back to text)\n\n\n\nLookup a country in World Bank database, and format a string showing the growth rate of GDP over the last 2 years.\n(back to text)\n\n\n\nInstead of hard-coding the values above, try to use the country, GDP and year variables you previously defined.\n(back to text)\n\n\n\nCreate a new string and use formatting to produce each of the following statements\n\n“The 1st quarter revenue was 110M”\n\n“The 2nd quarter revenue was 95M”\n\n“The 3rd quarter revenue was 100M”\n\n“The 4th quarter revenue was 130M”\n\n(back to text)\n\n\n\nWithout typing the commands, determine whether the following statements are true or false.\nOnce you have evaluated whether the command is True or False, run the code in Python.\n\nx = 2\ny = 2\nz = 4\n\n# Statement 1\nx &gt; z\n\n# Statement 1\nx == y\n\n# Statement 3\n(x &lt; y) and (x &gt; y)\n\n# Statement 4\n(x &lt; y) or (x &gt; y)\n\n# Statement 5\n(x &lt;= y) and (x &gt;= y)\n\n# Statement 6\nTrue and ((x &lt; z) or (x &lt; y))\n\n\n# code here!\n\n(back to text)\n\n\n\nFor each of the code cells below, think carefully about what you expect to be returned before evaluating the cell.\nThen evaluate the cell to check your intuitions.\nNOTE: For now, do not worry about what the [ and ] mean – they allow us to create lists which we will learn about in an upcoming lecture.\n\nall([True, True, True])\n\n\nall([False, True, False])\n\n\nall([False, False, False])\n\n\nany([True, True, True])\n\n\nany([False, True, False])\n\n\nany([False, False, False])\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#first-steps",
    "href": "tutorials/session_1/qe_basics_answers.html#first-steps",
    "title": "Basics",
    "section": "",
    "text": "We are ready to begin writing code!\nIn this section, we will teach you some basic concepts of programming and where to search for help.\n\n\nThe first thing we will learn is the idea of variable assignment.\nVariable assignment associates a value to a variable.\nBelow, we assign the value “Hello World” to the variable x\n\nx = \"Hello World\"\n\n\nx\n\n'Hello World'\n\n\nOnce we have assigned a value to a variable, Python will remember this variable as long as the current session of Python is still running.\nNotice how writing x into the prompt below outputs the value “Hello World”.\n\nx\n\n'Hello World'\n\n\nHowever, Python returns an error if we ask it about variables that have not yet been created.\n\n# uncomment (delete the # and the space) the line below and run\ny\n\nNameError: name 'y' is not defined\n\n\nIt is also useful to understand the order in which operations happen.\nFirst, the right side of the equal sign is computed.\nThen, that computed value is stored as the variable to the left of the equal sign.\n\n\n\nSee exercise 1 in the exercise list.\nKeep in mind that the variable binds a name to something stored in memory.\nThe name can even be bound to a value of a completely different type.\n\nx = 2\nprint(x)\nx = \"something else\"\nprint(x)\n\n\n\n\nComments are short notes that you leave for yourself and for others who read your code.\nThey should be used to explain what the code does.\nA comment is made with the #. Python ignores everything in a line that follows a #.\nLet’s practice making some comments.\n\ni = 1  # Assign the value 1 to variable i\nj = 2  # Assign the value 2 to variable j\n\n# We add i and j below this line\ni + j\n\n3"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#functions",
    "href": "tutorials/session_1/qe_basics_answers.html#functions",
    "title": "Basics",
    "section": "",
    "text": "Functions are processes that take an input (or inputs) and produce an output.\nIf we had a function called f that took two arguments x and y, we would write f(x, y) to use the function.\nFor example, the function print simply prints whatever it is given. Recall the variable we created called x.\n\nprint(x)\n\nHello World\n\n\n\n\nWe can figure out what a function does by asking for help.\nIn Jupyter notebooks, this is done by placing a ? after the function name (without using parenthesis) and evaluating the cell.\nFor example, we can ask for help on the print function by writing print?.\nDepending on how you launched Jupyter, this will either launch\n\nJupyterLab: display the help in text below the cell.\n\nClassic Jupyter Notebooks: display a new panel at the bottom of your screen. You can exit this panel by hitting the escape key or clicking the x at the top right of the panel.\n\n\nprint?\n\n\nDocstring:\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\n# print? # remove the comment and &lt;Shift-Enter&gt;\n\n\n\n\nSee exercise 2 in the exercise list.\nJupyterLab also has a “Contextual Help” (previously called “Inspector”) window. To use,\n\nGo to the Commands and choose Contextual Help (or Inspector), or select &lt;Ctrl-I&gt; (&lt;Cmd-I&gt; for OSX users).\n\nDrag the new inspector pain to dock in the screen next to your code.\n\nThen, type print or any other function into a cell and see the help.\n\n\n# len? # remove the comment and &lt;Shift-Enter&gt;\n\nWe will learn much more about functions, including how to write our own, in a future lecture."
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#objects-and-types",
    "href": "tutorials/session_1/qe_basics_answers.html#objects-and-types",
    "title": "Basics",
    "section": "",
    "text": "Everything in Python is an object.\nObjects are “things” that contain 1) data and 2) functions that can operate on the data.\nSometimes we refer to the functions inside an object as methods.\nWe can investigate what data is inside an object and which methods it supports by typing . after that particular variable, then hitting TAB.\nIt should then list data and method names to the right of the variable name like this:\n\n\n\nhttps://datascience.quantecon.org/_static/introspection.png\n\n\nYou can scroll through this list by using the up and down arrows.\nWe often refer to this as “tab completion” or “introspection”.\nLet’s do this together below. Keep going down until you find the method split.\n\n# Type a period after `x` and then press TAB.\nx\n\nOnce you have found the method split, you can use the method by adding parenthesis after it.\nLet’s call the split method, which doesn’t have any other required parameters. (Quiz: how would we check that?)\n\nx.split()\n\nWe often want to identify what kind of object some value is– called its “type”.\nA “type” is an abstraction which defines a set of behavior for any “instance” of that type i.e. 2.0 and 3.0 are instances of float, where float has a set of particular common behaviors.\nIn particular, the type determines:\n\nthe available data for any “instance” of the type (where each instance may have different values of the data).\n\nthe methods that can be applied on the object and its data.\n\nWe can figure this out by using the type function.\nThe type function takes a single argument and outputs the type of that argument.\n\ntype(3)\n\n\ntype(\"Hello World\")\n\n\ntype([1, 2, 3])\n\nWe will learn more about each of these types (and others!) and how to use them soon, so stay tuned!"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#modules",
    "href": "tutorials/session_1/qe_basics_answers.html#modules",
    "title": "Basics",
    "section": "",
    "text": "Python takes a modular approach to tools.\nBy this we mean that sets of related tools are bundled together into packages. (You may also hear the term modules to describe the same thing.)\nFor example:\n\npandas is a package that implements the tools necessary to do scalable data analysis.\n\nmatplotlib is a package that implements visualization tools.\n\nrequests and urllib are packages that allow Python to interface with the internet.\n\nAs we move further into the class, being able to access these packages will become very important.\nWe can bring a package’s functionality into our current Python session by writing\n\nimport package\n\nOnce we have done this, any function or object from that package can be accessed by using package.name.\nHere’s an example.\n\nimport sys   # for dealing with your computer's system\nsys.version  # information about the Python version in use"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#exercise-2",
    "href": "tutorials/session_1/qe_basics_answers.html#exercise-2",
    "title": "Basics",
    "section": "",
    "text": "See exercise 3 in the exercise list.\n\n\nSome packages have long names (see matplotlib, for example) which makes accessing the package functionality somewhat inconvenient.\nTo ease this burden, Python allows us to give aliases or “nicknames” to packages.\nFor example we can write:\n\nimport package as p\n\nThis statement allows us to access the packages functionality as p.function_name rather than package.function_name.\nSome common aliases for packages are\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib as mpl\n\nimport datetime as dt\n\nWhile you can choose any name for an alias, we suggest that you stick to the common ones.\nYou will learn what these common ones are over time.\n\n\n\nSee exercise 4 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#good-code-habits",
    "href": "tutorials/session_1/qe_basics_answers.html#good-code-habits",
    "title": "Basics",
    "section": "",
    "text": "A common saying in the software engineering world is:\n\nAlways code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability.\n\nThis might be a dramatic take, but the most important feature of your code after correctness is readability.\nWe encourage you to do everything in your power to make your code as readable as possible.\nHere are some suggestions for how to do so:\n\nComment frequently. Leaving short notes not only will help others who use your code, but will also help you interpret your code after some time has passed.\n\nAnytime you use a comma, place a space immediately afterwards.\n\nWhitespace is your friend. Don’t write line after line of code – use blank lines to break it up.\n\nDon’t let your lines run too long. Some people reading your code will be on a laptop, so you want to ensure that they don’t need to scroll horizontally and right to read your code. We recommend no more than 80 characters per line."
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#numbers",
    "href": "tutorials/session_1/qe_basics_answers.html#numbers",
    "title": "Basics",
    "section": "",
    "text": "Python has two types of numbers.\n\nInteger (int): These can only take the values of the integers i.e. $ {, -2, -1, 0, 1, 2, } $\n\nFloating Point Number (float): Think of these as any real number such as $ 1.0 $, $ 3.1415 $, or $ -100.022358923223 $…\n\nThe easiest way to differentiate these types of numbers is to find a decimal place after the number.\nA float will have a decimal place, but an integer will not.\nBelow, we assign integers to the variables xi and zi and assign floating point numbers to the variables xf and zf.\n\nxi = 1\nxf = 1.0\nzi = 123\nzf = 1230.5  # Notice -- There are no commas!\nzf2 = 1_230.5  # If needed, we use `_` to separate numbers for readability"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#exercise-4",
    "href": "tutorials/session_1/qe_basics_answers.html#exercise-4",
    "title": "Basics",
    "section": "",
    "text": "See exercise 5 in the exercise list.\n\n\nYou can use Python to perform mathematical calculations.\n\na = 4\nb = 2\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\nprint(\"a ^ b is\", a^b)\n\nYou likely could have guessed all except the last two.\nPython uses **, not ^, for exponentiation (raising a number to a power)!\nNotice also that above +, - and ** all returned an integer type, but / converted the result to a float.\nWhen possible, operations between integers return an integer type.\nAll operations involving a float will result in a float.\n\na = 4\nb = 2.0\n\nprint(\"a + b is\", a + b)\nprint(\"a - b is\", a - b)\nprint(\"a * b is\", a * b)\nprint(\"a / b is\", a / b)\nprint(\"a ** b is\", a**b)\n\nWe can also chain together operations.\nWhen doing this, Python follows the standard order of operations — parenthesis, exponents, multiplication and division, followed by addition and subtraction.\nFor example,\n\nx = 2.0\ny = 3.0\nz1 = x + y * x\nz2 = (x + y) * x\n\nWhat do you think z1 is?\nHow about z2?\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe often want to use other math functions on our numbers. Let’s try to calculate sin(2.5).\n\nsin(2.5)\n\nAs seen above, Python complains that sin isn’t defined.\nThe problem here is that the sin function – as well as many other standard math functions – are contained in the math package.\nWe must begin by importing the math package.\n\nimport math\n\nNow, we can use math.[TAB] to see what functions are available to us.\n\n# uncomment, add a period (`.`) and pres TAB\n# math\n\n\n# found math.sin!\nmath.sin(2.5)\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\nYou are less likely to run into the following operators, but understanding that they exist is useful.\nFor two numbers assigned to the variables x and y,\n\nFloor division: x // y\n\nModulus division: x % y\n\nRemember when you first learned how to do division and you were asked to talk about the quotient and the remainder?\nThat’s what these operators correspond to…\nFloor division returns the number of times the divisor goes into the dividend (the quotient) and modulus division returns the remainder.\nAn example would be 37 divided by 7:\n\nFloor division would return 5 (7 * 5 = 35)\n\nModulus division would return 2 (2 + 35 = 37)\n\nTry it!\n\n37 // 7\n\n\n37 % 7"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#strings",
    "href": "tutorials/session_1/qe_basics_answers.html#strings",
    "title": "Basics",
    "section": "",
    "text": "Textual information is stored in a data type called a string.\nTo denote that you would like something to be stored as a string, you place it inside of quotation marks.\nFor example,\n\n\"this is a string\"  # Notice the quotation marks\n'this is a string'  # Notice the quotation marks\nthis is not a string  # No quotation marks\n\nYou can use either \" or ' to create a string. Just make sure that you start and end the string with the same one!\nNotice that if we ask Python to tell us the type of a string, it abbreviates its answer to str.\n\ntype(\"this is a string\")"
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#exercise-7",
    "href": "tutorials/session_1/qe_basics_answers.html#exercise-7",
    "title": "Basics",
    "section": "",
    "text": "See exercise 8 in the exercise list.\n\n\nSome of the arithmetic operators we saw in the numbers lecture also work on strings:\n\nPut two strings together: x + y.\n\nRepeat the string x a total of n times: n * x (or x * n).\n\n\nx = \"Hello\"\ny = \"World\"\n\n\nx + y\n\n\n3 * x\n\nWhat happens if we try * with two strings, or - or /?\nThe best way to find out is to try it!\n\na = \"1\"\nb = \"2\"\na * b\n\n\na - b\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\nWe can use many methods to manipulate strings.\nWe will not be able to cover all of them here, but let’s take a look at some of the most useful ones.\n\nx\n\n\nx.lower()  # Makes all letters lower case\n\n\nx.upper()  # Makes all letters upper case\n\n\nx.count(\"l\")  # Counts number of a particular string\n\n\nx.count(\"ll\")\n\n\n\n\nSee exercise 10 in the exercise list.\n\n\n\nSee exercise 11 in the exercise list.\n\n\n\nSometimes we’d like to reuse some portion of a string repeatedly, but still make some relatively small changes at each usage.\nWe can do this with string formatting, which done by using {} as a placeholder where we’d like to change the string, with a variable name or expression.\nLet’s look at an example.\n\ncountry = \"Vietnam\"\nGDP = 223.9\nyear = 2017\nmy_string = f\"{country} had ${GDP} billion GDP in {year}\"\nprint(my_string)\n\nRather than just substituting a variable name, you can use a calculation or expression.\n\nprint(f\"{5}**2 = {5**2}\")\n\nOr, using our previous example\n\nmy_string = f\"{country} had ${GDP * 1_000_000} GDP in {year}\"\nprint(my_string)\n\nIn these cases, the f in front of the string causes Python interpolate any valid expression within the {} braces.\n\n\n\nSee exercise 12 in the exercise list.\nAlternatively, to reuse a formatted string, you can call the format method (noting that you do not put f in front).\n\ngdp_string = \"{country} had ${GDP} billion in {year}\"\n\ngdp_string.format(country = \"Vietnam\", GDP = 223.9, year = 2017)\n\n\n\n\nSee exercise 13 in the exercise list.\n\n\n\nSee exercise 14 in the exercise list.\nFor more information on what you can do with string formatting (there is a lot that can be done…), see the official Python documentation on the subject."
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#booleans",
    "href": "tutorials/session_1/qe_basics_answers.html#booleans",
    "title": "Basics",
    "section": "",
    "text": "A boolean is a type that denotes true or false.\nAs you will soon see in the control flow chapter, using boolean values allows you to perform or skip operations depending on whether or not a condition is met.\nLet’s start by creating some booleans and looking at them.\n\nx = True\ny = False\n\ntype(x)\n\n\nx\n\n\ny\n\n\n\nRather than directly write True or False, you will usually create booleans by making a comparison.\nFor example, you might want to evaluate whether the price of a particular asset is greater than or less than some price.\nFor two variables x and y, we can do the following comparisons:\n\nGreater than: x &gt; y\n\nLess than: x &lt; y\n\nEqual to: ==\n\nGreater than or equal to: x &gt;= y\n\nLess than or equal to: x &lt;= y\n\nWe demonstrate these below.\n\na = 4\nb = 2\n\nprint(\"a &gt; b\", \"is\", a &gt; b)\nprint(\"a &lt; b\", \"is\", a &lt; b)\nprint(\"a == b\", \"is\", a == b)\nprint(\"a &gt;= b\", \"is\", a &gt;= b)\nprint(\"a &lt;= b\", \"is\", a &lt;= b)\n\n\n\n\nOccasionally, determining whether a statement is “not true” or “not false” is more convenient than simply “true” or “false”.\nThis is known as negating a statement.\nIn Python, we can negate a boolean using the word not.\n\nnot False\n\n\nnot True\n\n\n\n\nSometimes we need to evaluate multiple comparisons at once.\nThis is done by using the words and and or.\nHowever, these are the “mathematical” ands and ors – so they don’t carry the same meaning as you’d use them in colloquial English.\n\na and b is true only when both a and b are true.\n\na or b is true whenever at least one of a or b is true.\n\nFor example\n\nThe statement “I will accept the new job if the salary is higher and I receive more vacation days” means that you would only accept the new job if you both receive a higher salary and are given more vacation days.\n\nThe statement “I will accept the new job if the salary is higher or I receive more vacation days” means that you would accept the job if\n\nthey raised your salary, (2) you are given more vacation days, or\nthey raise your salary and give you more vacation days.\n\n\nLet’s see some examples.\n\nTrue and False\n\n\nTrue and True\n\n\nTrue or False\n\n\nFalse or False\n\n\n# Can chain multiple comparisons together.\nTrue and (False or True)\n\n\n\n\nSee exercise 15 in the exercise list.\n\n\n\nWe have seen how we can use the words and and or to process two booleans at a time.\nThe functions all and any allow us to process an unlimited number of booleans at once.\nall(bools) will return True if and only if all the booleans in bools is True and returns False otherwise.\nany(bools) returns True whenever one or more of bools is True.\nThe exercise below will give you a chance to practice.\n\n\n\nSee exercise 16 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_basics_answers.html#exercises",
    "href": "tutorials/session_1/qe_basics_answers.html#exercises",
    "title": "Basics",
    "section": "",
    "text": "What do you think the value of z is after running the code below?\n\nz = 3\nz = z + 4\nprint(\"z is\", z)\n\nz is 7\n\n\nI think the answer is: 7\n(back to text)\n\n\n\nRead about out what the len function does (by writing len?).\nWhat will it produce if we give it the variable x?\nCheck whether you were right by running the code len(x).\n(back to text)\nlen counts the number of objects in a collection and returns an integer. The string x has 11 elements, which should be the answer.\n\nlen?\n\n\nSignature: len(obj, /)\nDocstring: Return the number of items in a container.\nType:      builtin_function_or_method\n\n\n\n\nlen(x)\n\n11\n\n\n\n\n\nWe can use our introspection skills to investigate a package’s contents.\nIn the cell below, use tab completion to find a function from the time module that will display the local time.\nUse time.FUNC_NAME? (where FUNC_NAME is replaced with the function you found) to see information about that function and then call the function.\nLook for something to do with the word local\n\nimport time\n# your code here -- notice the comment!\n\n(back to text)\n\n\n\nTry running import time as t in the cell below, then see if you can call the function you identified above.\nDoes it work?\n(back to text)\n\n\n\nCreate the following variables:\n\nD: A floating point number with the value 10,000\n\nr: A floating point number with value 0.025\n\nT: An integer with value 30\n\nWe will use them in a later exercise.\n\n# your code here!\n\n(back to text)\n\n\n\nRemember the variables we created earlier?\nLet’s compute the present discounted value of a payment ($ D $) made in $ T $ years assuming an interest rate of 2.5%. Save this value to a new variable called PDV and print your output.\nThe formula is\n\\[\n\\text{PDV} = \\frac{D}{(1 + r)^T}\n\\]\n\n# your code here\n\n(back to text)\n\n\n\nVerify the “trick” where the percent difference ($ \\()\nbetween two numbers close to 1 can be well approximated by the difference\nbetween the log of the two numbers (\\) (x) - (y) $).\nUse the numbers x and y below.\nyou will want to use the math.log function\n\nx = 1.05\ny = 1.02\n\n(back to text)\n\n\n\nThe code below is invalid Python code\n\nx = 'What's wrong with this string'\n\nCan you fix it?\nTry creating a code cell below and testing things out until you find a solution.\n(back to text)\n\n\n\nUsing the variables x and y, how could you create the sentence Hello World?\nThink about how to represent a space as a string.\n(back to text)\n\n\n\nOne of our favorite (and most frequently used) string methods is replace.\nIt substitutes all occurrences of a particular pattern with a different pattern.\nFor the variable test below, use the replace method to change the c to a d.\nType test.replace? to get some help for how to use the method replace.\n\ntest = \"abc\"\n\n(back to text)\n\n\n\nSuppose you are working with price data and encounter the value \"\\$6.50\".\nWe recognize this as being a number representing the quantity “six dollars and fifty cents.”\nHowever, Python interprets the value as the string \"\\$6.50\". (Quiz: why is this a problem? Think about the examples above.)\nIn this exercise, your task is to convert the variable price below into a number.\nOnce the string is in a suitable format, you can call write float(clean_price) to make it a number.\n\nprice = \"$6.50\"\n\n(back to text)\n\n\n\nLookup a country in World Bank database, and format a string showing the growth rate of GDP over the last 2 years.\n(back to text)\n\n\n\nInstead of hard-coding the values above, try to use the country, GDP and year variables you previously defined.\n(back to text)\n\n\n\nCreate a new string and use formatting to produce each of the following statements\n\n“The 1st quarter revenue was 110M”\n\n“The 2nd quarter revenue was 95M”\n\n“The 3rd quarter revenue was 100M”\n\n“The 4th quarter revenue was 130M”\n\n(back to text)\n\n\n\nWithout typing the commands, determine whether the following statements are true or false.\nOnce you have evaluated whether the command is True or False, run the code in Python.\n\nx = 2\ny = 2\nz = 4\n\n# Statement 1\nx &gt; z\n\n# Statement 1\nx == y\n\n# Statement 3\n(x &lt; y) and (x &gt; y)\n\n# Statement 4\n(x &lt; y) or (x &gt; y)\n\n# Statement 5\n(x &lt;= y) and (x &gt;= y)\n\n# Statement 6\nTrue and ((x &lt; z) or (x &lt; y))\n\n\n# code here!\n\n(back to text)\n\n\n\nFor each of the code cells below, think carefully about what you expect to be returned before evaluating the cell.\nThen evaluate the cell to check your intuitions.\nNOTE: For now, do not worry about what the [ and ] mean – they allow us to create lists which we will learn about in an upcoming lecture.\n\nall([True, True, True])\n\n\nall([False, True, False])\n\n\nall([False, False, False])\n\n\nany([True, True, True])\n\n\nany([False, True, False])\n\n\nany([False, False, False])\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections_answers.html",
    "href": "tutorials/session_1/qe_collections_answers.html",
    "title": "Collections",
    "section": "",
    "text": "Prerequisites\n\nCore data types\n\nOutcomes\n\nOrdered Collections\n\nKnow what a list is and a tuple is\n\nKnow how to tell a list from a tuple\n\nUnderstand the range, zip and enumerate functions\n\nBe able to use common list methods like append, sort, and reverse\n\n\nAssociative Collections\n\nUnderstand what a dict is\n\nKnow the distinction between a dicts keys and values\n\nUnderstand when dicts are useful\n\nBe familiar with common dict methods\n\n\nSets (optional)\n\nKnow what a set is\n\nUnderstand how a set differs from a list and a tuple\n\nKnow when to use a set vs a list or a tuple\n\n\n\n\n\n\nA Python list is an ordered collection of items.\nWe can create lists using the following syntax\n\n[item1, item2, ...,  itemN]\n\nwhere the ... represents any number of additional items.\nEach item can be of any type.\nLet’s create some lists.\n\n# created, but not assigned to a variable\n[2.0, 9.1, \"a rose is a rose is a rose\"]\n\n[2.0, 9.1, 'a rose is a rose is a rose']\n\n\n\n# stored as the variable `x`\nx = [2.0, 9.1, 12.5]\nprint(\"x has type\", type(x))\nx\n\nx has type &lt;class 'list'&gt;\n\n\n[2.0, 9.1, 12.5]\n\n\n\n\nWe can access items in a list called mylist using mylist[N] where N is an integer.\nNote: Anytime that we use the syntax x[i] we are doing what is called indexing – it means that we are selecting a particular element of a collection x.\n\nx[1]\n\n9.1\n\n\nWait? Why did x[1] return 9.1 when the first element in x is actually 2.0?\nThis happened because Python starts counting at zero!\nLets repeat that one more time for emphasis Python starts counting at zero!\nTo access the first element of x we must use x[0]:\n\nx[0]\n\n2.0\n\n\nWe can also determine how many items are in a list using the len function.\n\nlen(x)\n\n3\n\n\nWhat happens if we try to index with a number higher than the number of items in a list?\n\n# uncomment the line below and run\nx[4]\n\nIndexError: list index out of range\n\n\nWe can check if a list contains an element using the in keyword.\n\n2.0 in x\n\nTrue\n\n\n\n1.5 in x\n\nFalse\n\n\nFor our list x, other common operations we might want to do are…\n\nx.reverse()\nx\n\n[12.5, 9.1, 2.0]\n\n\n\nnumber_list = [10, 25, 42, 1.0]\nprint(number_list)\nnumber_list.sort()\nprint(number_list)\n\n[10, 25, 42, 1.0]\n[1.0, 10, 25, 42]\n\n\nNote that in order to sort, we had to have all elements in our list be numbers (int and float), more on this below.\nWe could actually do the same with a list of strings. In this case, sort will put the items in alphabetical order.\n\nstr_list = [\"NY\", \"AZ\", \"TX\"]\nprint(str_list)\nstr_list.sort()\nprint(str_list)\n\n['NY', 'AZ', 'TX']\n['AZ', 'NY', 'TX']\n\n\nThe append method adds an element to the end of existing list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append(10)\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 10]\n\n\nHowever, if you call append with a list, it adds a list to the end, rather than the numbers in that list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, [20, 4]]\n\n\nTo combine the lists instead…\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.extend([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 20, 4]\n\n\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\n\n\nWhile most examples above have all used a list with a single type of variable, this is not required.\nLet’s carefully make a small change to the first example: replace 2.0 with 2\n\nx = [2, 9.1, 12.5]\n\nThis behavior is identical for many operations you might apply to a list.\n\nimport numpy as np\nx = [2, 9.1, 12.5]\nnp.mean(x) == sum(x)/len(x)\n\nHere we have also introduced a new module, Numpy, which provides many functions for working with numeric data.\nTaking this further, we can put completely different types of elements inside of a list.\n\n# stored as the variable `x`\nx = [2, \"hello\", 3.0]\nprint(\"x has type\", type(x))\nx\n\nTo see the types of individual elements in the list:\n\nprint(f\"type(x[0]) = {type(x[0])}, type(x[0]) = {type(x[1])}, type(x[2]) = {type(x[2])}\")\n\nWhile no programming limitations prevent this, you should be careful if you write code with different numeric and non-numeric types in the same list.\nFor example, if the types within the list cannot be compared, then how could you sort the elements of the list? (i.e. How do you determine whether the string “hello” is less than the integer 2, “hello” &lt; 2?)\n\nx = [2, \"hello\", 3.0]\n# uncomment the line below and see what happens!\n# x.sort()\n\nA few key exceptions to this general rule are:\n\nLists with both integers and floating points are less error-prone (since mathematical code using the list would work with both types).\n\nWhen working with lists and data, you may want to represent missing values with a different type than the existing values.\n\n\n\n\nOne function you will see often in Python is the range function.\nIt has three versions:\n\nrange(N): goes from 0 to N-1\n\nrange(a, N): goes from a to N-1\n\nrange(a, N, d): goes from a to N-1, counting by d\n\nWhen we call the range function, we get back something that has type range:\n\nr = range(5)\nprint(\"type(r)\", type(r))\n\ntype(r) &lt;class 'range'&gt;\n\n\nTo turn the range into a list:\n\n[*r]\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(r)\n\n[0, 1, 2, 3, 4]\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\nTuples are very similar to lists and hold ordered collections of items.\nHowever, tuples and lists have three main differences:\n\nTuples are created using parenthesis — ( and ) — instead of square brackets — [ and ].\n\nTuples are immutable, which is a fancy computer science word meaning that they can’t be changed or altered after they are created.\n\nTuples and multiple return values from functions are tightly connected, as we will see in functions.\n\n\nt = (1, \"hello\", 3.0)\nprint(\"t is a\", type(t))\nt\n\nt is a &lt;class 'tuple'&gt;\n\n\n(1, 'hello', 3.0)\n\n\n\ntuple(list(t))\n\n(1, 'hello', 3.0)\n\n\nWe can convert a list to a tuple by calling the tuple function on a list.\n\nprint(\"x is a\", type(x))\nprint(\"tuple(x) is a\", type(tuple(x)))\ntuple(x)\n\nWe can also convert a tuple to a list using the list function.\n\nlist(t)\n\nAs with a list, we access items in a tuple t using t[N] where N is an int.\n\nt[0]  # still start counting at 0\n\n1\n\n\n\nt[2]\n\n3.0\n\n\n\n\n\nSee exercise 3 in the exercise list.\nTuples (and lists) can be unpacked directly into variables.\n\nx, y = (1, \"test\")\nprint(f\"x = {x}, y = {y}\")\n\nThis will be a convenient way to work with functions returning multiple values, as well as within comprehensions and loops.\n\n\n\nShould you use a list or tuple?\nThis depends on what you are storing, whether you might need to reorder the elements, or whether you’d add new elements without a complete reinterpretation of the underlying data.\nFor example, take data representing the GDP (in trillions) and population (in billions) for China in 2015.\n\nchina_data_2015 = (\"China\", 2015, 11.06, 1.371)\n\nprint(china_data_2015)\n\nIn this case, we have used a tuple since: (a) ordering would be meaningless; and (b) adding more data would require a reinterpretation of the whole data structure.\nOn the other hand, consider a list of GDP in China between 2013 and 2015.\n\ngdp_data = [9.607, 10.48, 11.06]\nprint(gdp_data)\n\nIn this case, we have used a list, since adding on a new element to the end of the list for GDP in 2016 would make complete sense.\nAlong these lines, collecting data on China for different years may make sense as a list of tuples (e.g. year, GDP, and population – although we will see better ways to store this sort of data in the Pandas section).\n\nchina_data = [(2015, 11.06, 1.371), (2014, 10.48, 1.364), (2013, 9.607, 1.357)]\nprint(china_data)\n\nIn general, a rule of thumb is to use a list unless you need to use a tuple.\nKey criteria for tuple use are when you want to:\n\nensure the order of elements can’t change\n\nensure the actual values of the elements can’t change\n\nuse the collection as a key in a dict (we will learn what this means soon)\n\n\n\n\nTwo functions that can be extremely useful are zip and enumerate.\nBoth of these functions are best understood by example, so let’s see them in action and then talk about what they do.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nprint(\"type(z)\", type(z))\n\ntype(z) &lt;class 'zip'&gt;\n\n\nTo see what is inside z, let’s convert it to a list.\n\nlist(z)\n\n[(2013, 9.607), (2014, 10.48), (2015, 11.06)]\n\n\nNotice that we now have a list where each item is a tuple.\nWithin each tuple, we have one item from each of the collections we passed to the zip function.\nIn particular, the first item in z contains the first item from [2013, 2014, 2015] and the first item from [9.607, 10.48, 11.06].\nThe second item in z contains the second item from each collection and so on.\nWe can access an element in this and then unpack the resulting tuple directly into variables.\n\nl = list(zip(years, gdp_data))\nx, y = l[0]\nprint(f\"year = {x}, GDP = {y}\")\n\nNow let’s experiment with enumerate.\n\ne = enumerate([\"a\", \"b\", \"c\"])\nprint(\"type(e)\", type(e))\ne\n\ntype(e) &lt;class 'enumerate'&gt;\n\n\n&lt;enumerate at 0x7fef3c8a01c0&gt;\n\n\nAgain, we call list(e) to see what is inside.\n\nlist(e)\n\n[(0, 'a'), (1, 'b'), (2, 'c')]\n\n\nWe again have a list of tuples, but this time, the first element in each tuple is the index of the second tuple element in the initial collection.\nNotice that the third item is (2, 'c') because [\"a\", \"b\", \"c\"][2] is 'c'\n\n\n\nSee exercise 4 in the exercise list.\nAn important quirk of some iterable types that are not lists (such as the above zip) is that you cannot convert the same type to a list twice.\nThis is because zip, enumerate, and range produce what is called a generator.\nA generator will only produce each of its elements a single time, so if you call list on the same generator a second time, it will not have any elements to iterate over anymore.\nFor more information, refer to the Python documentation.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nl = list(z)\nprint(l)\nm = list(z)\nprint(m)\n\n\n\n\n\n\n\n\nA dictionary (or dict) associates keys with values.\nIt will feel similar to a dictionary for words, where the keys are words and the values are the associated definitions.\nThe most common way to create a dict is to use curly braces — { and } — like this:\n\n{\n    \"key1\": value1,\n    \"key2\": value2,\n    \"keyN\": valueN\n}\n\nwhere the ... indicates that we can have any number of additional terms.\nThe crucial part of the syntax is that each key-value pair is written key: value and that these pairs are separated by commas — ,.\nLet’s see an example using our aggregate data on China in 2015.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\nUnlike our above example using a tuple, a dict allows us to associate a name with each field, rather than having to remember the order within the tuple.\nOften, code that makes a dict is easier to read if we put each key: value pair on its own line. (Recall our earlier comment on using whitespace effectively to improve readability!)\nThe code below is equivalent to what we saw above.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\n\nMost often, the keys (e.g. “country”, “year”, “GDP”, and “population”) will be strings, but we could also use numbers (int, or float) or even tuples (or, rarely, a combination of types).\nThe values can be any type and different from each other.\n\n\n\nSee exercise 5 in the exercise list.\nThis next example is meant to emphasize how values can be anything – including another dictionary.\n\ncompanies = {\"AAPL\": {\"bid\": 175.96, \"ask\": 175.98},\n             \"GE\": {\"bid\": 1047.03, \"ask\": 1048.40},\n             \"TVIX\": {\"bid\": 8.38, \"ask\": 8.40}}\nprint(companies)\n\n\n\nWe can now ask Python to tell us the value for a particular key by using the syntax d[k], where d is our dict and k is the key for which we want to find the value.\nFor example,\n\nchina_data['country']\n\n'China'\n\n\n\n\nprint(f\"country = {china_data['country']}, population = {china_data['population']}\")\n\ncountry = China, population = 1.371\n\n\nNote: when inside of a formatting string, you can use ' instead of \" as above to ensure the formatting still works with the embedded code.\nIf we ask for the value of a key that is not in the dict, we will get an error.\n\n# uncomment the line below to see the error\n# china_data[\"inflation\"]\n\nWe can also add new items to a dict using the syntax d[new_key] = new_value.\nLet’s see some examples.\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.05%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n\n\nTo update the value, we use assignment in the same way (which will create the key and value as required).\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.051%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.051%'}\n\n\nOr we could change the type.\n\nchina_data[\"unemployment\"] = False or True\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': True}\n\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe can do some common things with dicts.\nWe will demonstrate them with examples below.\n\n# number of key-value pairs in a dict\nlen(china_data)\n\n5\n\n\n\n# get a list of all the keys\nlist(china_data.keys())\n\n['country', 'year', 'GDP', 'population', 'unemployment']\n\n\n\n# get a list of all the values\nlist(china_data.values())\n\n['China', 2015, 11.06, 1.371, True]\n\n\n\nmore_china_data = {\n    \"irrigated_land\": 690_070,\n    \"top_religions\":\n        {\"buddhist\": 18.2, \"christian\" : 5.1, \"muslim\": 1.8}\n}\n\n# Add all key-value pairs in mydict2 to mydict.\n# if the key already appears in mydict, overwrite the\n# value with the value in mydict2\nchina_data.update(more_china_data)\nchina_data\n\n{'country': 'China',\n 'year': 2015,\n 'GDP': 11.06,\n 'population': 1.371,\n 'unemployment': True,\n 'irrigated_land': 690070,\n 'top_religions': {'buddhist': 18.2, 'christian': 5.1, 'muslim': 1.8}}\n\n\n\ntype(china_data.get(\"book\"))\n\nNoneType\n\n\n\n# Get the value associated with a key or return a default value\n# use this to avoid the NameError we saw above if you have a reasonable\n# default value\nchina_data.get(\"irrigated_land\", \"Data Not Available\")\n\n690070\n\n\n\nchina_data.get(\"book\", \"Data Not Available\")\n\n'Data Not Available'\n\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\n\nPython has an additional way to represent collections of items: sets.\nSets come up infrequently, but you should be aware of them.\nIf you are familiar with the mathematical concept of sets, then you will understand the majority of Python sets already.\nIf you don’t know the math behind sets, don’t worry: we’ll cover the basics of Python’s sets here.\nA set is an unordered collection of unique elements.\nThe syntax for creating a set uses curly bracket { and }.\n\n{item1, item2, ..., itemN}\n\nHere is an example.\n\ns = {1, \"hello\", 3.0}\nprint(\"s has type\", type(s))\ns\n\n\n\n\nSee exercise 9 in the exercise list.\nAs with lists and tuples, we can check if something is in the set and check the set’s length:\n\nprint(\"len(s) =\", len(s))\n\"hello\" in s\n\nUnlike lists and tuples, we can’t extract elements of a set s using s[N] where N is a number.\n\n# Uncomment the line below to see what happens\n# s[1]\n\nThis is because sets are not ordered, so the notion of getting the second element (s[1]) is not well defined.\nWe add elements to a set s using s.add.\n\ns.add(100)\ns\n\n\ns.add(\"hello\") # nothing happens, why?\ns\n\nWe can also do set operations.\nConsider the set s from above and the set s2 = {\"hello\", \"world\"}.\n\ns.union(s2): returns a set with all elements in either s or s2\n\ns.intersection(s2): returns a set with all elements in both s and s2\n\ns.difference(s2): returns a set with all elements in s that aren’t in s2\n\ns.symmetric_difference(s2): returns a set with all elements in only one of s and s2\n\n\n\n\nSee exercise 10 in the exercise list.\nAs with tuples and lists, a set function can convert other collections to sets.\n\nx = [1, 2, 3, 1]\nset(x)\n\n\nt = (1, 2, 3, 1)\nset(t)\n\nLikewise, we can convert sets to lists and tuples.\n\nlist(s)\n\n\ntuple(s)\n\n\n\n\n\n\n\n\nIn the first cell, try y.append(z).\nIn the second cell try y.extend(z).\nExplain the behavior.\nWhen you are trying to explain use y.append? and y.extend? to see a description of what these methods are supposed to do.\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\ny.append(z)\nprint(y)\n\n['a', 'b', 'c', [1, 2, 3]]\n\n\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\ny.extend(z)\nprint(y)\n\n['a', 'b', 'c', 1, 2, 3]\n\n\n(back to text)\n\n\n\nExperiment with the other two versions of the range function.\n\n# try list(range(a, N)) -- you pick `a` and `N`\n\n\n# try list(range(a, N, d)) -- you pick `a`, `N`, and `d`\n\n(back to text)\n\n\n\nVerify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\n# change first element of t\n\n\n# appending to t\n\n\n# sorting t\n\n\n# reversing t\n\n(back to text)\n\n\n\nChallenging For the tuple foo below, use a combination of zip, range, and len to mimic enumerate(foo).\nVerify that your proposed solution is correct by converting each to a list and checking equality with ==.\nYou can see what the answer should look like by starting with list(enumerate(foo)).\n\nfoo = (\"good\", \"luck!\")\n\n(back to text)\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n\n(back to text)\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\n\n(back to text)\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\n#china_data.pop()\n\n(back to text)\n\n\n\nExplain what happens to the value you popped.\nExperiment with calling pop twice.\n\n# your code here\n\n(back to text)\n\n\n\nTry creating a set with repeated elements (e.g. {1, 2, 1, 2, 1, 2}).\nWhat happens?\nWhy?\n\n# your code here\n\n(back to text)\n\n\n\nTest out two of the operations described above using the original set we created, s, and the set created below s2.\n\ns2 = {\"hello\", \"world\"}\n\n\n# Operation 1\n\n\n# Operation 2\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections_answers.html#ordered-collections",
    "href": "tutorials/session_1/qe_collections_answers.html#ordered-collections",
    "title": "Collections",
    "section": "",
    "text": "A Python list is an ordered collection of items.\nWe can create lists using the following syntax\n\n[item1, item2, ...,  itemN]\n\nwhere the ... represents any number of additional items.\nEach item can be of any type.\nLet’s create some lists.\n\n# created, but not assigned to a variable\n[2.0, 9.1, \"a rose is a rose is a rose\"]\n\n[2.0, 9.1, 'a rose is a rose is a rose']\n\n\n\n# stored as the variable `x`\nx = [2.0, 9.1, 12.5]\nprint(\"x has type\", type(x))\nx\n\nx has type &lt;class 'list'&gt;\n\n\n[2.0, 9.1, 12.5]\n\n\n\n\nWe can access items in a list called mylist using mylist[N] where N is an integer.\nNote: Anytime that we use the syntax x[i] we are doing what is called indexing – it means that we are selecting a particular element of a collection x.\n\nx[1]\n\n9.1\n\n\nWait? Why did x[1] return 9.1 when the first element in x is actually 2.0?\nThis happened because Python starts counting at zero!\nLets repeat that one more time for emphasis Python starts counting at zero!\nTo access the first element of x we must use x[0]:\n\nx[0]\n\n2.0\n\n\nWe can also determine how many items are in a list using the len function.\n\nlen(x)\n\n3\n\n\nWhat happens if we try to index with a number higher than the number of items in a list?\n\n# uncomment the line below and run\nx[4]\n\nIndexError: list index out of range\n\n\nWe can check if a list contains an element using the in keyword.\n\n2.0 in x\n\nTrue\n\n\n\n1.5 in x\n\nFalse\n\n\nFor our list x, other common operations we might want to do are…\n\nx.reverse()\nx\n\n[12.5, 9.1, 2.0]\n\n\n\nnumber_list = [10, 25, 42, 1.0]\nprint(number_list)\nnumber_list.sort()\nprint(number_list)\n\n[10, 25, 42, 1.0]\n[1.0, 10, 25, 42]\n\n\nNote that in order to sort, we had to have all elements in our list be numbers (int and float), more on this below.\nWe could actually do the same with a list of strings. In this case, sort will put the items in alphabetical order.\n\nstr_list = [\"NY\", \"AZ\", \"TX\"]\nprint(str_list)\nstr_list.sort()\nprint(str_list)\n\n['NY', 'AZ', 'TX']\n['AZ', 'NY', 'TX']\n\n\nThe append method adds an element to the end of existing list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append(10)\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 10]\n\n\nHowever, if you call append with a list, it adds a list to the end, rather than the numbers in that list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, [20, 4]]\n\n\nTo combine the lists instead…\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.extend([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 20, 4]\n\n\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\n\n\nWhile most examples above have all used a list with a single type of variable, this is not required.\nLet’s carefully make a small change to the first example: replace 2.0 with 2\n\nx = [2, 9.1, 12.5]\n\nThis behavior is identical for many operations you might apply to a list.\n\nimport numpy as np\nx = [2, 9.1, 12.5]\nnp.mean(x) == sum(x)/len(x)\n\nHere we have also introduced a new module, Numpy, which provides many functions for working with numeric data.\nTaking this further, we can put completely different types of elements inside of a list.\n\n# stored as the variable `x`\nx = [2, \"hello\", 3.0]\nprint(\"x has type\", type(x))\nx\n\nTo see the types of individual elements in the list:\n\nprint(f\"type(x[0]) = {type(x[0])}, type(x[0]) = {type(x[1])}, type(x[2]) = {type(x[2])}\")\n\nWhile no programming limitations prevent this, you should be careful if you write code with different numeric and non-numeric types in the same list.\nFor example, if the types within the list cannot be compared, then how could you sort the elements of the list? (i.e. How do you determine whether the string “hello” is less than the integer 2, “hello” &lt; 2?)\n\nx = [2, \"hello\", 3.0]\n# uncomment the line below and see what happens!\n# x.sort()\n\nA few key exceptions to this general rule are:\n\nLists with both integers and floating points are less error-prone (since mathematical code using the list would work with both types).\n\nWhen working with lists and data, you may want to represent missing values with a different type than the existing values.\n\n\n\n\nOne function you will see often in Python is the range function.\nIt has three versions:\n\nrange(N): goes from 0 to N-1\n\nrange(a, N): goes from a to N-1\n\nrange(a, N, d): goes from a to N-1, counting by d\n\nWhen we call the range function, we get back something that has type range:\n\nr = range(5)\nprint(\"type(r)\", type(r))\n\ntype(r) &lt;class 'range'&gt;\n\n\nTo turn the range into a list:\n\n[*r]\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(r)\n\n[0, 1, 2, 3, 4]\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\nTuples are very similar to lists and hold ordered collections of items.\nHowever, tuples and lists have three main differences:\n\nTuples are created using parenthesis — ( and ) — instead of square brackets — [ and ].\n\nTuples are immutable, which is a fancy computer science word meaning that they can’t be changed or altered after they are created.\n\nTuples and multiple return values from functions are tightly connected, as we will see in functions.\n\n\nt = (1, \"hello\", 3.0)\nprint(\"t is a\", type(t))\nt\n\nt is a &lt;class 'tuple'&gt;\n\n\n(1, 'hello', 3.0)\n\n\n\ntuple(list(t))\n\n(1, 'hello', 3.0)\n\n\nWe can convert a list to a tuple by calling the tuple function on a list.\n\nprint(\"x is a\", type(x))\nprint(\"tuple(x) is a\", type(tuple(x)))\ntuple(x)\n\nWe can also convert a tuple to a list using the list function.\n\nlist(t)\n\nAs with a list, we access items in a tuple t using t[N] where N is an int.\n\nt[0]  # still start counting at 0\n\n1\n\n\n\nt[2]\n\n3.0\n\n\n\n\n\nSee exercise 3 in the exercise list.\nTuples (and lists) can be unpacked directly into variables.\n\nx, y = (1, \"test\")\nprint(f\"x = {x}, y = {y}\")\n\nThis will be a convenient way to work with functions returning multiple values, as well as within comprehensions and loops.\n\n\n\nShould you use a list or tuple?\nThis depends on what you are storing, whether you might need to reorder the elements, or whether you’d add new elements without a complete reinterpretation of the underlying data.\nFor example, take data representing the GDP (in trillions) and population (in billions) for China in 2015.\n\nchina_data_2015 = (\"China\", 2015, 11.06, 1.371)\n\nprint(china_data_2015)\n\nIn this case, we have used a tuple since: (a) ordering would be meaningless; and (b) adding more data would require a reinterpretation of the whole data structure.\nOn the other hand, consider a list of GDP in China between 2013 and 2015.\n\ngdp_data = [9.607, 10.48, 11.06]\nprint(gdp_data)\n\nIn this case, we have used a list, since adding on a new element to the end of the list for GDP in 2016 would make complete sense.\nAlong these lines, collecting data on China for different years may make sense as a list of tuples (e.g. year, GDP, and population – although we will see better ways to store this sort of data in the Pandas section).\n\nchina_data = [(2015, 11.06, 1.371), (2014, 10.48, 1.364), (2013, 9.607, 1.357)]\nprint(china_data)\n\nIn general, a rule of thumb is to use a list unless you need to use a tuple.\nKey criteria for tuple use are when you want to:\n\nensure the order of elements can’t change\n\nensure the actual values of the elements can’t change\n\nuse the collection as a key in a dict (we will learn what this means soon)\n\n\n\n\nTwo functions that can be extremely useful are zip and enumerate.\nBoth of these functions are best understood by example, so let’s see them in action and then talk about what they do.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nprint(\"type(z)\", type(z))\n\ntype(z) &lt;class 'zip'&gt;\n\n\nTo see what is inside z, let’s convert it to a list.\n\nlist(z)\n\n[(2013, 9.607), (2014, 10.48), (2015, 11.06)]\n\n\nNotice that we now have a list where each item is a tuple.\nWithin each tuple, we have one item from each of the collections we passed to the zip function.\nIn particular, the first item in z contains the first item from [2013, 2014, 2015] and the first item from [9.607, 10.48, 11.06].\nThe second item in z contains the second item from each collection and so on.\nWe can access an element in this and then unpack the resulting tuple directly into variables.\n\nl = list(zip(years, gdp_data))\nx, y = l[0]\nprint(f\"year = {x}, GDP = {y}\")\n\nNow let’s experiment with enumerate.\n\ne = enumerate([\"a\", \"b\", \"c\"])\nprint(\"type(e)\", type(e))\ne\n\ntype(e) &lt;class 'enumerate'&gt;\n\n\n&lt;enumerate at 0x7fef3c8a01c0&gt;\n\n\nAgain, we call list(e) to see what is inside.\n\nlist(e)\n\n[(0, 'a'), (1, 'b'), (2, 'c')]\n\n\nWe again have a list of tuples, but this time, the first element in each tuple is the index of the second tuple element in the initial collection.\nNotice that the third item is (2, 'c') because [\"a\", \"b\", \"c\"][2] is 'c'\n\n\n\nSee exercise 4 in the exercise list.\nAn important quirk of some iterable types that are not lists (such as the above zip) is that you cannot convert the same type to a list twice.\nThis is because zip, enumerate, and range produce what is called a generator.\nA generator will only produce each of its elements a single time, so if you call list on the same generator a second time, it will not have any elements to iterate over anymore.\nFor more information, refer to the Python documentation.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nl = list(z)\nprint(l)\nm = list(z)\nprint(m)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections_answers.html#associative-collections",
    "href": "tutorials/session_1/qe_collections_answers.html#associative-collections",
    "title": "Collections",
    "section": "",
    "text": "A dictionary (or dict) associates keys with values.\nIt will feel similar to a dictionary for words, where the keys are words and the values are the associated definitions.\nThe most common way to create a dict is to use curly braces — { and } — like this:\n\n{\n    \"key1\": value1,\n    \"key2\": value2,\n    \"keyN\": valueN\n}\n\nwhere the ... indicates that we can have any number of additional terms.\nThe crucial part of the syntax is that each key-value pair is written key: value and that these pairs are separated by commas — ,.\nLet’s see an example using our aggregate data on China in 2015.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\nUnlike our above example using a tuple, a dict allows us to associate a name with each field, rather than having to remember the order within the tuple.\nOften, code that makes a dict is easier to read if we put each key: value pair on its own line. (Recall our earlier comment on using whitespace effectively to improve readability!)\nThe code below is equivalent to what we saw above.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\n\nMost often, the keys (e.g. “country”, “year”, “GDP”, and “population”) will be strings, but we could also use numbers (int, or float) or even tuples (or, rarely, a combination of types).\nThe values can be any type and different from each other.\n\n\n\nSee exercise 5 in the exercise list.\nThis next example is meant to emphasize how values can be anything – including another dictionary.\n\ncompanies = {\"AAPL\": {\"bid\": 175.96, \"ask\": 175.98},\n             \"GE\": {\"bid\": 1047.03, \"ask\": 1048.40},\n             \"TVIX\": {\"bid\": 8.38, \"ask\": 8.40}}\nprint(companies)\n\n\n\nWe can now ask Python to tell us the value for a particular key by using the syntax d[k], where d is our dict and k is the key for which we want to find the value.\nFor example,\n\nchina_data['country']\n\n'China'\n\n\n\n\nprint(f\"country = {china_data['country']}, population = {china_data['population']}\")\n\ncountry = China, population = 1.371\n\n\nNote: when inside of a formatting string, you can use ' instead of \" as above to ensure the formatting still works with the embedded code.\nIf we ask for the value of a key that is not in the dict, we will get an error.\n\n# uncomment the line below to see the error\n# china_data[\"inflation\"]\n\nWe can also add new items to a dict using the syntax d[new_key] = new_value.\nLet’s see some examples.\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.05%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n\n\nTo update the value, we use assignment in the same way (which will create the key and value as required).\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.051%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.051%'}\n\n\nOr we could change the type.\n\nchina_data[\"unemployment\"] = False or True\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': True}\n\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe can do some common things with dicts.\nWe will demonstrate them with examples below.\n\n# number of key-value pairs in a dict\nlen(china_data)\n\n5\n\n\n\n# get a list of all the keys\nlist(china_data.keys())\n\n['country', 'year', 'GDP', 'population', 'unemployment']\n\n\n\n# get a list of all the values\nlist(china_data.values())\n\n['China', 2015, 11.06, 1.371, True]\n\n\n\nmore_china_data = {\n    \"irrigated_land\": 690_070,\n    \"top_religions\":\n        {\"buddhist\": 18.2, \"christian\" : 5.1, \"muslim\": 1.8}\n}\n\n# Add all key-value pairs in mydict2 to mydict.\n# if the key already appears in mydict, overwrite the\n# value with the value in mydict2\nchina_data.update(more_china_data)\nchina_data\n\n{'country': 'China',\n 'year': 2015,\n 'GDP': 11.06,\n 'population': 1.371,\n 'unemployment': True,\n 'irrigated_land': 690070,\n 'top_religions': {'buddhist': 18.2, 'christian': 5.1, 'muslim': 1.8}}\n\n\n\ntype(china_data.get(\"book\"))\n\nNoneType\n\n\n\n# Get the value associated with a key or return a default value\n# use this to avoid the NameError we saw above if you have a reasonable\n# default value\nchina_data.get(\"irrigated_land\", \"Data Not Available\")\n\n690070\n\n\n\nchina_data.get(\"book\", \"Data Not Available\")\n\n'Data Not Available'\n\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\n\nPython has an additional way to represent collections of items: sets.\nSets come up infrequently, but you should be aware of them.\nIf you are familiar with the mathematical concept of sets, then you will understand the majority of Python sets already.\nIf you don’t know the math behind sets, don’t worry: we’ll cover the basics of Python’s sets here.\nA set is an unordered collection of unique elements.\nThe syntax for creating a set uses curly bracket { and }.\n\n{item1, item2, ..., itemN}\n\nHere is an example.\n\ns = {1, \"hello\", 3.0}\nprint(\"s has type\", type(s))\ns\n\n\n\n\nSee exercise 9 in the exercise list.\nAs with lists and tuples, we can check if something is in the set and check the set’s length:\n\nprint(\"len(s) =\", len(s))\n\"hello\" in s\n\nUnlike lists and tuples, we can’t extract elements of a set s using s[N] where N is a number.\n\n# Uncomment the line below to see what happens\n# s[1]\n\nThis is because sets are not ordered, so the notion of getting the second element (s[1]) is not well defined.\nWe add elements to a set s using s.add.\n\ns.add(100)\ns\n\n\ns.add(\"hello\") # nothing happens, why?\ns\n\nWe can also do set operations.\nConsider the set s from above and the set s2 = {\"hello\", \"world\"}.\n\ns.union(s2): returns a set with all elements in either s or s2\n\ns.intersection(s2): returns a set with all elements in both s and s2\n\ns.difference(s2): returns a set with all elements in s that aren’t in s2\n\ns.symmetric_difference(s2): returns a set with all elements in only one of s and s2\n\n\n\n\nSee exercise 10 in the exercise list.\nAs with tuples and lists, a set function can convert other collections to sets.\n\nx = [1, 2, 3, 1]\nset(x)\n\n\nt = (1, 2, 3, 1)\nset(t)\n\nLikewise, we can convert sets to lists and tuples.\n\nlist(s)\n\n\ntuple(s)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections_answers.html#exercises",
    "href": "tutorials/session_1/qe_collections_answers.html#exercises",
    "title": "Collections",
    "section": "",
    "text": "In the first cell, try y.append(z).\nIn the second cell try y.extend(z).\nExplain the behavior.\nWhen you are trying to explain use y.append? and y.extend? to see a description of what these methods are supposed to do.\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\ny.append(z)\nprint(y)\n\n['a', 'b', 'c', [1, 2, 3]]\n\n\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\ny.extend(z)\nprint(y)\n\n['a', 'b', 'c', 1, 2, 3]\n\n\n(back to text)\n\n\n\nExperiment with the other two versions of the range function.\n\n# try list(range(a, N)) -- you pick `a` and `N`\n\n\n# try list(range(a, N, d)) -- you pick `a`, `N`, and `d`\n\n(back to text)\n\n\n\nVerify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\n# change first element of t\n\n\n# appending to t\n\n\n# sorting t\n\n\n# reversing t\n\n(back to text)\n\n\n\nChallenging For the tuple foo below, use a combination of zip, range, and len to mimic enumerate(foo).\nVerify that your proposed solution is correct by converting each to a list and checking equality with ==.\nYou can see what the answer should look like by starting with list(enumerate(foo)).\n\nfoo = (\"good\", \"luck!\")\n\n(back to text)\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n\n(back to text)\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\n\n(back to text)\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\n#china_data.pop()\n\n(back to text)\n\n\n\nExplain what happens to the value you popped.\nExperiment with calling pop twice.\n\n# your code here\n\n(back to text)\n\n\n\nTry creating a set with repeated elements (e.g. {1, 2, 1, 2, 1, 2}).\nWhat happens?\nWhy?\n\n# your code here\n\n(back to text)\n\n\n\nTest out two of the operations described above using the original set we created, s, and the set created below s2.\n\ns2 = {\"hello\", \"world\"}\n\n\n# Operation 1\n\n\n# Operation 2\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_7/Classification.html",
    "href": "tutorials/session_7/Classification.html",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\nUpdate: the test.csv file now contains the score that should be predicted.\nImport training set and validation sets\nWe follow a three sets approach and define the following sets: - data used for the developing the model (dataset from train.csv) will be split into: - training dataset (variable called train) - test data set (variable called tes - data use\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\nDescribe the dataset.\nHow is the credit category encoded? Create a new variable representing it with values 0,1,2.\nReencode all categorical variables as dummy variables. Remove variables that are not useful for the analysis.\nMake several plots about the dataset (histograms, correlation plots, …)\nTip: there are cool ideas here\nSplit the train dataset into a df_train and a df_test dataset.\nImplement a logistic regression.\nCompute the confusion matrix using the test set. Comment\nPerform the same analysis with other classification methods and compare their performance using the test set.\nWhich one would you choose? Test its performance on the validation set\n\n\n\nWith the same database, without using the credit score, implement a k-means clustering algorithm.\nAre the clusters related to the credit score?"
  },
  {
    "objectID": "tutorials/session_7/Classification.html#predicting-the-credit-score",
    "href": "tutorials/session_7/Classification.html#predicting-the-credit-score",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\nUpdate: the test.csv file now contains the score that should be predicted.\nImport training set and validation sets\nWe follow a three sets approach and define the following sets: - data used for the developing the model (dataset from train.csv) will be split into: - training dataset (variable called train) - test data set (variable called tes - data use\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\nDescribe the dataset.\nHow is the credit category encoded? Create a new variable representing it with values 0,1,2.\nReencode all categorical variables as dummy variables. Remove variables that are not useful for the analysis.\nMake several plots about the dataset (histograms, correlation plots, …)\nTip: there are cool ideas here\nSplit the train dataset into a df_train and a df_test dataset.\nImplement a logistic regression.\nCompute the confusion matrix using the test set. Comment\nPerform the same analysis with other classification methods and compare their performance using the test set.\nWhich one would you choose? Test its performance on the validation set"
  },
  {
    "objectID": "tutorials/session_7/Classification.html#segmenting-the-bank-clients",
    "href": "tutorials/session_7/Classification.html#segmenting-the-bank-clients",
    "title": "Classification and clustering",
    "section": "",
    "text": "With the same database, without using the credit score, implement a k-means clustering algorithm.\nAre the clusters related to the credit score?"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html",
    "href": "tutorials/session_3/Phillips_curve.html",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "The Philips Curve was initially discovered as a statistical relationship between unemployment and inflation. The original version used historical US data.\n\n\n\nPhilips Curve\n\n\nOur goal here is to visually inspect the Philips curve using recent data, for several countries.\nIn the process we will learn to: - import dataframes, inspect them, merge them, clean the resulting data - use matplotlib to create graphs - bonus: experiment with other plotting libraries\n\n\nWe start by loading library dbnomics which contains all the data we want. It is installed already on the nuvolos server.\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series([\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n])\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\nShow the first rows of each database. Make a list of all columns.\nCompute standard statistics for all variables\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \n\n# table_1.query(\"Country=='France'\")\n\n\n# option 2: by using groupby\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum available interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the resulting “cylindric” database.\nOur DataFrame is now ready for further analysis !\n\n\n\nOur goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\nExpand the above command to make the plot nicer (label, title, grid, …)\n\n\n\nThe following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\nStore the result in df_fr as a new column reg_unemployment\nAdd the regression line to the scatter plot.\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\nAny comment on these results?\n\n\n\nAltair is a visualization library (based on Vega-lite) which offers a different syntax to make plots.\nIt is well adapted to the exploration phase, as it can operate on a full database (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries (for instance colors)?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # add something here\n)\nchart\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\n# modify the following code\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\n#run first then modify the following code\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\nchart_2 | legend\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n\n\n\nAnother popular option is the plotly library for nice-looking interactive plots. Combined with dash or shiny, it can be used to build very powerful interactive interfaces.\n\nimport plotly.express as px\n\n\nfig = px.scatter(df, x='unemployment', y='inflation', color='country', title=\"Philips Curves\")\nfig\n\nImprove the graph above in any way you like"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html#importing-the-data",
    "href": "tutorials/session_3/Phillips_curve.html#importing-the-data",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "We start by loading library dbnomics which contains all the data we want. It is installed already on the nuvolos server.\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series([\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n])\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\nShow the first rows of each database. Make a list of all columns.\nCompute standard statistics for all variables\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \n\n# table_1.query(\"Country=='France'\")\n\n\n# option 2: by using groupby\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum available interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the resulting “cylindric” database.\nOur DataFrame is now ready for further analysis !"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html#plotting-using-matplotlib",
    "href": "tutorials/session_3/Phillips_curve.html#plotting-using-matplotlib",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Our goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\nExpand the above command to make the plot nicer (label, title, grid, …)"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html#visualizing-the-regression",
    "href": "tutorials/session_3/Phillips_curve.html#visualizing-the-regression",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "The following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\nStore the result in df_fr as a new column reg_unemployment\nAdd the regression line to the scatter plot.\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\nAny comment on these results?"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html#bonus-visualizing-data-using-altair",
    "href": "tutorials/session_3/Phillips_curve.html#bonus-visualizing-data-using-altair",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Altair is a visualization library (based on Vega-lite) which offers a different syntax to make plots.\nIt is well adapted to the exploration phase, as it can operate on a full database (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries (for instance colors)?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # add something here\n)\nchart\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\n# modify the following code\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\n#run first then modify the following code\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\nchart_2 | legend\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve.html#bonus-2-plotly-express",
    "href": "tutorials/session_3/Phillips_curve.html#bonus-2-plotly-express",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Another popular option is the plotly library for nice-looking interactive plots. Combined with dash or shiny, it can be used to build very powerful interactive interfaces.\n\nimport plotly.express as px\n\n\nfig = px.scatter(df, x='unemployment', y='inflation', color='country', title=\"Philips Curves\")\nfig\n\nImprove the graph above in any way you like"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html",
    "href": "tutorials/session_3/Philips_curve_pablo.html",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "The Philips Curve was initially discovered as a statistical relationship between unemployment and inflation. The original version used historical US data.\n\n\n\nPhilips Curve\n\n\nOur goal here is to visually inspect the Philips curve using recent data, for several countries.\nIn the process we will learn to: - import dataframes, inspect them, merge them, clean the resulting data - use matplotlib to create graphs - bonus: experiment with other plotting libraries\n\n\nWe start by loading library dbnomics which contains all the data we want. It is installed already on the nuvolos server.\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series(\n    [\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n    ]\n)\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\nThe data comes from dbnomics. Provider is OECD. Database is “Data Live dataset” for inflation, and “Main Economic Indicators Publication” for unemployement.\nData is for several countries (Germany, France, USA, Great Britain).\n\ninflation: Consumer Price Index for all goods and services (total), in annual growth rate, measured every quarter\nunemployment: Labour Force Survey - quarterly rates , workers aged 15 or over\n\nShow the first rows of each database. Make a list of all columns.\n\ntable_1.head(2)\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nLOCATION\nINDICATOR\nSUBJECT\nMEASURE\nFREQUENCY\nCountry\nIndicator\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q1\n1956-01-01\n1.746324\n1.746324\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q2\n1956-04-01\n1.838658\n1.838658\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n\n\n\n\n\n\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\ntable_2.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'SUBJECT', 'MEASURE',\n       'FREQUENCY', 'Country', 'Subject', 'Measure', 'Frequency'],\n      dtype='object')\n\n\nCompute standard statistics for all variables\n\ntable_1.describe()\n\n\n\n\n\n\n\n\nperiod\nvalue\n\n\n\n\ncount\n1084\n1083.000000\n\n\nmean\n1989-09-30 18:46:29.667896704\n3.891054\n\n\nmin\n1956-01-01 00:00:00\n-1.623360\n\n\n25%\n1972-10-01 00:00:00\n1.680341\n\n\n50%\n1989-10-01 00:00:00\n2.724924\n\n\n75%\n2006-10-01 00:00:00\n5.002851\n\n\nmax\n2023-07-01 00:00:00\n26.565810\n\n\nstd\nNaN\n3.591647\n\n\n\n\n\n\n\n\ntable_2.describe()\n\n\n\n\n\n\n\n\nperiod\noriginal_value\nvalue\n\n\n\n\ncount\n817\n817.000000\n817.000000\n\n\nmean\n1994-11-08 05:59:33.561811584\n6.098632\n6.098632\n\n\nmin\n1955-01-01 00:00:00\n0.373774\n0.373774\n\n\n25%\n1979-07-01 00:00:00\n4.366667\n4.366667\n\n\n50%\n1996-07-01 00:00:00\n5.833333\n5.833333\n\n\n75%\n2011-01-01 00:00:00\n7.996948\n7.996948\n\n\nmax\n2023-10-01 00:00:00\n13.066667\n13.066667\n\n\nstd\nNaN\n2.550835\n2.550835\n\n\n\n\n\n\n\nAverage inflation over the period for all the countries is 3.89. Average unemployement over the period for all the countries is 6.09.\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \n\n# we want to extract a subdataframe for each country\n\n\nind = table_1['Country'] == 'France' \ntable_1_fr = table_1[ ind ]\n\n\n# what are the unique values taken by the column country ?\n\n# set(table_1['Country']) # pure python\ntable_1['Country'].unique()\n\narray(['France', 'United Kingdom', 'United States', 'Germany'],\n      dtype=object)\n\n\n\ntable_1_fra = table_1.query(\"Country=='France'\")\ntable_1_gbr = table_1.query(\"Country=='United Kingdom'\")\ntable_1_deu = table_1.query(\"Country=='Germany'\")\ntable_1_usa = table_1.query(\"Country=='United States'\")\n\n\nd = dict()\nfor country in [\"France\", \"United Kingdom\", \"Germany\", \"United States\"]:\n    d[country] = table_1.query(f\"Country=='{country}'\")\n\n\n# list comprehension\n[table_1.query(f\"Country=='{country}'\") for country in table_1['Country'].unique()];\n\n\n# dictionary comprehension\nd = {country: table_1.query(f\"Country=='{country}'\") for country in table_1['Country'].unique()}\n\n\nfor k,v in d.items():\n    print(f\"{k}, mean: {v['value'].mean()}\")\n\nFrance, mean: 4.218004559985239\nUnited Kingdom, mean: 5.003996036162361\nUnited States, mean: 3.678550917822878\nGermany, mean: 2.659118566666667\n\n\n\n# option 2: by using groupby\n\ntable_1.groupby(\"Country\")['value'].agg('mean')\n\nCountry\nFrance            4.218005\nGermany           2.659119\nUnited Kingdom    5.003996\nUnited States     3.678551\nName: value, dtype: float64\n\n\n\n#standard devition\ntable_1.groupby(\"Country\")['value'].agg('std')\n\nCountry\nFrance            3.853190\nGermany           1.866131\nUnited Kingdom    4.768593\nUnited States     2.779505\nName: value, dtype: float64\n\n\n\ntable_1.groupby(\"Country\")['value'].agg(['mean','std'])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nCountry\n\n\n\n\n\n\nFrance\n4.218005\n3.853190\n\n\nGermany\n2.659119\n1.866131\n\n\nUnited Kingdom\n5.003996\n4.768593\n\n\nUnited States\n3.678551\n2.779505\n\n\n\n\n\n\n\n\ntable_1.groupby(\"Country\")['value'].agg('describe')\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\nFrance\n271.0\n4.218005\n3.853190\n-0.423247\n1.660842\n2.670692\n5.765484\n18.565260\n\n\nGermany\n270.0\n2.659119\n1.866131\n-0.922850\n1.421377\n2.107098\n3.441268\n8.580543\n\n\nUnited Kingdom\n271.0\n5.003996\n4.768593\n-0.453172\n2.000000\n3.244983\n6.133533\n26.565810\n\n\nUnited States\n271.0\n3.678551\n2.779505\n-1.623360\n1.784109\n3.023983\n4.523969\n14.505600\n\n\n\n\n\n\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\n# we have two dataframes with similar columns\n\n\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\ntable_2.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'SUBJECT', 'MEASURE',\n       'FREQUENCY', 'Country', 'Subject', 'Measure', 'Frequency'],\n      dtype='object')\n\n\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\n\ntable.columns\n\nIndex(['@frequency_x', 'provider_code_x', 'dataset_code_x', 'dataset_name_x',\n       'series_code_x', 'series_name_x', 'original_period_x', 'period',\n       'original_value_x', 'value_x', 'LOCATION_x', 'INDICATOR', 'SUBJECT_x',\n       'MEASURE_x', 'FREQUENCY_x', 'Country', 'Indicator', 'Subject_x',\n       'Measure_x', 'Frequency_x', '@frequency_y', 'provider_code_y',\n       'dataset_code_y', 'dataset_name_y', 'series_code_y', 'series_name_y',\n       'original_period_y', 'original_value_y', 'value_y', 'LOCATION_y',\n       'SUBJECT_y', 'MEASURE_y', 'FREQUENCY_y', 'Subject_y', 'Measure_y',\n       'Frequency_y'],\n      dtype='object')\n\n\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\n\ntable.groupby('country')[ ['unemployment', 'inflation'] ].agg('mean')\n\n\n\n\n\n\n\n\nunemployment\ninflation\n\n\ncountry\n\n\n\n\n\n\nFrance\n8.680560\n1.664349\n\n\nGermany\n4.989272\n2.730136\n\n\nUnited Kingdom\n6.705114\n5.404707\n\n\nUnited States\n5.880812\n3.678551\n\n\n\n\n\n\n\n\n# the resulting dataframe sitll has horrible column names\ntable.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\ndate\noriginal_value_x\ninflation\n...\noriginal_period_y\noriginal_value_y\nunemployment\nLOCATION_y\nSUBJECT_y\nMEASURE_y\nFREQUENCY_y\nSubject_y\nMeasure_y\nFrequency_y\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q1\n2003-01-01\n2.366263\n2.366263\n...\n2003-Q1\n7.922234\n7.922234\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q2\n2003-04-01\n1.912854\n1.912854\n...\n2003-Q2\n8.089598\n8.089598\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n2\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q3\n2003-07-01\n1.93227\n1.932270\n...\n2003-Q3\n8.036090\n8.036090\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n3\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q4\n2003-10-01\n2.184437\n2.184437\n...\n2003-Q4\n8.349410\n8.349410\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n4\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2004-Q1\n2004-01-01\n1.800087\n1.800087\n...\n2004-Q1\n8.518631\n8.518631\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n\n\n5 rows × 36 columns\n\n\n\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\n# there are some nas in the dataframe\nsum(df['inflation'].isna())\n\n1\n\n\n\ntable[\n    ['inflation', 'unemployment'] # list of columns to select\n];\ntable[['inflation', 'unemployment']];\n\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum available interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the resulting “cylindric” database.\nOur DataFrame is now ready for further analysis !\n\n\n\nOur goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\n\ndf_fr = df.query(\"country=='France'\")\n\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\n\n\n\n\n\n\n\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\n\n\n\n\n\n\n\n\nExpand the above command to make the plot nicer (label, title, grid, …)\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment\")\nplt.ylabel(\"Inflation\")\nplt.grid()\n\n\n\n\n\n\n\n\n\n\n\nThe following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ninflation\nR-squared:\n0.435\n\n\nModel:\nOLS\nAdj. R-squared:\n0.428\n\n\nMethod:\nLeast Squares\nF-statistic:\n62.46\n\n\nDate:\nWed, 07 Feb 2024\nProb (F-statistic):\n1.17e-11\n\n\nTime:\n10:39:59\nLog-Likelihood:\n-120.37\n\n\nNo. Observations:\n83\nAIC:\n244.7\n\n\nDf Residuals:\n81\nBIC:\n249.6\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.7054\n1.024\n9.479\n0.000\n7.668\n11.743\n\n\nunemployment\n-0.9263\n0.117\n-7.903\n0.000\n-1.160\n-0.693\n\n\n\n\n\n\nOmnibus:\n9.014\nDurbin-Watson:\n0.241\n\n\nProb(Omnibus):\n0.011\nJarque-Bera (JB):\n10.906\n\n\nSkew:\n0.520\nProb(JB):\n0.00428\n\n\nKurtosis:\n4.439\nCond. No.\n79.0\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\n0     2.366810\n1     2.211777\n2     2.261342\n3     1.971104\n4     1.814349\n        ...   \n78    3.064908\n79    3.055976\n80    3.141252\n81    2.926298\n82    2.888369\nLength: 83, dtype: float64\n\n\nStore the result in df_fr as a new column reg_unemployment\n\n# df_fr.loc['reg_inflation'] = result.predict(df_fr['unemployment'])\n\n\n# no error message for full index specification\ndf_fr.loc[:,'reg_inflation'] = result.predict(df_fr['unemployment'])\n\n/tmp/ipykernel_63599/2161117277.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_fr.loc[:,'reg_inflation'] = result.predict(df_fr['unemployment'])\n\n\n\ndf_fr.head()\n\n\n\n\n\n\n\n\ndate\ncountry\ninflation\nunemployment\nreg_inflation\n\n\n\n\n0\n2003-01-01\nFrance\n2.366263\n7.922234\n2.366810\n\n\n1\n2003-04-01\nFrance\n1.912854\n8.089598\n2.211777\n\n\n2\n2003-07-01\nFrance\n1.932270\n8.036090\n2.261342\n\n\n3\n2003-10-01\nFrance\n2.184437\n8.349410\n1.971104\n\n\n4\n2004-01-01\nFrance\n1.800087\n8.518631\n1.814349\n\n\n\n\n\n\n\nAdd the regression line to the scatter plot.\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\nplt.plot(df_fr['unemployment'], df_fr['reg_inflation'])\n\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment\")\nplt.ylabel(\"Inflation\")\nplt.grid()\n\n\n\n\n\n\n\n\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\ncountry\ninflation\nunemployment\n\n\n\n\n0\n2003-01-01\nFrance\n2.366263\n7.922234\n\n\n1\n2003-04-01\nFrance\n1.912854\n8.089598\n\n\n2\n2003-07-01\nFrance\n1.932270\n8.036090\n\n\n3\n2003-10-01\nFrance\n2.184437\n8.349410\n\n\n4\n2004-01-01\nFrance\n1.800087\n8.518631\n\n\n\n\n\n\n\n\ncountries_list = df['country'].unique()\ncountries_list\n\narray(['France', 'United Kingdom', 'United States', 'Germany'],\n      dtype=object)\n\n\n\nfrom statsmodels.formula import api as sm\n\nfor country in countries_list:\n    print(f\"country=='{country}'\")\n    df_country = df.query(f\"country=='{country}'\")\n    model = sm.ols(formula='inflation ~ unemployment', data=df_country)\n    result = model.fit()\n    \n    df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n    \n    # create a scatter plot\n    plt.plot(df_country['unemployment'], df_country['inflation'], '.', label=country, alpha=0.5)\n    plt.plot(df_country['unemployment'], df_country['reg_inflation'])\n    \n    plt.title(\"Philips Curve\")\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.legend()\n    plt.grid()\n\ncountry=='France'\ncountry=='United Kingdom'\ncountry=='United States'\ncountry=='Germany'\n\n\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n\n\n\n\n\n\n\n\n\n\n# maybe nicer with subplots\n\nplt.subplot(2,2,1)\nplt.subplot(2,2,2)\nplt.subplot(2,2,3)\nplt.subplot(2,2,4)\n\n\n\n\n\n\n\n\n\n# i = 0\n\n# for country in countries_list:\n\n#     i = i + 1\n\n# enumeration syntax\n\nfor i,country in enumerate(countries_list):\n    \n    print(f\"country=='{country}'\")\n    df_country = df.query(f\"country=='{country}'\")\n    model = sm.ols(formula='inflation ~ unemployment', data=df_country)\n    result = model.fit()\n    \n    df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n    \n    # create a scatter plot\n\n    plt.subplot(2,2, i+1)\n    plt.plot(df_country['unemployment'], df_country['inflation'], '.', label=country, alpha=0.5)\n    plt.plot(df_country['unemployment'], df_country['reg_inflation'])\n    \n    plt.title(\"Philips Curve\")\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.legend()\n    plt.grid()\nplt.tight_layout()\n\ncountry=='France'\ncountry=='United Kingdom'\ncountry=='United States'\ncountry=='Germany'\n\n\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n\n\n\n\n\n\n\n\n\nAny comment on these results?\n\n\n\nAltair is a visualization library (based on Vega-lite) which offers a different syntax to make plots.\nIt is well adapted to the exploration phase, as it can operate on a full database (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\n\nchart = alt.Chart(df).mark_point()\nchart\n\n\n\n\n\n\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries (for instance colors)?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # add something here\n)\nchart\n\n\n\n\n\n\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\n# modify the following code\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n\n\n\n\n\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\n#run first then modify the following code\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\nchart_2 | legend\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'selection_multi' is deprecated.  Use 'selection_point'\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n\n\n\n\n\n\n\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n\n\n\n\n\n\n\n\n\n\n\nAnother popular option is the plotly library for nice-looking interactive plots. Combined with dash or shiny, it can be used to build very powerful interactive interfaces.\n\nimport plotly.express as px\n\n\nfig = px.scatter(df, x='unemployment', y='inflation', color='country', title=\"Philips Curves\")\nfig\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/plotly/express/_core.py:2065: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])\n\n\n                                                \n\n\nImprove the graph above in any way you like"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html#importing-the-data",
    "href": "tutorials/session_3/Philips_curve_pablo.html#importing-the-data",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "We start by loading library dbnomics which contains all the data we want. It is installed already on the nuvolos server.\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series(\n    [\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n    ]\n)\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\nThe data comes from dbnomics. Provider is OECD. Database is “Data Live dataset” for inflation, and “Main Economic Indicators Publication” for unemployement.\nData is for several countries (Germany, France, USA, Great Britain).\n\ninflation: Consumer Price Index for all goods and services (total), in annual growth rate, measured every quarter\nunemployment: Labour Force Survey - quarterly rates , workers aged 15 or over\n\nShow the first rows of each database. Make a list of all columns.\n\ntable_1.head(2)\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nLOCATION\nINDICATOR\nSUBJECT\nMEASURE\nFREQUENCY\nCountry\nIndicator\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q1\n1956-01-01\n1.746324\n1.746324\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q2\n1956-04-01\n1.838658\n1.838658\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n\n\n\n\n\n\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\ntable_2.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'SUBJECT', 'MEASURE',\n       'FREQUENCY', 'Country', 'Subject', 'Measure', 'Frequency'],\n      dtype='object')\n\n\nCompute standard statistics for all variables\n\ntable_1.describe()\n\n\n\n\n\n\n\n\nperiod\nvalue\n\n\n\n\ncount\n1084\n1083.000000\n\n\nmean\n1989-09-30 18:46:29.667896704\n3.891054\n\n\nmin\n1956-01-01 00:00:00\n-1.623360\n\n\n25%\n1972-10-01 00:00:00\n1.680341\n\n\n50%\n1989-10-01 00:00:00\n2.724924\n\n\n75%\n2006-10-01 00:00:00\n5.002851\n\n\nmax\n2023-07-01 00:00:00\n26.565810\n\n\nstd\nNaN\n3.591647\n\n\n\n\n\n\n\n\ntable_2.describe()\n\n\n\n\n\n\n\n\nperiod\noriginal_value\nvalue\n\n\n\n\ncount\n817\n817.000000\n817.000000\n\n\nmean\n1994-11-08 05:59:33.561811584\n6.098632\n6.098632\n\n\nmin\n1955-01-01 00:00:00\n0.373774\n0.373774\n\n\n25%\n1979-07-01 00:00:00\n4.366667\n4.366667\n\n\n50%\n1996-07-01 00:00:00\n5.833333\n5.833333\n\n\n75%\n2011-01-01 00:00:00\n7.996948\n7.996948\n\n\nmax\n2023-10-01 00:00:00\n13.066667\n13.066667\n\n\nstd\nNaN\n2.550835\n2.550835\n\n\n\n\n\n\n\nAverage inflation over the period for all the countries is 3.89. Average unemployement over the period for all the countries is 6.09.\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# option 1: by using pandas boolean selection \n\n# we want to extract a subdataframe for each country\n\n\nind = table_1['Country'] == 'France' \ntable_1_fr = table_1[ ind ]\n\n\n# what are the unique values taken by the column country ?\n\n# set(table_1['Country']) # pure python\ntable_1['Country'].unique()\n\narray(['France', 'United Kingdom', 'United States', 'Germany'],\n      dtype=object)\n\n\n\ntable_1_fra = table_1.query(\"Country=='France'\")\ntable_1_gbr = table_1.query(\"Country=='United Kingdom'\")\ntable_1_deu = table_1.query(\"Country=='Germany'\")\ntable_1_usa = table_1.query(\"Country=='United States'\")\n\n\nd = dict()\nfor country in [\"France\", \"United Kingdom\", \"Germany\", \"United States\"]:\n    d[country] = table_1.query(f\"Country=='{country}'\")\n\n\n# list comprehension\n[table_1.query(f\"Country=='{country}'\") for country in table_1['Country'].unique()];\n\n\n# dictionary comprehension\nd = {country: table_1.query(f\"Country=='{country}'\") for country in table_1['Country'].unique()}\n\n\nfor k,v in d.items():\n    print(f\"{k}, mean: {v['value'].mean()}\")\n\nFrance, mean: 4.218004559985239\nUnited Kingdom, mean: 5.003996036162361\nUnited States, mean: 3.678550917822878\nGermany, mean: 2.659118566666667\n\n\n\n# option 2: by using groupby\n\ntable_1.groupby(\"Country\")['value'].agg('mean')\n\nCountry\nFrance            4.218005\nGermany           2.659119\nUnited Kingdom    5.003996\nUnited States     3.678551\nName: value, dtype: float64\n\n\n\n#standard devition\ntable_1.groupby(\"Country\")['value'].agg('std')\n\nCountry\nFrance            3.853190\nGermany           1.866131\nUnited Kingdom    4.768593\nUnited States     2.779505\nName: value, dtype: float64\n\n\n\ntable_1.groupby(\"Country\")['value'].agg(['mean','std'])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nCountry\n\n\n\n\n\n\nFrance\n4.218005\n3.853190\n\n\nGermany\n2.659119\n1.866131\n\n\nUnited Kingdom\n5.003996\n4.768593\n\n\nUnited States\n3.678551\n2.779505\n\n\n\n\n\n\n\n\ntable_1.groupby(\"Country\")['value'].agg('describe')\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\nFrance\n271.0\n4.218005\n3.853190\n-0.423247\n1.660842\n2.670692\n5.765484\n18.565260\n\n\nGermany\n270.0\n2.659119\n1.866131\n-0.922850\n1.421377\n2.107098\n3.441268\n8.580543\n\n\nUnited Kingdom\n271.0\n5.003996\n4.768593\n-0.453172\n2.000000\n3.244983\n6.133533\n26.565810\n\n\nUnited States\n271.0\n3.678551\n2.779505\n-1.623360\n1.784109\n3.023983\n4.523969\n14.505600\n\n\n\n\n\n\n\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\n# we have two dataframes with similar columns\n\n\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\ntable_2.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'SUBJECT', 'MEASURE',\n       'FREQUENCY', 'Country', 'Subject', 'Measure', 'Frequency'],\n      dtype='object')\n\n\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\n\ntable.columns\n\nIndex(['@frequency_x', 'provider_code_x', 'dataset_code_x', 'dataset_name_x',\n       'series_code_x', 'series_name_x', 'original_period_x', 'period',\n       'original_value_x', 'value_x', 'LOCATION_x', 'INDICATOR', 'SUBJECT_x',\n       'MEASURE_x', 'FREQUENCY_x', 'Country', 'Indicator', 'Subject_x',\n       'Measure_x', 'Frequency_x', '@frequency_y', 'provider_code_y',\n       'dataset_code_y', 'dataset_name_y', 'series_code_y', 'series_name_y',\n       'original_period_y', 'original_value_y', 'value_y', 'LOCATION_y',\n       'SUBJECT_y', 'MEASURE_y', 'FREQUENCY_y', 'Subject_y', 'Measure_y',\n       'Frequency_y'],\n      dtype='object')\n\n\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\n\ntable.groupby('country')[ ['unemployment', 'inflation'] ].agg('mean')\n\n\n\n\n\n\n\n\nunemployment\ninflation\n\n\ncountry\n\n\n\n\n\n\nFrance\n8.680560\n1.664349\n\n\nGermany\n4.989272\n2.730136\n\n\nUnited Kingdom\n6.705114\n5.404707\n\n\nUnited States\n5.880812\n3.678551\n\n\n\n\n\n\n\n\n# the resulting dataframe sitll has horrible column names\ntable.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\ndate\noriginal_value_x\ninflation\n...\noriginal_period_y\noriginal_value_y\nunemployment\nLOCATION_y\nSUBJECT_y\nMEASURE_y\nFREQUENCY_y\nSubject_y\nMeasure_y\nFrequency_y\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q1\n2003-01-01\n2.366263\n2.366263\n...\n2003-Q1\n7.922234\n7.922234\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q2\n2003-04-01\n1.912854\n1.912854\n...\n2003-Q2\n8.089598\n8.089598\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n2\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q3\n2003-07-01\n1.93227\n1.932270\n...\n2003-Q3\n8.036090\n8.036090\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n3\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2003-Q4\n2003-10-01\n2.184437\n2.184437\n...\n2003-Q4\n8.349410\n8.349410\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n4\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n2004-Q1\n2004-01-01\n1.800087\n1.800087\n...\n2004-Q1\n8.518631\n8.518631\nFRA\nLRUNTTTT\nSTSA\nQ\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n\n\n5 rows × 36 columns\n\n\n\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\n# there are some nas in the dataframe\nsum(df['inflation'].isna())\n\n1\n\n\n\ntable[\n    ['inflation', 'unemployment'] # list of columns to select\n];\ntable[['inflation', 'unemployment']];\n\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\nWhat is the maximum available interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the resulting “cylindric” database.\nOur DataFrame is now ready for further analysis !"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html#plotting-using-matplotlib",
    "href": "tutorials/session_3/Philips_curve_pablo.html#plotting-using-matplotlib",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Our goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\n\ndf_fr = df.query(\"country=='France'\")\n\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation']) # missing 'o'\n\n\n\n\n\n\n\n\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\n\n\n\n\n\n\n\n\nExpand the above command to make the plot nicer (label, title, grid, …)\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment\")\nplt.ylabel(\"Inflation\")\nplt.grid()"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html#visualizing-the-regression",
    "href": "tutorials/session_3/Philips_curve_pablo.html#visualizing-the-regression",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "The following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ninflation\nR-squared:\n0.435\n\n\nModel:\nOLS\nAdj. R-squared:\n0.428\n\n\nMethod:\nLeast Squares\nF-statistic:\n62.46\n\n\nDate:\nWed, 07 Feb 2024\nProb (F-statistic):\n1.17e-11\n\n\nTime:\n10:39:59\nLog-Likelihood:\n-120.37\n\n\nNo. Observations:\n83\nAIC:\n244.7\n\n\nDf Residuals:\n81\nBIC:\n249.6\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.7054\n1.024\n9.479\n0.000\n7.668\n11.743\n\n\nunemployment\n-0.9263\n0.117\n-7.903\n0.000\n-1.160\n-0.693\n\n\n\n\n\n\nOmnibus:\n9.014\nDurbin-Watson:\n0.241\n\n\nProb(Omnibus):\n0.011\nJarque-Bera (JB):\n10.906\n\n\nSkew:\n0.520\nProb(JB):\n0.00428\n\n\nKurtosis:\n4.439\nCond. No.\n79.0\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\n0     2.366810\n1     2.211777\n2     2.261342\n3     1.971104\n4     1.814349\n        ...   \n78    3.064908\n79    3.055976\n80    3.141252\n81    2.926298\n82    2.888369\nLength: 83, dtype: float64\n\n\nStore the result in df_fr as a new column reg_unemployment\n\n# df_fr.loc['reg_inflation'] = result.predict(df_fr['unemployment'])\n\n\n# no error message for full index specification\ndf_fr.loc[:,'reg_inflation'] = result.predict(df_fr['unemployment'])\n\n/tmp/ipykernel_63599/2161117277.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_fr.loc[:,'reg_inflation'] = result.predict(df_fr['unemployment'])\n\n\n\ndf_fr.head()\n\n\n\n\n\n\n\n\ndate\ncountry\ninflation\nunemployment\nreg_inflation\n\n\n\n\n0\n2003-01-01\nFrance\n2.366263\n7.922234\n2.366810\n\n\n1\n2003-04-01\nFrance\n1.912854\n8.089598\n2.211777\n\n\n2\n2003-07-01\nFrance\n1.932270\n8.036090\n2.261342\n\n\n3\n2003-10-01\nFrance\n2.184437\n8.349410\n1.971104\n\n\n4\n2004-01-01\nFrance\n1.800087\n8.518631\n1.814349\n\n\n\n\n\n\n\nAdd the regression line to the scatter plot.\n\n# create a scatter plot\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\nplt.plot(df_fr['unemployment'], df_fr['reg_inflation'])\n\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment\")\nplt.ylabel(\"Inflation\")\nplt.grid()\n\n\n\n\n\n\n\n\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\ncountry\ninflation\nunemployment\n\n\n\n\n0\n2003-01-01\nFrance\n2.366263\n7.922234\n\n\n1\n2003-04-01\nFrance\n1.912854\n8.089598\n\n\n2\n2003-07-01\nFrance\n1.932270\n8.036090\n\n\n3\n2003-10-01\nFrance\n2.184437\n8.349410\n\n\n4\n2004-01-01\nFrance\n1.800087\n8.518631\n\n\n\n\n\n\n\n\ncountries_list = df['country'].unique()\ncountries_list\n\narray(['France', 'United Kingdom', 'United States', 'Germany'],\n      dtype=object)\n\n\n\nfrom statsmodels.formula import api as sm\n\nfor country in countries_list:\n    print(f\"country=='{country}'\")\n    df_country = df.query(f\"country=='{country}'\")\n    model = sm.ols(formula='inflation ~ unemployment', data=df_country)\n    result = model.fit()\n    \n    df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n    \n    # create a scatter plot\n    plt.plot(df_country['unemployment'], df_country['inflation'], '.', label=country, alpha=0.5)\n    plt.plot(df_country['unemployment'], df_country['reg_inflation'])\n    \n    plt.title(\"Philips Curve\")\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.legend()\n    plt.grid()\n\ncountry=='France'\ncountry=='United Kingdom'\ncountry=='United States'\ncountry=='Germany'\n\n\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/2982871465.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n\n\n\n\n\n\n\n\n\n\n# maybe nicer with subplots\n\nplt.subplot(2,2,1)\nplt.subplot(2,2,2)\nplt.subplot(2,2,3)\nplt.subplot(2,2,4)\n\n\n\n\n\n\n\n\n\n# i = 0\n\n# for country in countries_list:\n\n#     i = i + 1\n\n# enumeration syntax\n\nfor i,country in enumerate(countries_list):\n    \n    print(f\"country=='{country}'\")\n    df_country = df.query(f\"country=='{country}'\")\n    model = sm.ols(formula='inflation ~ unemployment', data=df_country)\n    result = model.fit()\n    \n    df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n    \n    # create a scatter plot\n\n    plt.subplot(2,2, i+1)\n    plt.plot(df_country['unemployment'], df_country['inflation'], '.', label=country, alpha=0.5)\n    plt.plot(df_country['unemployment'], df_country['reg_inflation'])\n    \n    plt.title(\"Philips Curve\")\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.legend()\n    plt.grid()\nplt.tight_layout()\n\ncountry=='France'\ncountry=='United Kingdom'\ncountry=='United States'\ncountry=='Germany'\n\n\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n/tmp/ipykernel_63599/566381394.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_country.loc[:,'reg_inflation'] = result.predict(df_country['unemployment'])\n\n\n\n\n\n\n\n\n\nAny comment on these results?"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html#bonus-visualizing-data-using-altair",
    "href": "tutorials/session_3/Philips_curve_pablo.html#bonus-visualizing-data-using-altair",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Altair is a visualization library (based on Vega-lite) which offers a different syntax to make plots.\nIt is well adapted to the exploration phase, as it can operate on a full database (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\n\nchart = alt.Chart(df).mark_point()\nchart\n\n\n\n\n\n\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries (for instance colors)?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # add something here\n)\nchart\n\n\n\n\n\n\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\n# modify the following code\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n\n\n\n\n\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\n#run first then modify the following code\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\nchart_2 | legend\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'selection_multi' is deprecated.  Use 'selection_point'\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n\n\n\n\n\n\n\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)"
  },
  {
    "objectID": "tutorials/session_3/Philips_curve_pablo.html#bonus-2-plotly-express",
    "href": "tutorials/session_3/Philips_curve_pablo.html#bonus-2-plotly-express",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "Another popular option is the plotly library for nice-looking interactive plots. Combined with dash or shiny, it can be used to build very powerful interactive interfaces.\n\nimport plotly.express as px\n\n\nfig = px.scatter(df, x='unemployment', y='inflation', color='country', title=\"Philips Curves\")\nfig\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/plotly/express/_core.py:2065: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])\n\n\n                                                \n\n\nImprove the graph above in any way you like"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis_correction.html",
    "href": "tutorials/session_8/sentiment_analysis_correction.html",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The following code processes the Lending Club Dataset from https://www.kaggle.com/datasets/mariiagusarova/preprocessed-lending-club-dataset-v2.\nRun and comment the following instructions (fix them if needed). Inspect he dataframe?\n\n#\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\nimport numpy as np\n\n\n# \nloan = pd.read_csv('loans.csv', low_memory=True)\n\n\nloan.head(2)\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\n0\n3600.0\n1.0\n24.0\n10.0\n55000.0\n0.0\n5.91\n4.0\n29.7\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20000.0\n2.0\n14.0\n10.0\n63000.0\n0.0\n10.78\n10.0\n56.2\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 66 columns\n\n\n\n\nloan.describe()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\ncount\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n...\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n\n\nmean\n14685.697457\n1.268249\n21.268733\n6.145857\n72400.496752\n0.207527\n18.575478\n6.705665\n52.912851\n8.344402\n...\n0.002113\n0.015782\n0.085755\n0.007478\n0.027221\n0.002114\n0.022196\n0.013875\n0.003502\n0.002349\n\n\nstd\n8280.771514\n0.443048\n12.763837\n3.662421\n25589.248556\n0.405536\n8.199811\n5.820888\n23.374794\n4.340655\n...\n0.045922\n0.124630\n0.280002\n0.086153\n0.162728\n0.045934\n0.147320\n0.116971\n0.059071\n0.048409\n\n\nmin\n1000.000000\n1.000000\n1.000000\n0.000000\n35000.990000\n0.000000\n-1.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n8000.000000\n1.000000\n12.000000\n3.000000\n51500.000000\n0.000000\n12.510000\n2.000000\n35.700000\n5.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13000.000000\n1.000000\n21.000000\n7.000000\n67600.000000\n0.000000\n18.130000\n5.000000\n53.200000\n8.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20000.000000\n2.000000\n31.000000\n10.000000\n90000.000000\n0.000000\n24.360000\n10.000000\n70.700000\n11.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n40000.000000\n2.000000\n65.000000\n10.000000\n144999.000000\n1.000000\n45.000000\n25.000000\n148.000000\n35.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 66 columns\n\n\n\nThe dataset contains many information about borrowers from an unspecified instutitution. We can observe that categorical data (like add_state) has already been encoded using dummy variables for each value. As a result the number of regressors is fairly large (66). On the other hands, the number of observations is also very large.\nRun and comment the following instructions (fix them if needed). What kind of modeling exercise is performed?\n\nloan['loan_status'].value_counts()\n\nloan_status\n0.0    672377\n1.0    176077\nName: count, dtype: int64\n\n\nThe objective here is to build a model to predict which loans will default (loan_status=1). It is a classification exercise. Since the number of regressors is fairly large it is natural to look for a machine learning approcah.\n\n#\nfeatures = loan.columns.to_list()\nfeatures.remove('loan_status')\n\n\n#\ndf_train, df_test = train_test_split(loan, test_size=0.25, random_state=42)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n#\nclf = LogisticRegression()\nclf.fit(df_train[features], df_train['loan_status'])\n\n\n#\ny_pred = clf.predict(df_test[features])\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(df_test[features], df_test['loan_status'])))\n\nAccuracy of logistic regression classifier on test set: 0.79\n\n\n\n#\ncal = sklearn.metrics.confusion_matrix(df_test['loan_status'], y_pred, labels=clf.classes_)\nprint(cal)\n\n[[165146   2918]\n [ 41439   2611]]\n\n\n\n#\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cal, display_labels=clf.classes_)\ndisp.plot()\n\n\n\n\n\n\n\n\nThe database contains information about many clients.\nFor the confusion matrix that was just computed compute accuracy, precision, recall and f1 score (lookup the definitions if needed).\n\n# copmute the different statistics (by hand or programmatically)\n\nComment on the model validity.\n\n\n\nWe use the News Sentiment Dataset from Kaggle.\n\nImport Dataset as a pandas dataframe. Remove rows where selected_text is not available.\n\n\n# the following command checks the current working directory \n# it should end with session_8\n%pwd\n\n'/home/pablo/Teaching/escp/dbe/tutorials/session_8'\n\n\n\nimport pandas\ndf = pandas.read_csv(\"Tweets.csv\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n\n\n\n\n\n\n# we can count the number of non available values for `text`\nsum(df['selected_text'].isna())\n\n1\n\n\n\ndf = df[df['selected_text'].notna()]\n\n\n# check it worked\nsum(df['selected_text'].isna())\n\n0\n\n\n\nDescribe Dataset (text and graphs). What is the distribution of the various sentiment values?\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\ncount\n27480\n27480\n27480\n27480\n\n\nunique\n27480\n27480\n22463\n3\n\n\ntop\ncb774db0d1\nI`d have responded, if I were going\ngood\nneutral\n\n\nfreq\n1\n1\n199\n11117\n\n\n\n\n\n\n\nThere are 2780 tweets. Only the tweets, not any context information.\n\n# information about the distrbution of sentiments\nsentiments = df['sentiment']\n\n\ncounts = sentiments.value_counts()\ncounts\n\nsentiment\nneutral     11117\npositive     8582\nnegative     7781\nName: count, dtype: int64\n\n\n\n# we can convert in percentages...\ncounts / sum(counts) * 100 \n\nsentiment\nneutral     40.454876\npositive    31.229985\nnegative    28.315138\nName: count, dtype: float64\n\n\n\n# ... or use the normalize option\nsentiments.value_counts(normalize=True)\n\nsentiment\nneutral     0.404549\npositive    0.312300\nnegative    0.283151\nName: proportion, dtype: float64\n\n\nThe distribution of tweet sentiment is rather well balanced: majority of tweets are neutreal with about 30% positive and 30% negative tweets.\n\nimport seaborn as sns\nsns.histplot(sentiments)\n\n\n\n\n\n\n\n\n\nCount the number of tweets mentionng trump.\n\n\n# this is a series where a line is True only if df['text'] contains \"trump\"\nind = df['text'].str.contains(\"trump\", na=False) | df['text'].str.contains(\"Trump\", na=False) \nind\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n27476    False\n27477    False\n27478    False\n27479    False\n27480    False\nName: text, Length: 27480, dtype: bool\n\n\n\n# which tweets contain word \"trump\"\ndf[ind]\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n1589\n6c5505a37c\nEnjoy! Family trumps everything\nEnjoy! Family trumps everything\npositive\n\n\n6235\n234a562dd9\nLOL I love my MacBook too. Oh and my iMac. Ca...\nlove\npositive\n\n\n14005\n464eafe267\nHow to get a $40 trumpet book - get caught in ...\ncaught\nnegative\n\n\n14615\nfc16472bdf\nSick kid trumps advance planning. Bummer\nSick\nnegative\n\n\n14671\n3314e0f981\nGrrr, I can`t even practice Trumpet or vocals ...\nneck hurt to\nnegative\n\n\n22230\n2762b6624b\n_fan Been busy trumping your cheese omlette wi...\n_fan Been busy trumping your cheese omlette wi...\nneutral\n\n\n\n\n\n\n\n\n# how many tweets contain word \"trump\"\nsum(ind)\n\n6\n\n\n\nSplit Dataset into training, and test set.\n\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n\nimport sklearn\n# here the data set is very big, we can choose a relatively small test set\ndf_train, df_test  = sklearn.model_selection.train_test_split(df, test_size=0.1)\n\n\ndf\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n...\n...\n...\n...\n...\n\n\n27476\n4eac33d1c0\nwish we could come see u on Denver husband l...\nd lost\nnegative\n\n\n27477\n4f4c4fc327\nI`ve wondered about rake to. The client has ...\n, don`t force\nnegative\n\n\n27478\nf67aae2310\nYay good for both of you. Enjoy the break - y...\nYay good for both of you.\npositive\n\n\n27479\ned167662a5\nBut it was worth it ****.\nBut it was worth it ****.\npositive\n\n\n27480\n6f7127d9d7\nAll this flirting going on - The ATG smiles...\nAll this flirting going on - The ATG smiles. Y...\nneutral\n\n\n\n\n27480 rows × 4 columns\n\n\n\n\nsum((df['selected_text'].isna()))\n\n0\n\n\n\ndf_train.shape\n\n(24732, 4)\n\n\n\n\n\nThe goal is now to to build a tweet classifier to predict a tweet sentiment, without any human input.\n\nExtract features from the training dataset. What do you do with non-words / punctuation?\n\n(hint: check the CountVectorizer function and the tutorial on sklearn webpage.)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n\n\nX = count_vect.fit_transform(df_train['selected_text'])\n\n\n# X is a matrix of \"features\", each one corresponding to the counts of specific tokens\nX.shape\n\n(24732, 16556)\n\n\n\n# the matrix is sparse: only non zero entries are stored\n# check for the first entry\nX[0,:]\n\n&lt;1x16556 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 17 stored elements in Compressed Sparse Row format&gt;\n\n\n\nfor e in X[0,:]: print(e)\n\n  (0, 8148) 1\n  (0, 3282) 1\n  (0, 7626) 1\n  (0, 11111)    1\n  (0, 1075) 1\n  (0, 12974)    1\n  (0, 2795) 1\n  (0, 8854) 1\n  (0, 15797)    1\n  (0, 15953)    1\n  (0, 7856) 1\n  (0, 13398)    1\n  (0, 3770) 1\n  (0, 8347) 1\n  (0, 1650) 1\n  (0, 9880) 1\n  (0, 4783) 1\n\n\n\n# we can actually print the processed tokens\nprint(count_vect.vocabulary_)\ninvert_voc = {v:k for k,v in count_vect.vocabulary_.items()}\n\n{'just': 8148, 'chillin': 3282, 'in': 7626, 'pjs': 11111, 'after': 1075, 'short': 12974, 'but': 2795, 'long': 8854, 'week': 15797, 'why': 15953, 'is': 7856, 'someone': 13398, 'continually': 3770, 'knocking': 8347, 'at': 1650, 'my': 9880, 'door': 4783, 'ouch': 10572, 'back': 1831, 'sick': 13043, 'http': 7406, 'twitpic': 15084, 'com': 3576, '4w6lf': 340, 'bbq': 2018, 'time': 14628, 'again': 1088, 'baby': 1819, 'alex': 1181, 'miss': 9552, 'you': 16416, 'ily': 7579, 'good': 6503, 'night': 10117, 'ah': 1114, 'not': 10232, 'doing': 4747, 'cbeebies': 3069, 'epg': 5277, 'by': 2824, 'any': 1414, 'chance': 3147, 'are': 1525, 'hehe': 7090, 'need': 9999, 'pinkyponk': 11077, 'crash': 3935, 'warnings': 15690, 'your': 16424, 'journey': 8094, 'home': 7271, 'hope': 7317, 'fl': 5840, 'omg': 10429, 'disco': 4625, 'packed': 10674, 'allergies': 1203, 'must': 9860, 'be': 2029, 'sleeping': 13210, 'walking': 15651, 'work': 16129, 'wasn': 15704, 'nice': 10097, 'cof': 3526, 'hate': 6968, 'electronics': 5135, 'making': 9118, 'an': 1322, 'imovie': 7601, 'of': 10356, 'college': 3563, 'im': 7581, 'school': 12625, 'right': 12239, 'now': 10260, 'fav': 5641, 'everyone': 5389, 'here': 7135, 'we': 15759, 're': 11822, 'off': 10360, 'to': 14684, 'party': 10788, 'have': 6982, 'myself': 9891, 'barely': 1943, 'hrs': 7395, 'sleep': 13205, 'and': 1326, 'the': 14461, 'lord': 8900, 'blessed': 2322, 'me': 9301, 'with': 16041, 'what': 15869, 'feels': 5685, 'like': 8688, 'it': 7873, 'came': 2899, 'sore': 13448, 'throat': 14559, 'pumpin': 11607, 'out': 10580, 'this': 14517, 'paper': 10744, 'gift': 6375, 'love': 8932, 'that': 14453, 'quiet': 11706, 'office': 10371, 'haha': 6812, 'totally': 14794, 'more': 9702, 'efficient': 5103, 'pic': 11015, 'donnie': 4770, 'one': 10446, 'siouxsinner': 13124, 'took': 14751, 'last': 8503, 'launch': 8528, 'word': 16125, 'has': 6960, 'he': 7015, 'got': 6557, 'solo': 13383, 'album': 1172, 'comin': 3598, 'too': 14748, 'yeaaa': 16321, 'so': 13340, 'classy': 3414, 'economy': 5051, 'depressing': 4434, 'fave': 5643, 'tired': 14655, 'over': 10608, 'worked': 16132, 'lol': 8832, 'wish': 16030, 'had': 6802, 'book': 2435, 'dirty': 4605, 'sad': 12464, 'sorry': 13458, 'offline': 10377, 'for': 5956, 'll': 8777, 'tweet': 15039, 'later': 8510, 'jeff': 7989, 'will': 15982, 'hard': 6932, 'from': 6118, '42nd': 292, 'pats': 10827, 'philly': 10977, 'am': 1263, 'mmm': 9592, 'cheesesteak': 3232, 'boyfriend': 2531, 'vacation': 15414, 'happy': 6925, 'mother': 9738, 'day': 4263, 'know': 8350, 'maybe': 9274, 'then': 14480, 'forgotten': 5980, 'about': 904, 'christmas': 3338, 'july': 8128, 'homey': 7286, 'missed': 9554, 'da': 4153, 'bus': 2781, 'writing': 16194, 'some': 13392, 'mild': 9475, 'wild': 15979, 'articles': 1583, 'want': 15667, 'go': 6467, 'peru': 10944, 'summer': 13998, 'ahhhhhhh': 1132, 'hopefully': 7322, 'yesyesyes': 16375, 'there': 14488, 'hey': 7151, 'guys': 6776, 'should': 12990, 'invite': 7812, 'moody': 9685, 'only': 10452, 'friend': 6096, 'these': 14494, 'does': 4735, 'hurt': 7462, 'much': 9811, 'would': 16167, 'ride': 12230, 'superman': 14047, 'cute': 4113, 'horrible': 7337, 'prefer': 11390, '80': 483, 'singstar': 13113, 'all': 1199, 'words': 16127, 'watched': 15716, 'final': 5767, 'break': 2571, 'prison': 11459, 'episode': 5281, 'was': 15698, 'great': 6641, 'farewell': 5606, 'dearly': 4302, 'follow': 5920, 'friday': 6091, 'following': 5928, 'people': 10900, 'followers': 5924, 'woot': 16121, 'followfriday': 5925, 'feeling': 5683, 'really': 11856, 'cool': 3809, 'oh': 10383, 'forgot': 5979, 'ferry': 5703, 'turns': 15015, 'around': 1558, 'before': 2098, 'going': 6488, 'sitting': 13141, 'sun': 14009, 'other': 10565, 'side': 13055, 'deck': 4327, 'closed': 3460, 'how': 7381, 'dare': 4219, 'apologize': 1458, 'uk': 15189, 'being': 2112, 'gone': 6500, 'while': 15905, 'don': 4762, 'america': 1295, 'never': 10062, 'touring': 14806, 'awesome': 1763, 'lmao': 8782, 'ha': 6787, 'bummed': 2748, 'aw': 1743, 'nick': 10104, 'his': 7208, 'heart': 7050, 'broken': 2641, 'poor': 11272, 'pleasant': 11159, 'surprise': 14083, 'still': 13770, 'working': 16135, 'on': 10444, 'music': 9850, 'grades': 6592, 'outside': 10593, 'behind': 2110, 'big': 2211, 'gray': 6633, 'cloud': 3473, 'thanks': 14440, 'thank': 14434, 'fkn': 5839, 'ugly': 15181, 'matches': 9244, 'mood': 9682, 'unfortunatly': 15256, 'punchy': 11611, 'due': 4939, 'two': 15125, 'early': 5005, 'morning': 9712, 'pager': 10685, 'events': 5371, 'ashleighhh': 1604, 'get': 6348, 'newww': 10080, 'cd': 3074, 'funky': 6170, 'reel': 11949, 'awful': 1776, 'pretty': 11426, 'icky': 7514, 'getting': 6353, 'ready': 11835, 'think': 14509, 'catching': 3039, 'cold': 3543, 'yay': 16305, 'gotta': 6564, 'pull': 11600, 'hour': 7372, 'shift': 12912, 'holla': 7257, 'ya': 16273, 'sometime': 13404, 'tomorrow': 14729, 'peace': 10863, 'new': 10064, 'born': 2487, 'single': 13110, 'our': 10577, 'little': 8758, 'puppy': 11620, 'basset': 1978, 'hound': 7370, 'actress': 983, 'cryin': 4040, 'could': 3870, 'bed': 2080, 'nine': 10137, 'once': 10445, 'make': 9112, 'sleepdeprived': 13206, 'luck': 8980, 'half': 6849, 'term': 14391, 'isn': 7865, 'enough': 5244, 'bless': 2321, 'her': 7131, 'dropped': 4896, 'lei': 8607, 'cemetary': 3100, 'least': 8579, 'wont': 16098, 'hav': 6980, 'put': 11640, 'up': 15314, 'wiv': 16055, 'him': 7197, 'no': 10161, 'problemo': 11474, 'poooooooor': 11267, 'sheeeep': 12883, '20': 128, 'john': 8058, 'drop': 4894, 'something': 13402, 'man': 9137, 'can': 2911, 'hang': 6898, 'instead': 7741, 'macaroni': 9052, 'cheese': 3229, 'please': 11161, 'cubbies': 4060, 'even': 5365, '500': 366, 'else': 5159, 'matter': 9260, 'beat': 2046, 'dem': 4398, 'bums': 2753, 'chicago': 3259, 'baseball': 1962, 'ore': 10533, 'play': 11143, 'audition': 1697, 'anna': 1362, 'dont': 4773, 'into': 7789, 'feel': 5680, 'bad': 1850, 'kids': 8280, 'disappointment': 4617, 'roof': 12327, 'decal': 4313, 'saving': 12576, 'gaahhhh': 6218, 'rock': 12293, 'honored': 7300, 'le': 8559, 'mont': 9669, 'they': 14496, 'rich': 12217, 'passed': 10799, 'pairung': 10702, 'as': 1594, 'well': 15833, '10': 22, 'minutes': 9529, 'class': 3408, 'its': 7888, 'kinda': 8296, 'cause': 3053, 'fun': 6156, 'adore': 1033, 'went': 15841, 'vending': 15453, 'maching': 9060, 'bugles': 2721, 'perhaps': 10924, 'sign': 13068, 'snack': 13302, 'anyway': 1429, 'probably': 11471, 'same': 12511, 'thing': 14504, 'wrong': 16199, 'paul': 10831, 'scheuring': 12621, 'series': 12784, 'finale': 5768, 'sucked': 13945, 'bank': 1922, 'teller': 14358, 'definitely': 4361, 'hitting': 7219, 'interested': 7768, 'dear': 4301, 'dance': 4197, 'tonight': 14738, 'look': 8868, 'quick': 11703, 'post': 11321, 'dashboard': 4238, 'pleasure': 11171, 'apparently': 1461, 'tv': 15024, 'fixed': 5831, 'plus': 11196, 'refunded': 11964, 'cost': 3858, 'buy': 2811, 'downside': 4822, 'tomato': 14718, 'soup': 13479, 'tastes': 14278, 'red': 11925, 'peppers': 10907, 'stressed': 13848, 'pugsly': 11595, 'missing': 9559, 'invisible': 7810, 'car': 2955, 'helps': 7123, 'boost': 2457, 'recycling': 11923, 'honest': 7290, 'gonna': 6501, 'ats': 1665, 'charmingly': 3192, 'where': 15896, 'did': 4532, 'leave': 8582, 'citeh': 3382, 'jersey': 8007, 'connection': 3731, 'gw': 6777, 'dutch': 4969, 'might': 9463, 'do': 4719, 'art': 1578, 'children': 3271, 'promised': 11523, 'birthday': 2249, 'brekkie': 2590, 'problem': 11472, 'bugger': 2716, 'food': 5940, 'house': 7374, 'stuff': 13893, 'precariously': 11382, 'close': 3459, 'ricotta': 12225, 'didn': 4535, 'help': 7118, 'fx': 6205, 'windows': 16002, '1000x': 26, 'better': 2170, 'than': 14433, 'vista': 15551, 'far': 5604, 'win': 15995, 'reborn': 11872, 'perfect': 10913, 'waking': 15642, 'star': 13685, 'wars': 15696, 'end': 5208, 'title': 14667, 'song': 13411, 'sorta': 13461, 'also': 1246, 'creepy': 3977, 'fine': 5778, 'shrug': 13018, 'send': 12747, 'loves': 8951, 'revenge': 12183, 'possibility': 11318, 'come': 3583, 'friends': 6100, 'takes': 14222, 'error': 5312, 'page': 10683, 'yall': 16291, 'ni99as': 10091, 'loopjazz': 8893, 'yess': 16364, 'ez': 5523, 'most': 9735, 'bejï': 2114, '½n': 16540, 'taylor': 14297, 'swift': 14148, 'joe': 8053, 'jonas': 8073, 'looked': 8870, 'together': 14704, 'broke': 2640, 'able': 902, 'recovering': 11919, 'italian': 7875, 'cruise': 4022, 'mediterenean': 9332, 'finished': 5787, 'weeding': 15791, 'flower': 5884, 'weeds': 15792, 'grow': 6699, 'legitimate': 8604, 'plants': 11136, 'rubbish': 12394, 'eh': 5113, 'sure': 14074, 'daw': 4258, 'guesting': 6741, 'ni': 10090, 'cha': 3123, 'monday': 9652, 'pendulum': 10891, 'goodbyes': 6508, 'suck': 13942, 'shut': 13027, 'plz': 11199, 'toy': 14822, 'story': 13810, 'jb': 7976, '3d': 268, 'movie': 9766, '2moro': 206, 'aaaand': 865, 'funny': 6180, 'actually': 987, 'were': 15843, 'decisive': 4326, 'nah': 9915, 'next': 10081, 'honey': 7293, 'ma': 9038, 'fault': 5639, 'confusin': 3711, 'nobody': 10167, 'write': 16190, 'wrk': 16196, 'gtta': 6729, 'tonite': 14742, 'mobile': 9605, 'advert': 1050, 'trafalger': 14845, 'square': 13631, 'looks': 8874, 'lot': 8911, 'except': 5426, 'if': 7539, 'load': 8794, 'pigeons': 11041, 'best': 2156, 'bring': 2616, 'show': 13002, 'denial': 4408, 'very': 15475, 'powerful': 11355, 'thinking': 14512, 'having': 6989, 'lunch': 8999, 'soon': 13422, 'quite': 11713, 'such': 13941, 'shame': 12848, 've': 15441, 'coz': 3906, 'different': 4550, 'aren': 1527, 'left': 8595, 'knee': 8338, 'somehow': 13397, 'hurts': 7468, 'walk': 15648, 'supposed': 14068, 'sometimes': 13405, 'busy': 2793, 'recognize': 11895, 'always': 1259, 'family': 5580, 'god': 6473, 'vegas': 15445, 'without': 16049, '_war_mt': 834, 'longer': 8855, 'blue': 2369, 'fantastic': 5600, 'likes': 8691, 'face': 5535, 'battery': 1996, 'die': 4539, 'daddy': 4165, 'see': 12716, 'driving': 4890, 'insane': 7714, 'talk': 14232, 'hips': 7205, 'lovely': 8948, 'sunniest': 14026, 'ages': 1100, 'exams': 5419, 'yes': 16360, 'sir': 13126, 'ok': 10398, 'hungry': 7449, 'fat': 5628, 'aha': 1115, 'jake': 7938, 'thomas': 14530, 'precious': 11384, 'when': 15892, 'say': 12582, 'white': 15921, 'light': 8679, 'every': 5382, 'ghost': 6366, 'whisperer': 15915, 'failed': 5549, 'miserably': 9544, 'cuties': 4118, 'tried': 14921, 'ftp': 6139, 's3': 12451, 'use': 15365, 'fireftp': 5803, 'firefox': 5802, 'browser': 2659, 'addin': 1000, 'mention': 9387, 'support': 14062, 'their': 14471, 'site': 13135, 'though': 14540, 'meetings': 9346, 'afternoon': 1078, 'started': 13700, 'fill': 5756, 'golden': 6492, 'times': 14634, 'hmmm': 7227, 'skip': 13172, 'grocery': 6682, 'store': 13804, 'head': 7017, 'walmart': 15658, 'potentially': 11337, 'direct': 4595, 'message': 9403, 'hell': 7107, 'complaining': 3648, '27': 180, 'answer': 1389, 'ask': 1611, 'ew': 5409, 'tongue': 14735, 'kiss': 8314, 'waiting': 15633, 'front': 6119, 'dps': 4830, 'julian': 8125, 'take': 14217, 'test': 14407, 'loneliness': 8848, 'bah': 1865, 'fb': 5655, 'fan': 5584, '7500': 469, 'diiinner': 4568, 'sooo': 13426, 'thunderrrr': 14580, 'cuddling': 4068, 'blanky': 2300, 'during': 4965, 'storm': 13807, 'whats': 15876, 'voice': 15569, 'shot': 12987, 'luv': 9016, 'true': 14954, 'forward': 6005, 'ugh': 15172, 'goes': 6485, 'life': 8670, 'cardboard': 2965, 'box': 2524, 'ch': 3122, 'appreciat': 1487, 'anything': 1427, 'gd': 6298, 'excited': 5434, 'made': 9068, '100x': 28, 'chocolates': 3307, 'stuck': 13882, 'yum': 16458, 'lucky': 8986, 'swimming': 14151, 'tanning': 14253, 'heaven': 7065, 'playing': 11151, 'infamous': 7676, 'hear': 7045, 'unfortunately': 15254, 'yeahhh': 16327, 'wasnt': 15705, 'thereeeeeeeeeee': 14490, 'woooooo': 16114, 'seeing': 12718, 'yo': 16395, 'reply': 12084, 'booooo': 2451, 'us': 15361, 'lmfaoooo': 8788, 'boring': 2485, 'account': 943, 'local': 8804, 'comic': 3596, 'ran': 11775, '4k': 328, 'gosh': 6552, 'happend': 6914, 'yesterday': 16373, 'license': 8659, 'hated': 6969, 'sounds': 13477, 'scared': 12597, 'cannot': 2933, 'wait': 15630, 'glad': 6428, 'copy': 3830, 'shipped': 12930, 'shows': 13012, 'today': 14693, 'tune': 15000, 'plans': 11129, 'cancelled': 2924, 'hmv': 7233, 'raises': 11767, 'hand': 6881, 'caffeine': 2850, 'although': 1256, 'caveat': 3060, 'normally': 10218, 'read': 11831, 'non': 10187, 'fiction': 5730, 'recent': 11880, 'years': 16336, 'novels': 10257, 'woke': 16076, 'wanna': 15664, 'stay': 13726, 'walked': 15649, 'room': 12329, 'bug': 2714, 'flew': 5860, 'eye': 5518, 'change': 3152, 'name': 9931, 'under': 15224, 'settings': 12805, 'pain': 10689, 'sky': 13181, 'until': 15309, 'awww': 1787, 'wanted': 15669, 'greg': 6661, 'watching': 15719, 'milk': 9482, 'decent': 4318, 'australian': 1715, 'find': 5774, 'trust': 14960, 'american': 1296, 'nerrrvous': 10040, 'yummy': 16464, 'thx': 14586, 'accidents': 937, 'boo': 2428, 'soggy': 13371, 'wished': 16031, 'cockermouth': 3514, '45': 296, 'jesus': 8014, 'christ': 3334, 'sunny': 14028, 'muuch': 9871, 'shall': 12844, 'web': 15772, 'won': 16087, 'let': 8633, 'reads': 11834, 'tweets': 15054, 'confident': 3700, 'she': 12878, 'pretenders': 11422, 'listening': 8746, 'favourite': 5650, 'year': 16333, 'weather': 15767, 'widget': 15964, 'imac': 7583, 'iphone': 7828, 'predict': 11386, 'rainy': 11764, 'season': 12697, 'comes': 3589, 'tuna': 14998, 'noodles': 10194, 'smashed': 13269, 'potato': 11334, 'cakes': 2861, 'winksy': 16014, 'says': 12587, 'dissappointed': 4671, 'past': 10807, 'few': 5716, 'days': 4267, 'st': 13650, 'killing': 8289, 'jean': 7984, 'possible': 11319, 'case': 3015, 'h1n1': 6785, 'ft': 6138, 'knox': 8357, 'ky': 8406, 'tiny': 14644, 'cc': 3070, 'gnyq7': 6466, 'note': 10234, 'info': 7683, 'purposes': 11630, 'panic': 10733, 'jï': 8156, 'bday': 2028, 'beautiful': 2059, 'aunt': 1704, 's2': 12450, 'mess': 9402, 'sensible': 12759, 'airport': 1155, 'wdw': 15758, 'trip': 14925, 'worse': 16158, 'fingered': 5784, 'coke': 3539, 'machine': 9057, 'drinking': 4880, 'diet': 4544, 'pepsi': 10908, 'dr': 4831, 'pepper': 10905, 'used': 15366, 'correction': 3849, 'legal': 8597, 'wooowww': 16116, 'full': 6152, 'idea': 7518, 'found': 6015, 'tinyurl': 14647, 'qlrcec': 11665, 'frenchies': 6076, 'set': 12802, 'precedent': 11383, 'loading': 8797, 'bought': 2507, 'plastic': 11138, 'personal': 10938, 'wine': 16007, 'bottles': 2504, 'jewel': 8018, 'rooftop': 12328, 'boozin': 2467, 'been': 2093, 'beating': 2052, 'irregularly': 7852, 'ever': 5373, 'since': 13103, 'explode': 5484, 'or': 10521, 'implode': 7606, 'yeah': 16325, 'done': 4766, 'several': 12815, 'dumerils': 4948, 'boa': 2385, 'prob': 11467, 'wonder': 16088, 'hognose': 7244, 'needs': 10007, 'bigger': 2212, 'blah': 2287, 'yea': 16319, 'max': 9265, 'trivun': 14936, 'poked': 11227, 'undertanding': 15238, 'none': 10188, 'watch': 15715, 'point': 11219, 'hd': 7014, 'stace': 13653, 'wants': 15672, 'hack': 6797, 'directions': 4598, 'multiple': 9827, 'social': 13351, 'net': 10050, 'sites': 13137, 'add': 992, 'chose': 3328, 'mu': 9804, 'hilarious': 7191, 'mine': 9503, 'gross': 6687, '_vegetable': 829, 'mean': 9306, 'yu': 16451, 'doctor': 4726, 'tuesday': 14990, 'yup': 16467, 'everything': 5395, 'loved': 8935, 'mom': 9636, 'rul': 12407, 'amazing': 1281, 'marks': 9201, 'roommate': 12333, 'era': 5293, 'wontons': 16099, 'sent': 12761, 'abrams': 906, 'surrounded': 14089, 'meee': 9338, 'tooooooo': 14758, 'bored': 2476, 'shinning': 12925, 'ate': 1652, 'chocolate': 3306, 'honour': 7301, 'cheering': 3224, 'talks': 14238, '_dunk': 626, 'partyends': 10789, 'blog': 2342, '1654': 92, 'release': 12019, 'stubb': 13879, 'dude': 4937, 'hax0r': 6999, 'cut': 4111, 'pro': 11466, 'tell': 14357, 'stable': 13652, 'checked': 3212, 'hubster': 7415, 'pass': 10798, 'lazy': 8552, 'caravan': 2959, 'running': 12424, 'clever': 3436, 'seems': 12724, 'call': 2877, 'ur': 15347, 'shattered': 12871, 'tre': 14892, 'hood': 7303, 'claim': 3397, 'thats': 14455, 'atl': 1657, 'theme': 14476, 'which': 15904, 'aint': 1149, 'youtube': 16437, 'vid': 15500, 'posted': 11324, 'lay': 8545, 'swineflu': 14154, 'illegal': 7565, 'law': 8538, 'abiding': 899, 'citizens': 3385, 'didnt': 4536, 'cry': 4038, 'er': 5292, 'tummy': 14995, 'ache': 956, 'taste': 14276, 'enjoy': 5231, '_gaba': 648, 'evrytime': 5408, 'listen': 8742, 'tat': 14280, 'plce': 11157, 'reminded': 12044, 'mojojojo': 9629, 'dexter': 4502, 'lab': 8416, 'stopped': 13800, 'coffee': 3529, 'maryland': 9223, 'rest': 12137, 'area': 1526, 'five': 5828, 'entire': 5260, 'middle': 9455, 'schools': 12627, 'heck': 7071, 'audrey': 1700, 'bestie': 2160, 'loving': 8958, 'may': 9272, 'holiday': 7254, 'foot': 5948, 'gym': 6782, 'study': 13889, 'ooooo': 10478, 'using': 15378, 'version': 15472, 'twidget': 15069, 'mac': 9049, 'heading': 7027, 'downtown': 4825, 'drinks': 4881, 'dancing': 4202, 'goin': 6487, 'brothers': 2655, 'live': 8760, 'rocking': 12297, 'sneezing': 13317, 'frequently': 6081, 'manager': 9143, 'switched': 14159, 'schedule': 12617, 'closing': 3467, 'pre': 11381, 'twitter': 15089, 'formal': 5983, 'invitation': 7811, 'speak': 13525, 'edu2': 5075, 'yaoo': 16297, 'sonnnn': 13416, 'andy': 1335, 'fonzie': 5938, 'gomez': 6497, 'sry': 13643, 'figure': 5743, 'works': 16141, 'finally': 5769, 'setting': 12804, 'switch': 14158, 'care': 2970, 'fans': 5595, 'hero': 7142, 'rocks': 12298, 'socks': 13358, 'cartoons': 3010, 'haven': 6985, 'eating': 5036, 'breakfast': 2572, 'programs': 11510, 'changed': 3153, 'viewing': 15508, 'evening': 5366, 'jay': 7969, 'way': 15746, 'cardinal': 2968, 'fallen': 5568, 'nest': 10049, 'attacked': 1669, 'bird': 2241, 'dog': 4739, 'leaving': 8586, 'everyday': 5386, 'fact': 5542, 'city': 3386, 'earlier': 5002, 'mondays': 9653, 'sleepy': 13216, 'awake': 1746, 'tweetdeck': 15043, 'keeps': 8218, 'crashing': 3938, 'nj': 10151, 'avoiding': 1742, 'nascar': 9952, 'ima': 7582, 'habits': 6796, 'incredibly': 7654, 'sweeeet': 14133, 'brake': 2552, 'nothing': 10240, 'stress': 13846, 'dream': 4858, 'loooong': 8881, 'weekend': 15799, 'inlawing': 7697, 'movies': 9768, 'trek': 14903, 'flick': 5861, 'top': 14763, 'molar': 9630, 'extracted': 5511, 'swollen': 14161, 'numb': 10281, 'toothache': 14761, 'sinus': 13123, 'suppose': 14067, '4th': 333, 'waited': 15631, 'wind': 15998, 'blowing': 2362, 'through': 14563, 'tumbleweed': 14993, 'old': 10408, 'crackerack': 3919, 'couldnt': 3872, 'southend': 13486, '2nd': 211, 'tickets': 14597, 'ill': 7564, 'tour': 14804, 'bg': 2191, 'emo': 5186, 'jesse': 8009, 'mccartney': 9286, 'tends': 14374, 'internet': 7777, 'sweetie': 14142, 'aliens': 1194, 'boobie': 2430, 'craig': 3922, 'rise': 12256, '11': 43, 'bedtime': 2085, 'grown': 6702, 'ups': 15335, 'because': 2071, 'couldn': 3871, 'access': 930, 'properly': 11535, 'shiny': 12926, 'studying': 13891, 'computing': 3668, 'feck': 5669, 'changing': 3157, 'plant': 11130, 'science': 12631, 'confused': 3710, 'girl': 6399, 'sit': 13134, 'double': 4798, 'decker': 4328, 'neck': 9995, 'down': 4809, 'ppl': 11362, 'who': 15927, 'level': 8642, 'above': 905, 'imagine': 7588, '30': 223, 'okay': 10401, 'sane': 12535, 'population': 11288, 'world': 16145, 'strawberry': 13835, 'margaritas': 9181, 'baked': 1878, 'stuffed': 13894, 'shrimp': 13015, 'effects': 5101, 'worth': 16161, 'wear': 15763, 'jacket': 7924, 'cos': 3855, 'notice': 10242, 'mark': 9193, 'shirt': 12934, 'drunk': 4912, 'specifically': 13534, 'i36': 7494, 'tinypic': 14645, 'mwz6uo': 9878, 'jpg': 8103, 'outfit': 10586, 'q63obq': 11655, 'ahhh': 1127, 'soooo': 13427, 'kill': 8283, 'gilbert': 6386, 'sullivan': 13986, 'cheer': 3222, 'th': 14426, 'serenading': 12779, 'expensive': 5471, 'miserable': 9543, 'smile': 13279, 'fed': 5671, 'happily': 6921, 'stupid': 13904, 'mane': 9148, 'um': 15196, 'gave': 6291, 'nope': 10212, 'intended': 7759, 'start': 13699, 'habit': 6795, 'fast': 5624, 'ends': 5216, 'meet': 9343, 'sports': 13603, 'three': 14551, 'legged': 8601, 'race': 11732, 'mr': 9787, 'pettigrew': 10952, 'said': 12488, 'aloud': 1232, 'grandpa': 6612, 'telling': 14361, 'human': 7432, 'bodies': 2399, 'med': 9320, 'hotter': 7365, 'wearing': 15765, 'shorts': 12984, 'snl': 13327, 'justin': 8151, 'timberlake': 14627, 'dreamt': 4864, 'committed': 3616, 'suicide': 13977, 'drip': 4882, 'stand': 13675, 'freaked': 6047, 'testing': 14410, 'dawn': 4261, 'carlingford': 2985, 'lough': 8919, 'place': 11113, 'involves': 7819, 'tho': 14528, 'picture': 11027, 'sucks': 13951, 'lacking': 8426, 'trousers': 14949, 'thunder': 14579, 'thighs': 14502, 'daughtry': 4251, 'upload': 15326, 'pulled': 11602, 'muscle': 9843, 'twitterbugs': 15093, 'loooooooooong': 8884, 'lie': 8663, 'spent': 13558, 'wife': 15969, 'tangled': 14248, 'wheels': 15891, 'daughter': 4249, 'hit': 7213, 'dislocation': 4657, 'fracture': 6024, 'resulted': 12151, 'failure': 5554, 'phone': 10990, 'catch': 3037, 'aww': 1785, 'cant': 2935, 'till': 14625, 'sydney': 14167, 'soooooo': 13429, 'sleeeeepy': 13203, 'already': 1237, 'wow': 16170, 'hurtling': 7467, 'assembling': 1623, 'figuring': 5745, 'alarm': 1168, 'loud': 8916, 'blew': 2324, 'ear': 4998, 'drum': 4907, 'josette': 8083, 'across': 970, 'pond': 11251, 'nowhere': 10262, 'exciting': 5438, 'goddamnit': 6478, 'row': 12371, 'run': 12416, 'corrupted': 3853, 'singing': 13109, 'away': 1753, 'fml': 5900, 'dad': 4161, 'moneyz': 9656, 'gets': 6351, 'finish': 5786, 'welcome': 15829, 'easy': 5030, 'offer': 10365, 'lost': 8910, 'both': 2497, 'hates': 6974, 'sound': 13473, 'played': 11147, 'shaw': 12874, 'computer': 3666, 'bum': 2745, 'ultimate': 15192, 'exicted': 5457, 'radiator': 11743, 'boiled': 2407, '000': 1, 'depressed': 4431, 'boarding': 2389, 'uss': 15380, 'enterprise': 5251, 'warp': 15691, 'speed': 13541, 'ahead': 1124, '4x4s': 364, 'shizz': 12944, '2gether': 199, 'yr': 16445, 'realized': 11850, 'shout': 12998, 'player': 11148, 'hahahah': 6818, 'cooperating': 3822, 'mad': 9063, 'flash': 5850, 'awoke': 1783, '12': 50, 'crazy': 3952, 'nervous': 10044, 'entering': 5250, 'pogue': 11217, 'according': 942, 'yet': 16376, 'seem': 12721, 'tearrrrrrrr': 14315, 'serendipity': 12780, 'hours': 7373, 'lets': 8636, 'happier': 6919, 'sadly': 12476, 'fever': 5714, 'lemonade': 8615, 'hairspray': 6845, 'nutritious': 10293, 'delayed': 4377, 'malibu': 9127, 'doesn': 4736, 'text': 14416, 'silly': 13084, 'ring': 12243, 'hugs': 7426, 'sacramento': 12460, 'continue': 3771, 'server': 12792, '2008': 135, 'r2': 11726, 'unleashed': 15282, 'sigh': 13061, 'window': 16001, 'ladies': 8433, 'suffer': 13963, 'scenario': 12607, 'men': 9375, 'experience': 5473, 'felt': 5694, 'pee': 10875, 'cried': 3986, 'jonathon': 8076, 'ross': 12351, 'audience': 1691, 'anthem': 1397, 'high': 7175, 'dry': 4914, 'blip': 2330, 'fm': 5899, '79fcr': 473, 'actual': 986, 'heartbreak': 7052, 'purple': 11627, 'converse': 3790, 'game': 6244, 'sorrry': 13456, 'jackass': 7922, 'congrats': 3719, 'drink': 4876, 'hahaha': 6817, 'pleased': 11162, 'wit': 16038, 'tha': 14427, 'turnout': 15014, 'common': 3618, 'remember': 12040, 'nic': 10095, 'wouldn': 16169, 'online': 10451, 'clearly': 3432, 'kept': 8233, 'sunburned': 14015, 'volcano': 15572, 'nd': 9984, 'nyt': 10312, 'sux': 14111, 'burned': 2766, 'bit': 2255, 'news': 10074, 'truth': 14962, 'hiding': 7173, 'eyes': 5521, 'paramore': 10756, 'decode': 4332, 'butterfly': 2802, 'dead': 4289, 'late': 8508, 'congratulations': 3724, 'leah': 8568, 'mothers': 9740, 'duuuude': 4972, 'tim': 14626, 'vs': 15594, 'animated': 1354, 'either': 5123, 'adult': 1041, 'swim': 14149, 'g4': 6212, 'neither': 10029, 'suffering': 13965, 'ish': 7858, 'loveee': 8937, 'breakin': 2575, 'laughed': 8524, 'update': 15318, 'crying': 4041, 'spineless': 13573, 'makes': 9115, 'downloaded': 4815, 'parnoid': 10774, 'dnt': 4717, 'cuddle': 4067, 'visiting': 15546, 'friendster': 6102, 'facebook': 5537, 'poorly': 11274, 'ree': 11942, 'heally': 7039, 'hoping': 7329, 'rain': 11756, 'stops': 13802, 'sunburnt': 14017, 'arms': 1551, 'itching': 7879, 'hello': 7112, 'worry': 16156, 'soooooooooo': 13434, 'palm': 10715, 'niggas': 10112, 'talkin': 14236, 'bout': 2516, 'bm': 2381, '745': 467, 'poop': 11269, 'nite': 10148, 'weird': 15817, 'synced': 14178, 'research': 12122, 'guilt': 6747, 'yah': 16287, 'ordered': 10530, 'asus': 1643, 'eee': 5082, 'pc': 10854, 'gon': 6498, 'ignore': 7546, 'txt': 15134, 'unless': 15283, '6th': 458, 'street': 13840, 'wedding': 15786, 'eventful': 5369, 'third': 14514, 'freelesson': 6064, 'freistunde': 6072, 'surprised': 14084, 'choice': 3310, 'project': 11513, 'believe': 2119, 'kind': 8295, 'relationship': 12004, 'thought': 14542, '50': 365, 'sigjeans': 13066, 'fugees': 6150, 'keeping': 8217, 'company': 3633, 'troubles': 14946, 'ain': 1148, 'grandparents': 6613, 'ilocos': 7570, 'moved': 9763, 'nueva': 10274, 'ecija': 5045, 'mate': 9246, 'pills': 11055, 'cuz': 4126, 'nuts': 10294, 'whatever': 15874, 'pung': 11614, 'jk': 8038, 'retarded': 12158, '16th': 94, 'learned': 8574, 'lesson': 8630, 'nyc': 10309, 'huh': 7428, 'nearly': 9992, 'causes': 3054, 'mic': 9441, 'problems': 11475, 'euruko': 5358, 'hold': 7247, 'bottom': 2505, 'brother': 2654, 'knows': 8355, 'haters': 6973, 'taking': 14224, 'sang': 12537, 'xbox': 16245, 'lips': 8730, 'beer': 2094, 'karla': 8183, 'leno': 8619, 'mourning': 9758, 'hon': 7289, 'discriminating': 4637, 'wierd': 15966, 'jus': 8146, 'warm': 15684, 'cozy': 3907, 'inside': 7721, 'nanay': 9938, 'reading': 11833, 'card': 2964, 'hugged': 7420, 'knew': 8340, 'babies': 1818, 'booo': 2448, 'tgif': 14424, 'finger': 5783, 'traffic': 14847, 'jam': 7940, 'bet': 2163, 'fall': 5567, 'asleep': 1617, 'sorries': 13452, 'afford': 1067, 'raining': 11761, 'cats': 3046, 'dogs': 4744, 'mysore': 9892, 'thankfully': 14436, 'pigs': 11046, 'swines': 14155, 'comeback': 3584, 'bob': 2393, 'proctor': 11486, 'advice': 1053, 'freaks': 6050, 'funnnn': 6179, 'talinda': 14231, 'tyler': 15139, 'slept': 13219, 'nephew': 10035, 'yrs': 16446, 'naps': 9946, 'age': 1093, 'concur': 3685, 'check': 3211, 'ones': 10448, 'tod': 14690, 'showed': 13004, 'fluid': 5890, 'decreasing': 4336, 'slightly': 13226, 'doesnt': 4737, 'anytime': 1428, 'ive': 7903, 'uploading': 15328, 'photos': 11000, 'cell': 3095, 'nothin': 10238, 'candid': 2929, 'ness': 10048, 'bitbetter': 2256, 'share': 12857, 'bartender': 1957, 'ly': 9027, 'ba3nf': 1811, 'disappointed': 4615, 'hopes': 7326, 'lovin': 8957, 'itsucks': 7891, 'sensors': 12760, 'vocï': 15563, 'que': 11687, 'sumiu': 13991, 'forever': 5972, 'msn': 9794, 'fiftythousand': 5739, 'hugh': 7423, 'laurie': 8534, 'needle': 10004, 'haystack': 7004, 'whole': 15933, 'somethign': 13400, '200': 129, 'updates': 15320, 'alotment': 1231, 'hangers': 6899, 'closet': 3465, '_ricardo': 784, 'road': 12270, 'indies': 7663, 'rule': 12408, 'curious': 4091, 'benjamin': 2143, 'button': 2806, 'another': 1388, 'block': 2335, 'cross': 4010, 'fro': 6114, 'bloody': 2354, 'boiling': 2409, 'hardcoded': 6933, '650mb': 428, 'brain': 2549, '90s': 515, 'despite': 4469, 'amount': 1315, 'forgive': 5977, 'politicians': 11241, 'pics': 11025, 'lots': 8913, 'stories': 13806, 'goodies': 6510, 'memories': 9371, 'concrete': 3684, 'landfill': 8469, 'recycler': 11922, 'fuckkk': 6145, 'sleepppppppp': 13213, 'mummy': 9831, 'symphonic': 14174, 'pawing': 10839, 'thru': 14571, 'elle': 5152, 'saw': 12580, 'innit': 7702, 'safely': 12482, 'euro': 5354, 'winner': 16015, 'approve': 1495, 'heartedly': 7056, 'priceless': 11433, 'falls': 5574, '1pm': 122, 'replying': 12086, 'sweet': 14137, 'booth': 2463, 'newport': 10073, 'heb': 7070, 'thanxs': 14447, 'mooooooooornin': 9690, 'legs': 8606, 'whenever': 15893, 'rains': 11762, 'motivated': 9745, 'thinks': 14513, '66sbz': 433, 'adorable': 1032, 'ooh': 10468, 'driver': 4886, 'gloomy': 6446, 'sulking': 13985, 'tumblr': 14994, 'gru3some': 6714, 'save': 12573, 'xd51qx88b': 16248, 'beyond': 2182, 'cheap': 3205, 'yoooouuu': 16411, 'confsuing': 3708, 'yearling': 16335, 'pet': 10948, 'died': 4540, 'exhausted': 5451, 'nvr': 10302, 'bidor': 2208, 'wantan': 15668, 'mee': 9337, 'drinkin': 4879, 'cham': 3140, 'ping': 11068, 'puts': 11642, 'stole': 13789, 'occasionally': 10337, 'edward': 5079, 'cullen': 4073, 'obama': 10320, 'mail': 9094, 'ordering': 10531, 'aswered': 1646, 'calls': 2882, 'beatwittyparty': 2056, 'relaxing': 12014, 'uminaa': 15201, 'pink': 11071, 'hardcore': 6934, 'jamie': 7943, 'everyoneeeee': 5391, 'xxxxxxloser': 16269, 'holy': 7269, 'se': 12685, 'beats': 2054, 'heat': 7060, 'guava': 6734, 'juice': 8122, 'handi': 6888, 'water': 15721, 'beach': 2033, 'tubes': 14987, '9th': 536, '10th': 41, 'june': 8137, 'venue': 15464, 'washed': 15700, 'sleeve': 13217, 'keep': 8214, 'tea': 14305, 'alas': 1169, '_o': 760, 'uh': 15182, 'visit': 15544, 'austin': 1712, 'gigantic': 6379, 'moving': 9771, 'decision': 4324, 'closer': 3463, 'louis': 8921, 'portland': 11298, 'register': 11978, 'presale': 11404, 'saturday': 12565, 'mayhem': 9277, 'wake': 15637, 'comm': 3601, 'thanxxx': 14449, 'gods': 6483, 'sake': 12494, 'ik': 7556, 'opted': 10514, 'strike': 13860, '12th': 58, 'altaf': 1248, 'bhai': 2197, 'unveil': 15313, 'happened': 6915, 'karachi': 8179, 'open': 10492, 'experiement': 5472, 'remembered': 12041, 'pentecost': 10898, 'uselessly': 15371, 'complicated': 3656, 'marty': 9217, 'mcflyy': 9293, 'agaaaaaaiiiin': 1086, 'maaaam': 9042, 'chickened': 3264, 'interesting': 7769, 'beauty': 2062, 'massacre': 9233, 'australia': 1714, 'florida': 5880, 'misses': 9555, 'perfectly': 10916, 'goo': 6502, 'twitterverse': 15114, 'entertained': 5253, 'boredd': 2477, 'forrest': 5993, 'baron': 1951, 'fpu': 6022, 'thn': 14522, 'drugs': 4905, 'turn': 15009, 'pumpkin': 11608, 'mornings': 9714, 'exist': 5458, 'weren': 15845, 'hot': 7360, 'faith': 5561, 'picked': 11018, 'taco': 14199, 'guitar': 6752, 'agree': 1107, 'usb': 15364, 'key': 8247, 'fancy': 5588, 'packaging': 10673, 'fire': 5797, 'flames': 5847, 'dragonforce': 4838, '102': 29, 'kiis': 8281, 'ryan': 12445, 'meowmie': 9394, 'money': 9654, 'pay': 10842, 'them': 14475, 'real': 11839, 'seventh': 12814, 'gorilla': 6550, 'pod': 11208, 'lego': 8605, 'knight': 8342, 'helmet': 7117, 'whoishonorsociety': 15932, 'pajama': 10705, 'pants': 10740, 'official': 10374, 'twilight': 15072, 'saga': 12486, 'ended': 5210, 'empty': 5200, 'lulu': 8994, 'slanted': 13192, 'farmer': 5609, 'market': 9196, 'trying': 14967, 'cat': 3034, 'figured': 5744, 'sink': 13116, 'cupboard': 4081, 'target': 14265, 'safety': 12485, 'locks': 8812, 'heavy': 7069, 'part': 10777, 'reason': 11862, 'aladdin': 1167, 'eclipse': 5046, 'mints': 9524, 'soo': 13420, 'sunshine': 14038, 'headaches': 7021, 'cleaned': 3423, 'list': 8740, 'blocked': 2337, 'girls': 6406, 'spammer': 13510, 'di': 4516, 'bella': 2127, 'roasters': 12276, 'asked': 1612, 'job': 8046, 'person': 10937, 'needed': 10002, 'horrifying': 7342, 'skateboard': 13160, 'candy': 2931, 'nvd': 10300, 'prize': 11465, 'concert': 3680, 'ar': 1507, '15': 78, 'qualifies': 11677, 'gun': 6758, '21': 147, 'parts': 10787, 'ake': 1164, 'lazing': 8551, 'congratss': 3721, 'btw': 2688, 'nathanfillion': 9961, 'flight': 5865, 'control': 3783, 'score': 12637, 'each': 4994, 'darn': 4228, 'angry': 1349, 'random': 11777, 'asian': 1609, 'paws': 10841, 'air': 1150, 'sushi': 14098, 'chinese': 3287, 'restaurants': 12141, 'apple': 1474, 'katie': 8192, 'cheery': 3227, 'free': 6055, 'fieldnotes': 5734, 'travel': 14885, 'north': 10220, '3rd': 279, 'own': 10645, 'purchases': 11625, 'yard': 16298, 'sales': 12500, 'large': 8494, 'crates': 3940, 'refrigerator': 11961, '35': 253, 'ment': 9382, 'disappointing': 4616, 'report': 12089, 'updating': 15321, 'supertarget': 14053, 'squeeky': 13634, 'hereeeeee': 7136, 'imma': 7594, 'georgia': 6341, 'la': 8412, 'grrrrrr': 6710, 'shower': 13005, 'brb': 2568, 'mins': 9522, 'awwwwwwww': 1794, 'dried': 4874, 'wats': 15733, 'inning': 7701, 'mum': 9829, 'bacon': 1847, 'smoke': 13288, 'stars': 13695, 'ja': 7917, 'binks': 2234, 'crappy': 3933, 'motivation': 9747, 'revise': 12187, 'odd': 10348, 'mitchel': 9576, 'musso': 9858, 'dosent': 4792, 'pumped': 11606, 'mind': 9498, 'reckon': 11890, 'stop': 13799, 'hair': 6836, 'greying': 6668, 'retire': 12161, 'gill': 6387, 'dark': 4222, '_c': 591, 'taken': 14219, 'twatter': 15028, 'lately': 8509, 'replies': 12083, 'superb': 14044, 'rehearsal': 11990, 'rob': 12278, 'delucca': 4395, 'thingy': 14508, 'month': 9676, 'learning': 8576, 'songs': 13413, 'minute': 9527, 'delicious': 4386, 'losing': 8908, 'neighbors': 10024, 'give': 6416, 'brazil': 2565, 'presentation': 11411, 'shouldve': 12996, 'given': 6420, 'wed': 15784, 'overtime': 10629, 'bgt': 2194, 'awkward': 1781, 'moment': 9637, 'loose': 8895, 'faster': 5625, 'move': 9761, 'feet': 5686, 'correct': 3847, 'misconstrued': 9542, 'smell': 13271, 'pancakes': 10724, 'toast': 14686, 'cooking': 3807, 'awwww': 1789, 'slow': 13242, 'certain': 3113, 'ahhhh': 1129, 'smart': 13264, 'avoid': 1739, 'happening': 6916, 'super': 14043, 'toured': 14805, 'philippines': 10973, 'nashville': 9953, 'employee': 5197, 'forbid': 5957, 'usually': 15385, 'along': 1229, 'commenting': 3609, 'nachos': 9910, 'himself': 7198, 'clear': 3429, 'birds': 2243, 'docks': 4724, 'shopping': 12969, 'recontinue': 11906, 'breathless': 2584, 'scent': 12611, 'huge': 7419, 'hits': 7217, 'bummer': 2749, 'officially': 10375, 'dust': 4966, 'grounded': 6692, 'nicer': 10101, 'drive': 4883, 'careful': 2974, 'blackberry': 2279, 'explain': 5481, 'intrigued': 7791, 'calling': 2881, 'shitshow': 12940, 'lying': 9029, 'looooooooves': 8885, '_fred6': 645, 'guess': 6736, 'option': 10519, 'mhm': 9435, 'om': 10421, 'nom': 10186, 'gutted': 6771, 'beside': 2153, 'pool': 11261, 'spending': 13557, 'lonely': 8849, 'dressed': 4869, 'granny': 6614, 'twitting': 15118, 'familiar': 5579, 'goodnight': 6514, 'hating': 6976, 'prom': 11519, 'p9': 10667, 'danica': 4211, 'team': 14311, 'spongebob': 13593, 'inspirational': 7729, 'gots': 6562, 'beathing': 2049, 'suit': 13978, 'victoia': 15497, 'secret': 12708, 'exchange': 5430, 'bottoms': 2506, 'weeks': 15803, 'revision': 12190, 'biology': 2238, 'chemistry': 3246, 'plenty': 11179, 'gold': 6491, 'willing': 15987, 'jumping': 8132, 'trampoline': 14863, 'thud': 14575, 'shouldn': 12994, 'eaten': 5033, 'cookie': 3805, 'myloc': 9887, '1xiz': 127, 'migawd': 9462, 'surf': 14076, 'anymore': 1420, 'modem': 9618, 'withdrawals': 16044, 'tweeple': 15035, 'recommend': 11899, 'www': 16217, 'audiomicro': 1693, 'tr': 14829, 'gwoy': 6780, 'gwpx': 6781, 'looking': 8872, 'betterrrrrrrr': 2172, 'rocked': 12294, 'strapped': 13829, 'ohio': 10388, 'california': 2876, 'parks': 10771, 'nmr7pc': 10160, 'hi': 7163, 'frenchie': 6075, 'bullet': 2737, 'train': 14856, 'tokyo': 14711, 'gf': 6356, 'japan': 7956, 'thursday': 14584, 'sightseeing': 13065, 'gaijin': 6232, 'bulky': 2735, 'idol': 7531, 'exspecially': 5500, 'mommy': 9644, 'christoph': 3339, 'fridge': 6093, 'those': 14537, 'south': 13485, 'mowing': 9774, 'coco': 3516, 'blame': 2293, 'goose': 6544, 'gotcha': 6559, 'feelin': 5682, 'tron': 14941, 'zone': 16507, 'michelle': 9445, 'luke': 8990, 'quote': 11721, 'goodluck': 6511, 'dreading': 4856, 'trouble': 14945, 'voting': 15586, 'thankgod': 14437, 'crackberry': 3916, 'pray': 11375, 'zac': 16477, 'brown': 2657, 'virginia': 15528, 'nova': 10255, '_gutter': 661, 'trent': 14909, 'wore': 16128, 'bouta': 2517, 'donate': 4763, 'blood': 2353, 'ahhhhh': 1130, 'margs': 9182, 'brightens': 2610, 'lobster': 8803, 'dinner': 4583, 'mba': 9284, 'folks': 5918, 'terasse': 14388, 'pouring': 11346, 'mtl': 9800, 'eat': 5032, 'sandwhich': 12530, 'meeting': 9345, 'brag': 2544, 'itchy': 7880, 'sorrow': 13453, 'linkin': 8721, 'park': 10767, 'spend': 13555, 'student': 13883, 'burbank': 2760, 'fly': 5894, 'dc': 4279, 'enthusiasm': 5258, 'meeeee': 9341, 'annoyed': 1382, 'phew': 10970, 'anyone': 1422, 'runs': 12426, 'issueï': 7871, 'juz': 8154, 'gadgetopia': 6223, 'dm': 4710, 'email': 5165, 'basquash': 1976, '07': 10, 'sengoku': 12750, 'basara': 1960, 'valkyria': 15424, 'chronicles': 3342, 'requiemforthephantom': 12107, 'edenoftheeast': 5062, 'simpsons': 13098, 'nodding': 10171, 'weir': 15816, 'lady': 8435, 'plurk': 11195, 'rq5ru': 12380, 'viral': 15525, 'danny': 4214, 'xoxo': 16257, 'loooooove': 8887, 'revising': 12188, 'exam': 5418, 'website': 15782, 'c7yojg': 2837, 'idk': 7529, 'loopy': 8894, 'indeed': 7655, 'pigment': 11045, 'imagination': 7587, 'ncis': 9982, 'complete': 3650, 'first': 5809, 'jealous': 7981, 'tom': 14716, 'germany': 6344, 'fas': 5617, 'booted': 2462, 'tvs': 15026, 'maaaaan': 9040, 'aswell': 1645, 'joke': 8068, 'surely': 14075, 'bothered': 2499, 'sony': 13419, 'awe': 1756, 'inventing': 7801, 'melatonin': 9356, 'enthu': 5256, '_anshul': 569, 'speaks': 13529, 'asylum': 1649, 'insomnia': 7723, 'kickin': 8269, 'butt': 2798, 'contribute': 3779, 'destruction': 4478, 'language': 8477, 'finishing': 5788, 'magazine': 9076, 'article': 1582, 'cap': 2943, 'almost': 1225, 'sholders': 12958, 'enjoying': 5235, 'duber': 4928, 'merlin': 9400, 'bt': 2685, 'course': 3886, 'es': 5316, 'starwars': 13707, 'rocio': 12292, 'ian': 7497, 'important': 7608, 'polish': 11238, 'nail': 9918, 'handle': 6889, 'murdered': 9841, 'filipino': 5754, 'prof': 11495, 'gorayeb': 6546, 'migranes': 9467, 'practice': 11368, 'pools': 11263, 'threw': 14554, 'normal': 10217, 'boston': 2493, 'daft': 4168, 'recieve': 11886, 'million': 9489, 'invites': 7814, 'derny': 4444, 'sofie': 13365, 'cindy': 3368, 'eric': 5299, 'coloured': 3573, 'leg': 8596, 'filled': 5757, 'colour': 3572, 'truly': 14955, 'integrity': 7755, 'comment': 3605, 'ipod': 7831, 'touch': 14797, 'literally': 8754, 'falling': 5570, 'apart': 1444, 'junk': 8141, 'reluctant': 12033, 'finals': 5770, 'unfollowed': 15249, 'purpose': 11628, 'fishy': 5819, 'bouncing': 2509, 'rush': 12430, 'nauseous': 9973, 'revel': 12182, 'ago': 1106, 'homeless': 7276, 'afterjune': 1077, '1st': 123, 'nighttt': 10126, 'xxo': 16265, 'gurl': 6764, 'face2face': 5536, 'palisades': 10714, 'try': 14964, 'pei': 10884, 'wei': 15811, 'spelt': 13553, 'deadly': 4293, 'pure': 11626, 'escapism': 5319, 'achieve': 958, 'strange': 13823, 'taxi': 14296, 'bits': 2264, 'titanic': 14666, 'couple': 3884, 'iya': 7907, 'starting': 13702, 'krn': 8385, 'mitzy': 9580, 'baru': 1958, 'ber': 2149, 'tweeter': 15046, 'jg': 8022, 'nit': 10146, 'badoptus': 1858, '60k': 419, 'oscar': 10557, 'de': 4288, 'renta': 12066, 'east': 5024, 'village': 15511, 'shops': 12971, 'booked': 2437, 'museum': 9845, 'ooopps': 10480, '10am': 36, 'shizzle': 12946, 'gear': 6301, 'hospital': 7354, 'studio': 13888, 'x2wsw': 16237, 'desktop': 4463, 'mouse': 9759, 'ketboard': 8239, 'ot': 10562, 'second': 12706, 'monitor': 9659, 'completely': 3652, 'wii': 15973, 'vacuum': 15419, 'currently': 4098, 'posts': 11332, 'business': 2786, 'venturefile': 15463, 'alright': 1240, 'salad': 12495, 'swag': 14117, 'chili': 3275, 'minneapolis': 9516, 'refuse': 11967, 'masala': 9225, 'chaas': 3124, 'london': 8847, 'tourist': 14807, 'managed': 9141, 'purchase': 11624, 'ovi': 10635, 'constant': 3743, 'wasup': 15713, 'givin': 6423, 'lil': 8695, 'odee': 10349, 'qot': 11669, 'some1': 13393, 'heard': 7046, 'lonq': 8866, 'crawling': 3946, 'wrist': 16189, 'massive': 9238, 'headache': 7020, 'blown': 2363, 'ahaha': 1118, 'dunno': 4958, 'freakin': 6048, 'parents': 10765, 'island': 7861, 'flowers': 5885, 'pouch': 11340, 'stealth': 13738, 'present': 11410, 'mall': 9129, 'hide': 7171, 'churro': 3350, 'ut': 15387, 'coolness': 3814, 'conversations': 3789, 'ybd0': 16315, 'thunderstorm': 14581, 'track': 14833, 'licnse': 8662, 'plates': 11140, 'renewed': 12061, 'mechanic': 9319, 'inspections': 7727, 'town': 14821, 'promise': 11522, 'broiler': 2639, 'burger': 2762, 'king': 8303, 'burgers': 2763, 'chicken': 3263, 'fries': 6103, 'argentina': 1532, 'otra': 10570, 'vez': 15480, 'tshwane': 14974, 'rates': 11797, 'service': 12793, 'endless': 5212, 'loop': 8892, 'fail': 5547, 'spammers': 13511, 'anychance': 1418, 'matthew': 9262, 'sarah': 12549, 'ghosts': 6367, 'girlfriends': 6402, 'brought': 2656, 'states': 13715, 'learn': 8573, 'talking': 14237, 'library': 8657, 'nevermind': 10063, 'twitteraddict': 15090, 'likely': 8690, 'canada': 2913, 'nikkie': 10132, 'payne': 10850, 'pervs': 10946, 'teens': 14336, 'brand': 2556, 'bo': 2384, 'alcohol': 1174, 'band': 1910, 'idiot': 7526, 'leisure': 8612, 'bay': 2006, 'uploaded': 15327, 'grrr': 6705, 'games': 6249, 'wishes': 16032, 'sowwie': 13492, 'paint': 10694, 'chin': 3285, 'convo': 3797, 'victim': 15495, 'smells': 13274, 'gorgeous': 6549, 'everybody': 5384, 'indo': 7666, 'thai': 14431, 'traditional': 14844, 'clothes': 3471, 'object': 10323, 'wat': 15714, 'arun': 1592, 'rele': 12017, 'lfpa': 8649, 'cop': 3825, 'theater': 14462, 'drawning': 4850, 'emails': 5167, 'input': 7709, 'rough': 12356, 'sounded': 13474, 'agitated': 1105, 'knowledge': 8353, 'neuroanatomy': 10060, 'special': 13532, 'icarly': 7506, 'damage': 4177, 'guides': 6744, 'many': 9164, 'lifesaver': 8673, 'cud': 4065, 'entertainment': 5255, 'romania': 12321, 'mtv': 9803, 'sunday': 14019, 'motherssss': 9742, 'authority': 1716, '2day': 193, 'stranger': 13825, 'beblessed': 2066, 'names': 9935, 'ff': 5719, 'feline': 5688, 'anyways': 1430, 'pictures': 11028, 'caturday': 3049, 'earth': 5016, 'led': 8590, 'bittt': 2266, 'wendy': 15840, 'french': 6074, 'begins': 2103, 'bouquets': 2513, 'em': 5164, 'arranged': 1562, 'momma': 9640, 'proud': 11547, 'shoutout': 13000, 'moms': 9648, 'appreciate': 1488, 'cares': 2977, 'ohhhh': 10386, 'plain': 11120, 'cheers': 3226, 'downloading': 4818, 'tf2': 14422, 'mere': 9398, '3g': 269, 'instantly': 7740, '500kb': 368, 'brighter': 2611, 'sandwiched': 12532, 'between': 2175, 'ham': 6867, 'months': 9678, 'ta': 14190, 'wking': 16061, 'wkend': 16060, 'funniest': 6175, 'blow': 2360, 'harder': 6937, 'thankyou': 14445, 'log': 8818, 'details': 4481, 'adam': 991, 'lambert': 8453, 'count': 3874, 'sheep': 12885, 'fairly': 5559, 'flat': 5853, 'route': 12365, 'available': 1726, 'fully': 6154, 'functional': 6158, 'bike': 2220, 'yikes': 16389, 'false': 5575, 'jst': 8109, 'scratch': 12651, 'wht': 15950, 'nonsense': 10191, 'perfs': 10922, 'reyah': 12197, 'fellow': 5692, 'homegirl': 7275, 'situations': 13145, 'push': 11634, 'happens': 6918, '_jayy': 697, 'history': 7212, 'essay': 5327, 'housemate': 7376, 'angel': 1339, 'demons': 4405, 'reminise': 12050, 'qfsag': 11659, 'shots': 12988, 'espresso': 5324, 'aaaaand': 861, 'harmless': 6949, '_vip': 831, 'bye': 2825, 'beard': 2040, 'artwork': 1590, '3am': 266, 'portfolio': 11297, 'meca': 9318, 'wednesday': 15788, 'milonzzi': 9494, 'review': 12185, 'ball': 1894, 'coached': 3503, 'youngster': 16422, 'fundamentals': 6161, 'steak': 13733, 'understands': 15237, 'cabin': 2843, 'stocked': 13785, '140': 72, 'pages': 10686, 'serious': 12785, 'react': 11827, 'worst': 16160, 'date': 4243, 'miles': 9478, 'bew': 2178, 'gtalk': 6726, 'pls': 11187, '600': 415, 'ruth': 12440, 'paid': 10688, 'bills': 2229, 'electricity': 5132, 'liquour': 8736, 'knife': 8341, 'straight': 13816, 'curse': 4099, 'women': 16084, 'unavailable': 15212, 'until10': 15310, 'reinstalled': 11997, 'ubuntu': 15159, 'laptop': 8489, 'ext4': 5501, 'filesystem': 5753, 'system': 14184, 'boot': 2459, 'grub': 6715, 'operability': 10499, 'seconds': 12707, 'shudve': 13023, 'told': 14712, 'happen': 6913, 'nigh': 10116, 'lovess': 8952, 'bliss': 2331, 'jpeg': 8102, 'format': 5984, 'board': 2388, 'terminology': 14394, 'related': 12003, 'twitters': 15111, 'cuba': 4059, 'gona': 6499, 'headin': 7026, 'chilis': 3276, 'presidente': 11416, 'callin': 2880, 'ice': 7507, 'wonderful': 16090, 'yeaup': 16339, 'allowed': 1218, 'nap': 9943, 'teachers': 14308, 'johnathan': 8059, 'marly': 9204, 'meds': 9336, 'addicted': 994, 'disspointed': 4673, 'tonked': 14743, '40': 285, 'overs': 10624, 'war': 15674, 'begin': 2099, 'fiuuhh': 5827, 'bath': 1987, 'twits': 15085, 'lili': 8696, 'homie': 7287, 'black': 2277, '75': 468, 'exact': 5415, 'empanadas': 5190, 'ending': 5211, 'antony': 1408, 'johnsons': 8062, 'woohoo': 16106, 'wreck': 16180, 'nutella': 10291, 'mochi': 9610, 'waste': 15710, 'sunburn': 14014, 'knees': 8339, 'yield': 16386, 'webkinz': 15780, 'club': 3479, 'horse': 7345, 'called': 2878, 'dollar': 4751, 'ded': 4338, 'eeeeeee': 5085, 'binder': 2232, 'pa': 10668, 'starts': 13703, 'briefing': 2607, 'coast': 3505, 'acct': 946, 'mornin': 9711, 'decided': 4321, 'whateverworks': 15875, 'buisiness': 2730, 'fridays': 6092, 'algebra': 1186, 'cereal': 3109, 'toys': 14824, 'thinkin': 14511, 'buyin': 2813, 'kinds': 8302, 'mo': 9602, 'golf': 6494, 'vp': 15591, 'fight': 5740, 'argh': 1534, '_d': 609, 'zwarte': 16520, 'maillot': 9097, 'enjoyed': 5234, '_marguerite': 734, 'dianne': 4524, 'claudia': 3416, 'tweetville': 15060, 'active': 978, 'abbie': 891, 'round': 12359, 'things': 14507, 'podcast': 11210, 'listenening': 8744, 'def': 4348, 'peolple': 10899, 'wallace': 15654, 'gromit': 6683, 'bbc1': 2013, 'chevy': 3253, 'dealership': 4297, 'utah': 15388, 'van': 15428, 'fr': 6023, 'angrrry': 1348, 'sportsmens': 13604, 'warehouse': 15682, 'field': 5733, 'points': 11224, 'arrows': 1574, 'noo': 10192, 'zoo': 16509, 'ru': 12391, '2night': 213, 'skype': 13184, 'ssshh': 13647, 'playlist': 11152, 'leaning': 8572, 'toward': 14816, 'serani': 12776, 'detalis': 4482, 'gruesome': 6717, 'pom': 11250, 'chi': 3257, '4wdgr': 347, 'blonde': 2351, 'girly': 6407, 'hella': 7108, 'sausage': 12572, 'mcmuffin': 9298, 'gt': 6724, 'pill': 11052, 'display': 4664, 'functioning': 6159, 'alzheimer': 1262, 'patients': 10821, 'noise': 10181, 'greenville': 6657, 'sport': 13601, 'charlotte': 3188, '_lisa': 723, 'rip': 12250, 'gaming': 6252, '13': 59, 'session': 12800, 'absolutely': 911, 'badu': 1859, '_peanut_': 769, 'liked': 8689, 'commute': 3624, 'pmsl': 11202, 'everywhere': 5397, 'wool': 16108, 'stash': 13709, '_josa': 701, '_diaz': 618, 'stores': 13805, 'sat': 12556, 'church': 3348, 'sing': 13105, 'choir': 3311, 'services': 12795, 'lacey': 8424, 'granulation': 6618, 'omgggg': 10433, 'hawaii': 6993, 'future': 6196, 'queens': 11690, 'kendra': 8228, 'cancel': 2920, 'rumbo': 12413, 'bachilleres': 1830, 'sunstroke': 14040, 'kickable': 8266, 'starbucks': 13687, 'discount': 4629, 'code': 3522, 'youstinkatrespondingtotexts': 16434, 'trailers': 14854, 'hannah': 6907, 'montana': 9671, 'trailer': 14853, 'grease': 6639, 'conaway': 3671, 'guy': 6775, 'kenickie': 8229, 'celebrity': 3091, 'rehab': 11989, 'apologies': 1456, 'fix': 5829, 'feelings': 5684, 'kev': 8242, 'issue': 7869, 'oxm': 10656, 'link': 8718, 'wholeheartedly': 15934, 'asprin': 1619, 'excellent': 5425, 'vote': 15583, 'votes': 15585, 'category': 3043, 'bee': 2086, 'keemie': 8212, 'guus': 6773, '2moz': 208, 'sooooooooo': 13433, 'wh': 15855, 'beginnings': 2102, 'grabe': 6583, 'norms': 10219, 'researched': 12123, 'dami': 4179, 'pala': 10709, 'flamenco': 5846, 'forms': 5987, 'sub': 13914, 'kaloka': 8170, 'attempted': 1673, 'solea': 13379, 'ang': 1337, 'rockin': 12296, 'river': 12260, 'waitin': 15632, 'boy': 2528, 'app': 1460, 'development': 4496, 'easywriter': 5031, 'electro': 5133, 'bahah': 1866, 'gareths': 6275, 'abroad': 908, 'niamh': 10093, 'coming': 3599, 'gareth': 6274, 'frown': 6123, 'aussie': 1710, 'muah': 9806, 'din': 4577, 'amy': 1321, 'lori': 8902, 'pricey': 11435, 'dress': 4868, 'pissed': 11088, 'twitterena': 15095, 'boiler': 2408, 'tubas': 14985, 'upsets': 15337, 'becasue': 2070, 'imm': 7593, 'uset': 15377, 'happiest': 6920, 'itl': 7885, '_m': 731, 'nature': 9969, 'hockey': 7238, 'ideas': 7520, 'places': 11117, 'member': 9365, 'sooooo': 13428, 'writers': 16192, 'foisting': 5912, 'pipe': 11081, 'dreams': 4863, 'onto': 10461, 'typically': 15145, 'bristol': 2620, '_shutupandsmile': 797, 'ow': 10638, 'avatar': 1729, 'tear': 14313, 'co': 3501, 'workers': 16133, 'bars': 1956, 'britain': 2622, 'shabby': 12831, 'nerdfriends': 10038, 'rochester': 12291, 'sunbathin': 14011, 'clicked': 3439, 'woops': 16118, 'dayem': 4264, 'twpp': 15127, 'coughing': 3869, 'impossible': 7613, 'herts': 7149, 'cheerios': 3225, 'scones': 12634, 'saying': 12585, 'goodbye': 6506, 'fantastically': 5601, 'drives': 4888, 'others': 10567, 'distracted': 4678, 'easier': 5020, 'doc': 4722, 'douchenozzle': 4804, '_hawt': 667, 'fitness': 5825, 'regime': 11975, 'hehehehe': 7093, 'wishing': 16036, 'realize': 11849, 'fac': 5534, 'resist': 12128, '__': 552, 'unfair': 15246, 'wa': 15604, 'graduation': 6597, 'cp': 3908, 'gl': 6426, 'awsome': 1784, 'youre': 16425, 'mis': 9536, 'users': 15375, 'google': 6522, 'chrome': 3341, 'teaching': 14310, 'messing': 9408, 'gaah': 6217, 'fish': 5814, 'fingers': 5785, 'croquettes': 4009, 'beans': 2038, 'tend': 14373, 'namaskar': 9928, 'namaste': 9929, 'marathi': 9169, 'naaaah': 9907, 'thro': 14558, 'mama': 9131, 'kfc': 8258, 'lake': 8446, 'ahahaha': 1120, 'deal': 4295, 'snuff': 13336, 'hermione': 7140, 'action': 976, 'm8zfx': 9037, 'pole': 11234, 'stunningly': 13903, 'piece': 11031, 'puffffy': 11588, 'leavinggggggg': 8587, 'nooooooooo': 10205, '105': 33, '18': 100, '24': 165, 'dk': 4708, 'profiles': 11499, 'range': 11781, 'roots': 12339, 'pinic': 11070, 'mcdonalds': 9289, 'smiles': 13281, 'slush': 13253, 'six': 13146, 'flags': 5841, 'wicked': 15960, '17': 96, 'popular': 11285, 'vaca': 15412, 'cayman': 3065, 'islands': 7862, 'cnt': 3499, 'iloveyou': 7575, 'fresh': 6082, 'blanco': 2295, '100': 23, 'puerto': 11587, 'rican': 12215, 'mami': 9133, 'muy': 9872, 'amor': 1312, 'yi': 16385, 'tu': 14984, 'crushed': 4031, 'upset': 15336, 'soothing': 13440, 'nursing': 10290, 'etc': 5340, 'nights': 10123, 'request': 12102, 'guests': 6742, 'apartment': 1445, 'downfall': 4812, 'upgraded': 15323, 'spotify': 13608, 'premium': 11398, 'exceeded': 5423, 'threshold': 14553, 'awesomeness': 1769, 'itunes': 7895, 'bipolar': 2240, 'lame': 8455, 'holding': 7249, 'candic': 2928, 'soft': 13366, 'sighh': 13062, 'walks': 15652, '_diva': 621, 'tracks': 14838, 'dancefloor': 4198, 'thanxx': 14448, 'iï': 7915, '½ï': 16555, '½m': 16537, '_emily': 629, 'carl': 2981, '10yr': 42, 'slowest': 13245, 'starving': 13706, 'interessanter': 7767, 'issues': 7870, 'tough': 14802, 'number': 10282, 'rung': 12420, 'wales': 15646, 'court': 3890, 'juss': 8147, 'lavender': 8537, 'camera': 2901, 'celebreting': 3089, 'aweso': 1761, 'sufferin': 13964, 'personally': 10941, 'decide': 4320, 'election': 5130, 'north40': 10221, 'reagan': 11838, '08': 14, 'tjefferson': 14671, 'troosevelt': 14942, 'hott': 7364, 'africa': 1073, 'woo': 16100, 'finding': 5776, 'nemo': 10033, 'bws': 2823, 'scents': 12612, 'learnt': 8577, 'manual': 9162, 'enj': 5230, 'besides': 2154, 'cleaning': 3426, 'packing': 10676, 'msg': 9792, 'comedy': 3586, 'central': 3103, 'endure': 5217, 'brief': 2606, 'scrubs': 12678, 'steps': 13750, 'checking': 3214, 'hearing': 7048, 'colleagues': 3558, 'lists': 8750, 'samsam': 12519, 'sexy': 12823, 'crooked': 4006, 'body': 2400, 'drained': 4840, 'classic': 3410, 'jt': 8110, 'willkommen': 15988, 'twitterland': 15104, 'flu': 5888, 'cough': 3868, 'promises': 11524, 'anywhere': 1433, 'snapppp': 13311, 'lollll': 8838, 'fool': 5943, 'tesco': 14406, 'nearby': 9990, 'lj': 8776, 'sware': 14120, 'en': 5202, 'mady': 9074, 'truck': 14952, 'palin': 10712, 'haircut': 6838, 'grey': 6667, 'surrender': 14088, 'd5mjyj': 4151, 'scan': 12592, 'photo': 10995, 'shop': 12966, 'holly': 7263, 'gabbie': 6220, 'haahaaa': 6792, 'jellybeaniesss': 7994, 'comfortable': 3592, 'tweeting': 15053, '5rylt': 404, 'stage': 13660, 'theem': 14468, 'dangerous': 4207, 'avid': 1736, 'main': 9098, 'lack': 8425, 'folk': 5916, 'nina': 10136, 'lastnight': 8505, 'answerr': 1394, 'destrey': 4473, 'videos': 15502, 'bound': 2510, 'stressful': 13850, 'wrote': 16200, 'cmf': 3494, 'ads': 1037, 'march': 9173, 'deleted': 4382, 'wtf': 16204, 'arrived': 1570, '150': 79, 'fellows': 5693, 'pin': 11059, 'hole': 7251, 'todaaaaay': 14692, 'film': 5762, 'cw8wp2': 4129, 'utter': 15393, 'major': 9107, 'texts': 14421, 'citibank': 3383, 'grannys': 6615, 'burp': 2773, 'frog': 6115, 'banged': 1918, 'chips': 3297, 'sale': 12499, 'low': 8963, 'budget': 2705, 'video': 15501, 'annoying': 1383, 'hands': 6892, 'headed': 7025, 'search': 12692, 'c4237j': 2835, 'montday': 9673, 'award': 1747, 'ceremony': 3110, '4jkvl': 327, 'earning': 5010, 'cash': 3017, 'coboyf': 3510, 'paypal': 10851, 'cashouts': 3022, 'minimum': 9512, 'companion': 3632, 'bff': 2186, 'alreadyyyy': 1238, 'realy': 11858, 'sounding': 13476, 'lurgy': 9010, 'rope': 12340, 'bro': 2634, 'proudly': 11548, 'representing': 12097, 'father': 5632, 'badly': 1855, 'gr8': 6578, 'airbrushed': 1151, 'messed': 9405, 'eerrrr': 5096, 'seen': 12725, 'bcoz': 2025, 'george': 6339, 'however': 7385, 'loveeee': 8938, 'dcd': 4281, 'seniors': 12752, 'character': 3173, 'designs': 4459, 'sneak': 13313, 'peak': 10865, 'oooh': 10471, 'regular': 11988, 'wave': 15737, 'exactly': 5416, 'northern': 10223, 'emirates': 5181, 'richter': 12221, 'lotsa': 8914, 'seriously': 12786, 'motherï': 9743, '½s': 16546, 'yyyyuck': 16474, '_autism': 575, 'bright': 2609, 'refuses': 11969, 'wires': 16024, 'sort': 13460, 'graph': 6621, 'botega': 2496, 'branded': 2557, 'public': 11580, 'menace': 9376, 'state': 13712, 'paychecks': 10845, 'fair': 5556, 'worried': 16154, 'printer': 11452, 'newest': 10070, 'glade': 6429, 'debian': 4308, 'onoir': 10457, 'montreal': 9680, 'anxiety': 1412, 'denmark': 4410, 'yukky': 16455, 'collecting': 3560, 'tacos': 14200, 'pasadena': 10796, 'huaaahhhhh': 7410, 'jupaa': 8142, 'resaaaa': 12115, 'awas': 1752, 'kaliaaannn': 8169, 'buttfuck': 2804, 'yoooooooou': 16410, 'task': 14271, 'concern': 3677, 'quotess': 11723, 'afraid': 1071, 'comments': 3610, 'mp': 9781, 'expenses': 5470, 'hopelessly': 7325, 'average': 1733, 'salary': 12498, 'insensitive': 7717, 'bag': 1861, 'bathrooms': 1990, 'cent': 3101, 'mmmm': 9593, 'granada': 6602, 'bowl': 2520, 'crew': 3982, 'bowling': 2522, 'fisch': 5812, '4wn29': 356, 'gm': 6455, 'bankruptcy': 1926, 'loss': 8909, 'offset': 10378, 'gains': 6237, 'tax': 14294, 'wise': 16028, 'lates': 8513, 'pfff': 10954, 'sucksss': 13953, 'myrtle': 9890, 'sc': 12589, 'woeiwoeiwoei': 16074, 'yep': 16357, 'fic': 5728, 'unopened': 15292, 'pack': 10670, 'goat': 6470, 'starin': 13692, 'til': 14621, 'abt': 915, 'cr': 3912, 'chan': 3146, 'sentiments': 12765, 'sophomore': 13442, 'junior': 8139, 'bringing': 2617, 'esgiq': 5320, 'stayin': 13728, '630': 422, '100th': 27, 'matters': 9261, 'anybody': 1417, 'win7': 15997, 'sp2': 13501, 'haiiii': 6833, 'sankq': 12541, 'fineee': 5780, 'js': 8108, 'checkup': 3216, 'rib': 12212, 'fakin': 5565, 'kisha': 8313, 'dood': 4778, 'locked': 8810, 'vimeo': 15514, 'signup': 13075, 'dojo': 4748, 'workshop': 16143, 'munich': 9838, 'ct83ub': 4057, 'hurry': 7461, 'nico': 10108, 'landed': 8468, 'addy': 1008, 'fwd': 6204, 'stat': 13711, 'priority': 11456, 'praying': 11378, 'humble': 7434, 'gives': 6422, 'grace': 6585, 'onscreen': 10459, 'keyboard': 8248, 'dammit': 4183, 'hahahahahahhah': 6827, 'gratiss': 6628, 'milo': 9493, 'saddest': 12470, 'bother': 2498, 'ermm': 5307, 'upper': 15331, 'completley': 3654, 'flo': 5870, 'rida': 12227, '_erincharde': 634, 'afterthought': 1081, 'spurs': 13625, 'champs': 3145, 'kitty': 8324, 'sling': 13227, 'hanging': 6901, 'killed': 8284, 'horde': 7335, 'gears': 6302, 'sittin': 13140, 'lb': 8555, 'dummy': 4949, 'drag': 4833, 'feb': 5668, '09': 16, '56pm': 384, 'gmt': 6458, '805': 485, 'including': 7642, 'somewhere': 13407, 'twice': 15068, 'outvoted': 10602, 'screwed': 12670, 'san': 12520, 'diego': 4541, 'whether': 15900, 'holes': 7252, 'shoes': 12955, 'harley': 6945, 'december': 4316, 'onoger': 10456, 'providence': 11555, 'handy': 6896, 'joy': 8096, 'serial': 12783, 'fraudster': 6043, 'retriever': 12167, 'living': 8773, 'keen': 8213, 'ears': 5015, 'ashley': 1605, 'wee': 15789, 'cafe': 2848, 'latteeeeeeeeeee': 8519, '_gm': 654, 'plan': 11121, 'vicious': 15493, 'circl': 3377, 'dramatic': 4844, 'wassup': 15708, 'dvd': 4974, 'dawson': 4262, 'creek': 3971, 'trash': 14881, 'repops': 12088, 'michigan': 9446, 'hahha': 6829, 'leacing': 8560, 'wi': 15959, 'envy': 5270, 'instant': 7739, 'marketing': 9197, 'empire': 5193, 'bonus': 2427, 'recoup': 11914, 'investment': 7808, 'less': 8629, 'vur': 15598, 'megainternetwealth': 9349, 'megaredpacket': 9351, 'stoked': 13788, 'paisley': 10704, 'cone': 3694, '_y_tony': 846, 'pad': 10680, 'ragoons': 11750, 'yumb': 16459, 'david': 4255, '_from_hell': 646, 'difficult': 4555, 'thankies': 14438, 'scotland': 12642, 'lived': 8763, 'europe': 5356, 'cranking': 3932, 'spreadsheets': 13615, 'postieeee': 11327, 'andshehopes': 1333, 'blogspot': 2347, '2009': 136, '05': 7, 'kewpie': 8246, 'html': 7404, 'shining': 12923, 'valid': 15423, 'excuse': 5445, 'forth': 5999, 'kidding': 8275, 'quoting': 11724, 'delight': 4387, 'amber': 1287, 'backround': 1842, 'design': 4453, 'scroll': 12675, 'bottem': 2502, 'click': 3437, 'image': 7584, 'slapton': 13194, 'borin': 2484, 'woman': 16083, 'dar': 4218, 'iloveyoumoreeee': 7576, 'hellotxt': 7115, 'booming': 2446, 'hoes': 7241, 'rarely': 11791, 'dye': 4979, 'aaaagggessss': 863, 'todays': 14694, 'deucie': 4489, 'sitter': 13139, 'fab': 5530, 'beautifuul': 2061, 'planning': 11128, 'hgtv': 7160, '236am': 161, 'decor': 4334, 'amazi': 1279, '_pe': 768, 'phillip': 10975, 'hasn': 6963, 'scored': 12638, 'mojito': 9627, 'unhappy': 15261, 'ambulance': 1289, 'temporary': 14369, 'address': 1004, 'm8': 9036, 'domain': 4757, 'isss': 7868, 'boredddd': 2478, 'morningg': 9713, 'goodness': 6513, 'create': 3960, 'tons': 14744, 'drama': 4842, 'babes': 1817, 'isnt': 7866, 'mikey': 9472, 'cousin': 3891, 'caught': 3051, 'bouquet': 2512, 'geesh': 6310, 'nside': 10267, 'joint': 8067, 'bf': 2183, 'confuzzled': 3715, 'shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit': 12916, 'explore': 5486, 'snake': 13305, 'garden': 6269, 'imposible': 7612, 'moodle': 9683, 'intro': 7793, 'training': 14859, 'bookings': 2440, 'squashed': 13632, 'hq': 7392, 'scoundrels': 12645, 'aching': 961, 'honestly': 7291, 'hug': 7418, 'consider': 3737, 'yultron': 16457, 'meanest': 9307, 'extinction': 5508, 'frogs': 6116, 'portsmouth': 11302, 'terrified': 14403, 'pensioners': 10897, 'expect': 5465, 'suprises': 14071, 'screw': 12669, 'solutions': 13385, 'badmicrosoft': 1856, 'hopefuly': 7323, 'sisters': 13133, 'timeline': 14633, 'loveliness': 8946, 'wondering': 16094, 'colossal': 3571, 'spiritualsmarts': 13578, '1560': 81, 'published': 11583, 'digest': 4559, 'weekly': 15802, 'virge': 15526, 'dying': 4983, 'obvious': 10331, 'aidan': 1141, 'judges': 8115, 'fourth': 6019, 'spelled': 13549, 'metallica': 9413, 'amused': 1319, 'reviews': 12186, 'canadian': 2914, 'incident': 7636, 'near': 9989, 'relying': 12034, 'lacks': 8428, 'reception': 11882, 'giving': 6424, 'rad': 11742, 'michele': 9444, 'teehee': 14332, 'halo': 6860, 'eww': 5411, 'atm': 1663, 'roast': 12274, 'unappealing': 15211, 'tattoo': 14288, 'owwwwwww': 10654, 'download': 4814, 'rofl': 12306, 'kristin': 8382, 'bone': 2421, 'join': 8064, 'draw': 4847, 'neuro': 10059, 'fxs3l': 6206, 'ton': 14733, 'homework': 7285, '4wrrp': 359, 'pg': 10959, '237': 162, 'laying': 8548, 'wash': 15699, 'mommas': 9641, 'callum': 2883, 'someday': 13396, 'salivary': 12502, 'gland': 6434, 'frm': 6113, 'flies': 5864, 'alison': 1196, 'queen': 11689, 'growing': 6700, 'ashleigh': 1603, 'teacher': 14307, 'myspace': 9894, 'janedurkin': 7948, 'tease': 14317, 'lessons': 8631, 'patiently': 10820, 'mega': 9348, 'redpacket': 11936, 'epic': 5278, 'lappytop': 8488, 'baterrry': 1986, 'construction': 3747, 'smh': 13278, 'lo': 8792, 'hence': 7126, 'smiley': 13282, 'hong': 7296, 'monsters': 9668, 'regret': 11984, 'boooo': 2450, 'strained': 13820, 'cup': 4080, 'yfrog': 16380, 'ehhmyj': 5115, 'success': 13937, 'sooooooooooooooooo': 13436, 'bs': 2681, '4real': 331, 'deadass': 4290, 'herrreeeee': 7145, 'lisa': 8738, 'usa': 15362, 'oowweee': 10486, 'china': 3286, 'wuz': 16211, 'poppin': 11281, 'lipstic': 8731, 'pumps': 11609, 'effect': 5099, 'met': 9410, 'shortest': 12982, 'line': 8711, 'fcb': 5656, 'tunapuna': 14999, 'tellers': 14359, 'disappeared': 4612, '14': 71, 'whoop': 15944, 'daaaaang': 4155, 'cobra': 3511, 'cam': 2892, 'dragged': 4834, 'craving': 3941, 'satisfy': 12562, 'meant': 9313, 'sentimental': 12764, 'ental': 5247, 'fren': 6073, 'lorraine': 8903, 'climb': 3445, 'shoulder': 12992, 'burnt': 2772, 'hangover': 6902, 'ol': 10407, 'english': 5227, 'fry': 6136, 'wives': 16056, 'tips': 14651, 'cya': 4133, 'warning': 15689, 'signal': 13069, 'lyndon': 9031, '4pm': 330, 'via': 15485, 'tasmania': 14275, 'wweeeeooo': 16216, 'downhill': 4813, 'welcomes': 15832, 'christian': 3336, 'grimmy': 6678, 'cars': 3006, 'deed': 4342, 'deserves': 4452, 'somatic': 13390, 'drawing': 4848, 'lanham': 8480, 'leopard': 8625, 'breeding': 2587, 'ground': 6690, 'montanna': 9672, 'shes': 12904, 'lonelyyyy': 8850, 'oooo': 10474, 'ughh': 15173, '2stop': 218, 'killin': 8288, 'ra': 11729, 'neighbor': 10022, 'paved': 10837, 'forced': 5960, 'zeke': 16489, 'astroturf': 1642, 'drain': 4839, 'spray': 13612, 'green': 6652, 'dreadful': 4854, 'fruits': 6129, 'visible': 15542, 'magically': 9082, 'ry': 12443, 'bebo': 2067, 'putting': 11643, 'edinburgh': 5065, 'haggis': 6810, 'hes': 7150, 'xox': 16256, 'slower': 13244, 'mexico': 9426, 'aeropuerto': 1059, 'shocked': 12952, 'departed': 4421, 'relaxin': 12013, 'looooong': 8882, 'bhr': 2198, 'hosts': 7359, 'rqpl7': 12382, 'small': 13261, 'doses': 4793, 'commercial': 3611, 'shoot': 12964, 'dirt': 4603, 'vacations': 15416, 'woken': 16077, 'targeted': 14266, 'uses': 15376, 'ie6': 7536, 'core': 3837, 'default': 4349, 'trend': 14906, 'ruilen': 12403, 'panthers': 10738, 'saints': 12492, 'crossed': 4012, 'qld': 11663, 'teams': 14312, 'math': 9251, 'lunchhhhhhhh': 9003, 'fifteen': 5736, 'awwwwweeee': 1791, 'rotate': 12353, 'outsider': 10595, 'pub': 11579, 'likey': 8693, 'grateful': 6626, 'differe': 4547, 'role': 12310, 'suggestion': 13975, 'convince': 3795, 'aweful': 1759, 'taltal': 14243, 'aol': 1437, '96': 527, 'added': 993, 'coincidence': 3536, 'tal': 14225, 'chocked': 3305, 'arm': 1547, 'ahh': 1125, 'amounts': 1316, 'msgs': 9793, 'ack': 965, 'order': 10529, 'press': 11417, '107': 34, 'no0o0o0o': 10162, 'waldo': 15645, 'ckc': 3392, 'chilling': 3283, 'accident': 933, 'pr': 11366, 'pacific': 10669, 'tomorow': 14728, 'besties': 2161, 'mcdo': 9287, 'mains': 9102, '2nite': 215, 'debating': 4306, 'harry': 6957, 'potter': 11338, 'bwahahahaha': 2821, 'totes': 14796, 'giggle': 6380, 'howmany': 7386, 'sorryyyyyy': 13459, 'chapter': 3171, 'kiyosaki': 8327, 'hire': 7206, 'cleaner': 3424, 'lov': 8926, 'skills': 13166, 'voted': 15584, 'yours': 16429, 'hiss': 7210, 'arik': 1541, 'popped': 11280, 'reall': 11852, 'hiccups': 7165, 'relief': 12023, 'nooooooooooo': 10207, 'sf': 12825, 'ahahahahahahahaha': 1122, 'jeans': 7985, 'emergency': 5180, 'plez': 11180, '4us': 334, '630p': 423, 'sprints': 13622, 'impressed': 7615, 'keys': 8257, 'looooove': 8888, 'shortcakefai': 12975, 'dang': 4204, 'dave': 4252, 'whining': 15910, 'heyy': 7155, 'yuup': 16471, 'hows': 7388, 'enjoyable': 5232, 'dizzy': 4702, 'calories': 2889, 'balk': 1893, 'disappoint': 4614, 'boys': 2533, '__d': 557, 'awayyyyy': 1755, 'attended': 1678, 'innovation': 7706, 'seminar': 12744, 'picnic': 11023, '2007': 134, 'rumored': 12415, 'screen': 12661, 'kindle': 8300, 'wyfi': 16227, 'k12': 8157, 'textbooks': 14417, 'amorsote': 1313, 'iced': 7510, 'sluggish': 13249, 'soak': 13341, '30mins': 231, 'wall': 15653, 'sharing': 12861, 'migraine': 9465, 'coffe': 3527, 'strong': 13868, 'hahah': 6816, 'boonies': 2447, 'map': 9165, 'transition': 14871, 'bkite': 2272, '07kjr': 13, 'nooooooo': 10203, 'dumb': 4946, 'scheduled': 12618, 'exhausting': 5452, 'qt': 11673, 'mow': 9772, 'lawn': 8540, 'marco': 9176, 'smoking': 13292, 'weed': 15790, 'philosophical': 10979, 'ohh': 10384, 'attack': 1668, '4d': 308, 'cassie': 3028, 'wimbledon': 15993, 'imax': 7590, 'successful': 13938, 'death': 4303, 'evil': 5403, 'homely': 7277, 'freddie': 6054, 'grilled': 6674, 'breast': 2580, 'reefried': 11947, 'gnite': 6465, 'twitties': 15117, 'freshman': 6083, 'alllllll': 1210, 'cqc': 3911, 'brazilian': 2566, 'humor': 7440, 'program': 11504, 'aston': 1639, 'spanish': 13513, 'min': 9497, 'outstanding': 10597, 'tc': 14300, 'r6rfc': 11728, 'tool': 14752, 'smilin': 13283, '1000': 24, '__diamond': 558, 'period': 10926, 'stain': 13662, 'blues': 2372, '_henrie': 672, 'bathroom': 1989, 'weirdest': 15819, 'wateva': 15729, 'michael': 9443, 'scofiled': 12633, 'noooooo': 10202, 'writer': 16191, 'tooth': 14760, 'tucking': 14988, 'mogwai': 9621, 'repeat': 12073, 'eeeeeeeeeee': 5086, 'luddite': 8989, 'theatre': 14464, 'younger': 16421, 'sis': 13127, 'mary': 9221, 'poppins': 11283, 'judge': 8113, 'pillow': 11053, 'nanna': 9940, 'bubble': 2694, 'baths': 1991, 'ahahah': 1119, 'holyyyyyyy': 7270, 'lumberjack': 8996, 'lipton': 8733, 'sparkling': 13518, 'telecom': 14348, 'sister': 13132, 'tonigh': 14737, 'monkey': 9661, 'russian': 12435, 'roulette': 12358, 'fabulous': 5532, 'interracial': 7780, 'ba': 1809, 'gphone': 6572, 'signed': 13071, 'io': 7822, 'sticking': 13763, 'clients': 3443, 'record': 11907, 'locations': 8808, 'young': 16420, 'sum': 13989, 'attending': 1679, 'cic': 3359, 'orientation': 10544, 'hearin': 7047, 'loggade': 8820, 'visst': 15550, '400e': 287, 'cache': 2846, 'lï': 9034, '½rdags': 16544, 'alone': 1228, 'mid': 9453, 'detox': 4487, 'piss': 11087, 'bruce': 2663, 'healthy': 7042, 'scary': 12606, 'maggots': 9079, 'rat': 11794, 'avoidance': 1740, 'continuous': 3774, 'flow': 5883, 'trick': 14917, 'loosen': 8896, 'gremlins': 6663, 'talented': 14229, 'omj': 10437, 'tonights': 14739, 'yanks': 16295, 'lakers': 8448, 'accounts': 945, 'county': 3882, 'raly': 11771, 'tak': 14216, 'quotes': 11722, 'pieces': 11032, 'ady': 1055, 'yoga': 16397, 'smith': 13285, 'amherst': 1301, 'mini': 9508, 'reunion': 12178, 'built': 2729, 'bridge': 2604, 'boards': 2390, 'nails': 9920, 'starwarsday': 13708, 'flickr': 5863, 'safe': 12481, 'returned': 12172, 'huntsville': 7454, 'sam': 12509, 'houston': 7379, 'grave': 6630, 'dipped': 4592, 'strawberries': 13834, 'considering': 3740, 'yarn': 16301, 'recently': 11881, 'necessary': 9994, 'angelina': 1341, 'spam': 13509, 'checks': 3215, 'spooky': 13597, 'pork': 11291, 'stir': 13780, 'rice': 12216, 'breaks': 2578, 'tip': 14648, 'bottle': 2503, 'securely': 12711, 'tightly': 14616, 'knw': 8359, 'maan': 9046, 'become': 2077, 'omelettes': 10426, 'chapathis': 3168, 'busted': 2791, 'lip': 8729, 'nurse': 10288, 'rid': 12226, 'event': 5368, 'management': 9142, 'prerequisites': 11403, 'dis': 4606, 'charger': 3177, 'verizon': 15469, 'fone': 5933, 'semester': 12742, 'pity': 11101, 'liesboystell': 8668, 'henna': 7128, 'redhead': 11929, 'lookin': 8871, 'boat': 2391, 'whew': 15902, 'kicking': 8270, 'holidays': 7255, 'whiles': 15906, 'power': 11352, 'cutting': 4121, 'aaaawww': 868, 'alcoholic': 1175, 'beverage': 2177, '_skies': 800, 'grrrrri': 6709, 'justice': 8149, 'cantttt': 2939, 'tormented': 14775, 'showing': 13009, '22': 153, 'especially': 5322, 'loads': 8798, 'jobs': 8050, 'themed': 14477, 'brickman': 2601, 'atleast': 1661, 'joined': 8065, 'invent': 7799, 'flavour': 5858, 'competion': 3640, 'entry': 5264, 'recognised': 11893, 'sweetpotatoe': 14143, 'seasalt': 12696, 'robert': 12282, 'pattinson': 10829, 'conchords': 3683, 'limo': 8705, 'yayy': 16309, 'fo': 5902, 'battlestar': 2002, 'galactica': 6239, 'channel': 3159, 'rebellion': 11868, 'became': 2068, 'gaeta': 6224, 'elephant': 5140, 'painting': 10697, 'shortly': 12983, 'west': 15848, 'apartments': 1446, 'ponying': 11255, 'minds': 9501, 'pushing': 11637, 'heels': 7085, 'livelovesing': 8765, 'embarassed': 5169, 'result': 12150, 'gettin': 6352, 'ak': 1161, 'flop': 5875, '2k6': 202, 'raised': 11766, '25': 167, 'rag': 11748, 'bets': 2166, 'raise': 11765, 'managaed': 9139, 'eels': 5094, 'donuts': 4775, 'comfort': 3591, 'fu': 6142, 'pleeez': 11177, 'bamf': 1905, 'ing': 7685, 'robsessedpattinson': 12288, 'biggest': 2213, 'burrito': 2775, 'asada': 1595, 'ooo': 10470, 'blahh': 2288, 'stomachace': 13792, 'girlys': 6408, 'wayy': 15749, 'kirsty': 8311, 'manually': 9163, 'oooiifull': 10472, 'mp4': 9783, 'shifts': 12914, 'orphan': 10552, 'lambs': 8454, 'cheated': 3209, 'zoombezi': 16510, 'buuuut': 2809, 'muahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 9808, 'dati': 4246, 'lian': 8655, 'eina': 5118, 'gelli': 6316, 'zero': 16496, 'francis': 6032, 'fit': 5824, 'implying': 7607, 'subtle': 13930, 'usual': 15384, 'remote': 12053, 'fighting': 5741, 'heerlen': 7086, 'shouts': 13001, 'peoples': 10902, 'twitterville': 15115, 'attention': 1680, 'length': 8617, 'type': 15142, 'mans': 9161, 'amazon': 1285, 'ce': 3077, 'grade': 6589, 'mock': 9611, 'lovel': 8944, 'ave': 1731, 'sweetest': 14140, 'obviously': 10332, 'kudo': 8393, 'concept': 3674, 'adjusting': 1014, 'obvs': 10333, 'tryna': 14969, 'noooooooo': 10204, 'seasons': 12698, 'files': 5752, 'hurray': 7459, 'phillll': 10976, 'wazzuppppp': 15753, 'semi': 12743, 'older': 10409, 'stack': 13656, 'talent': 14228, 'altered': 1251, 'peeps': 10880, 'twitterworld': 15116, 'jackson': 7929, 'rathbone': 11798, 'russians': 12436, 'quotation': 11720, 'rug': 12399, 'tied': 14606, 'unny': 15289, 'blatently': 2304, 'rather': 11799, 'thoughts': 14544, 'loz': 8967, 'alive': 1198, 'gandhi': 6255, 'books': 2443, 'charged': 3176, 'sean': 12690, 'teasing': 14320, 'wings': 16010, 'font': 5935, 'fonts': 5937, 'qw': 11725, 'makers': 9114, 'laugh': 8523, 'sees': 12726, 'amaze': 1275, 'annnnnnddd': 1375, 'bites': 2261, 'unfixable': 15248, 'forget': 5975, 'sunglasses': 14024, 'thanx': 14446, 'filthy': 5766, 'jimmy': 8032, 'lasted': 8504, 'soaked': 13342, 'hiking': 7189, 'wand': 15661, 'hawthorn': 6998, 'kraken': 8379, 'heartstrings': 7059, 'whoaa': 15929, 'toledo': 14713, 'pizza': 11107, 'enlightened': 5240, 'inspired': 7732, 'letters': 8638, 'hill': 7193, 'piano': 11014, 'lemme': 8613, 'mates': 9249, 'millsy': 9492, 'yahoo': 16289, 'yups': 16470, 'pruning': 11564, 'unfollowers': 15250, 'omgogmgo': 10434, 'wakeup': 15640, 'noooo': 10200, 'banquet': 1931, 'sb': 12588, '239k': 164, 'players': 11149, 'brick': 2599, 'kick': 8265, 'a9': 851, '_precious06': 775, 'talked': 14233, 'grandma': 6610, 'hindi': 7200, 'manged': 9149, 'software': 13370, 'install': 7734, 'willl': 15989, 'nighty': 10127, 'favorit': 5646, 'mommys': 9645, 'afternoons': 1079, 'happiness': 6922, 'milan': 9474, 'bfoyf': 2190, 'geneva': 6326, 'fam': 5576, 'kirk': 8309, 'id': 7517, 'clubbing': 3480, 'holdin': 7248, 'imo': 7600, 'icant': 7505, 'sober': 13346, 'professional': 11497, 'interns': 7779, 'sukks': 13984, 'telenovelas': 14351, 'braid': 2547, 'brewer': 2593, 'jordie': 8079, 'disliking': 4656, 'legit': 8603, 'aaaaaaaaaahhhhhhhh': 855, 'melt': 9360, 'brrrrrr': 2662, 'cleared': 3430, 'lah': 8443, 'linda': 8708, '27th': 181, '5th': 405, 'gangstarr': 6258, 'exgirl': 5449, 'nas': 9950, 'texas': 14415, 'eeekkkk': 5090, 'goofin': 6521, 'knowing': 8351, 'mouth': 9760, 'sissy': 13129, 'subway': 13934, 'letting': 8640, 'hw': 7478, 'tia': 14592, 'rose': 12345, 'sh': 12830, 'prove': 11549, 'cream': 3955, 'vancouver': 15430, 'throwing': 14569, 'canucks': 2940, 'polite': 11239, 'chosen': 3329, 'jab': 7919, 'knitting': 8343, 'needles': 10005, 'patrick': 10823, 'urgh': 15352, '_bambu': 579, 'mending': 9379, 'accomplish': 940, 'congratulatory': 3725, 'thankful': 14435, 'thnkn': 14525, 'virtus': 15535, 'treviso': 14911, 'futurshow': 6197, 'forza': 6007, 'ragazzi': 11749, 'erased': 5295, 'pfft': 10956, 'helicopters': 7105, 'imaginary': 7586, 'penis': 10893, 'drake': 4841, 'lovliest': 8961, 'ruined': 12405, 'doctors': 4727, 'inability': 7628, 'warn': 15687, 'tests': 14411, 'congrat': 3718, 'bach': 1829, 'theres': 14491, 'assignment': 1629, 'havent': 6986, 'concentrate': 3672, 'statue': 13722, 'venetian': 15455, 'screamed': 12657, 'p2v': 10665, 'migrates': 9468, 'vmware': 15558, 'vsp': 15596, 'magento': 9078, 'config': 3701, 'lemon': 8614, 'ranch': 11776, 'yself': 16449, 'heroism': 7144, 'badges': 1853, 'achievement': 959, 'favorite': 5647, 'hopeless': 7324, 'desk': 4461, 'lifespan': 8674, 'chel': 3238, 'nicole': 10110, 'indianapolis': 7661, 'buzzed': 2818, 'awhile': 1780, 'niceee': 10098, 'stickersss': 13762, 'woop': 16117, 'itself': 7889, 'country': 3879, 'preschool': 11405, 'kiddies': 8273, 'dunkin': 4956, 'hee': 7076, 'group': 6693, 'ripped': 12252, 'dentention': 4413, 'anxious': 1413, 'boreedd': 2481, 'cwpm': 4131, 'society': 13354, 'crimson': 3991, 'tide': 14602, 'sauce': 12570, 'tuner': 15002, 'deserve': 4450, 'posh': 11307, 'tatler': 14286, 'kk': 8328, 'challenge': 3136, 'sail': 12489, 'grad': 6588, 'eventhough': 5370, 'cook': 3801, 'sculpting': 12680, 'koi': 8365, 'dangerously': 4208, 'fell': 5689, 'paris': 10766, 'logs': 8829, 'resturants': 12149, 'predictable': 11387, 'cloudy': 3475, 'washing': 15702, 'messages': 9404, 'quilt': 11708, 'laid': 8444, 'finds': 5777, 'hardest': 6938, 'gained': 6235, 'kgs': 8259, 'shite': 12938, 'fornicating': 5989, 'nose': 10229, 'gaaay': 6216, 'accidentaly': 935, 'slammed': 13191, 'trunk': 14958, 'delete': 4381, 'network': 10056, 'cyber': 4134, 'kidnap': 8278, 'festival': 5706, 'stockholm': 13786, 'relay': 12015, 'honor': 7299, '06': 8, 'uncle': 15218, 'howard': 7382, 'firefly': 5801, 'nathan': 9960, 'fillion': 5761, 'tnx': 14683, 'progressive': 11512, 'smiling': 13284, 'piggies': 11042, 'hmpf': 7231, '6am': 453, 'dmp': 4711, 'definately': 4358, 'pm': 11201, 'circle': 3378, 'ab': 886, 'zum': 16518, 'spooohort': 13600, 'buying': 2814, 'twitterparty': 15107, 'laundry': 8531, 'sparkly': 13519, 'awwwwwwwwww': 1795, 'awwwww': 1790, 'groove': 6685, 'heeelllppppp': 7081, 'meeeeeeee': 9342, '_pearson': 770, 'fallow': 5572, 'babysitting': 1824, 'paionks': 10699, 'obsessed': 10327, 'addison': 1002, 'exclamation': 5439, 'throw': 14564, 'twitterrr': 15110, 'cervical': 3116, 'cancer': 2926, 'recession': 11883, 'clue': 3483, 'fafsa': 5546, 'form': 5982, 'ann': 1361, 'arbor': 1514, 'detroit': 4488, 'metro': 9421, 'computers': 3667, 'massages': 9235, 'son': 13409, 'grab': 6580, 'cover': 3895, 'realised': 11844, 'sending': 12749, 'staying': 13729, 'hve': 7475, 'docent': 4723, 'hvng': 7477, 'zombie': 16505, 'aftrn': 1084, 'shld': 12947, 'lebron': 8588, 'mvp': 9874, 'nba': 9978, 'lifetime': 8676, 'talktalk': 14240, 'broadband': 2636, 'workin': 16134, 'ova': 10606, 'sum1': 13990, 'dice': 4531, 'heroes': 7143, 'classics': 3411, 'timeless': 14632, 'numbers': 10283, 'performed': 10920, 'positive': 11312, 'tennessee': 14382, 'alot': 1230, 'tragic': 14850, 'mysterious': 9898, 'lights': 8685, 'messy': 9409, 'ponytail': 11256, 'preschoolers': 11407, 'hundred': 7443, 'weigh': 15812, 'lbs': 8556, 'gta4': 6725, 'meh': 9352, 'djandyw': 4704, 'coursework': 3889, 'mileyyyyyy': 9481, 'calm': 2884, 'breath': 2581, 'dragonball': 4837, 'listing': 8748, 'slide': 13222, 'hungover': 7447, 'sickening': 13046, 'regression': 11983, 'process': 11479, 'motion': 9744, 'possibe': 11317, 'gathering': 6289, 'corner': 3841, 'regretting': 11987, 'collection': 3561, 'ironclad': 7845, 'determination': 4484, 'uttered': 15394, 'ical': 7504, 'farrrrr': 5613, 'pdhpe': 10860, 'flats': 5854, 'helping': 7121, 'often': 10381, 'dummyhead': 4950, 'natsmith88': 9965, 'prepare': 11400, 'yourself': 16431, 'meditate': 9330, 'musical': 9851, 'actor': 981, 'neighbours': 10026, 'quitting': 11715, 'private': 11462, 'beta': 2164, 'blackberries': 2278, 'sucky': 13955, 'ticks': 14600, 'pakistan': 10707, '07kbq': 12, 'heeeeey': 7079, '38': 264, 'sickkkk': 13050, 'previous': 11429, 'understand': 15234, 'defo': 4365, 'uggo': 15171, 'lapit': 8485, 'na': 9905, 'ko': 8361, 'magout': 9086, 'fated': 5631, 'rrj4e': 12384, 'yt': 16450, 'partner': 10785, 'stubbed': 13880, 'giddy': 6374, 'invention': 7802, 'television': 14356, 'influence': 7682, '_ernman': 635, 'hej': 7101, 'malena': 9126, 'lycka': 9028, 'eurovision': 5357, 'united': 15274, 'kingdom': 8304, 'belly': 2129, 'assistant': 1632, 'focus': 5906, 'kid': 8272, 'particularly': 10782, 'pcd': 10855, 'painful': 10691, 'inflammed': 7679, 'aiming': 1147, 'asking': 1614, 'linked': 8719, 'developing': 4495, 'patient': 10819, 'homeschooling': 7281, 'dd': 4283, 'pitcher': 11097, 'roar': 12273, 'poorer': 11273, 'mexican': 9425, 'mx': 9879, 'automatically': 1720, 'caramels': 2958, 'crack': 3915, 'deadline': 4291, 'hectic': 7073, 'promo': 11527, 'orbicule': 10526, 'translation': 14874, 'norwegian': 10226, 'icecream': 7509, 'chris': 3332, 'pine': 11064, 'nudge': 10272, 'highlight': 7179, 'sold': 13376, 'rl': 12264, 'photoshop': 11002, 'nkotb': 10156, 'tish': 14663, 'crv': 4037, 'pick': 11017, 'duckie': 4933, 'match': 9242, 'handball': 6883, 'profile': 11498, 'loaded': 8795, 'variety': 15434, 'daily': 4171, 'comics': 3597, 'freshner': 6084, 'horriblel': 7338, 'grossed': 6688, 'ethan': 5343, 'coooooooool': 3819, 'dooooooooown': 4782, 'patience': 10818, 'virtue': 15533, 'attempt': 1672, 'alochol': 1227, 'thrown': 14570, 'teenagers': 14335, 'encouragement': 5207, 'bluetooth': 2373, 'device': 4497, 'greedy': 6649, 'adults': 1043, 'burn': 2765, 'irony': 7850, 'inventor': 7803, 'ford': 5962, 'mustang': 9862, 'hahaaaha': 6815, 'necklace': 9996, 'weekends': 15800, 'injured': 7692, 'passing': 10801, 'germs': 6345, 'surviving': 14094, 'rachael': 11734, 'classes': 3409, 'ohhh': 10385, 'wowwww': 16171, 'empitome': 5195, 'spirit': 13575, 'cake': 2860, 'rooting': 12338, 'treat': 14895, 'chem': 3242, 'physics': 11012, 'freaaaaaaak': 6045, 'compliment': 3658, 'endearing': 5209, 'completed': 3651, 'hunt': 7452, 'betting': 2173, 'slip': 13229, 'tenner': 14381, 'newcastle': 10069, 'relegated': 12021, 'tw': 15027, 'lovers': 8950, 'hills': 7194, 'index': 7657, 'lover': 8949, 'trudy': 14953, 'burnet': 2767, 'hasnt': 6964, 'occasional': 10336, 'reruns': 12114, 'bury': 2780, 'ours': 10578, 'fourteen': 6018, 'ehhh': 5114, 'dentist': 4414, 'root': 12337, 'canal': 2916, 'host': 7355, '_w': 833, 'interview': 7785, 'chat': 3198, 'laura': 8532, 'wna': 16069, 'tan': 14246, 'chilled': 3280, 'leavin': 8585, 'stripper': 13864, 'suppoort': 14061, 'freezing': 6069, 'damnit': 4187, 'stp': 13815, 'step': 13743, 'vhs': 15483, 'connecting': 3730, 'harddrive': 6935, 'mon': 9650, 'oven': 10607, 'dam': 4176, 'zach': 16478, 'canceled': 2921, 'spring': 13617, 'emoticons': 5187, 'reno': 12063, 'sabi': 12455, 'nga': 10086, 'lighting': 8682, 'hazardous': 7007, 'health': 7040, 'happ': 6911, 'mmot': 9598, 'wishin': 16035, '00': 0, 'highschool': 7183, '57am': 386, 'promising': 11525, 'october': 10345, 'rc': 11815, 'sa': 12453, 'western': 15849, 'digital': 4565, 'caviar': 3061, '1tb': 124, 'sata': 12557, '300': 224, 'ncq': 9983, '32mb': 243, 'appreciated': 1489, 'reward': 12193, 'weak': 15760, 'participated': 10780, 'lit': 8751, 'foa': 5903, 'diff': 4546, 'storys': 13811, 'neva': 10061, 'tools': 14754, 'darling': 4227, 'edit': 5067, 'suite': 13980, 'prince': 11443, 'persia': 10935, 'recogns': 11896, 'assignments': 1630, 'installing': 7736, 'redo': 11935, 'claimed': 3398, 'media': 9323, 'stats': 13721, 'tch': 14301, 'mm': 9590, 'jeffree': 7990, 'rawks': 11808, 'difficulties': 4556, 'stepped': 13749, 'addresses': 1006, 'ne': 9986, 'miley': 9480, 'cse': 4049, 'rfid': 12202, 'guinea': 6749, 'extension': 5506, 'modern': 9620, 'studies': 13887, 'subject': 13918, 'guessing': 6738, 'supertramp': 14054, '5jucn': 400, 'jess': 8008, 'props': 11540, 'paying': 10848, 'heartbroken': 7054, 'burnsy': 2771, 'comparison': 3636, 'rocky': 12301, 'mountains': 9755, 'enlgland': 5239, '_k': 703, 'question': 11698, 'storms': 13809, 'partyyy': 10792, 'religious': 12027, 'supernatural': 14049, 'busiest': 2785, 'boss': 2491, 'prep': 11399, 'gifts': 6377, 'bfast': 2184, 'tires': 14659, 'inspection': 7726, 'sticker': 13761, '590': 388, 'repair': 12071, 'partying': 10791, '_baby': 578, 'obsession': 10328, 'editing': 5069, 'whoa': 15928, 'aaahaha': 870, 'unread': 15300, 'ditzy': 4689, 'skits': 13176, 'sadness': 12477, 'awards': 1749, '_idance19': 682, 'thiss': 14518, 'blastinggg': 2303, 'minnie': 9518, 'overjoyed': 10618, 'jleno': 8040, 'minus': 9526, 'vita': 15553, 'rickbaker24': 12223, 'posting': 11329, 'tehe': 14343, 'spread': 13613, 'vibes': 15488, 'outty': 10601, 'couuuurse': 3893, 'fika': 5748, 'thes': 14493, 'hurtin': 7465, '_carter': 595, 'blind': 2326, 'mums': 9833, 'fiona': 5795, 'enought': 5245, 'megan': 9350, 'suitcase': 13979, 'pisses': 11089, 'amara': 1271, 'qqe7b': 11671, 'heres': 7137, 'goodmorning': 6512, 'baptized': 1934, 'graduate': 6594, 'dal': 4174, 'den': 4407, 'officials': 10376, 'twiiter': 15070, 'erm': 5305, 'minibus': 9510, 'useless': 15370, 'laodicean': 8483, 'kavya': 8198, 'spelling': 13552, 'crown': 4016, 'spell': 13548, 'transformers': 14870, 'colombia': 3566, 'ms': 9791, 'yayz': 16314, 'x3': 16238, 'horribly': 7339, 'jackie': 7926, 'embraced': 5178, 'appeal': 1463, 'quality': 11678, 'audio': 1692, 'artists': 1587, 'grizzly': 6681, 'bears': 2043, 'adventures': 1049, 'swine': 14153, 'scaring': 12603, 'thnx': 14527, 'tweeten': 15045, 'tix': 14669, 'deals': 4299, '28': 182, '43': 293, 'blerg': 2319, 'glasses': 6438, 'color': 3567, 'carded': 2966, 'gamestop': 6250, 'ego': 5112, 'refusal': 11966, 'upon': 15329, 'realisation': 11842, 'morrissey': 9723, 'gig': 6378, 'brixton': 2631, 'postponed': 11331, 'monthsish': 9679, 'tto': 14980, 'tafe': 14204, 'tooo': 14755, 'characters': 3174, 'gambit': 6243, 'fold': 5914, 'dayy': 4271, 'travelled': 14887, 'grew': 6666, 'genoese': 6329, 'pasta': 10808, 'sgt': 12829, 'hughes': 7424, 'bacteria': 1849, 'meningitis': 9380, 'disinfected': 4653, 'female': 5696, 'latrine': 8517, 'ski': 13162, 'heh': 7089, 'differences': 4549, 'b3': 1807, 'b4': 1808, 'smelly': 13275, 'vixon': 15556, 'kadi': 8160, 'amazinq': 1283, 'qirls': 11661, 'niqht': 10143, 'quess': 11696, 'qreat': 11672, 'dresses': 4870, 'neglected': 10021, 'surgery': 14081, 'mostley': 9736, 'yellow': 16356, '_read': 779, 'background': 1834, 'apps': 1497, 'stressin': 13852, '_waters': 835, 'hahahahah': 6820, 'indian': 7659, 'cowboy': 3901, 'carnival': 2990, 'costco': 3860, 'print': 11451, 'photobook': 10996, 'weee': 15793, 'blinds': 2328, 'glare': 6435, 'style': 13908, 'dual': 4924, 'reject': 11998, 'mfm7tl': 9429, 'defending': 4352, 'bleach': 2307, 'doggie': 4740, 'forest': 5970, 'programmed': 11507, 'fotos': 6012, 'showr': 13011, 'tidy': 14604, 'hse': 7398, 'stamp': 13674, 'skin': 13168, 'medicine': 9328, 'laughing': 8525, 'laughter': 8527, 'gud': 6735, 'noticed': 10243, '_shediddy': 795, 'phped': 11006, 'successfuly': 13940, 'ems': 5201, 'anywho': 1434, 'roooooooooom': 12336, 'dsi': 4919, 'aim': 1146, '_is_here': 691, 'hun': 7442, 'edna': 5074, 'goldfish': 6493, 'spain': 13508, 'tiff': 14610, 'fuzzy': 6203, '_shep': 796, 'relax': 12009, 'translated': 14873, 'swedish': 14131, 'screaming': 12659, 'noes': 10173, 'standby': 13678, 'plug': 11188, 'fuse': 6191, 'duty': 4971, 'students': 13885, 'union': 15269, 'punters': 11617, 'members': 9366, 'clueles': 3485, 'qantas': 11657, 'lounge': 8924, 'flyertalk': 5896, 'bloggers': 2343, 'panel': 10730, 'listed': 8741, '2pm': 217, '1e15': 118, 'bea09': 2032, 'freaking': 6049, 'spank': 13514, 'cavs': 3064, 'anniversary': 1372, 'stanley': 13683, 'steemer': 13741, '800': 484, 'seattle': 12703, 'matt': 9257, 'spots': 13610, 'memphis': 9374, 'visited': 15545, 'icon': 7515, 'restaurant': 12140, 'settle': 12806, 'hamburger': 6869, 'godawful': 6475, 'mizmind': 9585, 'highness': 7182, 'crystalmariedontluvspiteanymore': 4043, 'appreciation': 1490, '4w8l1': 343, 'haaha': 6791, 'bangs': 1920, 'oops': 10483, 'thaank': 14429, 'lies': 8667, 'embedded': 5175, 'hampstead': 6877, 'constituency': 3745, 'apples': 1476, 'rio': 12248, 'bravo': 2564, 'hunting': 7453, 'sense': 12755, 'aparently': 1443, 'sight': 13064, 'sampler': 12518, 'snot': 13332, 'hubby': 7413, 'ticketttttss': 14598, 'tahong': 14210, 'x24ke': 16230, 'partied': 10783, 'updated': 15319, 'skins': 13170, 'nightmare': 10121, '82k': 490, 'css': 4054, 'woe': 16073, 'mulching': 9823, 'bunny': 2758, 'survive': 14092, 'struggle': 13875, 'screwing': 12671, 'lovee': 8936, 'phogs': 10989, 'edited': 5068, 'docs': 4725, 'appt': 1499, 'hallooo': 6855, 'bayern': 2007, 'stau': 13725, '_l': 714, 'exhibit': 5453, 'factor': 5543, 'child': 3268, 'droped': 4895, '120': 51, 'smarted': 13266, 'bussiness': 2788, 'breaky': 2579, 'russtle': 12437, '_prototype09': 776, 'americanidolislove': 1298, 'ughhhhhh': 15176, 'youu': 16438, 'cuute': 4122, 'goitn': 6490, 'cleanin': 3425, 'somethin': 13401, 'totem': 14795, 'phd': 10967, 'taunting': 14293, '_ii': 685, 'foggy': 5911, 'tiger': 14611, 'claws': 3417, 'nutz': 10299, 'skint': 13171, 'backup': 1845, 'renae': 12056, 'abducted': 895, 'pregnant': 11392, 'changes': 3155, 'sesh': 12797, 'lined': 8712, 'c4s': 2836, 'otherwise': 10568, 'emailed': 5166, 'zuccini': 16515, 'heads': 7034, 'awwwh': 1788, 'directory': 4601, 'toll': 14715, 'highly': 7181, 'correctly': 3851, 'attacking': 1670, 'tei': 14344, 'sency': 12746, 'ghina': 6365, 'tricked': 14918, 'differing': 4553, 'mandy': 9147, 'robbie': 12280, 'monk': 9660, 'jumps': 8133, 'developed': 4492, 'asda': 1600, 'tweetstats': 15056, 'confirmed': 3704, 'suspected': 14100, 'maccy': 9055, 'extra': 5509, 'sudden': 13957, 'evr': 5406, 'brngs': 2633, 'starbux': 13689, 'ten': 14372, 'bucks': 2699, 'boulder': 2508, 'ruled': 12410, 'travelling': 14888, 'ahhhhhh': 1131, 'realx': 11857, 'hamdemic': 6870, 'aporkalypse': 1459, 'parmageddon': 10773, 'rules': 12411, 'tension': 14384, '_cheryl': 598, 'teleport': 14353, 'nonetheless': 10189, 'sunnybank': 14029, 'sep': 12768, 'austira': 1713, 'wrap': 16176, 'ashamed': 1601, 'timid': 14636, 'osocute': 10558, 'bashfulness': 1967, 'quit': 11712, 'chzlw': 3353, '_n_nouns': 751, 'ikr': 7559, 'boystown': 2535, 'eastwood': 5029, '3hours': 270, 'cambie': 2893, 'feedback': 5676, 'bogged': 2403, 'eugh': 5350, 'britains': 2623, 'meagan': 9304, 'mindy': 9502, 'blanket': 2297, 'shed': 12880, 'lint': 8724, 'skirt': 13175, 'roll': 12311, 'stone': 13794, 'charity': 3181, 'cystic': 4144, 'fibrosis': 5727, 'thrilled': 14555, 'teen': 14333, 'sikaflex': 13079, 'caulk': 3052, 'ich': 7511, 'auch': 1688, 'zu': 16514, 'pinkpop': 11076, 'mitchell': 9577, 'tryed': 14965, 'twitterfon': 15098, 'inconclusive': 7645, 'rays': 11810, 'moments': 9638, 'intense': 7760, 'genius': 6328, 'sheer': 12886, 'insanity': 7716, 'sanity': 12539, 'seats': 12702, 'powers': 11357, 'withdrawl': 16045, 'symptons': 14176, 'darrius': 4233, 'symptoms': 14175, 'minne': 9515, 'syamptoms': 14165, 'scores': 12640, 'no1': 10163, 'slot': 13239, '306': 228, 'spectacular': 13537, 'nutt': 10295, 'collar': 3555, 'leash': 8578, 'matched': 9243, 'winter': 16019, 'ps': 11565, 'paranoid': 10757, 'arriving': 1572, 'drat': 4846, 'means': 9312, 'wolftrap': 16081, 'thedailyshow': 14466, 'nooooo': 10201, 'shud': 13020, 'throooooooooooooo': 14561, 'twitts': 15120, 'respond': 12133, 'diver': 4692, 'theory': 14484, '70': 460, 'questions': 11700, '35mins': 255, 'ea': 4991, 'grouchy': 6689, 'rants': 11786, 'ace': 949, 'hardly': 6940, 'waves': 15740, 'shape': 12855, 'sms': 13299, 'solution': 13384, 'promotion': 11529, 'itagg': 7874, '½6': 16527, 'smoky': 13294, 'missus': 9566, 'drizzle': 4891, 'bruised': 2667, 'german': 6343, 'follower': 5923, 'hermine': 7139, 'sudetenland': 13959, 'sweden': 14130, 'refugee': 11963, '1948': 108, 'whitsun': 15925, 'tube': 14986, 'deeeesearted': 4343, 'ns2l55': 10266, 'aceness': 952, 'extreme': 5513, 'cheesy': 3234, 'victoria': 15499, 'slippers': 13231, 'chauncey': 3203, 'sac': 12457, 'speech': 13538, 'careless': 2976, 'dreaming': 4861, 'funerals': 6166, 'education': 5077, 'circus': 3379, 'offtopic': 10380, 'inappropriate': 7629, 'bacardi': 1827, 'phuket': 11009, 'thailand': 14432, 'singapore': 13106, 'sooner': 13423, 'moses': 9728, 'girlfriend': 6401, 'placed': 11114, 'dos': 4789, 'screenshot': 12667, 'qlzp2': 11666, 'kiran': 8308, 'land': 8466, 'jd': 7980, 'yaaaaay': 16277, 'colored': 3569, 'volleyball': 15574, 'everyones': 5393, 'kojikun': 8367, 'seastar': 12699, 'brian': 2597, 'sug': 13970, 'mistake': 9568, 'skipping': 13174, 'carleigh': 2983, 'barley': 1948, 'hr': 7393, 'bby': 2019, 'kitten': 8320, 'babe': 1815, 'nxt': 10305, 'wk': 16059, 'fallin': 5569, 'fatty': 5637, 'cali': 2874, 'filming': 5763, 'musicans': 9852, 'legion': 8602, 'dan': 4193, 'leader': 8562, 'festive': 5708, 'projection': 11514, 'complaints': 3649, '4jcfg': 318, 'cycle': 4136, 'within': 16048, 'quarry': 11681, 'shipley': 12928, 'glen': 6439, 'baildon': 1872, 'moor': 9694, 'woods': 16102, '_x': 844, 'hah': 6811, 'tory': 14785, 'watty': 15736, 'louie': 8920, 'thq': 14548, 'billed': 2226, 'rpg': 12376, 'keyed': 8252, 'shibuya': 12909, 'contact': 3754, 'lenses': 8621, 'shinjuku': 12924, 'stick': 13759, 'hawks': 6996, 'mighty': 9464, 'blackhawks': 2280, 'friendss': 6101, 'aka': 1162, 'tidied': 14603, 'hoovered': 7314, 'out2': 10581, 'bakery': 1880, 'followed': 5922, 'mea': 9302, 'daaay': 4157, 'stung': 13900, 'stinging': 13774, 'nettles': 10055, 'shin': 12920, 'cable': 2844, 'soz': 13499, 'geography': 6337, 'cz': 4146, 'whitney': 15924, 'dangggg': 4209, 'celebrate': 3085, 'rained': 11759, 'devon': 4500, 'harde': 6936, 'nat': 9956, 'goon': 6524, 'subo': 13922, 'britians': 2624, 'contacts': 3757, 'naw': 9975, 'tame': 14244, 'costume': 3864, 'voyager': 15589, 'medical': 9325, 'uni': 15263, 'welcomeee': 15831, 'backkkkk': 1837, 'standing': 13680, 'treatments': 14898, 'script': 12674, 'amaaazing': 1265, 'ladder': 8431, 'collapse': 3553, 'hittin': 7218, 'ole': 10413, 'dusty': 4968, 'trail': 14852, 'morgan': 9707, 'efforts': 5107, 'gentleman': 6331, 'agreement': 1111, 'whoever': 15931, 'wins': 16017, 'pays': 10852, 'sample': 12517, 'frustrated': 6133, 'scare': 12596, 'imaging': 7589, 'staring': 13693, 'faces': 5541, 'basically': 1969, 'insurance': 7751, 'bugs': 2722, 'shine': 12921, 'tad': 14202, 'selfish': 12735, 'rewrite': 12194, 'bosses': 2492, 'summy': 14002, 'cocoa': 3517, 'crispies': 3995, 'chemo': 3247, 'recover': 11916, 'bumbed': 2746, 'laker': 8447, 'welchs': 15827, 'grape': 6619, 'toronto': 14778, '48': 301, 'bluedart': 2371, 'shipment': 12929, 'deliver': 4391, 'databases': 4242, 'ph': 10961, 'macaron': 9051, 'smackdown': 13259, 'aware': 1750, 'pride': 11437, 'blank': 2296, 'sweeet': 14134, 'headlining': 7031, 'doubt': 4801, 'scratched': 12652, 'intentando': 7762, 'intentarlo': 7763, 'daddio': 4164, 'wolverine': 16082, 'ooaf': 10463, 'jackman': 7927, 'sctrahc': 12679, 'creammm': 3956, 'pair': 10700, 'txting': 15136, 'resurrect': 12154, 'replace': 12077, 'tekzilla': 14347, 'ginger': 6395, 'biscuits': 2253, 'urgently': 15351, 'weight': 15814, 'scales': 12591, 'wknd': 16062, 'prayin': 11377, '4wlgi': 355, 'drems': 4867, 'x2eb3': 16234, 'frappuccino': 6039, 'mc': 9285, 'sim': 13088, 'posit': 11308, 'softees': 13368, 'stranded': 13822, 'warren': 15694, 'tech': 14321, 'spec': 13531, 'tempted': 14370, 'leica': 8608, 'palawan': 10710, 'karen': 8182, 'donation': 4765, 'banner': 1929, 'productive': 11493, 'keychain': 8250, 'user': 15372, 'differs': 4554, 'keychains': 8251, '501': 369, 'owner': 10647, 'excitement': 5436, 'kiddin': 8274, 'shy': 13036, 'describe': 4446, 'whispergifts': 15916, 'bridal': 2602, 'registry': 11982, 'workplace': 16140, 'sleeved': 13218, 'grecia': 6646, 'fershure': 5704, 'gaggles': 6227, 'commuters': 3625, 'squished': 13638, 'sellout': 12740, 'fluke': 5892, 'projector': 11515, 'heavens': 7068, 'certainly': 3114, 'warmed': 15685, 'dressing': 4871, 'twtvite': 15129, '3koyqo': 273, 'aptw': 1503, 'engagements': 5222, 'bold': 2412, 'borriing': 2489, 'stuffy': 13897, 'disagree': 4608, 'balancing': 1884, 'chair': 3132, 'sixth': 13148, 'sneers': 13315, 'hung': 7444, 'balls': 1899, 'butttt': 2808, 'august': 1702, '7th': 482, 'feelers': 5681, 'harrassed': 6954, 'twit': 15081, 'knda': 8337, 'hallucinating': 6858, 'boohoo': 2433, 'embarrassing': 5173, 'phonograph': 10994, 'industry': 7672, 'apm': 1453, 'berry': 2151, 'rabbit': 11730, 'ribena': 12214, '_vet': 830, 'von': 15581, 'jo': 8043, 'exsausted': 5499, 'sweaty': 14129, 'chalky': 3134, 'iusedtobescaredof': 7900, 'cooolooorss': 3816, 'composition': 3662, 'tumor': 14996, 'grandas': 6606, 'probable': 11470, 'kisses': 8315, '33': 244, 'omgg': 10431, 'jello': 7992, 'delirious': 4389, 'jimmie': 8031, 'cg': 3120, 'admit': 1020, 'smooth': 13295, 'nor': 10213, 'cajun': 2859, 'refreshed': 11959, 'gotten': 6565, 'lead': 8561, 'sadd': 12465, 'hols': 7268, 'chomp': 3316, 'terribly': 14401, 'chill': 3277, 'bylaurenluke': 2829, 'cudamo': 4066, 'biotch': 2239, 'feed': 5675, '0ut': 20, 'somebody': 13395, 'lappy': 8487, 'norm': 10215, 'attempts': 1675, 'extend': 5503, 'inner': 7700, 'closure': 3468, 'ref': 11950, 'pare': 10761, 'portraits': 11301, 'bangbang': 1917, 'streak': 13836, 'lng': 8791, '13pqtw': 69, 'foolish': 5944, 'iiight': 7553, 'yuck': 16452, 'maids': 9093, 'lc': 8557, '18hrs': 104, '22hrs': 156, 'reminds': 12048, 'catche': 3038, 'anti': 1400, 'gravity': 6632, 'chamber': 3141, '_cheshire_cat_': 599, 'idiotat': 7527, 'tove_liden': 14814, 'nks': 10157, 'tove': 14813, 'ddoodm': 4286, 'shares': 12860, 'czhzb3': 4148, 'draft': 4832, 'rpzmx': 12379, 'moring': 9708, 'stomach': 13791, 'invaders': 7797, 'base': 1961, 'kickass': 8267, 'phast': 10966, 'thick': 14499, '_h786': 665, 'restrictions': 12147, 'suffers': 13966, 'jetsons': 8017, 'finltstones': 5790, 'thumb': 14576, 'pot': 11333, '970': 528, 'shidduch': 12910, 'anythgin': 1424, 'fastsmallballbuster': 5627, 'weirdherout': 15820, 'visionboard': 15543, 'superpower': 14050, 'whalen': 15864, 'peter': 10949, 'gunna': 6761, 'smallville': 13263, 'incredible': 7653, 'reach': 11825, 'injury': 7694, '_ohh': 764, 'thread': 14549, 'results': 12152, 'announced': 1377, 'prd': 11380, 'recommendatiion': 11900, 'lecture': 8589, 'tank': 14251, 'guild': 6745, 'omelets': 10424, 'butter': 2800, 'olive': 10415, 'oil': 10394, 'spits': 13581, 'napkin': 9944, 'jet': 8015, 'carbon': 2960, 'footprint': 5953, '_guy': 662, 'xmen': 16251, 'continues': 3773, 'mazie': 9281, 'kristina': 8383, 'ericka': 5301, 'robin': 12283, 'liers': 8666, 'metabolism': 9411, 'choose': 3318, 'digs': 4566, 'extremely': 5514, 'yummm': 16461, 'hangin': 6900, 'ashington': 1602, 'sept': 12771, 'northumberland': 10224, 'academy': 920, 'lancaster': 8464, 'hel': 7102, 'rosary': 12344, 'mucking': 9814, 'rig': 12237, 'demo': 4403, 'envious': 5267, 'chang': 3151, 'thay': 14460, 'anh': 1351, 'gi': 6368, 'ca': 2839, 'amp': 1317, 'heated': 7061, 'dyed': 4980, 'indie': 7662, 'need2learn': 10000, 'tunes': 15003, '1hour': 119, 'fuunn': 6198, 'az': 1802, 'reques': 12101, 'james': 7942, '_west': 837, 'hubs': 7414, 'missions': 9563, 'brings': 2618, 'daze': 4275, 'canberra': 2919, 'lotion': 8912, 'mohawk': 9622, 'dhcp': 4513, 'bacoor': 1848, 'wahaha': 15626, 'wifi': 15972, 'toinks': 14709, 'wiih': 15974, 'camp': 2905, 'fires': 5805, 'charlene': 3183, 'yeay': 16340, 'inglewood': 7686, '2ndary': 212, 'friendly': 6098, 'ozzy': 10661, 'forgottten': 5981, 'meaning': 9309, 'insonmia': 7725, 'maaaaaan': 9039, 'oui': 10575, 'brushing': 2677, 'bonjour': 2424, 'seriuosly': 12787, 'backache': 1832, 'psh': 11567, 'brilliant': 2614, 'sloooooooooowlyyyyyyyyyyyyy': 13236, 'popcornss': 11277, 'whasup': 15868, 'jen': 7997, 'caption': 2951, 'animal': 1352, 'planet': 11125, 'tiime': 14619, 'fascinating': 5618, 'bedroom': 2082, 'ignored': 7547, 'showin': 13008, 'genes': 6325, 'doente': 4734, 'teniece': 14380, 'pop': 11275, 'taskbar': 14272, 'corrupt': 3852, 'chkdisk': 3301, 'anita': 1357, 'pankraz': 10736, 'liek': 8665, 'famous': 5583, 'luckkyy': 8984, 'trix': 14937, 'engines': 5224, 'mana': 9138, 'accel': 923, 'gooooood': 6535, 'pshh': 11568, 'redskins': 11938, 'jansen': 7954, 'twitterberry': 15091, 'faults': 5640, 'ducked': 4932, 'ga': 6215, 'pleasantly': 11160, 'hhahaa': 7162, 'ahah': 1117, 'lolllllyyyyyyyy': 8839, '2026': 142, '2164': 149, '6790': 439, '9128': 519, 'credit': 3969, 'cue': 4070, 'kenny': 8231, 'underrated': 15232, 'porridge': 11294, 'gloopy': 6447, 'kicked': 8268, 'beckyyy': 2076, 'chackin': 3126, 'discuss': 4639, '87': 494, 'degrees': 4373, 'hagg': 6809, 'bomb': 2416, 'jack': 7920, 'whataburger': 15870, 'cabanaaaaaa': 2841, 'lm': 8781, 'ny': 10306, '2ft': 196, 'laud': 8522, '4a': 305, 'weddin': 15785, 'spike': 13567, 'mahn': 9090, '700': 461, 'compared': 3635, 'ppls': 11364, 'robots': 12286, 'ttfn': 14978, 'waitt': 15636, 'scratchy': 12655, 'biased': 2202, 'hopeful': 7320, 'tweetbeaks': 15042, 'rblpnqte': 11814, 'sheesh': 12887, 'rb': 11812, 'delux': 4397, '5z36j': 409, 'offf': 10368, 'ws': 16202, 'oeiras': 10355, 'portugal': 11303, 'hoorah': 7312, 'e3': 4989, 'g4tv': 6214, 'coverage': 3896, 'comcast': 3582, 'basic': 1968, 'ways': 15748, 'brooke': 2646, 'skate': 13159, 'seshion': 12798, 'arrands': 1560, 'faq': 5603, 'languages': 8478, 'resting': 12144, 'pune': 11613, 'enjoye': 5233, 'nanny': 9941, 'iater': 7500, 'pudding': 11586, 'tis': 14661, 'col': 3540, 'polka': 11243, 'dots': 4797, 'glam': 6431, 'badluck': 1854, 'sidewalk': 13058, 'harvard': 6959, 'cobblestones': 3509, 'buddy': 2704, 'teased': 14318, 'puggy': 11593, 'pughug': 11594, 'ac': 919, 'tmr': 14677, 'foodland': 5941, 'heaps': 7044, 'spa': 13502, 'fort': 5997, 'collins': 3565, 'casa': 3014, 'kent': 8232, 'ale': 1176, 'reached': 11826, 'nasal': 9951, 'trimmer': 14924, 'query': 11693, 'carla': 2982, 'illness': 7567, 'belfast': 2116, 'clean': 3422, 'building': 2728, 'sayed': 12583, 'hassan': 6965, 'looool': 8880, 'ana': 1323, '7ag': 478, 'ele': 5129, 'ye6le3ni': 16318, 'taqa3od': 14262, 'memory': 9373, 'rap': 11787, 'loser': 8907, 'aight': 1142, 'pau': 10830, 'au': 1686, 'blaisdell': 2291, 'arena': 1528, 'cheehee': 3218, 'awwrrite': 1786, 'pocket': 11206, 'suggestions': 13976, 'replacing': 12079, 'ken': 8226, 'onion': 10450, 'chive': 3300, 'self': 12734, 'whoops': 15945, 'lifeline': 8672, 'neat': 9993, 'gary': 6279, 'ticker': 14594, 'morrrning': 9725, 'ti': 14590, '_ianne': 681, 'terrance': 14397, 'center': 3102, 'admitting': 1022, 'plots': 11185, 'romance': 12320, 'feminist': 5699, 'whaaaaaat': 15858, 'waah': 15614, 'aunts': 1707, 'hopfully': 7327, 'wisdom': 16027, 'teeth': 14337, 'startin': 13701, 'goto': 6561, 'lonovala': 8865, 'frenz': 6079, 'transport': 14877, 'arrngmnts': 1573, 'wrkg': 16197, 'wxnwa': 16222, 'yeee': 16342, 'venessa': 15454, 'internal': 7775, 'spare': 13516, 'loft': 8817, '4ward': 345, 'industrial': 7671, 'estate': 5332, 'pirate': 11083, 'ffancy': 5720, 'mile': 9476, 'understandable': 15235, 'chillaxin': 3278, 'bankholiday': 1924, 'everbody': 5374, 'plane': 11123, 'fanboy': 5586, 'jason': 7964, 'mraz': 9788, 'morrison': 9721, 'jealousy': 7983, 'tangible': 14247, 'safesex': 12484, 'topic': 14765, 'gossip': 6554, 'blessings': 2323, 'metal': 9412, 'bat': 1983, 'socked': 13357, 'reckless': 11889, 'kuwait': 8401, 'suv': 14109, 'ticket': 14595, 'function': 6157, 'grrh': 6704, 'kut3': 8400, 'pit': 11095, 'witch': 16039, 'upstate': 15342, 'hick': 7167, 'ethnic': 5345, '_grubb': 659, 'kevin': 8243, 'joining': 8066, 'carrying': 3005, 'backpack': 1840, 'ontario': 10460, 'saddens': 12468, 'delegate': 4380, 'vogue': 15568, 'model': 9615, 'rachel': 11735, 'merh': 9399, 'wasted': 15711, 'impromptu': 7620, 'costa': 3859, 'deciding': 4323, 'workout': 16139, 'du': 4923, 'jour': 8090, '000th': 2, 'stayed': 13727, 'annivarsary': 1371, 'hanson': 6908, 'niceeeee': 10099, 'convinced': 3796, 'les': 8627, 'spat': 13522, 'rite': 12259, 'flor': 5877, 'grets': 6665, 'easports': 5022, 'madden': 9065, 'funeral': 6165, 'aye': 1800, 'ffwd': 5723, 'aac': 878, 'markers': 9195, 'tyring': 15151, 'aruba': 1591, 'soul': 13469, 'fest': 5705, 'bam': 1901, 'ouuhh': 10603, 'tight': 14614, 'bithday': 2262, 'pierre': 11037, 'bl': 2275, 'gogol': 6486, 'bordello': 2472, 'par': 10748, 'prolly': 11517, 'gb': 6296, 'fashioned': 5621, 'civilization': 3388, 'ii': 7552, 'every1': 5383, 'laser': 8500, 'tagging': 14208, 'dominos': 4761, 'martabak': 9213, 'tempting': 14371, 'fattening': 5636, 'comedian': 3585, 'ada': 990, 'acara': 921, 'menarik': 9377, 'lain': 8445, 'vip': 15523, 'ttg': 14979, 'yg': 16381, 'dikasih': 4570, 'tasks': 14274, 'approach': 1492, 'strangers': 13826, 'crowd': 4014, 'hmm': 7226, 'twittter': 15121, 'boredom': 2480, 'split': 13585, 'duties': 4970, 'osu': 10560, 'ath': 1653, 'trng': 14939, 'rcption': 11817, 'chatting': 3201, 'rummage': 12414, 'mcfly': 9291, 'shee': 12881, 'applied': 1480, 'mercedez': 9395, 'snitchsneeker': 13325, 'chest': 3251, 'yew': 16377, 'prooved': 11532, 'bumme': 2747, 'marking': 9200, '135': 64, 'layed': 8546, '23': 158, 'jobfield': 8047, 'unlucky': 15287, 'attempting': 1674, 'return': 12171, 'dayyy': 4272, 'ng': 10085, '_magazine': 732, 'reality': 11847, 'cs4': 4048, 'wishlist': 16037, 'whered': 15897, 'funnier': 6174, 'theyve': 14498, 'gooood': 6529, 'fusion': 6192, 'directly': 4599, 'banking': 1925, 'pal': 10708, 'witless': 16050, 'episodes': 5282, 'nbc': 9979, 'theofficenbc': 14483, 'gervais': 6347, 'skillz': 13167, '_shawn': 794, 'ikea': 7557, 'rooms': 12334, 'twitterfriends': 15100, 'headbutt': 7023, 'football': 5950, 'refreshing': 11960, 'fuming': 6155, 'ebay': 5038, 'agai': 1087, 'wolfram': 16080, 'alpha': 1233, 'cuil': 4072, 'bud': 2700, 'lime': 8701, 'ep': 5274, 'hassn': 6966, 'recommended': 11903, 'harney': 6951, 'sons': 13418, 'steady': 13732, 'income': 7644, 'cab': 2840, 'mastered': 9240, 'assume': 1635, 'heffer': 7088, 'morris': 9720, 'leopold': 8626, 'mapped': 9166, 'served': 12791, 'transparency': 14876, 'opaqueness': 10490, 'birdy': 2244, 'lap': 8484, 'eve': 5364, 'al': 1166, 'hers': 7147, 'twins': 15076, 'asthma': 1638, 'bot': 2494, 'nauseas': 9971, 'chick': 3261, 'fila': 5750, 'crampin': 3927, 'birdies': 2242, 'decrease': 4335, 'sinhalenfoss': 13115, 'ep22': 5275, 'kpor': 8377, 'blocking': 2339, 'cops': 3829, 'tori': 14774, 'remains': 12035, 'latest': 8514, 'channels': 3160, 'view': 15506, 'makin': 9117, 'lenny': 8618, 'bulgaria': 2733, 'maroon': 9205, 'jane': 7947, 'albums': 1173, 'aaaaw': 867, 'tennis': 14383, 'emotional': 5188, 'kan': 8173, 'smfh': 13277, 'reactin': 11828, 'pilots': 11057, 'airline': 1153, 'unpaid': 15294, 'd53dmn': 4150, 'parody': 10775, 'moyles': 9776, 'booomb': 2449, 'mocking': 9612, '12hr': 55, 'ugg': 15166, 'kaylen': 8202, 'finna': 5791, 'uuurgg': 15400, 'marley': 9202, 'earphones': 5012, 'wnna': 16070, 'fkin': 5836, 'costly': 3861, 'jewelry': 8020, 'designer': 4456, '1600th': 89, 'contacting': 3756, 'peaceful': 10864, 'everythin': 5394, 'rushing': 12431, '103f': 31, 'fevered': 5715, 'preschooler': 11406, 'havin': 6988, 'corona': 3845, 'woulda': 16168, 'buddies': 2703, 'pero': 10932, 'yata': 16303, 'boyssss': 2534, 'rafting': 11747, 'tripics': 14926, 'ap': 1439, 'gov': 6567, 'total': 14793, 'nerd': 10037, 'cases': 3016, 'beasted': 2045, 'bctiny': 2026, 'po4me': 11205, 'milestones': 9479, 'projects': 11516, 'contest': 3765, 'groupies': 6695, 'stalkers': 13670, 'buzz': 2817, 'reunited': 12179, 'satisfied': 12561, 'hurting': 7466, 'nellie': 10032, 'blisters': 2334, 'bu': 2690, 'blast': 2301, 'arghhhh': 1536, 'clutters': 3490, 'shhh': 12907, 'litle': 8755, 'yumm': 16460, 'unstoppable': 15307, 'refusn': 11970, 'dissapointment': 4670, 'np': 10264, 'svm3r': 14115, 'fucken': 6144, 'recorded': 11908, 'telly': 14363, 'sesion': 12799, 'sorce': 13446, 'sincerely': 13104, 'yonkers': 16405, 'newspaper': 10078, 'furloughed': 6186, 'noooootttttttt': 10210, 'ls': 8969, 'taker': 14220, 'splodge': 13586, 'ketchup': 8240, 'arrival': 1568, 'ship': 12927, '_lou27': 728, 'glasgow': 6436, 'doh': 4745, 'exclusive': 5440, 'cruel': 4021, 'stunk': 13901, 'headphones': 7032, 'buggy': 2720, 'staff': 13659, 'patrons': 10826, 'blueberry': 2370, '_supernatural_': 814, '4w8cw': 342, 'mishaaaaaaaa': 9548, 'campaign': 2906, 'held': 7103, 'essential': 5329, 'psps': 11572, 'il': 7560, 'delays': 4378, 'cancellations': 2923, 'bite': 2260, 'sympathize': 14172, 'ganna': 6260, '_erica': 633, 'lonnngg': 8864, '26': 176, 'beatrice': 2053, 'guide': 6743, 'hallway': 6859, 'worries': 16155, 'ambers': 1288, '4am': 306, 'cooked': 3802, 'todayy': 14695, 'bounds': 2511, 'whyd': 15954, 'bailey': 1873, 'clipped': 3450, 'billing': 2227, 'fee': 5674, 'countries': 3878, 'setup': 12809, '5000': 367, 'mamma': 9135, 'bore': 2475, 'hat': 6967, 'mir': 9532, 'echt': 5044, 'angetan': 1343, '90210': 512, '½sst': 16548, 'grï': 6723, '½en': 16531, 'maine': 9099, 'effing': 5105, 'torta': 14780, 'queer': 11691, 'conscience': 3736, 'efteling': 5109, 'expected': 5466, 'silverlight': 13086, 'content': 3761, 'vine': 15516, 'aspx': 1620, 'tony': 14746, 'santa': 12543, 'cruz': 4036, 'jamba': 7941, 'gah': 6228, 'roseburg': 12347, 'dreamin': 4860, 'hook': 7305, 'marlon': 9203, 'brando': 2559, 'godfather': 6482, 'browsing': 2660, 'terrible': 14400, 'simplicity': 13095, 'useful': 15368, 'watchin': 15718, 'gilmore': 6389, 'girlz': 6409, 'accomplished': 941, 'boi': 2405, 'jammin': 7946, 'stephens': 13747, 'almond': 1222, 'festivities': 5709, 'earl': 5001, 'bev': 2176, '_zol': 849, 'password': 10805, 'medium': 9334, 'heffas': 7087, 'jasmin': 7963, 'beginning': 2101, 'nita': 10147, 'excit': 5432, '2me': 205, 'u2': 15155, 'bettering': 2171, 'ach': 954, 'tire': 14654, 'reaaaally': 11824, 'batman': 1992, 'loove': 8898, 'chores': 3325, 'apt': 1501, 'hallo': 6854, 'sheffield': 12889, 'romeo': 12324, 'juliet': 8126, 'bryant': 2680, 'wallinwood': 15656, 'montagues': 9670, 'capulets': 2953, '_miss': 744, 'vh1': 15482, 'preview': 11428, 'keypad': 8256, 'sundays': 14020, '½ve': 16551, 'sj': 13155, '45pm': 298, 'cinelux': 3369, 'almaden': 1220, 'contractions': 3776, 'wxidk': 16219, 'psp': 11570, 'fathers': 5633, 'userid': 15373, 'vishnupsp': 15540, 'heylo': 7154, 'johnn': 8060, 'awesomee': 1764, 'cuzz': 4127, 'webcast': 15776, 'logged': 8821, 'jars': 7961, 'duper': 4964, 'kelly': 8224, 'tao': 14257, 'inxs': 7821, 'bass': 1977, 'yelling': 16355, 'tet': 14412, 'outing': 10588, 'marwell': 9220, 'marines': 9190, 'protect': 11542, 'serve': 12790, 'bif': 2210, 'eom': 5272, 'dishes': 4651, 'rep': 12070, 'cramps': 3929, 'november': 10258, 'maintenance': 9105, 'despair': 4465, 'manors': 9160, 'proved': 11550, 'psyched': 11574, 'gkr': 6425, 'logical': 8824, 'hollered': 7260, '3x': 282, 'dnw': 4718, 'roses': 12349, 'ins': 7713, 'daa': 4154, 'crewww': 3983, 'malaria': 9122, 'affects': 1065, 'bell': 2126, 'wormy': 16151, 'labyrinth': 8423, 'tomorrowland': 14730, 'confirmation': 3703, 'tours': 14811, 'tww': 15132, 'solving': 13388, 'logic': 8823, 'puzzles': 11645, 'englands': 5226, 'becoming': 2079, 'lamer': 8459, 'crosse': 4011, 'discrimination': 4638, 'frustrating': 6134, 'advanced': 1045, 'outdoor': 10584, 'shelter': 12895, 'glove': 6451, 'helpline': 7122, 'meann': 9311, 'medisoft': 9329, 'hammock': 6874, 'tree': 14901, 'husband': 7469, 'mf88dz': 9428, 'nt': 10270, 'mayb': 9273, 'networking': 10057, 'kennel': 8230, 'goblet': 6472, 'menu': 9391, 's0ulja': 12449, 'b0y': 1804, 'te': 14304, 'trending': 14907, 'soulja': 13471, 'itsjeff': 7890, 'garbo': 6267, 'fake': 5563, '_2gnt': 548, 'skill': 13165, 'bio': 2235, 'suckkkk': 13949, 'bahaha': 1867, 'tribbles': 14914, 'tracking': 14835, 'devices': 4498, 'htc': 7401, 'costs': 3862, 'jazzercise': 7974, 'mrs': 9789, 'underwood': 15239, 'chaperone': 3170, 'agent': 1098, 'indoors': 7669, 'buffett': 2713, 'ftw': 6141, 'appear': 1465, 'perfume': 10923, 'tarot': 14267, 'cards': 2969, 'storysize': 13812, 'liner': 8713, 'glue': 6453, 'poster': 11325, 'earnt': 5011, 'financial': 5772, 'grand': 6603, 'wheres': 15898, 'nfg': 10084, 'cow': 3900, 'epicfail': 5280, 'goals': 6469, 'knock': 8345, 'charla': 3182, 'stevie': 13756, 'reschedule': 12116, 'competition': 3641, 'sims': 13099, 'luna': 8998, '6pm': 456, 'grovelled': 6698, '9am': 532, 'mann': 9156, 'das': 4236, 'ist': 7872, 'lustig': 9014, 'armer': 1549, 'macs': 9062, 'charging': 3179, 'garantiefall': 6265, 'cleanup': 3428, 'teratoma': 14389, 'cavity': 3063, 'experiencing': 5475, 'male': 9125, 'pattern': 10828, 'baldness': 1887, '4jg09': 321, 'gh': 6359, 'attend': 1676, 'battling': 2003, 'moviesss': 9769, 'sex': 12820, 'howz': 7390, 'estrella05azul': 5337, 'wordpress': 16126, 'impressive': 7618, 'macbook': 9053, 'blink': 2329, 'billy': 2230, 'bragg': 2545, 'colin': 3549, 'blunstone': 2375, 'dump': 4951, 'hacked': 6798, 'pressure': 11419, 'subside': 13928, 'seizures': 12729, 'tumors': 14997, 'niggling': 10113, 'adoarble': 1025, 'booboo': 2431, 'md': 9300, 'noises': 10182, 'arguing': 1538, 'entertaining': 5254, 'cholesterol': 3314, 'lunchbreak': 9000, 'lactose': 8429, 'oopse': 10484, 'earnest': 5009, '40404': 289, '32665': 242, 'commentary': 3607, 'daves': 4253, 'shatner': 12868, 'frosties': 6121, 'centre': 3104, 'knackers': 8336, 'sickkkkk': 13051, 'emma': 5183, 'diversity': 4693, 'cherokee': 3249, 'ability': 900, 'grammar': 6600, 'nazi': 9977, 'elsewhere': 5161, 'intentionally': 7764, 'simply': 13096, 'malakas': 9120, 'ulan': 15190, 'behave': 2107, 'thaw': 14459, 'nicely': 10100, 'liking': 8694, 'alice': 1191, 'ramei': 11774, 'excellant': 5424, 'scene': 12608, 'translate': 14872, 'xmind': 16252, 'pie': 11030, 'buttercup': 2801, 'married': 9208, 'hehehehehe': 7095, 'includes': 7641, 'shoe': 12954, 'trading': 14842, 'songgoeswrongs': 13412, '400': 286, 'sumone': 14003, 'sweetheart': 14141, 'gahhh': 6231, 'dunnooooooo': 4959, 'confuzzzledd': 3716, '_aid16': 567, 'rub': 12392, 'cuuuute': 4124, 'gfail': 6357, 'battles': 2001, 'amazingly': 1282, 'fathom': 5634, 'ignorent': 7548, 'aholes': 1136, 'overturn': 10630, 'prop': 11533, '92': 522, 'turning': 15012, 'roo': 12326, 'oop': 10482, 'returning': 12173, 'floor': 5874, 'calcutta': 2867, 'delhi': 4383, 'lucknow': 8985, 'absence': 909, 'hur': 7455, 'spirituality': 13577, 'realizing': 11851, 'potty': 11339, 'knocked': 8346, 'spot': 13607, 'wefollow': 15809, 'perth': 10943, 'eleven': 5145, 'barbeque': 1938, 'station': 13718, 'stink': 13776, 'birthdays': 2250, 'bra': 2539, 'anyhow': 1419, 'spite': 13580, 'outta': 10598, 'space': 13503, 'xo': 16255, 'https': 7407, 'bb': 2011, 'spicy': 13562, 'lentil': 8622, 'gap': 6262, 'aaahh': 871, 'spose': 13606, 'weeek': 15795, 'fasho': 5623, 'shelters': 12897, 'natalyy': 9958, 'rotten': 12355, 'averages': 1734, 'facepanda': 5540, 'iis': 7555, 'geordanos': 6338, 'thin': 14503, 'crust': 4032, 'olives': 10417, 'peperoni': 10904, 'mushroom': 9847, 'maria': 9183, 'accidentally': 934, 'disconnected': 4627, 'catalog': 3035, 'jitsu': 8034, 'appearing': 1468, 'photoshoot': 11001, '116': 44, 'amadeus': 1266, 'mozart': 9777, 'chorus': 3327, 'ey': 5516, 'speedo': 13543, 'kudos': 8394, 'disgusting': 4649, 'goddamn': 6477, 'loooove': 8889, 'godddd': 6480, 'dothebouncy': 4796, 'smf': 13276, 'shameless': 12850, 'plugging': 11190, 'rangers': 11783, 'forum': 6003, 'bk': 2271, 'sumtime': 14007, 'soaps': 13344, 'rents': 12069, 'ahve': 1138, 'yessum': 16372, 'oct': 10344, 'speeds': 13545, 'unlimited': 15285, 'further': 6189, '34': 252, 'naman': 9927, 'films': 5764, 'insufficient': 7747, 'fundage': 6160, 'watchmen': 15720, 'keyboards': 8249, 'pohaku': 11218, '12st': 57, 'spinning': 13574, 'shack': 12832, 'localgovcamp': 8805, 'letter': 8637, 'contents': 3763, 'mybrute': 9881, 'yhana09': 16383, 'completing': 3653, 'quizzes': 11719, 'nigguh': 10114, 'bali': 1890, 'afterwards': 1082, 'smack': 13258, 'energy': 5220, 'hip': 7203, 'kicks': 8271, 'grrrr': 6706, 'bull': 2736, 'arthritus': 1581, 'hobo': 7237, 'cuy43t': 4125, 'shoo': 12960, 'woosoo': 16120, 'gant': 6261, 'driveing': 4884, 'nutts': 10297, 'goddaughters': 6479, 'categories': 3042, 'remove': 12054, 'heineken': 7098, 'yucky': 16453, 'adopt': 1027, 'slack': 13189, 'heartless': 7057, 'huwwts': 7472, 'texted': 14418, 'crawl': 3944, 'act': 973, 'lmfaoo': 8787, 'ht': 7400, 'monster': 9666, 'brush': 2676, 'possibly': 11320, 'noah': 10164, 'whahahah': 15862, 'definition': 4362, 'crackers': 3920, 'whale': 15863, 'visitor': 15548, 'reinstall': 11995, 'easily': 5021, 'elusive': 5163, 'deff': 4354, 'weekened': 15801, 'instrumentals': 7746, 'shelby': 12891, 'twp': 15126, 'yeh': 16351, 'breaking': 2576, '_sml': 802, 'elitecamp': 5149, 'beaten': 2048, 'pulp': 11604, 'impression': 7616, 'topify': 14768, 'mong': 9657, '__marie': 561, 'sticks': 13765, 'hc': 7012, 'trade': 14839, 'racoons': 11741, 'skunks': 13180, 'livin': 8772, 'shooeessss': 12961, 'logging': 8822, '200km': 138, 'errands': 5310, 'avoided': 1741, 'cinder': 3367, 'blocks': 2341, 'dantas': 4215, 'porky': 11292, 'beavs': 2063, 'rconp': 11816, 'pulling': 11603, 'vegetarian': 15449, 'soda': 13360, 'majorly': 9108, 'cutback': 4112, 'carbs': 2963, 'yummyyy': 16465, 'proposed': 11538, '15th': 85, 'conversating': 3787, 'nintendogs': 10141, 'upsetting': 15338, 'roundtable': 12363, 'hamburg': 6868, 'detlev': 4485, 'fischer': 5813, 'accessibility': 931, 'bitv': 2267, 'xing': 16249, '333622': 250, 'awesom': 1762, 'snuggle': 13337, 'wut': 16209, 'harsh': 6958, 'archuleta': 1522, 'turned': 15010, '_finn_': 640, 'ding': 4579, 'beautifullll': 2060, 'eu': 5348, 'probs': 11478, 'satellite': 12559, 'provider': 11556, 'volunteering': 15578, 'syndrome': 14181, 'indiana': 7660, 'hu': 7408, '_newamerykah_': 753, 'alllllllllllllllllllllllllllll': 1213, 'wakenbake': 15638, 'ocean': 10341, 'glass': 6437, 'experts': 5478, 'exercise': 5447, '90': 511, 'jillian': 8028, 'optimistic': 10516, 'hunny': 7451, 'nasty': 9955, 'jig': 8025, 'shakalohana': 12837, 'wavez': 15741, 'surfin': 14079, 'trak': 14862, 'hoilday': 7245, 'zebra': 16487, 'maar': 9048, 'lurk': 9011, 'mode': 9614, 'lock': 8809, 'poured': 11345, 'whiskey': 15914, 'joan': 8044, '5jbp3': 394, 'nsty': 10268, 'remind': 12043, 'blooming': 2355, 'clematis': 3434, 'princess': 11447, 'lingerie': 8716, 'parties': 10784, 'paycheck': 10844, '_langley': 719, 'igbaras': 7542, 'cheeto': 3235, 'puffs': 11589, 'tiring': 14660, '77': 471, 'faves': 5644, 'unproductive': 15298, 'grandmother': 6611, '7am': 479, 'plotting': 11186, 'muahahaha': 9807, 'jackets': 7925, 'headach': 7019, 'bust': 2790, 'accompany': 939, 'cement': 3099, 'rows': 12373, 'pixels': 11105, 'sewing': 12819, 'divorcing': 4697, 'bear': 2039, 'hank': 6905, 'thompson': 14531, '5jboh': 393, 'stumpy': 13899, 'chemics': 3244, 'smelling': 13273, 'repellant': 12074, 'stinks': 13777, 'marykay': 9222, 'sendai': 12748, 'hopped': 7330, 'swwwaaaaggg': 14163, 'oooonnnn': 10477, 'england': 5225, '_uk': 826, 'myst': 9896, 'toooo': 14756, 'isis': 7860, 'tapeworm': 14260, 'wanting': 15671, 'chaper': 3169, 'beover': 2148, 'huggles': 7422, 'hovering': 7380, 'pissy': 11093, 'decisions': 4325, 'chauffer': 3202, 'tmrw': 14678, 'ned': 9998, 'beathroom': 2050, 'outsie': 10596, 'daylight': 4265, 'deadlines': 4292, 'cuppa': 4086, '_starr': 808, 'hawkesbury': 6995, 'windsor': 16005, 'dj': 4703, 'fcs': 5658, 'suggest': 13972, 'algae': 1185, 'antics': 1403, 'parella': 10762, 'marcia': 9175, 'twitte': 15088, 'celebrating': 3087, 'generations': 6324, 'bleeeeah': 2315, 'stairs': 13664, 'toe': 14701, 'wasssup': 15707, 'cocoliciousness': 3519, 'nhl': 10089, 'pens': 10896, 'kings': 8305, '2010': 140, 'errr': 5314, 'espressos': 5325, 'latte': 8518, 'noon': 10195, 'mish': 9546, 'ladie': 8432, 'jungle': 8138, 'hotel': 7361, '_mcflyy': 738, 'finaly': 5771, 'mwahaha': 9875, 'podcasting': 11212, 'snap': 13307, 'nod': 10170, 'thxs': 14587, 'whine': 15908, 'slo': 13235, 'intellectually': 7756, 'severing': 12817, 'disagreement': 4609, 'peppermint': 10906, 'mochas': 9609, 'frappachinos': 6038, 'addicti': 995, 'hoo': 7302, 'crossing': 4013, 'hacks': 6800, 'humbug': 7435, 'nh': 10088, 'bï': 2832, 'vï': 15601, '½o': 16543, 'kho': 8262, 'hypervenilating': 7489, 'keith': 8219, 'urban': 15348, 'belt': 2134, 'easactive': 5018, 'paused': 10835, 'resistance': 12129, 'torn': 14776, 'morn': 9709, 'mirror': 9534, 'luvvv': 9022, 'aquestion': 1506, 'moon': 9686, 'recommen': 11898, 'bes': 2152, 'rockstar': 12299, 'claps': 3403, 'forwar': 6004, 'kidney': 8279, 'tonsils': 14745, 'hilariously': 7192, 'champagne': 3142, 'slingalink': 13228, 'evice1': 5399, 'daughters': 4250, 'kindergarden': 8297, 'xd': 16247, 'viggo': 15510, 'miami': 9440, 'boyle': 2532, 'ridding': 12229, 'dangero': 4206, 'waaaaaaa': 15607, 'throbbing': 14560, 'nooooothing': 10209, 'studyhall': 13890, '_h': 664, 'daisy': 4173, 'amazin': 1280, 'bunch': 2755, 'homes': 7280, 'alrightttt': 1241, 'netball': 10051, 'nowww': 10263, '600th': 417, 'byeeeee': 2828, 'soothes': 13439, 'coldplay': 3545, 'twittering': 15103, 'jag': 7932, 'alternator': 1255, 'clock': 3452, 'frd': 6044, 'mission': 9561, 'yung': 16466, 'scariest': 12602, 'rick': 12222, 'thurs': 14583, 'nikeplus': 10130, 'arrive': 1569, 'acting': 974, 'rear': 11860, 'marilyn': 9188, 'seria': 12782, 'asses': 1625, 'overwhelmed': 10631, '_emily_young_': 630, 'okey': 10403, 'gaining': 6236, 'bread': 2570, 'peanut': 10866, 'dragging': 4835, 'lied': 8664, 'baddd': 1851, 'slap': 13193, 'stretch': 13855, 'meme': 9368, 'hop': 7316, 'por': 11290, 'favor': 5645, 'dzcpg3': 4987, 'chained': 3130, 'webcom': 15777, 'yul': 16456, 'chai': 3128, 'checkin': 3213, 'hottest': 7366, 'assing': 1631, 'g1': 6210, 'biznesssss': 2269, 'paniniii': 10735, 'ange': 1338, '_phillips': 773, 'stock': 13784, 'hoped': 7318, 'cheaptweet': 3207, 'syopvd': 14182, 'sin': 13101, 'shamed': 12849, 'unemployed': 15243, 'meself': 9401, 'whack': 15861, 'tweethug': 15050, '21u': 152, 'elephants': 5141, 'magnificent': 9085, 'pleease': 11174, 'sonetime': 13410, 'flavor': 5856, 'belgian': 2117, 'considered': 3739, 'truss': 14959, 'failing': 5551, 'strip': 13862, 'mikee': 9471, 'mike': 9470, 'miffed': 9461, 'tbh': 14299, 'vic': 15491, '_da_': 611, '60': 414, 'mangoes': 9151, 'baka': 1876, 'naglilihi': 9914, 'ka': 8158, 'lang': 8476, 'carmen': 2989, 'respect': 12131, 'dryer': 4915, 'kno': 8344, 'shineeeee': 12922, 'fk': 5835, 'replenished': 12081, 'bathing': 1988, 'suits': 13981, 'anthony': 1398, 'rapp': 11790, 'defective': 4350, 'skid': 13163, 'zoning': 16508, 'shul': 13025, 'jon': 8072, 'kate': 8187, 'drove': 4899, 'mazda': 9279, 'rx8': 12442, 'angela': 1340, 'utterly': 15395, 'yaaaay': 16278, 'tila': 14622, 'tierd': 14607, 'delivery': 4394, 'fringe': 6110, 'comix': 3600, 'radio': 11744, 'disney': 4660, 'guilty': 6748, 'yogulicious': 16398, 'sour': 13481, 'sally': 12503, 'competitor': 3645, 'chances': 3148, 'minimal': 9511, 'po': 11204, 'tail': 14212, '_01': 538, 'snow': 13333, 'solitaire': 13382, 'fiddle': 5731, 'playable': 11144, 'joker': 8069, 'ps3': 11566, 'exclusivity': 5441, 'arkham': 1545, 'twistory': 15080, 'diapers': 4525, 'feeding': 5678, 'iam': 7496, 'greaaat': 6637, 'adress': 1036, 'loco_crime_1st': 8813, 'wha': 15856, 'hacky': 6801, 'sack': 12458, 'sans': 12542, 'fuckn': 6146, 'drew': 4873, 'macdonalds': 9056, 'mango': 9150, 'medley': 9335, 'dammm': 4184, 'considerably': 3738, 'sorr': 13451, 'babysit': 1823, 'nare': 9948, 'nightall': 10118, 'frankie': 6035, 'claires': 3401, '_lord': 727, 'overcast': 10610, 'ohhhhh': 10387, 'brunch': 2672, 'wide': 15963, 'zimmer': 16501, 'frame': 6028, '_hall': 666, 'homies': 7288, 'answered': 1391, 'postal': 11322, 'upcoming': 15317, 'documents': 4729, 'jog': 8056, 'pat': 10813, 'darren': 4230, 'linkedin': 8720, 'darrenmonroe': 4231, 'tucson': 14989, '18th': 106, 'finest': 5782, 'vol': 15571, 'lookn': 8873, 'colorado': 3568, 'sunrise': 14032, 'caity': 2858, 'wossy': 16164, 'brum': 2670, 'controversial': 3785, 'estimate': 5334, 'timeframe': 14631, 'colab': 3541, 'sw': 14116, 'hopefull': 7321, 'hooray': 7313, 'healing': 7038, 'matey': 9250, 'ssssssssssmack': 13649, 'ww': 16213, 'digusted': 4567, 'wroclaw': 16198, 'poland': 11231, 'kosmo': 8374, 'eovvn': 5273, 'buried': 2764, 'entourage': 5262, 'bittersweet': 2265, 'bleaching': 2308, 'yeahhhh': 16328, 'ali': 1188, 'justwatched': 8152, 'escaped': 5318, 'discovery': 4635, 'imprisoned': 7619, 'newlyweds': 10072, 'loosing': 8897, 'bbbbrrrrrrrr': 2012, 'tweeters': 15047, '_rocks': 785, 'spurted': 13626, 'fanta': 5597, 'uno': 15290, 'rprl0': 12378, 'midnight': 9457, 'fri': 6087, 'lower': 8965, 'stings': 13775, 'sugar': 13971, 'havnet': 6991, 'butlers': 2797, 'hispanic': 7209, 'hendrix': 7127, 'cosmos': 3856, 'ebm': 5039, 'waaaay': 15611, 'katherine': 8189, 'zachy': 16479, 'triple': 14927, 'stocktwits': 13787, 'clutter': 3489, 'twittersphere': 15112, 'goody': 6520, 'iptv': 7834, 'application': 1478, 'compiled': 3646, 'slowly': 13246, 'sorting': 13463, 'musiq': 9857, 'soulchild': 13470, 'hamilton': 6871, 'ugggghhhh': 15167, 'begun': 2105, 'documentation': 4728, 'viewers': 15507, 'eagle': 4996, 'ustream': 15383, 'funnyy': 6181, 'original': 10546, 'obu': 10329, 'ryaaaaaaaaaaaaan': 12444, 'snjen': 13326, 'bride': 2603, 'pinch': 11061, 'foreigner': 5967, '2xjoc': 222, 'cloooseee': 3458, 'mocha': 9608, 'frapp': 6037, 'simple': 13094, 'safari': 12480, 'toolbar': 14753, 'calculator': 2865, 'calcs': 2864, 'havaianas': 6981, 'molded': 9632, 'watchers': 15717, 'shiggity': 12915, 'shwa': 13033, 'coda': 3521, '99usd': 531, 'sunscreen': 14034, 'simpson': 13097, 'shezza': 12905, 'paula': 10832, 'ohshnapsss': 10391, 'blair': 2289, 'yeeeah': 16343, 'bake': 1877, 'cookies': 3806, 'bck': 2024, 'thts': 14573, 'tricks': 14919, '_kearley': 705, 'egg': 5110, 'whites': 15922, 'grain': 6599, 'interviewed': 7786, 'wil': 15977, 'gardeners': 6270, 'statistic': 13720, 'playback': 11145, 'comet': 3590, 'sensex': 12757, 'travis': 14891, 'clark': 3406, 'hilaaaaarious': 7190, 'subscriptions': 13927, 'nzz': 10315, 'economist': 5050, 'costsavings': 3863, 'luckily': 8982, 'versions': 15473, 'hiya': 7222, 'exhilerating': 5455, 'william': 15985, 'kong': 8368, 'airplane': 1154, 'payday': 10846, 'fishes': 5815, 'kindest': 8299, 'roomies': 12332, 'clwn': 3491, 'celebrated': 3086, 'softball': 13367, 'robina': 12284, 'favourtie': 5652, 'axsujx': 1798, 'awes': 1760, 'gooooooooood': 6540, 'texting': 14420, 'virus': 15536, 'tweeps': 15037, 'worksheet': 16142, 'wb': 15754, 'sniffle': 13321, 'cornish': 3843, 'countryside': 3880, 'british': 2625, 'hol': 7246, 'ailun': 1145, '6rww': 457, 'reeeally': 11944, 'stumble': 13898, 'weeee': 15794, 'coordinating': 3824, 'lovebug': 8934, 'weepies': 15806, 'goodnights': 6515, 'agh': 1103, 'pleaser': 11169, 'foreigners': 5968, 'pretty_mess': 11427, 'psychology': 11576, 'statements': 13714, 'parallel': 10755, 'devastated': 4490, 'seat': 12700, 'dreamwidth': 4865, 'martin': 9214, 'japanese': 7957, 'rochelle': 12290, 'cater': 3044, 'produce': 11488, 'houses': 7377, 'smelled': 13272, 'alley': 1204, 'maaad': 9044, 'basement': 1964, 'whitley': 15923, 'diwali': 4698, 'saddi': 12471, 'dilli': 4573, 'disorder': 4661, 'jjj': 8037, 'gal': 6238, 'pals': 10719, 'phoenix': 10987, 'editions': 5071, 'greatest': 6642, 'purse': 11632, 'soulmate': 13472, 'frustraded': 6132, '_bob7': 587, 'vet': 15479, 'xrays': 16262, 'enter': 5248, 'giveaway': 6417, 'elmo': 5156, 'finshed': 5793, 'annnd': 1373, '44am': 295, 'sleeps': 13214, 'lift': 8677, 'splints': 13584, 'fey': 5718, 'slays': 13199, '30rock': 233, 'inbruges': 7632, 'hitman': 7216, 'tale': 14227, 'pale': 10711, 'shirtless': 12935, 'footballers': 5951, 'attractive': 1685, 'tweetie': 15051, 'ghetto': 6363, 'seo': 12766, 'voodoo': 15582, 'noarchive': 10165, 'hides': 7172, 'beast': 2044, 'riding': 12235, '_holden': 674, 'herself': 7148, 'decides': 4322, 'client': 3442, 'embracing': 5179, 'tears': 14316, 'shallow': 12845, 'airy': 1157, 'ttyl': 14982, 'moods': 9684, 'frickin': 6089, 'slithering': 13233, 'snakes': 13306, 'frightening': 6107, 'detail': 4480, 'preparing': 11401, 'sermons': 12788, 'powerpoint': 11356, 'slides': 13224, 'ed': 5057, 'speedracher': 13544, 'strongly': 13871, '12seconds': 56, 'spock': 13587, 'prototype': 11546, 'mentor': 9390, 'heyya': 7156, 'formulas': 5988, 'sudoku': 13960, 'theirs': 14472, 'giant': 6369, 'wet': 15852, 'stinky': 13778, 'loaner': 8801, 'bein': 2111, '7day': 480, 'moro': 9717, 'kimba': 8293, 'diaries': 4526, 'nicky': 10106, 'bette': 2169, 'blumenthal': 2374, 'toodaayy': 14749, 'researching': 12124, 'dislike': 4655, 'dublin': 4929, 'whatcha': 15871, 'originally': 10547, 'thankujesus': 14443, 'beyeblessed': 2180, 'traces': 14831, 'rdj': 11820, 'dreadfully': 4855, 'twitterology': 15106, 'offshore': 10379, 'behalf': 2106, 'cet0': 3117, 'searching': 12694, 'wavy': 15743, 'hairstyle': 6847, 'fondue': 5932, 'lava': 8536, '10pm': 40, 'rate': 11795, 'inputting': 7710, 'excitin': 5437, 'shade': 12834, 'native': 9964, 'wildflowers': 15980, 'notion': 10248, '_monk': 747, 'elp': 5158, 'medication': 9326, 'twilightguy': 15073, 'kalebnation': 8167, 'spamming': 13512, 'cashis': 3021, 'jenkins': 7998, 'shitttt': 12942, 'blockbuster': 2336, 'zealand': 16486, 'coraline': 3834, 'puke': 11596, 'ahaa': 1116, 'techno': 14326, 'backs': 1843, 'ui': 15187, 'snappier': 13309, 'bugging': 2719, 'launched': 8529, 'below': 2133, 'hadfr': 6804, 'smacked': 13260, '_in_forks': 687, 'static': 13716, 'bar': 1935, 'shatranjanpoli': 12869, '26498457': 178, 'andheri': 1329, '26733333': 179, 'ki': 8263, 'jai': 7934, 'ho': 7234, 'saddened': 12467, 'husker': 7470, '66st1': 435, 'freee': 6060, 'loongerrr': 8877, 'sadface': 12473, 'sob': 13345, 'girrrrrrrrlll': 6413, 'jeeeeez': 7986, 'misha': 9547, 'directing': 4596, 'whywhywhy': 15956, 'essays': 5328, 'killer': 8286, 'manage': 9140, 'champions': 3144, 'suffolk': 13969, 'mb': 9283, 'neil': 10028, 'jodi': 8051, 'terminator': 14393, 'trs': 14950, 'wikipedia': 15976, 'brighton': 2613, 'esp': 5321, 'iiii': 7554, 'handedly': 6885, 'crane': 3931, 'hairbrush': 6837, 'disinfect': 4652, 'beware': 2179, 'visialvoicemail': 15541, 'sync': 14177, 'seamless': 12689, 'toro': 14777, 'quebec': 11688, 'tripped': 14928, 'awarded': 1748, 'dork': 4788, 'appointment': 1483, 'earned': 5008, '_devil1': 617, 'quiz': 11716, 'wooden': 16101, 'couch': 3867, 'guessed': 6737, 'phoenixfm': 10988, 'php': 11005, 'herniated': 7141, 'disc': 4622, 'lluuvv': 8780, 'evenn': 5367, 'moreee': 9703, 'antidisestablishmentarianism': 1404, 'excruciating': 5442, 'heeder': 7077, 'irma': 7843, 'vep': 15467, 'rehearsals': 11991, 'moveable': 9762, 'fox': 6020, 'refresh': 11958, 'mimcy': 9495, 'mornining': 9715, 'easton': 5028, 'loner': 8851, 'difference': 4548, 'whatsoever': 15877, 'tpt': 14828, 'absolutly': 913, 'stressors': 13854, 'force': 5959, 'becase': 2069, 'foods': 5942, 'viruses': 15537, 'luvly': 9019, 'recory': 11912, 'disconnects': 4628, 'uber': 15157, 'steam': 13739, 'willdo': 15984, 'anythig': 1425, 'acknowledge': 966, 'inevitable': 7675, 'opendns': 10493, 'dns': 4716, 'goooooodmorning': 6537, 'sleeeeeeepz': 13201, 'myhouse': 9884, 'gusying': 6767, 'bourbon': 2514, 'branch': 2554, 'zeitgeist': 16488, 'shh': 12906, 'snob': 13328, 'scarred': 12605, 'amie': 1302, 'cody': 3525, 'ddyyd6': 4287, 'boooooooo': 2453, 'lopatcong': 8899, '4w1s0': 336, 'carey': 2978, '_bloggerific': 586, 'innocent': 7704, 'sell': 12738, 'profitable': 11501, 'graders': 6591, 'tigers': 14613, 'omfg': 10427, 'cleveland': 3435, 'grass': 6625, 'majors': 9109, 'rejoice': 12001, 'ultra': 15194, 'firmware': 5808, 'installed': 7735, 'ard': 1523, 'earn': 5007, 'dissapear': 4666, 'pastors': 10811, 'bris': 2619, 'per': 10909, 'distant': 4675, 'relative': 12006, 'heir': 7100, '5k': 401, '250k': 172, 'clouds': 3474, 'sleepover': 13211, 'generation': 6323, 'shudder': 13021, 'reclaim': 11891, 'funty': 6182, 'september': 12772, 'ridiculous': 12232, '16': 87, 'hungraaaaaaaaay': 7448, 'shorter': 12981, 'borgata': 2483, 'nyappy': 10307, 'boating': 2392, 'songwriting': 13414, 'cape': 2945, 'cod': 3520, 'gooooooood': 6539, 'tomoroo': 14727, 'giid': 6384, 'gni': 6463, 'linuxoutlaws': 8726, '4jcjj': 319, 'boagsie': 2386, 'folkestone': 5917, 'cinema': 3370, 'escape': 5317, 'longg': 8857, 'neighborhood': 10023, 'runners': 12422, 'ty': 15138, 'auntiegail': 1706, 'picking': 11020, 'vis': 15538, 'vests': 15478, 'auntie': 1705, 'gails': 6233, 'childminding': 3270, 'sic': 13040, 'attendances': 1677, 'boingo': 2410, 'connected': 3728, 'fuss': 6193, 'snail': 13304, 'minday': 9499, 'arlington': 1546, 'zeta': 16498, '5z4uq': 412, 'retardeddddddd': 12159, '1300': 61, 'sicky': 13054, 'headline': 7029, 'susan': 14096, 'quits': 11714, 'josh': 8084, 'lured': 9009, 'memorial': 9370, 'hurdles': 7457, 'currency': 4096, 'ironpoodonia': 7848, 'uuu': 15399, 'nou': 10253, 'yeey': 16349, 'epenis': 5276, 'gas': 6280, 'pissing': 11090, 'weiters': 15825, 'rf': 12201, 'eggs': 5111, 'stove': 13814, 'boil': 2406, 'worthless': 16162, 'jorx': 8081, 'jail': 7935, 'stalker': 13668, 'belongs': 2131, 'minnesota': 9517, '4more': 329, 'infection': 7678, 'higher': 7176, 'bing': 2233, 'smarter': 13267, 'classier': 3412, 'fillin': 5758, 'commence': 3604, 'pouting': 11348, 'returns': 12174, 'readysetgo': 11836, 'yeahh': 16326, 'uhhh': 15184, 'mia': 9439, 'evie': 5402, 'capture': 2952, 'violently': 15522, 'phrase': 11008, 'hater': 6972, 'yoda': 16396, 'psyching': 11575, 'deodorant': 4420, 'lungs': 9007, 'bleed': 2312, 'inhaled': 7688, 'constantly': 3744, 'females': 5697, '_rankin': 777, 'meat': 9317, '_tina': 821, '_adriii': 565, 'lindt': 8710, 'cafes': 2849, 'offhand': 10369, 'crowdsourcing': 4015, 'graphics': 6623, 'backgroung': 1835, 'ppicture': 11361, 'mostly': 9737, 'todd': 14698, 'mgmt': 9433, 'atbfm': 1651, 'solidsyn': 13381, 'realise': 11843, 'organiser': 10540, 'dun': 4954, 'performing': 10921, 'artist': 1586, 'djs': 4706, 'malaysian': 9124, 'laws': 8542, 'dolls': 4754, 'sailor': 12490, 'webdu': 15779, 'conf': 3695, 'depends': 4425, 'owe': 10639, 'jump': 8131, 'pout': 11347, 'maccies': 9054, 'difficulty': 4557, 'shiit': 12919, 'pig': 11039, 'saturdays': 12566, 'colouring': 3574, 'gymnastics': 6783, 'gils': 6390, 'eeee': 5084, 'agreed': 1109, 'brookings': 2647, 'geek': 6306, 'gender': 6319, 'mens': 9381, 'cryyy': 4045, 'katies': 8194, 'bronchitis': 2644, 'bangalore': 1916, 'ballerina': 1896, 'chucking': 3345, 'edumedia09': 5078, 'goooood': 6530, 'sg': 12827, 'sana': 12521, 'jurong': 8144, 'guttered': 6772, 'ruby': 12397, 'eliminated': 5147, 'nzntm': 10314, 'supporting': 14066, 'locals': 8806, 'hosanna': 7350, 'bleh': 2316, 'proper': 11534, 'coding': 3524, 'gimme': 6391, 'situation': 13143, 'mandriva': 9146, 'unloved': 15286, 'bin': 2231, 'stressd': 13847, 'ako': 1165, 'tayo': 14298, 'toasty': 14689, 'montel': 9675, 'mommie': 9642, 'uuuugh': 15401, '_era': 632, 'powerdvd': 11354, 'daisies': 4172, 'screenies': 12663, 'icons': 7516, 'protection': 11543, 'bonkers': 2425, 'invited': 7813, 'togethers': 14705, 'playin': 11150, 'security': 12712, 'wallet': 15655, 'goooosh': 6543, 'rt': 12390, 'lar': 8491, 'mainland': 9100, 'police': 11236, 'representatives': 12096, 'owm': 10644, 'tryinh': 14968, 'o2': 10317, 'goooooood': 6538, 'prejudice': 11396, 'gr': 6577, 'frank': 6034, 'pixies': 11106, 'vomiting': 15580, 'sniffly': 13323, 'bens': 2145, 'nightshift': 10124, 'soooooooooooooooooooooooooooooooooooo': 13437, 'txtin': 15135, 'audiotistic': 1695, 'soooory': 13438, 'rested': 12142, 'russel': 12433, 'ponderland': 11252, 'timing': 14637, 'exploring': 5487, 'atlas': 1660, 'hercules': 7133, 'craters': 3939, 'uma': 15197, 'oph': 10505, 'ser': 12775, 'caput': 2954, 'saturn': 12567, 'fogged': 5910, 'eyepiece': 5520, '2a': 187, 'alllllllllllllllll': 1212, '_recordings': 780, 'occuring': 10340, '8th': 509, 'hm': 7225, '_hypnotic': 677, 'ilost': 7571, 'iappreciate': 7498, 'supermodel': 14048, 'python': 11653, 'conversion': 3791, 'extranet': 5512, 'promote': 11528, 'jcdecaux': 7979, 'counts': 3881, 'sinner': 13119, 'femme': 5700, 'fatale': 5629, 'photographers': 10998, 'greastest': 6640, 'gode': 6481, 'ould': 10576, 'recording': 11909, 'blogtv': 2348, 'overnight': 10619, 'printing': 11453, 'shipping': 12931, 'urs': 15358, 'tomo': 14724, 'awareness': 1751, 'principle': 11450, 'mourners': 9757, 'arggghhhhhhhhhhh': 1533, 'fortuna': 6000, 'prints': 11454, 'goodbyeeee': 6507, 'dilemma': 4571, 'lag': 8442, 'spahkly': 13507, 'simon': 13092, 'personality': 10940, 'boyfrann': 2530, 'mines': 9504, 'sprung': 13624, 'pencils': 10889, 'sharp': 12865, 'sharpened': 12866, 'pencil': 10888, 'philosophy': 10980, 'funding': 6162, 'stalking': 13672, 'thumbs': 14577, 'wheat': 15885, 'potatoes': 11335, 'protein': 11544, 'nooo': 10198, 'crush': 4030, 'heehee': 7084, 'shadduppp': 12833, 'cinemas': 3371, 'butts': 2807, 'tortellini': 14781, 'notoriously': 10249, 'unreliable': 15303, 'grid': 6670, 'iqbal': 7836, 'dept': 4437, 'barry': 1954, 'gibb': 6372, 'williamssssss': 15986, 'lexus': 8647, 'hopw': 7331, 'calyx': 2891, 'cf2yuj': 3118, 'brainbone': 2550, 'c9ryqc': 2838, 'stereo': 13751, 'skyline': 13183, 'razr': 11811, 'sonny': 13417, 'mystery': 9899, 'thoughh': 14541, 'crusty': 4033, 'aawww': 885, 'insults': 7750, 'cries': 3987, 'clarify': 3405, 'poo': 11257, 'maï': 9282, 'asos': 1618, 'coupons': 3885, 'horseback': 7346, 'jodies': 8052, 'dudes': 4938, 'clowns': 3478, 'nesbitt': 10047, 'cutest': 4116, 'dslr': 4921, 'parent': 10763, 'trap': 14878, 'created': 3961, 'believed': 2120, 'laff': 8440, 'mass': 9232, 'papers': 10745, 'shitt': 12941, 'funkey': 6169, 'xmlrpc': 16254, 'codeignite': 3523, 'output': 10591, 'amf': 1300, 'zend': 16493, 'closely': 3461, 'interfere': 7771, 'insulting': 7749, 'fare': 5605, 'blockparty': 2340, '_d_': 610, 'increasingly': 7652, 'elevator': 5144, 'danas': 4196, 'roseanne': 12346, 'jerk': 8006, 'pets': 10951, 'attraction': 1684, 'assembly': 1624, 'required': 12109, 'traumatized': 14883, 'edge': 5063, 'table': 14193, 'bruise': 2666, 'solid': 13380, 'twisted': 15078, 'jared': 7959, 'yee': 16341, 'asylm': 1648, 'disorganized': 4663, 'everyond': 5388, 'hollaback': 7258, 'geocaching': 6335, '_song': 803, 'wondrous': 16096, 'beltaine': 2135, 'greek': 6651, 'resturant': 12148, 'johnny': 8061, 'oz': 10660, 'adding': 1001, 'property': 11536, 'ruin': 12404, 'bakers': 1879, 'spider': 13563, 'calming': 2886, 'fears': 5661, 'greeeeen': 6650, 'thorny': 14534, 'bushes': 2784, 'sayin': 12584, 'requirements': 12110, 'grtsat': 6713, 'bggeting': 2192, 'mcmcbuddy': 9297, 'rainbow': 11757, 'burritos': 2776, 'cancellation': 2922, 'beaumont': 2057, 'chillaxing': 3279, 'botcon': 2495, 'darl': 4225, 'mua': 9805, 'tracie': 14832, 'weaver': 15771, 'scrabbled': 12647, 'creativity': 3964, 'pinned': 11078, 'sarcy': 12552, 'sickness': 13053, 'toes': 14702, 'xuxu': 16263, 'aaauuuggghhh': 874, 'mcdonald': 9288, 'cheetos': 3236, 'hehehe': 7092, 'condom': 3693, 'boxes': 2526, 'geez': 6312, 'tow': 14815, 'ahhhhhhhh': 1133, 'pridelines': 11438, 'dresss': 4872, 'haul': 6978, 'wagner': 15623, 'bend': 2137, 'f1': 5526, 'dads': 4167, 'discussions': 4642, 'anurag': 1410, 'placement': 11115, 'charge': 3175, 'argument': 1539, 'aptitude': 1502, 'heeey': 7083, 'kitchen': 8317, 'brooklyn': 2648, 'horror': 7343, 'bigi': 2216, 'fashion': 5620, 'spree': 13616, 'sih': 13077, '____': 554, 'rated': 11796, 'youuuuuu': 16442, 'labs': 8422, 'stead': 13730, 'dissapoint': 4668, 'methinks': 9418, 'depresse': 4430, 'hoedown': 7240, 'throwdown': 14566, '_refugee_': 781, 'exiting': 5461, 'linux': 8725, 'amusing': 1320, 'rubiks': 12395, 'recyle': 11924, 'salvation': 12508, 'army': 1552, 'recycle': 11921, 'pervert': 10945, 'mario': 9191, 'kart': 8185, 'structure': 13874, 'suffication': 13967, 'overall': 10609, 'forecast': 5964, 'peckish': 10872, 'wah': 15625, 'pleaseeeeeeeeeeeeee': 11168, 'reuters': 12181, 'anyarticle': 1416, 'rdt': 11821, 'url': 15355, 'bigmoney': 2217, 'idus1284981420090508': 7534, 'renouncing': 12064, 'worms': 16150, 'un': 15206, 'lean': 8571, 'bandung': 1912, 'doggy': 4743, 'vegan': 15444, 'four': 6017, '5z3ij': 410, 'aila': 1144, 'cyclone': 4138, 'sunshade': 14036, 'balcony': 1885, 'labeled': 8418, 'electric': 5131, '28ï': 184, 'vancity': 15429, 'data': 4240, 'customer': 4109, 'entered': 5249, 'elses': 5160, 'wherever': 15899, 'reminding': 12047, 'chipotle': 3294, 'bunuelos': 2759, 'pen': 10886, '6urrxta8dom': 459, 'winning': 16016, 'crutch': 4034, 'tattoos': 14290, 'peircings': 10885, 'hahahaha': 6819, 'cheaper': 3206, 'carries': 3002, 'foreverrr': 5973, 'wal': 15643, 'mart': 9212, 'dubai': 4926, 'ughhh': 15174, 'doggone': 4742, 'ponderosa': 11253, 'progress': 11511, 'individually': 7665, 'loafing': 8800, '_fam': 637, 'julyish': 8129, 'mothersday': 9741, 'impose': 7610, 'formidable': 5986, 'waters': 15728, 'casual': 3032, 'offense': 10363, 'orange': 10525, 'beetle': 2097, 'whoo': 15938, '200lbs': 139, 'brody': 2638, 'ventura': 15462, '_gal': 649, 'debugging': 4311, 'decemberists': 4317, 'wreckers': 16182, 'tooooo': 14757, 'regrettin': 11986, 'duster': 4967, 'shoppping': 12970, 'greenhills': 6654, 'balikbayans': 1891, 'slum': 13250, 'landlords': 8472, 'backed': 1833, 'sewage': 12818, 'inspiration': 7728, '20gf': 143, 'insomniac': 7724, 'daphne': 4217, 'wif': 15968, 'meeee': 9340, 'toniight': 14741, 'bruh': 2665, 'erection': 5298, 'stroll': 13867, 'beaches': 2034, 'jmjb': 8042, 'pincode': 11063, 'expired': 5479, 'sea': 12686, 'duff': 4942, 'muthafuckin': 9868, 'moovie': 9697, 'sited': 13136, 'sara': 12548, 'cart': 3007, 'si': 13037, 'fog': 5909, '_aston': 573, 'applications': 1479, 'starring': 13694, 'wwe': 16215, 'wrestler': 16185, 'kane': 8174, 'york': 16413, 'distraction': 4680, 'smoothie': 13296, 'predicted': 11388, 'knowledgable': 8352, 'tp': 14825, 'tetep': 14413, 'kurang': 8396, '3bt': 267, 'buat': 2692, 'shooting': 12965, 'kamis': 8172, 'nih': 10128, 'huaaaa': 7409, 'gila': 6385, 'syapa': 14166, 'lg': 8650, 'craptastic': 3934, 'girrrrl': 6412, 'alyssa': 1261, 'weeps': 15808, 'sixty': 13149, 'seek': 12719, 'planned': 11127, 'anne': 1368, 'judy': 8118, 'mysweetebony': 9901, 'lmk': 8790, 'paysite': 10853, 'everyon': 5387, 'arent': 1529, 'abc': 894, 'tragedy': 14849, 'france': 6031, 'overcome': 10611, 'negative': 10016, 'reaction': 11829, 'netherlands': 10052, 'waaa': 15605, '_21thanks': 546, '_chapman': 597, 'whuffaoke': 15952, 'gillette': 6388, 'stadium': 13658, 'undergarments': 15227, 'recap': 11876, 'wasting': 15712, 'pinkberry': 11072, 'embarrass': 5171, 'yeaterday': 16338, 'hahahha': 6828, 'swear': 14122, 'mar': 9168, 'suss': 14105, 'chap': 3166, 'arabs': 1509, 'insist': 7722, 'ochh': 10343, 'blister': 2332, 'supose': 14056, 'yas': 16302, 'thaught': 14458, 'thee': 14467, 'lovley': 8960, 'nuttin': 10296, 'topman': 14769, 'lolipop': 8834, 'esploder': 5323, 'paperwork': 10747, 'pleaseeeee': 11165, 'birthdayyyy': 2251, 'bookmarked': 2441, 'vinegar': 15517, 'witdrawal': 16040, 'tie': 14605, 'demos': 4406, 'cher': 3248, 'warmly': 15686, 'disgusted': 4648, 'effort': 5106, 'shwasty': 13034, 'shloshed': 12949, 'siberia': 13039, '8mm': 507, 'maximum': 9268, 'listenin': 8745, 'girl5': 6400, 'aaron': 883, 'sunnys': 14030, '_127': 543, 'thorw': 14536, 'varsity': 15437, 'fanclub': 5587, 'sway': 14121, 'meow': 9393, 'trippin': 14929, '32': 241, 'tabby': 14192, 'shore': 12972, 'league': 8567, 'diabetes': 4517, 'mountain': 9754, 'dew': 4501, '_slamma_': 801, 'tixs': 14670, 'lolzor': 8845, 'naturally': 9967, 'farm': 5608, 'cents': 3106, 'comps': 3664, 'horay': 7334, 'preston': 11420, 'smaller': 13262, 'companies': 3631, 'itï': 7899, 'slight': 13225, 'hint': 7202, 'mamas': 9132, 'lunchtime': 9005, '37': 261, '1am': 116, '11am': 46, 'amanda': 1268, 'eastern': 5026, 'carolina': 2995, 'pancras': 10725, 'ahmazing': 1134, 'harm': 6946, 'afaik': 1061, 'awlll': 1782, 'summa': 13992, 'longest': 8856, 'trackk': 14836, 'everrr': 5379, 'strek': 13843, 'fitting': 5826, '5jg16': 397, 'grader': 6590, 'thowing': 14547, 'jokes': 8070, 'sicker': 13047, 'messes': 9407, '2000': 130, 'sanfran': 12536, '58': 387, 'cple': 3909, '86': 493, 'degr': 4371, 'dmv': 4713, 'makeup': 9116, 'sizes': 13153, 'bullying': 2744, 'hype': 7485, 'rainin': 11760, 'languish': 8479, 'habbo': 6794, 'cinematography': 3372, 'bai': 1871, 'kt': 8388, 'parade': 10751, 'float': 5871, 'popup': 11289, 'blocker': 2338, 'spidey': 13566, 'boxers': 2525, 'quasi': 11685, 'sequel': 12773, 'plot': 11184, 'terms': 14395, 'quinn': 11710, 'cstm': 4055, 'buaa': 2691, 'moneyy': 9655, 'fraser': 6040, '_jayr': 696, 'budgeting': 2706, 'tango': 14249, 'ug': 15165, 'loveeeee': 8939, 'profession': 11496, 'sterling': 13753, 'chad': 3127, 'dylan': 4984, 'cooper': 3820, 'pathetic': 10817, 'rearranging': 11861, 'shortening': 12980, 'poet': 11214, 'oprah': 10513, 'xbl': 16243, 'plzzzzz': 11200, 'belong': 2130, 'institute': 7743, 'compute': 3665, 'technology': 14328, 'twibes': 15066, 'pict': 11026, 'cantazaro': 2936, 'italy': 7876, 'calabria': 2863, 'lose': 8906, 'bak': 1875, 'neewwww': 10014, 'wardrobe': 15681, 'interventio': 7784, 'aahhh': 881, 'placements': 11116, 'upgrades': 15324, 'kohls': 8364, 'flip': 5867, 'flops': 5876, 'annnnd': 1374, 'accept': 925, 'inconvenience': 7647, 'arts': 1589, 'bahhhh': 1870, 'pwnage_org': 11648, 'pcfopc': 10856, 'sdk': 12684, 'kills': 8291, 'financially': 5773, 'ellipital': 5155, 'demand': 4400, 'operation': 10502, 'mischevious': 9539, 'pinwheel': 11080, 'queueing': 11702, 'cryy': 4044, 'deer': 4347, 'downsides': 4823, 'carnivore': 2991, 'goslings': 6553, 'trolley': 14940, 'pusher': 11636, 'sooooooo': 13430, 'yaaw': 16282, '5529634599': 379, '_ary': 571, 'aaaww': 876, 'courseeee': 3887, 'pooped': 11270, 'postin': 11328, 'wmiad': 16068, 'sweaters': 14125, 'curly': 4095, 'justify': 8150, 'curler': 4092, 'covered': 3897, 'carry': 3004, 'childhood': 3269, 'sutzkever': 14106, 'backorder': 1839, 'belated': 2115, 'ajc': 1160, 'maxwell': 9270, 'bill': 2225, 'fishing': 5818, 'teaser': 14319, 'chipolte': 3293, 'dibs': 4529, 'sulu': 13987, 'savvv': 12578, 'donot': 4772, 'fotolog': 6010, 'sokristen': 13374, 'luckiest': 8981, 'tweeterz': 15048, '_cabrera': 592, 'deprived': 4436, 'assumptions': 1637, 'phase': 10965, 'sings': 13112, 'owl': 10642, 'depress': 4429, 'summe': 13997, 'infect': 7677, 'brunswick': 2674, 'java': 7966, 'hmph': 7232, 'retweet': 12175, 'cici': 3360, 'baltimore': 1900, 'suburbs': 13933, 'yupp': 16468, 'rude': 12398, 'depending': 4424, 'witness': 16051, 'sweets': 14144, 'drank': 4845, 'cyderrrrrrrrr': 4139, 'ruining': 12406, 'instances': 7738, 'yyyyyyyyyoooooooooouuuuu': 16475, 'teevee': 14340, 'adventure': 1048, 'norf': 10214, 'yessir': 16365, 'status': 13723, 'cursing': 4101, 'juddday': 8112, 'location': 8807, 'ubertwitter': 15158, 'include': 7639, 'shriya': 13017, 'sneezy': 13318, 'buff': 2710, 'sophie': 13441, 'vips': 15524, 'acts': 985, 'believes': 2122, 'snoozing': 13329, 'stubborn': 13881, 'caring': 2980, 'temperature': 14368, 'opening': 10496, 'diamond': 4521, 'hurtful': 7463, 'glitter': 6443, 'misog': 9550, 'answers': 1395, 'curved': 4103, 'grading': 6593, 'desire': 4460, 'isle': 7863, 'mediterranean': 9333, 'presenting': 11413, 'owens': 10640, '8pm': 508, 'deserved': 4451, 'towed': 14817, 'redgie': 11928, 'splashtown': 13582, 'ripstick': 12254, 'retract': 12165, 'statement': 13713, 'thennnnnnnn': 14482, 'login': 8826, 'participate': 10779, 'carrie': 3000, 'cambodian': 2894, 'waitress': 15634, 'ti89': 14591, 'immune': 7599, 'favs': 5653, 'cameo': 2900, 'creams': 3957, 'thoe': 14529, 'faile': 5548, 'deceitful': 4314, 'unreal': 15301, 'steve': 13754, 'jones': 8077, 'channing': 3161, 'tatum': 14291, 'frozen': 6126, 'bones': 2422, 'capital': 2947, 'allah': 1200, 'hafiz': 6807, 'universe': 15279, '42': 291, 'hitchikers': 7215, 'baffles': 1860, 'shortened': 12979, 'urls': 15356, 'context': 3768, 'auto': 1717, 'expand': 5464, 'ihop': 7551, 'maths': 9253, 'singer': 13107, 'thanking': 14439, 'omgsh': 10435, 'camping': 2907, 'coupla': 3883, 'fist': 5822, 'biker': 2221, 'boby': 2397, 'contributing': 3781, 'retirement': 12163, 'employer': 5198, '_milano': 742, 'bleeds': 2314, 'mayer': 9276, 'modeling': 9616, 'phobia': 10985, 'phobias': 10986, '21life': 150, 'sentences': 12763, 'chester': 3252, 'stiles': 13769, 'doughnut': 4806, 'mourn': 9756, 'windy': 16006, 'eddy': 5060, 'dumbo': 4947, 'manhattan': 9152, 'nintendo': 10140, 'gameboy': 6245, '_marie': 735, 'hudgens': 7417, 'shuffle': 13024, 'wooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo': 16115, 'doubled': 4799, 'punk': 11615, 'muses': 9844, 'vivid': 15555, 'spiderweb': 13565, 'spiders': 13564, 'tynisha': 15140, 'keli': 8220, 'tynishakeli': 15141, 'zensify': 16495, 'landscape': 8474, 'typing': 15147, 'pleaseeee': 11164, 'concerned': 3678, 'batt': 1995, 'abby': 893, 'l8er': 8410, 'humira': 7438, 'enbrel': 5204, 'rheumy': 12204, 'cutie': 4117, 'exausted': 5420, 'xanax': 16241, 'logins': 8827, 'everytime': 5396, 'grueling': 6716, 'gordo': 6547, 'communicating': 3621, 'minty': 9525, 'oww': 10651, 'operate': 10500, 'schedules': 12620, 'soy': 13498, 'teets': 14339, 'cork': 3839, 'sevens': 12812, 'lam': 8450, 'heheheheh': 7094, 'yaaay': 16280, '1992': 112, 'coolio': 3812, 'poverty': 11350, 'moood': 9689, 'lameness': 8457, 'traitor': 14861, 'aphrodisiac': 1451, 'intake': 7753, 'bowlful': 2521, 'knackered': 8335, 'shouldnt': 12995, 'demi': 4401, 'tisdale': 14662, 'mak': 9110, 'bbm': 2017, 'phn': 10982, 'tmail': 14676, 'dated': 4244, 'peroni': 10933, 'awesomeeeeee': 1766, 'poopy': 11271, 'patrol': 10824, 'doggies': 4741, 'pounding': 11342, 'develop': 4491, 'remedies': 12039, 'twinge': 15075, '3333': 246, 'leaked': 8569, 'filling': 5759, 'ughhhhhhhh': 15177, 'juuuuuust': 8153, 'hai': 6831, '_call': 593, 'castle': 3030, 'chez': 3255, 'moi': 9623, 'intent': 7761, 'kathleen': 8190, '_souljaa': 804, 'fabric': 5531, 'audun': 1701, 'propped': 11539, 'fluffy': 5889, 'pillows': 11054, 'suckss': 13952, 'culture': 4075, 'increasing': 7651, 'india': 7658, 'govt': 6568, 'charges': 3178, 'rapes': 11788, 'lighter': 8681, 'flashed': 5851, 'unheard': 15262, 'pasty': 10812, 'freckle': 6052, 'bash': 1966, 'everybodys': 5385, 'qo': 11667, 'fridaaaayyyyy': 6090, 'sowwy': 13495, 'denton': 4416, 'musician': 9854, 'profit': 11500, '2215': 155, '2morrow': 207, 'cia': 3356, 'represent': 12095, 'actors': 982, 'careers': 2972, 'killers': 8287, 'jonathan': 8075, 'epitomised': 5283, 'fangirl': 5591, 'friendlyand': 6099, 'joey': 8055, 'eight': 5116, 'yourock': 16428, 'amen': 1292, 'brotha': 2653, 'pennsylvania': 10895, 'hsbc': 7397, 'savings': 12577, 'apy': 1504, '55': 378, 'underneath': 15228, 'mattress': 9263, '67kvt': 447, 'porter': 11296, 'haunting': 6979, 'forces': 5961, 'strategies': 13831, 'competitive': 3643, 'advantage': 1046, 'heavenly': 7066, 'siyal8r': 13150, 'dreambears': 4859, '33333': 247, 'hahahahaha': 6821, 'sociable': 13350, 'shaycarl': 12877, 'vlog': 15557, 'textin': 14419, 'beds': 2084, 'brandy': 2560, 'whiskers': 15913, 'conversation': 3788, 'multitask': 9828, 'mature': 9264, 'britney': 2626, 'lamb': 8451, 'shirts': 12936, 'cookers': 3804, 'koreans': 8373, 'buisnesses': 2731, 'mussoooooo': 9859, 'youï': 16443, '½re': 16545, '221': 154, 'dedicated': 4339, 'slice': 13220, 'sql': 13629, 'axed': 1797, 'ombra': 10423, 'mai': 9091, 'fï': 6208, 'largo': 8495, 'hï': 7493, '½ndel': 16541, 'scrap': 12649, 'showering': 13006, 'ecaytrade': 5043, 'vice': 15492, 'president': 11415, 'cultural': 4074, 'affairs': 1062, 'southwestern': 13490, 'bamboo': 1903, 'jade': 7931, 'mollie': 9635, 'kaelah': 8161, 'admire': 1018, 'vietnam': 15505, 'ratty': 11803, 'disturbed': 4685, 'toadtastic': 14685, 'rosie': 12350, 'goal': 6468, 'leav': 8581, 'rescheduled': 12117, 'crawled': 3945, 'givee': 6419, 'daniel': 4212, 'schuhmacher': 12629, 'studentfinance': 13884, 'pounds': 11343, 'maaaannnnnnnnn': 9043, 'tritonlink': 14933, 'maaaaannnn': 9041, 'holidayzzzzz': 7256, 'coworkers': 3905, 'followfridays': 5926, 'tantra': 14254, 'kirki': 8310, 'drowning': 4901, 'sorrows': 13454, 'ip': 7826, 'brittany': 2630, 'sinse': 13122, 'nuffin': 10275, 'charleston': 3185, 'bees': 2096, 'trapped': 14880, 'honeypot': 7295, 'bran': 2553, '2n': 210, 'kateage': 8188, 'owi': 10641, 'credo': 3970, 'brighi': 2608, 'twitt': 15087, 'heartache': 7051, 'ctrl': 4058, 'underscore': 15233, 'e71': 4990, 'ere': 5297, 'profy': 11503, 'ayt': 1801, 'uspln': 15379, 'cautious': 3056, 'youuuuu': 16441, 'savvy': 12579, 'thus': 14585, 'ss': 13644, 'ben': 2136, 'incl': 7637, 'coaches': 3504, 'terri': 14398, 'crisps': 3996, '_bbcrew': 581, 'whhaacck': 15903, '200f2': 137, '85': 492, 'tweddin': 15030, 'alllll': 1209, 'consumption': 3752, 'specials': 13533, 'bensons': 2146, '2001': 131, 'maniacs': 9153, 'lush': 9013, 'env2': 5265, 'pov': 11349, '99': 529, 'tataindicom': 14281, 'tatasky': 14283, 'airtel': 1156, 'withdraws': 16046, 'victims': 15496, 'brad': 2542, 'olympic': 10420, 'rack': 11739, 'lifeball': 8671, 'vienna': 15504, '17jiy8': 99, 'username': 15374, 'papa': 10741, 'helped': 7119, 'mill': 9486, 'regions': 11977, 'neti': 10053, 'vi': 15484, '5jfu9': 396, 'alittle': 1197, 'fear': 5660, 'stolen': 13790, 'bale': 1888, 'cotton': 3866, 'milkshake': 9484, 'uuuup': 15402, 'glorious': 6448, 'contemplate': 3759, 'awesomerer': 1770, 'happpppy': 6923, 'chocr': 3309, 'rounds': 12362, 'gourds': 6566, 'letdown': 8634, 'zombies': 16506, 'huggers': 7421, 'muhahahaaaa': 9821, 'murder': 9839, 'ruleaz': 12409, 'zic': 16500, 'ron': 12325, 'griffin': 6672, 'denver': 4417, 'nosebleeds': 10230, 'ahugs': 1137, 'deli': 4384, 'doable': 4720, '_kotenok': 712, 'pointless': 11222, 'nan': 9937, 'hte': 7402, 'jittery': 8035, 'musicboat': 9853, 'university': 15280, 'benicks': 2142, 'crashed': 3936, 'ttytomorrow': 14983, 'sngs': 13319, 'wks': 16064, 'csh': 4050, 'dayyyy': 4273, 'gu': 6730, 'keepin': 8216, 'missinmydgbigtyme': 9560, 'cheesecake': 3230, 'polar': 11232, '_kong': 710, 'prix': 11464, 'perks': 10927, 'towels': 14819, 'lovey': 8954, 'agreeable': 1108, 'yays': 16307, 'choked': 3313, 'retainers': 12156, 'discovered': 4634, 'shortcoming': 12976, 'integrated': 7754, 'subtract': 13932, 'limit': 8702, 'shampoo': 12851, 'sumthin': 14005, 'akankah': 1163, 'suatu': 13913, 'hari': 6942, 'ku': 8390, 'mendapatkannya': 9378, '201th': 141, 'remarks': 12036, 'blondes': 2352, 'nz': 10313, 'umm': 15202, 'massage': 9234, 'erin': 5304, 'baguio': 1864, 'damned': 4186, 'ungrateful': 15260, 'trades': 14841, 'comp': 3627, '3mb': 276, 'anthropomorphic': 1399, 'planter': 11132, 'm6sru3': 9035, '31st': 240, '930a': 524, '730p': 465, 'lube': 8974, 'welsh': 15837, 'ninja': 10139, 'dominicks': 4760, 'gummy': 6755, 'bre': 2569, 'relatives': 12008, 'loon': 8876, 'dario': 4221, 'tk': 14672, 'harlem': 6944, 'pollution': 11245, 'ethnicity': 5346, 'fellas': 5691, 'dueces': 4940, 'bagel': 1862, 'maxin': 9269, 'ftsk': 6140, 'rapids': 11789, 'alike': 1195, 'nervvoouus': 10045, 'pta': 11578, 'alerts': 1180, 'generally': 6321, 'stands': 13681, 'association': 1634, 'confuse': 3709, 'decluttering': 4331, 'wannnnaaaa': 15666, 'goooooo': 6534, 'blimey': 2325, 'nov': 10254, 'gether': 6349, 'jp': 8101, 'iv': 7901, 'sucker': 13946, 'lat': 8507, 'negativity': 10017, 'bump': 2750, 'seemingly': 12723, 'unable': 15209, 'muster': 9865, 'tsi': 14975, '160hp': 90, 'imtetsting': 7625, 'visual': 15552, 'chickadee': 3262, 'messenger': 9406, 'supp': 14057, 'princecharming': 11444, 'maternity': 9248, 'converter': 3793, 'converting': 3794, 'fanfic': 5590, 'worn': 16152, 'performance': 10918, 'randomly': 11778, 'hangup': 6903, 'morrisons': 9722, 'weirder': 15818, 'any1': 1415, 'tipping': 14650, 'unusual': 15312, 'activity': 980, 'bicycle': 2204, 'goood': 6525, 'seven': 12811, 'fraktastic': 6027, 'zoidberg': 16504, 'tweeted': 15044, 'bubblies': 2696, '_steve': 810, 'exeter': 5448, 'marked': 9194, 'funy': 6183, 'housework': 7378, 'gasp': 6284, 'replay': 12080, 'rankings': 11784, '_cupcake': 608, 'wrestlefest': 16184, 'roughnight': 12357, 'buttershots': 2803, 'faire': 5557, 'boreddddd': 2479, '_a4l': 562, 'arrange': 1561, 'archies': 1518, '5am': 389, 'lawnmower': 8541, 'stood': 13795, 'tall': 14241, 'carbonated': 2962, 'nofair': 10174, 'eyeshadows': 5522, 'hooked': 7306, 'koala': 8362, 'pubquizzing': 11585, '11e': 47, 'sinon': 13120, 'mardi': 9178, 'requested': 12103, 'guest': 6740, 'speaking': 13528, 'tweeples': 15036, 'acquainted': 968, 'spoke': 13592, 'sunshiines': 14037, 'nostalgia': 10231, 'leavers': 8583, '_writes': 843, 'camcorder': 2897, 'rove': 12369, 'mcmanus': 9296, 'footage': 5949, 'ohyeahhh': 10393, 'geeky': 6308, 'ellen': 5153, 'skott': 13179, 'gooodnight': 6527, 'hacker': 6799, 'carolyn': 2996, 'thereeeeeee': 14489, 'duetter': 4941, 'crepe': 3979, 'medal': 9321, 'drivin': 4889, 'glued': 6454, 'copier': 3827, 'goooooodmoring': 6536, 'greaattt': 6638, 'toddler': 14699, '_val_4_now': 828, 'ferris': 5702, 'wheel': 15888, 'ri': 12210, 'section': 12709, 'gino': 6398, 'laughs': 8526, 'medhurst': 9322, 'kashtam': 8186, 'similey': 13091, 'csk': 4052, 'irish': 7841, 'upstairs': 15341, 'biz': 2268, 'receive': 11877, 'handbags': 6882, 'heheee': 7091, 'fresno': 6085, 'wednesay': 15787, 'shepards': 12901, 'bush': 2783, 'baptist': 1933, 'missionaries': 9562, '_cath': 596, 'whos': 15947, 'monica': 9658, 'iono': 7824, 'l8r': 8411, 'soetimes': 13362, 'lives': 8769, 'deep': 4345, 'communications': 3622, 'roomie': 12331, 'survivor': 14095, 'denny': 4411, 'jessie': 8012, 'mosquito': 9733, 'mentioned': 9388, 'undrstand': 15242, 'busays': 2782, 'tagalog': 14207, 'waray': 15676, 'initials': 7691, 'grumpy': 6721, 'blamed': 2294, 'receiving': 11879, '_roe': 786, 'chunky': 3347, 'beef': 2089, 'srry': 13641, 'urselff': 15360, 'yayyyyyyy': 16312, 'hungryyyyyy': 7450, 'expecting': 5467, 'broadway': 2637, 'walc': 15644, 'ers': 5315, 'reminiscing': 12049, 'uggh': 15169, 'miller': 9488, 'cubs': 4063, 'leading': 8564, 'chica': 3258, 'goosebumps': 6545, 'tones': 14734, 'dealz': 4300, 'freaky': 6051, 'becomin': 2078, 'shopaholic': 12967, 'organ': 10538, 'ejamming': 5124, 'undeveloped': 15240, 'sprint': 13621, '8330': 491, 'nightmares': 10122, 'perky': 10928, 'chipped': 3295, 'girlies': 6404, 'bothering': 2500, 'desperation': 4468, 'whoah': 15930, 'effective': 5100, 'rendered': 12057, 'magic': 9080, 'funfunfun': 6167, 'childrens': 3272, 'yuuum': 16472, 'android': 1332, 'pushed': 11635, 'chemist': 3245, 'runway': 12427, 'parked': 10768, 'fs': 6137, 'yoooooooooooooooooooou': 16409, 'thas': 14451, 'bouts': 2518, 'ericson': 5302, 'touched': 14799, 'asia': 1608, 'topics': 14767, 'telephone': 14352, 'barn': 1949, 'hay': 7000, 'prices': 11434, 'brett': 2592, 'teach': 14306, 'blindly': 2327, 'label': 8417, 'tops': 14772, 'latin': 8516, 'nothings': 10241, 'aswel': 1644, 'lmfao': 8786, 'commands': 3603, 'educate': 5076, 'velvet': 15452, 'cupcakes': 4084, 'master': 9239, 'frosting': 6122, 'coolest': 3811, 'confusing': 3712, 'scratches': 12653, 'henpecking': 7129, 'cheating': 3210, 'blond': 2350, 'karma': 8184, 'dawg': 4259, 'hampa': 6876, 'hatiku': 6975, 'whatz': 15883, 'waaaaaaaaaah': 15608, 'unpopular': 15297, 'chantal': 3163, 'tittle': 14668, 'breathe': 2582, 'lurveeeeee': 9012, 'outfits': 10587, 'spoilers': 13589, 'speller': 13550, 'craziness': 3951, 'w00t': 15602, 'funds': 6164, 'drunken': 4913, 'longing': 8861, 'hoodie': 7304, 'brrr': 2661, 'adelaide': 1009, 'marie': 9187, 'aiza': 1159, '_live': 724, 'halls': 6857, 'throte': 14562, 'demon': 4404, 'pinkpawsforlife': 11075, 'org': 10537, 'wooo': 16109, 'nuggets': 10277, 'gob': 6471, 'phoned': 10991, 'craziier': 3950, 'sigma': 13067, 'splendid': 13583, 'ev': 5359, 'boom': 2445, 'sta': 13651, 'focused': 5907, 'skinned': 13169, 'tweetsuite': 15057, 'acs': 971, 'ther': 14485, 'ecstatic': 5053, 'mutual': 9870, 'admiration': 1017, 'somewhat': 13406, 'whaaat': 15859, 'herd': 7134, 'somalions': 13389, 'stuffted': 13896, 'forbidden': 5958, 'cholocate': 3315, '_lokelani': 726, 'hitched': 7214, 'cuidalo': 4071, 'beeen': 2088, 'onee': 10447, 'additional': 1003, 'stepmom': 13748, 'sweating': 14126, 'driven': 4885, 'auditioning': 1698, 'southpark': 13488, '2hear': 200, 'kerry': 8237, 'gmtv': 6459, 'hyper': 7487, 'idiots': 7528, 'juggler': 8120, 'bandwagon': 1913, 'wr': 16174, 'tape': 14258, 'fungus': 6168, 'munch': 9834, 'watson': 15734, 'gigs': 6382, 'rly': 12267, 'disaster': 4618, 'breathing': 2583, 'believing': 2123, 'waaah': 15612, 'masseuse': 9236, 'yourname': 16427, 'tricky': 14920, 'moncton': 9651, 'jill': 8027, 'idkk': 7530, '_g': 647, 'rugby': 12400, '_mar_mey': 733, 'disbanding': 4620, 'americana': 1297, 'options': 10520, 'wacky': 15618, 'amigo': 1303, 'ofcourse': 10359, 'experiment': 5476, 'supervise': 14055, 'midterms': 9459, 'wipe': 16020, 'slate': 13196, 'jelly': 7993, 'runnin': 12423, 'pdx': 10861, 'fired': 5798, 'plays': 11155, 'iplayer': 7830, 'supporters': 14065, 'athletic': 1656, 'dinna': 4582, 'hink': 7201, 'shown': 13010, 'elgin': 5146, 'nae1': 9912, 'oll': 10418, 'applebees': 1475, 'shadow': 12836, 'realistic': 11846, 'rolling': 12316, 'burden': 2761, 'russ': 12432, '_dj': 622, 'xmas': 16250, 'keswick': 8238, 'cheesecakes': 3231, 'pilot': 11056, 'burnin': 2768, '04': 6, 'source': 13482, 'lad': 8430, 'mite': 9578, 'sumthn': 14006, 'doin': 4746, 'ay': 1799, 'suuuuck': 14108, 'peas': 10870, 'nakuh': 9926, 'grabeh': 6584, 'twitchy': 15083, 'joeman': 8054, 'mor': 9699, 'g1freaks': 6211, 'togetha': 14703, 'navy': 9974, 'nailpolish': 9919, 'stained': 13663, 'shiiite': 12918, 'rhyming': 12208, 'creeping': 3976, 'charlie': 3187, 'neglect': 10020, 'wnt': 16071, 'zsg': 16513, 'orillia': 10550, 'arsenal': 1577, 'mitch': 9575, 'awwwwwww': 1793, 'sats': 12564, 'teddy': 14329, 'humid': 7436, 'organizing': 10543, 'herrrrr': 7146, 'fletcher': 5859, 'disturbance': 4684, 'sapinsidetrack': 12547, 'palo': 10718, 'alto': 1257, 'lines': 8714, 'unmuted': 15288, 'opinion': 10507, 'saved': 12575, 'sexxxy': 12822, 'tiiiiiimmmmmmeee': 14618, 'blushing': 2380, 'headrush': 7033, 'favour': 5649, 'tweetilicious': 15052, 'schoool': 12628, 'hittt': 7220, 'beck': 2073, 'overdue': 10613, 'overstuffed': 10628, 'grill': 6673, 'congratulation': 3723, 'almighty': 1221, 'yey': 16378, 'starbuck': 13686, 'deeply': 4346, 'cheeks': 3220, 'rebel': 11867, 'meets': 9347, 'dac': 4159, 'pantera': 10737, 'cowboys': 3902, 'paracetamol': 10750, 'swallow': 14118, 'sanzz': 12546, 'ex': 5414, 'baristas': 1944, 'commonalities': 3619, 'socialising': 13353, 'tomor': 14726, 'fate': 5630, 'ysa': 16448, 'drums': 4910, '_uh_knee': 825, 'charm': 3189, 't4': 14188, 'brownie': 2658, 'rsvp': 12389, 'qiuqiu': 11662, 'degree': 4372, '90mph': 514, 'basis': 1972, 'cld': 3419, 'furious': 6185, 'mph': 9785, 'forte': 5998, 'mystic': 9900, 'bientï': 2209, '½t': 16550, 'lire': 8737, 'quincy': 11709, 'fashionisthenextcity': 5622, 'tee': 14331, 'limited': 8704, 'edition': 5070, 'twic': 15067, '_daarling': 612, 'distance': 4674, 'tiki': 14620, 'oo': 10462, 'opened': 10494, 'lauren': 8533, 'conrad': 3735, 'andswere': 1334, 'quesadiaas': 11694, 'bombbb': 2418, 'animals': 1353, '_kryptik': 713, 'statuses': 13724, 'wel': 15826, 'arbit': 1512, 'ireland': 7840, 'x2bp2': 16232, 'gang': 6257, 'mwahs': 9876, 'stretches': 13856, 'selection': 12731, 'flavors': 5857, 'roomate': 12330, 'naina': 9921, '3wordsaftersex': 281, 'effy': 5108, '90mm': 513, 'mamiya': 9134, 'mf': 9427, 'lens': 8620, 'shocking': 12953, 'suxs': 14112, 'dosen': 4791, 'sedaris': 12714, 'insulted': 7748, '35th': 256, '2getha': 198, 'hahaa': 6813, 'td': 14302, 'battled': 1998, 'cs3': 4047, 'processor': 11481, 'nambu': 9930, 'unintuitive': 15267, 'interminable': 7773, 'sandman': 12528, 'wot': 16165, 'partyin': 10790, '2nyt': 216, 'taps': 14261, 'capes': 2946, 'build': 2726, 'drinker': 4877, 'shake': 12839, 'hid': 7169, 'confidence': 3699, 'ies': 7538, 'martwo': 9216, 'accadentally': 922, 'xvd1wankt': 16264, 'struggling': 13877, 'jelous': 7995, 'position': 11310, 'jummy': 8130, 'ewwwww': 5413, 'marra': 9206, 'ecxcited': 5056, 'annabel': 1363, 'hearts': 7058, 'arrest': 1564, 'wavves': 15742, 'meltdown': 9361, 'barcelona': 1941, 'drummer': 4908, 'logically': 8825, 'motorways': 9753, 'walnut': 15659, 'becca': 2072, 'yaay': 16283, 'flown': 5886, 'clamped': 3402, 'laffy': 8441, 'taffy': 14205, 'momies': 9639, 'sk': 13156, 'dha': 4511, 'cece': 3081, 'chanqes': 3162, 'los': 8905, 'tab': 14191, 'randoms': 11779, 'licking': 8661, 'crumbs': 4027, 'twirl': 15077, 'wrapper': 16178, 'zammo': 16482, '_garner': 652, 'suggested': 13973, 'fannie': 5593, 'darwin': 4235, 'fetch': 5712, 'ouchie': 10574, 'excedrine': 5422, 'compare': 3634, 'notes': 10237, 'haih': 6832, 'unc': 15217, 'cuts': 4119, 'rejected': 11999, 'upgrade': 15322, 'panicky': 10734, 'sec': 12705, 'ojcf5l': 10396, 'cassandra': 3026, 'wilcox': 15978, 'bloss': 2357, 'bucket': 2697, 'doors': 4784, 'byee': 2827, 'rainstorm': 11763, 'cheat': 3208, 'gooooonight': 6533, 'arnold': 1555, 'rus': 12429, '_cullen8': 607, 'uglier': 15179, 'realllllllllly': 11853, 'cycling': 4137, 'campjitterbug': 2908, 'thnks': 14526, 'clara': 3404, 'sotomayor': 13468, 'branches': 2555, 'damit': 4180, 'hahhh': 6830, 'nerve': 10041, 'askin': 1613, '5hours': 392, 'helll': 7110, 'selling': 12739, 'exhibition': 5454, 'gallery': 6240, 'otara': 10563, 'birmingham': 2246, 'whoooooo': 15942, 'spcn': 13524, 'opff': 10504, 'bbl8r': 2016, 'tells': 14362, 'pretending': 11423, 'assassinate': 1621, 'eaat': 4992, 'subtly': 13931, 'temme': 14364, 'firangs': 5796, 'steal': 13735, 'vocab': 15559, 'relieve': 12024, 'speeches': 13539, 'foad': 5904, 'taught': 14292, 'asians': 1610, 'ge': 6300, 'strep': 13845, 'shifting': 12913, 'ghey': 6364, 'ughhhh': 15175, 'sheli': 12893, 'flood': 5872, 'murdere': 9840, 'accidently': 936, 'cured': 4090, 'hunger': 7446, 'h8': 6786, 'withdrew': 16047, 'permission': 10931, 'idp': 7532, 'camps': 2909, 'idprelief': 7533, 'vinyl': 15521, 'clad': 3395, 'playset': 11156, 'mfr': 9430, 'warranty': 15693, 'swing': 14156, 'slots': 13240, 'climbing': 3446, 'amish': 1305, 'arrggh': 1566, 'kava': 8197, 'liam': 8654, '_pase': 767, 'kaleidoscope': 8168, 'norma': 10216, 'terabyte': 14387, 'contacted': 3755, '2ce': 191, 'response': 12135, 'systems': 14185, 'smoothly': 13297, 'upkeep': 15325, 'crisis': 3994, 'eavy': 5037, 'doe': 4733, 'firts': 5811, 'impressions': 7617, 'ageing': 1094, 'tytn': 15153, 'aaawww': 877, 'soooooooo': 13431, 'unfortunetly': 15257, 'tengo': 14379, 'dinero': 4578, 'luvv': 9020, 'excedrin': 5421, 'vent': 15460, 'domestic': 4758, 'artery': 1579, 'hagen': 6808, 'daz': 4274, 'websites': 15783, 'ethics': 5344, 'kilkenny': 8282, 'clown': 3476, 'interrupt': 7781, 'blankets': 2298, 'hostage': 7356, '_joy': 702, 'entirely': 5261, 'pizzas': 11109, 'price': 11431, 'wireless': 16023, 'drops': 4898, 'thesis': 14495, 'joyfull': 8097, 'shoulda': 12991, 'pierced': 11033, 'piercings': 11035, 'aaah': 869, 'incarnated': 7633, 'mutant': 9867, 'ghalib': 6361, 'unanswered': 15210, 'nigguhs': 10115, 'ilh': 7562, 'florence': 5879, 'flasher': 5852, 'zack': 16480, 'alpine': 1234, 'chattin': 3200, 'foreve': 5971, 'zavaroni': 16485, 'sp': 13500, 'shields': 12911, 'barrymore': 1955, 'garage': 6264, 'storage': 13803, 'upbeat': 15315, 'backk': 1836, 'hadn': 6805, 'hallmark': 6853, 'fails': 5553, 'nyeh': 10310, 'signing': 13072, 'fannish': 5594, 'inquisition': 7711, 'dot': 4795, 'shelf': 12892, 'cameras': 2903, 'mash': 9227, 'warhammer': 15683, 'uwl7m': 15407, 'spoiled': 13588, 'donï': 4776, 'canï': 2941, 'severe': 12816, 'ryt': 12447, 'dougie': 4808, 'poynter': 11359, 'replyed': 12085, 'ticketless': 14596, 'payat': 10843, 'melbourne': 9357, 'ugggh': 15168, 'dancers': 4200, 'blogging': 2344, 'passion': 10802, 'chit': 3299, 'mummyyyyyyyy': 9832, 'wkp': 16063, 'feature': 5666, 'heeeey': 7080, 'boc': 2398, 'raj': 11769, 'squad': 13630, 'gyms': 6784, 'machines': 9059, 'indulge': 7670, 'allow': 1217, 'thicke': 14500, 'serenade': 12778, 'naked': 9925, 'woooh': 16110, 'beers': 2095, 'awesomness': 1773, 'sourness': 13484, 'stalkersaturday': 13671, 'gloom': 6445, 'whoohoo': 15940, 'dyeing': 4982, 'tilaaa': 14623, '_troy': 823, '_mcloven': 739, 'garbage': 6266, 'popping': 11282, 'pwnd': 11649, 'sides': 13057, 'goodniqht': 6516, 'hilly': 7195, 'lmbo': 8784, 'litterally': 8757, 'lalala': 8449, 'whilst': 15907, 'chf': 3256, '670': 437, 'hardware': 6941, 'wud': 16207, 'overheating': 10616, 'avatarcamp': 1730, 'hotttie': 7368, 'poooh': 11266, 'kerri': 8236, 'locker': 8811, 'restoration': 12145, 'banks': 1927, 'proposals': 11537, 'avocets': 1738, 'gut': 6768, 'exception': 5428, 'chillen': 3281, 'csla': 4053, 'pacquiao': 10679, 'rerun': 12113, 'tackle': 14198, 'mare': 9179, 'paradice': 10753, 'heyyy': 7158, 'rr': 12383, '_world': 841, 'brewers': 2594, 'leafs': 8566, 'oliver': 10416, 'anooyed': 1387, 'pla': 11112, '4wh0o': 353, 'posterrr': 11326, 'faking': 5566, 'newark': 10065, 'crochet': 4003, 'stitches': 13782, 'afgan': 1068, 'lamentablemente': 8458, 'paso': 10797, 'jrztwitterlunch': 8107, 'developement': 4493, '13tolife': 70, 'mentioning': 9389, 'links': 8722, 'contests': 3767, 'reminder': 12045, 'ummm': 15204, 'pisssing': 11091, 'whens': 15895, 'lova': 8927, 'bbff': 2014, 'twenty': 15062, 'superstar': 14051, '2b': 190, 'lyk': 9030, 'othaa': 10564, 'gurrrrrl': 6765, 'rieger': 12236, 'begonia': 2104, 'basket': 1974, 'seemed': 12722, 'casualties': 3033, 'discography': 4626, 'teardrops': 14314, 'christa': 3335, '07k6e': 11, 'pril': 11439, 'yeps': 16358, 'ihate': 7550, '_nesmith': 752, 'lisette': 8739, 'alejandra': 1178, 'entrepreneurs': 5263, '4w425': 338, 'parrrtty': 10776, 'twister': 15079, 'tipsy': 14652, 'desperate': 4466, 'adjustment': 1015, 'perez': 10912, 'littttttle': 8759, '160': 88, 'hurrah': 7458, 'va': 15411, 'muggy': 9819, 'cursed': 4100, 'reinforced': 11994, 'sorts': 13464, 'newly': 10071, '20min': 144, 'rundown': 12419, 'paste': 10809, 'tosser': 14790, 'flap': 5848, '10000000000': 25, '_the_moon': 817, 'jim': 8029, 'fruitbat': 6128, 'cldnt': 3420, 'recipe': 11887, 'cig': 3362, 'sticky': 13766, 'attitude': 1682, 'iznt': 7911, 'perils': 10925, 'tragedies': 14848, 'funn': 6171, 'dancer': 4199, 'lammmeeee': 8461, 'tournament': 14809, 'destroy': 4474, 'quil': 11707, 'grinning': 6680, 'tragidy': 14851, 'wirting': 16025, 'almos': 1224, 'listenint': 8747, 'afh': 1069, 'sacto': 12463, 'proflowers': 11502, 'fiasco': 5725, 'itell': 7882, 'sosososo': 13467, 'cory': 3854, 'weiss': 15824, '67jzp': 445, 'concerts': 3682, 'experienced': 5474, 'apathy': 1448, 'empathy': 5192, '_fc': 639, 'alias': 1190, 'approximately': 1496, '117th': 45, 'extended': 5504, 'recoverable': 11917, 'sweep': 14136, 'lovato': 8931, 'trader': 14840, '71st': 463, 'firemen': 5804, 'wth': 16206, 'pause': 10834, 'creating': 3962, 'frequent': 6080, '½h': 16534, 'borrow': 2490, 'anythin': 1426, 'arg': 1531, 'eirtaku': 5121, 'bots': 2501, 'hii': 7186, 'thatsï': 14456, '½mee': 16538, '_lipz': 722, 'ravenclaw': 11806, 'verse': 15471, 'blazing': 2305, '25mbps': 174, 'faa': 5529, 'thaa': 14428, 'tru': 14951, 'orc': 10527, 'hilton': 7196, 'beeman': 2092, 'commentaries': 3606, 'mayyyybe': 9278, 'charicee': 3180, 'doomed': 4780, 'assholes': 1627, 'acing': 964, 'crooning': 4007, 'peewee': 10883, 'napping': 9945, 'sfo': 12826, 'missin': 9558, 'camden': 2898, 'prego': 11393, 'smuts': 13300, 'sicckkkk': 13042, 'metaverse': 9414, 'virtual': 15530, 'worlds': 16146, 'gamier': 6251, 'terra': 14396, 'usage': 15363, 'tn': 14681, 'metaverseu': 9415, 'wabble': 15615, 'sickkk': 13049, 'ichat': 7512, 'critique': 4001, 'activities': 979, 'minutos': 9530, 'biting': 2263, 'varnish': 15435, 'gain': 6234, 'tag': 14206, 'thousand': 14545, 'corpes': 3846, '95': 526, 'overrated': 10621, 'fcukkk': 5659, 'marvelous': 9219, 'routines': 12368, 'leeds': 8593, 'stamford': 13673, 'sweety': 14145, 'waht': 15627, 'hugz': 7427, 'xxxxxxxxxx': 16272, 'goooooddd': 6531, 'chale': 3133, 'ver': 15468, 'define': 4359, 'evernote': 5375, 'sd': 12682, 'fllwng': 5869, 'thm': 14521, 'evernote_eyefi': 5376, 'mpxn': 9786, 'julio': 8127, 'bac': 1826, '15minutes': 84, 'shallowness': 12846, 'annoy': 1381, 'straighten': 13817, 'pj': 11110, 'fyi': 6207, '_03': 539, 'tupac': 15006, 'lyrics': 9033, 'jacked': 7923, 'sr': 13640, '234': 159, 'sunblock': 14013, 'venezuela': 15456, 'classmates': 3413, 'marriage': 9207, 'graceful': 6586, 'accepted': 928, 'freckles': 6053, 'oooohhh': 10476, 'chino': 3290, 'apathetic': 1447, 'elizabeth': 5150, 'squeeze': 13635, 'investigated': 7806, 'ermintrude': 5306, 'nabbed': 9909, 'ble': 2306, 'stanky': 13682, 'sunset': 14035, 'cliffs': 3444, 'headlights': 7028, 'spotlight': 13609, '5jib6': 399, 'godamn': 6474, 'planted': 11131, 'backyard': 1846, 'armhole': 1550, 'involving': 7820, 'omgaah': 10430, 'creepering': 3975, 'announce': 1376, 'marathon': 9170, 'downstairs': 4824, 'hollie': 7261, 'steel': 13740, 'wharra': 15867, 'ccna': 3072, 'ccnp': 3073, 'ccie': 3071, 'nishiki': 10145, 'alhamdulilah': 1187, 'nafa': 9913, 'sanjaya': 12540, 'hahahahahaha': 6822, 'cure': 4089, 'spontaneously': 13596, 'spontaneity': 13595, 'rediculous': 11931, 'hrdest': 7394, 'celtics': 3098, 'redsox': 11939, 'platypuses': 11142, 'mammals': 9136, 'breezy': 2589, 'tasty': 14279, 'avail': 1724, 'tkd': 14673, 'instructor': 7744, 'dodger': 4730, '2maro': 204, 'ewl': 5410, 'hives': 7221, 'allergic': 1202, 'itches': 7878, 'yeeee': 16344, 'nets': 10054, 'paiseh': 10703, 'xbox720': 16246, 'lousy': 8925, 'embarrased': 5170, '15gb': 83, 'srsly': 13642, 'lettuce': 8641, 'spinach': 13572, 'happpy': 6924, 'hooping': 7311, 'distractions': 4681, 'believers': 2121, 'warped': 15692, 'surprising': 14086, 'bazillions': 2009, 'americans': 1299, 'douchebag': 4803, 'invented': 7800, 'sensibility': 12758, 'thier': 14501, 'guns': 6763, '5z5kz': 413, 'immensely': 7597, 'annual': 1385, '_a_hart': 563, 'wrox': 16201, 'aparantly': 1442, 'soccer': 13349, 'defited': 4363, 'peed': 10876, 'sheets': 12888, 'underpants': 15230, 'trees': 14902, 'craz': 3947, 'dï': 4988, '½a': 16529, 'sol': 13375, 'grrrrrrr': 6711, '½why': 16552, '½whyyy': 16553, 'cramming': 3925, 'exciteeeddd': 5435, '395': 265, 'hungary': 7445, 'nurburgring': 10287, 'mtfye3': 9799, 'simoneserhan': 13093, 'hollyoaks': 7265, 'quashed': 11684, 'twibble': 15065, 'based': 1963, 'phones': 10992, 'requires': 12111, 'washer': 15701, 'kitchenfire': 8318, 'kay': 8199, 'destroyalllines': 4475, '_news': 755, 'teignmouth': 14345, 'dawlish': 4260, 'hmmmm': 7228, 'alaska': 1170, 'debt': 4310, 'hidden': 7170, 'file': 5751, 'nott': 10250, 'throwin': 14568, 'rah': 11751, 'illi': 7566, 'shalonda': 12847, 'bangin': 1919, 'repetative': 12075, 'passyunk': 10806, '145': 74, '165': 91, 'yawn': 16304, 'starfleet': 13691, 'gate': 6286, 'presen': 11408, '3mdce': 277, 'destination': 4472, 'quarters': 11683, 'bookstore': 2444, 'helpful': 7120, 'josiah': 8087, 'roller': 12314, 'blading': 2286, 'contrary': 3778, 'tributary': 14915, 'endlessly': 5213, 'malicious': 9128, '_jolene': 700, 'yuk': 16454, 'fa': 5528, 'thoug': 14539, 'dent': 4412, 'theyre': 14497, 'celebritytweet': 3092, 'cuckoo': 4064, 'chirping': 3298, 'scrambled': 12648, 'fartingloud': 5616, 'delish': 4390, 'eeeeeeek': 5087, 'ahhhaaaaa': 1128, 'nola': 10185, 'screenshots': 12668, 'released': 12020, 'funnn': 6178, 'celtic': 3097, 'borde': 2471, 'frasier': 6041, 'twelve': 15061, 'aaaaaawwwesome': 859, 'cheeky': 3221, 'handheld': 6887, 'detour': 4486, 'frighten': 6106, '_sprigge': 805, 'skydivers': 13182, 'thousands': 14546, 'planes': 11124, 'fifties': 5737, 'silence': 13081, '_laertesgirl': 717, 'woodvine': 16103, 'zoe': 16503, 'thorne': 14533, 'programme': 11506, 'moreton': 9705, 'stroppy': 13872, 'teenager': 14334, 'bladder': 2285, 'sorehead': 13449, 'creators': 3966, 'developers': 4494, 'hving': 7476, 'sandwichesss': 12534, 'grillz': 6675, 'plaque': 11137, 'unity': 15276, 'mcr': 9299, 'yaa': 16274, 'fond': 5930, 'kittens': 8321, 'balloon': 1898, 'duh': 4943, 'necks': 9997, 'spoilt': 13590, '4days': 310, 'competitions': 3642, 'hk': 7224, 'twitterers': 15096, 'leather': 8580, 'size': 13151, 'boogah': 2432, 'contain': 3758, 'happenning': 6917, 'stackeoverflow': 13657, 'youuu': 16439, 'wattup': 15735, 'jv': 8155, 'twurl': 15131, 'nl': 10158, 'ogzbd': 10382, 'sweat': 14124, 'contracts': 3777, 'uti': 15391, '_duerden': 625, 'jealousmuch': 7982, 'outttt': 10600, 'dragon': 4836, 'attracted': 1683, 'tabloid': 14195, 'headlines': 7030, 'labor': 8421, 'investigation': 7807, 'ec': 5042, 'gamers': 6248, 'webdesign': 15778, 'punch': 11610, 'nikki': 10131, 'bracelet': 2540, 'grea': 6635, 'dix': 4699, 'gush': 6766, 'unbelievable': 15215, 'tscc': 14972, '_envy': 631, 'sunburnnn': 14016, 'achurley': 962, 'nauseatingly': 9972, 'saddd': 12466, 'resolve': 12130, 'questioning': 11699, 'scoreless': 12639, 'sized': 13152, 'snickers': 13320, 'screamin': 12658, 'nefuew': 10015, 'forehead': 5966, 'klutz': 8332, '_lynn': 730, '_raquel': 778, 'yal': 16290, 'freedom': 6059, 'whever': 15901, 'notices': 10244, 'framework': 6029, 'sl': 13187, 'product': 11491, 'registration': 11981, 'comms': 3620, 'bruises': 2668, 'allmothers': 1215, '_ladie': 716, 'swamp': 14119, 'observatory': 10326, 'reports': 12092, 'connecticut': 3729, 'warbler': 15677, 'metzger': 9424, 'magee': 9077, 'sleeep': 13204, 'disappearing': 4613, 'kaila': 8164, 'ocampo': 10334, 'rainbowholic': 11758, 'eerie': 5095, 'demad': 4399, 'jailbreaking': 7936, 'stacie': 13655, 'pple': 11363, 'dayquil': 4266, 'equals': 5287, 'dieing': 4542, 'dozen': 4827, 'amazzzing': 1286, 'didgeridoo': 4534, 'missy': 9567, 'higgins': 7174, 'voegele': 15567, 'nocturnals': 10169, 'kellie': 8223, 'bffls': 2187, 'dates': 4245, 'brokennn': 2642, 'vry': 15593, 'capacity': 2944, 'sneezed': 13316, 'prime': 11442, 'minister': 9513, '21st': 151, 'appology': 1485, 'recall': 11875, 'flame': 5845, 'stream': 13837, 'riot': 12249, 'smitten': 13286, 'tryin': 14966, '669l2': 431, '511q': 371, 'jijr': 8026, 'hulz': 7431, '511r': 372, '2ve': 220, 'xpg0': 16260, '_davis': 614, 'conference': 3696, 'ban': 1907, '_beachmex': 582, 'bare': 1942, 'coach': 3502, 'definently': 4360, 'sexify': 12821, '67i82': 444, 'bribe': 2598, 'geje': 6314, 'rqaoe': 12381, 'underpaid': 15229, 'theraflu': 14486, 'drowsy': 4902, 'watering': 15727, 'cancelling': 2925, 'javaon': 7967, 'guinness': 6751, 'coogars': 3800, 'blokey': 2349, 'towel': 14818, 'dash': 4237, 'budurl': 2707, 'f9p5': 5527, 'collect': 3559, 'acquired': 969, 'freecycle': 6057, 'turtles': 15017, '4wehl': 350, 'yosemite': 16415, 'throwback': 14565, 'gott': 6563, 'shuts': 13029, 'basil': 1971, 'greatness': 6643, 'telegraph': 14349, '_elmo_': 628, 'awhh': 1779, 'fof': 5908, 'recovered': 11918, 'x116r': 16229, 'potential': 11336, 'ilove': 7572, 'physically': 11011, 'delicio': 4385, 'toilet': 14707, 'yayyyyyyyyyyy': 16313, 'urself': 15359, '4sx96': 332, 'smokers': 13290, 'vandalize': 15431, '14th': 77, 'deaths': 4304, '_foster': 643, 'themes': 14478, 'mariners': 9189, 'trustyfotografie': 14961, 'irked': 7842, 'kristine': 8384, 'be4': 2030, 'waraw': 15675, 'taipei': 14215, 'eluded': 5162, 'earlyer': 5006, 'thot': 14538, 'spill': 13568, 'corn': 3840, 'flakes': 5843, 'harmon': 6950, '_pierce': 774, 'nx': 10304, '01': 4, 'ncc': 9981, '1701': 97, 'popcorn': 11276, 'peeling': 10878, 'champion': 3143, 'defunct': 4367, 'cebu': 3080, 'gems': 6318, 'haileys': 6835, 'prayers': 11376, 'backseat': 1844, 'crud': 4020, 'celebs': 3093, 'frikken': 6109, 'baaaaaaaaaaackkkkkkkk': 1812, 'chops': 3321, 'lolly': 8840, 'gna': 6460, 'broom': 2649, 'standards': 13677, 'seatbelt': 12701, 'whispers': 15918, 'swiffer': 14147, 'snoring': 13331, 'eva': 5360, 'whip': 15911, 'hellyeah': 7116, 'err': 5309, 'timthumb': 14639, 'listning': 8749, 'kurt': 8397, 'spin': 13571, 'differents': 4552, 'goingg': 6489, 'vermouth': 15470, 'argue': 1537, 'complain': 3647, 'baadly': 1814, 'crop': 4008, 'fountain': 6016, 'youth': 16436, 'supastition': 14042, 'imeem': 7591, 'kpeqpg7vuy': 8376, 'laminate': 8460, 'footy': 5954, 'pour': 11344, 'dt': 4922, 'bklyn': 2274, 'umbrellaless': 15199, 'dum': 4945, 'diggin': 4562, 'gotdamn': 6560, 'angle': 1345, '_jenkins': 699, 'explosion': 5488, 'gandang': 6254, 'mas': 9224, 'mahal': 9089, 'sya': 14164, 'eeet': 5092, 'charming': 3191, 'shutters': 13030, 'relocate': 12031, 'challanges': 3135, 'photography': 10999, 'neko': 10031, 'offering': 10367, 'stevens': 13755, 'waterhouse': 15726, 'jhy': 8023, 'catbook': 3036, '6402509': 425, 'presence': 11409, 'subconscience': 13916, 'parcs': 10759, 'internets': 7778, 'alwaysss': 1260, 'cuuuba': 4123, 'upshot': 15339, 'uhh': 15183, 'nuh': 10278, 'scout': 12646, 'athletes': 1655, 'joanna': 8045, 'hayes': 7001, 'heather': 7063, 'bown': 2523, 'digging': 4563, '_greenwizard': 657, 'shorty': 12986, 'anywayss': 1431, '_oliver': 766, 'jamies': 7944, 'reflecting': 11954, 'sunlight': 14025, 'improperly': 7621, 'hammy': 6875, 'turtle': 15016, 'contract': 3775, 'spazz': 13523, 'relaxant': 12010, 'narcotic': 9947, 'moist': 9624, 'looong': 8879, 'ofc': 10357, 'genuinely': 6334, 'nuthin': 10292, 'waaaa': 15606, 'lemsip': 8616, 'apaently': 1441, 'ittt': 7894, 'sunfay': 14023, 'afterall': 1076, 'reali': 11840, 'deadpool': 4294, 'reynolds': 12198, 'lucas': 8976, 'czech': 4147, 'republic': 12099, 'collector': 3562, 'logan': 8819, 'bc': 2022, 'reds': 11937, 'hollywood': 7266, 'nai': 9917, 'trapeze': 14879, 'teaches': 14309, 'whooo': 15941, 'hoooo': 7308, 'journalism': 8092, 'olina': 10414, 'shoreee': 12973, 'foo': 5939, 'kontakt': 8369, 'exploit': 5485, 'kindred': 8301, 'racers': 11733, 'hammered': 6873, 'dramathon': 4843, 'marc': 9172, 'travels': 14889, 'mills': 9491, 'martini': 9215, 'fcking': 5657, 'overseas': 10625, 'reiki': 11992, 'reiki88': 11993, 'aitn': 1158, 'starburst': 13688, 'stu': 13878, 'lantz': 8482, 'hearn': 7049, '1030': 30, 'norton': 10225, 'mmmmm': 9594, 'loveeeeeeeeeeeeeeeeeeeeee': 8941, 'raimi': 11755, 'arrgghhhggguuuiiissshhhh': 1567, 'adoring': 1034, 'sash': 12553, 'sil': 13080, 'jacqueline': 7930, 'wilson': 15992, 'cbbc': 3068, 'ram': 11772, 'designated': 4454, 'angie': 1344, '_ahoy': 566, 'career': 2971, 'scrubbing': 12677, 'amigui': 1304, 'moan': 9604, 'whinge': 15909, 'trains': 14860, 'seann': 12691, 'scott': 12643, 'proof': 11530, 'ofcoooursee': 10358, 'pluss': 11197, 'incentive': 7634, 'directv': 4602, 'ooops': 10481, 'ridiculously': 12233, 'parking': 10770, 'bedalii': 2081, 'doll': 4750, 'streetcar': 13841, 'plis': 11181, '253ce': 173, 'doberman': 4721, 'skies': 13164, 'yards': 16300, 'noone': 10196, 'embossers': 5176, 'papersource': 10746, 'dijon': 4569, 'vcr': 15440, 'jessica': 8011, 'alba': 1171, '138': 65, '468': 299, 'movement': 9764, '9followers': 533, 'pooch': 11258, '_tickle': 820, 'pan': 10722, 'doubles': 4800, 'dish': 4650, '_connors': 603, '1999': 115, 'complex': 3655, 'condolences': 3692, 'vega': 15443, 'fireeeeee': 5799, 'robs': 12287, 'sammy': 12516, 'microsize': 9450, 'debby': 4307, 'burns': 2770, 'reaaaallly': 11823, 'copies': 3828, '1doe': 117, 'princes': 11446, 'crabs': 3914, '4d8': 309, 'horten': 7349, 'moss': 9734, 'kï': 8408, '182': 102, 'general': 6320, 'admission': 1019, 'written': 16195, 'references': 11952, 'strikes': 13861, 'burton': 2779, 'sod': 13359, 'nugget': 10276, 'sorrryy': 13457, 'broth': 2652, 'sorrrry': 13455, 'kl': 8329, 'bbyshower': 2021, 'jays': 7973, 'sharon': 12864, '_kinda_guy': 709, 'politics': 11242, 'makerfaire': 9113, 'forgets': 5976, 'lovelovelove': 8947, 'sickk': 13048, 'sistah': 13131, 'lax': 8544, 'sources': 13483, 'tails': 14213, 'youself': 16433, 'lynne': 9032, 'mmmmmmmm': 9596, 'charmer': 3190, 'haox': 6909, 'weirdos': 15823, 'individual': 7664, 'vehicle': 15451, 'qfc': 11658, 'broad': 2635, 'taxes': 14295, 'images': 7585, 'weï': 15853, 'grumbles': 6719, 'eink': 5119, 'shortcut': 12977, 'burst': 2777, 'moldy': 9633, 'squishy': 13639, 'reasons': 11864, 'cracker': 3918, 'kaitlyn': 8165, 'bobbi': 2395, 'lewis': 8644, 'tantrum': 14255, 'apologetic': 1455, 'cor': 3833, 'baccck': 1828, 'poolside': 11264, 'toniht': 14740, 'proves': 11553, '3pjnc': 278, 'pointed': 11220, 'involved': 7818, 'iphon': 7827, 'maxim': 9267, 'treats': 14899, 'loaders': 8796, '_control': 604, 'babyy': 1825, '333': 245, 'tuuneee': 15023, 'sooooooooooo': 13435, 'consuming': 3750, 'engrossing': 5228, 'salute': 12507, 'af': 1060, 'hairs': 6844, 'inboxes': 7631, 'headsup': 7035, 'brazillians': 2567, 'expression': 5496, 'eqbwe': 5286, 'cussing': 4106, 'slutted': 13254, 'favorites': 5648, 'diary': 4527, 'clashes': 3407, 'uswitch': 15386, 'euphoria': 5352, 'allll': 1208, '9pm': 535, 'enna': 5243, 'kodumai': 8363, 'idhu': 7524, 'arrr': 1575, 'bwahahaha': 2820, 'inflicted': 7681, 'iloveyoutwoooo': 7577, 'painted': 10696, 'looming': 8875, 'panda': 10727, 'redi': 11930, 'frens': 6078, '2gather': 197, 'manchester': 9144, 'wayyyyy': 15751, 'alien': 1193, 'congratses': 3720, 'tew': 14414, 'stunning': 13902, 'pineapple': 11065, 'pies': 11038, 'pos': 11305, 'mayday': 9275, '_cantus_': 594, 'brewing': 2595, '½20': 16525, 'dippin': 4593, 'iz': 7909, 'levels': 8643, 'topped': 14770, 'jenson': 8004, 'leigh': 8610, 'evo': 5404, 'typos': 15148, 'dealer': 4296, 'sentence': 12762, 'uuuups': 15403, 'doesnï': 4738, 'booored': 2456, 'expert': 5477, 'fightstar': 5742, 'mercury': 9397, 'dbm4n6': 4278, 'cnkhev': 3497, 'musicmonday': 9856, 'freemusic': 6065, 'rash': 11792, 'whyyyy': 15957, 'livechat': 8762, 'cus': 4104, 'weekdays': 15798, 'reupload': 12180, 'aaaaaahhhhhhhh': 858, 'sweeeeeeet': 14132, 'etha': 5342, 'sens': 12753, 'matic': 9254, 'bradie': 2543, 'nitentdo': 10149, 'ds': 4916, 'showcase': 13003, 'excercise': 5429, 'shemar': 12899, 'moore': 9695, 'embassy': 5174, 'madame': 9064, 'tussaud': 15018, 'nothinbg': 10239, 'streets': 13842, 'dooo': 4781, 'tweetshrinking': 15055, 'tweed': 15031, 'tenenen': 14376, 'tenen': 14375, 'whooaaa': 15939, 'overwheolming': 10634, 'itus': 7896, 'worrying': 16157, 'visits': 15549, 'saaaaaaaaaaaaaad': 12454, 'charley': 3186, 'horses': 7347, 'directors': 4600, 'uhhhg': 15185, 'lovingly': 8959, 'vengaboys': 15457, 'zane': 16483, 'lowe': 8964, 'wolf': 16079, 'gladiators': 6430, '_boduch': 588, '67': 436, 'voucher': 15587, 'onna': 10453, 'sleeeeeeeepy': 13200, 'wholey': 15936, 'evolution': 5405, 'wholely': 15935, 'composed': 3661, 'hateeee': 6971, 'eek': 5093, 'beatweetup': 2055, 'badge': 1852, 'counter': 3875, 'nerves': 10042, 'westin': 15850, 'discounts': 4630, 'anime': 1356, 'expoï': 5494, 'provides': 11557, 'hotels': 7362, 'expo': 5490, '2396': 163, 'nowadays': 10261, 'oregon': 10534, 'sandiego': 12527, 'cda': 3075, 'convert': 3792, 'mp3': 9782, 'dinners': 4584, 'fintster': 5794, '_thrifty': 819, 'pairs': 10701, 'aberdeen': 897, 'forsaken': 5994, 'su0yr': 13912, 'footyball': 5955, 'llama': 8778, 'riah': 12211, 'shakas': 12838, 'againn': 1089, 'salads': 12496, 'cydia': 4140, 'reload': 12030, 'springboard': 13618, 'whattaburger': 15880, 'initial': 7690, 'rant': 11785, 'twune': 15130, 'felicia': 5687, 'carrrr': 3003, '08kaifj': 15, 'shark': 12862, 'micah': 9442, 'provided': 11554, 'blacks': 2284, 'overhere': 10617, 'dub': 4925, 'loveees': 8942, 'temp': 14365, '15c': 82, '30c': 230, 'sickee': 13044, 'breakingg': 2577, 'glitch': 6442, 'mix': 9581, 'delivered': 4392, 'ducks': 4935, 'magical': 9081, 'northbound': 10222, 'tuning': 15004, 'raising': 11768, 'handz': 6897, 'limp': 8706, 'coquitlam': 3832, 'logo': 8828, 'shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit': 12917, 'meeeaaannn': 9339, 'tai': 14211, 'rblpn': 11813, '5z10': 408, 'churchill': 3349, 'downs': 4821, 'goods': 6517, 'announcement': 1378, 'trekked': 14904, 'talkers': 14235, 'unabashedly': 15208, 'byw': 2831, 'spoon': 13598, 'screening': 12664, 'el': 5126, 'capitan': 2948, 'guniea': 6760, 'eachother': 4995, 'eeeeevvveeerrrr': 5088, 'hash': 6962, 'tags': 14209, 'novemeber': 10259, '_haze': 670, 'ray': 11809, 'appeared': 1467, 'aots': 1438, 'nekkid': 10030, 'religion': 12026, 'wen': 15839, 'venus': 15466, 'thngs': 14523, 'stuf': 13892, '_sis': 799, 'ging': 6394, 'summaa': 13993, 'declined': 4330, 'thehodge': 14470, 'inclusion': 7643, 'mono': 9663, 'dh': 4510, 'crumbling': 4026, 'graduates': 6596, 'cudve': 4069, 'kdg': 8208, 'cheek': 3219, 'americ': 1294, 'lingering': 8717, 'sipping': 13125, 'partyyyy': 10793, 'bang': 1915, 'pleaseeeeee': 11166, 'contentment': 3762, '7pm': 481, 'dat': 4239, 'enthusiastic': 5259, '888': 498, 'cubes': 4061, 'vacationing': 15415, 'clogged': 3454, 'mooorrreeee': 9693, 'movin': 9770, 'arghhh': 1535, 'korean': 8372, 'canazarro': 2918, 'bricked': 2600, 'secured': 12710, 'element': 5137, 'dom': 4756, 'cob': 3508, 'alexi': 1183, 'mh': 9434, 'lok': 8831, 'headeache': 7024, 'toda': 14691, 'grice': 6669, 'kc': 8206, 'pque': 11365, 'las': 8498, 'pinas': 11060, 'kenan': 8227, 'distorted': 4676, 'disapointed': 4610, 'mmmmmm': 9595, 'jazzy': 7975, 'catchy': 3040, 'krogers': 8386, 'journaling': 8091, 'shish': 12937, 'pennies': 10894, 'balance': 1883, 'millionaire': 9490, 'hellooooooo': 7114, '_augustine': 574, 'coles': 3547, 'gashes': 6282, 'steakhouse': 13734, 'woowoo': 16122, 'danger': 4205, 'adsense': 1038, '47': 300, '060': 9, 'inr': 7712, 'tigerheat': 14612, '_liljess_x': 721, 'nonlong': 10190, 'gardens': 6272, 'impulse': 7624, 'buys': 2815, 'envelope': 5266, '2d': 192, 'vids': 15503, 'disabled': 4607, 'wishhh': 16034, 'prehistoric': 11395, 'patricia': 10822, 'cope': 3826, 'filter': 5765, 'gracious': 6587, 'wrecking': 16183, 'soaking': 13343, 'joys': 8100, 'ulcers': 15191, '4jaw9': 313, 'chelsey': 3241, 'affected': 1064, 'behaved': 2108, 'appreci': 1486, '_sb': 792, 'procrastinating': 11484, 'cornwall': 3844, 'phenomenal': 10968, 'heheï': 7096, '½i': 16535, 'confit': 3705, 'duck': 4931, 'canad': 2912, 'burning': 2769, 'piglets': 11044, 'chucks': 3346, 'ashton': 1607, 'slacking': 13190, 'pilates': 11050, 'fastest': 5626, 'gch': 6297, 'chiodos': 3291, 'dissapointed': 4669, 'heyyhoo': 7157, 'heyhey': 7153, 'dull': 4944, 'uncool': 15222, 'discouraged': 4631, 'todayyyyyy': 14697, 'starsailor': 13696, 'duong': 4962, '19': 107, 'cnn': 3498, 'foto': 6009, 'finden': 5775, 'hea': 7016, 'bracket': 2541, 'dallas': 4175, 'odessey': 10351, 'ideal': 7519, 'terrific': 14402, 'aid': 1140, '2x2924': 221, '1x2610': 126, '3x2500': 283, '1841': 103, 'desktops': 4464, 'slpy': 13248, 'zzzz': 16521, 'luncheon': 9001, 'planters': 11133, 'ou': 10571, 'bom': 2415, 'apetite': 1450, 'nobodys': 10168, 'dips': 4594, 'francisco': 6033, 'djspy': 4707, 'playdate': 11146, 'transmission': 14875, 'aah': 880, 'woofers': 16105, 'diagram': 4519, 'mainstream': 9103, 'adoption': 1030, 'curve': 4102, 'grr': 6703, 'discussion': 4641, 'loll': 8835, 'kansai': 8175, 'current': 4097, 'groups': 6696, 'juniors': 8140, 'korea': 8371, 'refs': 11962, 'yao': 16296, 'playoffs': 11154, 'cruising': 4024, 'wcf': 15757, 'subscribe': 13925, 'everr': 5378, '30stm': 236, 'weeooow': 15804, 'pfffffffffffffffffffffftttttttt': 10955, 'tourney': 14810, 'coool': 3815, 'wasnï': 15706, 'eeeh': 5089, 'whattttever': 15882, 'choc': 3304, 'chip': 3292, 'voicemail': 15570, 'ffs': 5722, 'suse': 14097, 'shweeeeet': 13035, 'cï': 4149, 'qua': 11674, 'jmccartney': 8041, 'twittpic': 15119, 'diddy': 4533, 'spooning': 13599, 'opportunity': 10509, 'wakes': 15639, '67xv3': 450, 'former': 5985, 'pawnshop': 10840, 'jimi': 8030, 'cashing': 3020, 'sickly': 13052, 'yippeee': 16391, 'enlargement': 5238, 'steph': 13744, 'improve': 7622, '2837': 183, 'heavennn': 7067, 'stare': 13690, 'chandler': 3150, 'meal': 9305, 'corey': 3838, '30pm': 232, 'noones': 10197, 'dcgeyv': 4282, 'elet': 5142, 'br': 2538, 'dampen': 4191, 'sliverlight': 13234, 'mariahs': 9186, 'pci': 10857, 'sets': 12803, 'cytheria': 4145, 'hulu': 7430, 'thaanks': 14430, 'wokking': 16078, 'amateur': 1273, 'ark': 1544, 'builder': 2727, 'political': 11240, 'affiliation': 1066, 'wana': 15660, 'manning': 9158, 'norwood': 10227, 'haaaaaa': 6789, 'yaaaaaaay': 16275, '_in': 686, 'raul': 11804, 'julia': 8124, 'wimpers': 15994, 'hyped': 7486, 'mmemarko': 9591, 'schilderweb': 12622, 'homepage': 7279, 'pritchard': 11460, 'olds': 10412, 'hairloss': 6842, 'voyed': 15590, 'slumdog': 13251, 'wbt6': 15755, 'cooks': 3808, 'happeh': 6912, 'blankie': 2299, 'icebox': 7508, 'stomatch': 13793, 'wacom': 15619, 'gorgeou': 6548, 'donny': 4771, 'nottt': 10252, 'commercials': 3613, 'dared': 4220, 'wealthy': 15762, 'minted': 9523, 'plural': 11194, 'mope': 9698, 'nos': 10228, 'livewriter': 8770, 'accessible': 932, 'sniffles': 13322, 'jaunty': 7965, 'jackalope': 7921, 'intrepid': 7790, 'ibex': 7501, 'among': 1310, 'giveaways': 6418, 'tati': 14285, 'emal': 5168, 'nastiness': 9954, 'bestfriends': 2159, 'wheniwerealad': 15894, 'explanations': 5483, 'pleeezzze': 11178, 'madeleines': 9069, 'baking': 1881, 'answerer': 1392, '67gzx': 441, 'presents': 11414, 'sisa': 13128, 'afterwork': 1083, 'blogs': 2346, 'tuned': 15001, 'kwijt': 8403, 'pinging': 11069, 'opera': 10498, 'kellynn': 8225, '789': 472, 'pacman': 10678, 'stronger': 13869, 'upwards': 15345, '600k': 416, 'restart': 12138, 'offered': 10366, 'shivashankar': 12943, 'spellin': 13551, 'inertial': 7674, 'oneself': 10449, 'unofficially': 15291, 'fi': 5724, 'chota': 3330, 'reflexie': 11956, 'arcade': 1515, 'areply': 1530, 'buggin': 2718, 'sole': 13378, 'supporter': 14064, 'credi': 3968, 'basics': 1970, 'analysis': 1324, 'theatres': 14465, 'twittttty': 15122, 'bores': 2482, 'bullied': 2739, 'needing': 10003, 'math11': 9252, 'acct1b': 947, 'bio19': 2236, 'dreary': 4866, 'rubbing': 12393, 'jessi': 8010, 'quiero': 11705, 'rosa': 12342, 'guadalupe': 6731, 'damm': 4182, 'virgins': 15529, 'reactions': 11830, 'stroking': 13866, 'welts': 15838, 'ie': 7535, '30s': 234, 'earbud': 4999, 'ceased': 3079, 'shure': 13026, 'workyy': 16144, '3lin': 274, 'uninvited': 15268, 'deid': 4374, 'bd': 2027, 'sundayyyy': 14021, 'inshalla': 7719, 'devo': 4499, 'astor': 1640, 'dope': 4785, 'smashing': 13270, 'reznor': 12200, 'mariqueen': 9192, 'creme': 3978, 'brulee': 2669, 'tiramisu': 14653, 'models': 9617, 'banjo': 1921, 'tooie': 14750, 'springfield': 13619, 'noisy': 10183, 'piggls': 11043, 'pickles': 11021, 'needless': 10006, 'drool': 4893, '33hus': 251, 'yeyah': 16379, 'fink': 5789, 'popeye': 11278, 'hopin': 7328, 'helsinki': 7124, 'presentations': 11412, 'ankle': 1360, 'stupido': 13906, 'catwalk': 3050, 'janes': 7950, 'sm': 13257, 'sweatshirt': 14128, 'doughnuts': 4807, 'gardening': 6271, 'paintings': 10698, 'mi': 9438, 'ideia': 7521, '_ming': 743, 'gg': 6358, 'sch': 12613, 'attachment': 1667, 'yan': 16292, 'kinny': 8307, 'jayem': 7971, 'b2b': 1806, 'krisisdnb': 8380, 'asx': 1647, 'worm': 16149, 'volt': 15575, 'dxd': 4978, 'didnï': 4538, 'ood': 10465, 'diana': 4522, 'tutor': 15020, 'c1': 2833, 'c2': 2834, 'lulz': 8995, 'notting': 10251, 'prick': 11436, 'touching': 14801, 'handshakes': 6893, 'pokes': 11230, 'ys': 16447, 'hairrr': 6843, 'treasures': 14894, 'seos': 12767, 'submitting': 13921, 'information': 7684, 'pains': 10693, 'handful': 6886, 'wellllllllllllll': 15835, 'pllleeeaaassseeeeee': 11183, 'munching': 9836, 'rent': 12065, 'unexpected': 15245, 'complications': 3657, 'grapes': 6620, '4wauk': 346, 'ic': 7503, 'iyaa': 7908, 'lasts': 8506, 'buddi': 2702, 'blasting': 2302, 'celine': 3094, 'dion': 4587, 'untill': 15311, 'rides': 12231, 'procrastination': 11485, 'assumption': 1636, 'rum': 12412, 'gin': 6392, 'preferred': 11391, 'previously': 11430, 'whit': 15919, 'fidel': 5732, 'shemms': 12900, 'wxsgy': 16223, 'sanctity': 12522, 'tainted': 14214, '360': 258, 'mercenaries': 9396, 'explosions': 5489, 'database': 4241, 'superstition': 14052, 'frost': 6120, 'nofakery': 10175, 'blacklisted': 2282, 'warner': 15688, 'bros': 2651, 'straw': 13833, 'manly': 9155, 'babygirl': 1822, 'crazier': 3949, 'appearance': 1466, 'itouch': 7887, 'noggin': 10176, 'rey': 12196, 'mysterio': 9897, 'sleepin': 13208, 'aga': 1085, 'waterfront': 15724, 'princess_i': 11448, 'butineedhelp': 2796, 'csi': 4051, 'aus': 1709, 'silicone': 13083, 's05e04': 12448, 'evryone': 5407, 'pleaseee': 11163, 'toss': 14787, 'pd': 10859, 'kittie': 8322, 'berlin': 2150, 'holllaaa': 7262, 'analytics': 1325, 'tostitos': 14792, 'piknik': 11049, 'dip': 4590, 'unfortunate': 15253, '_la_mania': 715, 'mask': 9230, 'whoooops': 15943, 'confusion': 3713, 'competetion': 3639, 'counting': 3877, 'jibber': 8024, 'rodney': 12304, 'chasing': 3197, 'fireflies': 5800, 'stfu': 13757, 'moooooorning': 9691, 'depot': 4428, 'plywood': 11198, 'apprentice': 1491, 'upppp': 15332, 'tourture': 14812, 'lovies': 8956, 'thinker': 14510, 'loool': 8878, 'killen': 8285, 'recharge': 11884, 'aaahhh': 872, 'showers': 13007, 'meanwhile': 9315, 'bigweekend': 2219, 'awaits': 1745, 'misse': 9553, 'ties': 14608, 'rebound': 11873, 'meetin': 9344, 'jewelery': 8019, 'pooling': 11262, 'gremlin': 6662, 'challenging': 3139, 'slipped': 13230, 'unplugging': 15296, 'ouchhhhhh': 10573, 'frkn': 6112, 'watir': 15732, 'wiki': 15975, 'hydro': 7484, 'abandoned': 888, 'niley': 10133, 'doppppe': 4786, 'moshie': 9731, 'moshhhh': 9730, 'blistered': 2333, 'yaaaaaay': 16276, 'jaydiohead': 7970, 'twt': 15128, '91610': 521, 'vegetables': 15448, 'hellboy': 7109, 'acsm': 972, 'unfathomable': 15247, 'bedrooms': 2083, 'revive': 12192, 'spirits': 13576, 'bun': 2754, 'vicodin': 15494, 'bjaday': 2270, 'drivers': 4887, '_mean': 740, 'niece': 10111, 'lite': 8752, 'p250': 10662, 'vcenter': 15439, 'x64': 16239, 'compability': 3628, 'matrix': 9256, 'grabbing': 6582, 'carafe': 2956, 'caffiene': 2851, 'amcmain': 1290, 'teh': 14342, 'vague': 15421, 'southland': 13487, 'raw': 11807, 'misunderstood': 9573, 'nephews': 10036, 'cowering': 3903, 'pigeon': 11040, 'feathers': 5665, 'sittting': 13142, 'tyra': 15149, 'scream': 12656, 'rrrrr': 12386, 'richmond': 12220, 'conditions': 3689, 'goodtimes': 6518, 'pitch': 11096, 'legends': 8600, '6nkpuz': 455, 'manned': 9157, 'overspend': 10626, '11th': 49, 'saget': 12487, 'kit': 8316, 'avaialble': 1723, 'dhc4hg': 4512, 'mod': 9613, 'giftcert': 6376, 'hipfabric': 7204, 'accent': 924, 'lucy': 8988, 'gweg': 6778, 'thoughtful': 14543, 'conti': 3769, 'yjpq': 16393, 'dadgum': 4166, 'nations': 9963, 'freight': 6071, 'carriers': 3001, 'fella': 5690, 'edmonton': 5073, 'nokia': 10184, 'mozilla': 9780, 'os': 10556, 'operating': 10501, 'journalists': 8093, 'prs': 11563, 'ironing': 7847, 'oceans': 10342, 'madly': 9071, 'monte': 9674, 'cristo': 3999, 'sandwich': 12531, 'bi': 2199, 'focals': 5905, 'irresponsible': 7854, 'transatlantic': 14865, 'flights': 5866, 'lolol': 8842, 'ohshit': 10390, 'pleaseeeeeeeeeee': 11167, '_rosie': 788, 'everrrr': 5380, 'bandwidth': 1914, 'scholl': 12624, 'sandal': 12525, 'inserts': 7718, 'abit': 901, 'loocie': 8867, 'soonish': 13425, 'ane': 1336, 'dining': 4581, 'plangi': 11126, 'lung': 9006, 'coat': 3506, 'cfs': 3119, 'midday': 9454, 'millenia': 9487, 'bulb': 2732, 'syncing': 14179, 'blairr': 2290, '80th': 488, 'fallower': 5573, 'xxxxxxxx': 16271, 'asbestos': 1598, 'backroom': 1841, 'processing': 11480, 'technique': 14325, 'pp': 11360, 'buffalo': 2711, 'worshipper': 16159, 'schack': 12614, '4w2ls': 337, 'cauzinhoooo': 3057, '_bandoni': 580, 'environmental': 5269, 'kbs': 8205, 'rescuers': 12120, 'dru': 4904, 'mickey': 9447, 'sequels': 12774, 'brutal': 2678, 'nate': 9959, 'judo': 8117, 'retail': 12155, 'therapy': 14487, 'awesomest': 1771, 'knooow': 8348, 'uve': 15405, 'avenue': 1732, 'fade': 5545, 'regrets': 11985, 'cigarettes': 3364, 'bands': 1911, 'headaaaaaaaaaaaache': 7018, 'ecpm': 5052, 'sly': 13256, 'keane': 8210, 'sunnn': 14027, 'finnalllyyy': 5792, 'dubbed': 4927, 'forwarding': 6006, 'germï': 6346, 'rodrï': 12305, '½guez': 16533, 'xxxrebelrebelxxx': 16266, 'pups': 11622, 'raven': 11805, 'thors': 14535, 'wonderfur': 16093, 'kittykisses': 8325, 'shitload': 12939, 'bananas': 1909, 'abandoning': 889, 'parvo': 10795, 'pup': 11618, 'moons': 9688, '94': 525, 'dndn': 4714, 'sanctuarysunday': 12524, 'sanctuary': 12523, 'requiem': 12106, 'hahahahahahahaa': 6824, 'vw': 15599, 'alriiightt': 1243, 'featured': 5667, 'ykyat': 16394, '37nnd': 263, 'pile': 11051, 'delay': 4376, 'cnyhp': 3500, 'touchin': 14800, 'starss': 13698, 'everyoneeeeeeeeee': 5392, 'items': 7884, 'etsy': 5347, 'beloved': 2132, 'barack': 1937, '626': 421, 'derbyshire': 4440, 'percy': 10911, 'thrower': 14567, 'gardner': 6273, 'residence': 12126, 'allo': 1216, 'winds': 16004, 'outfield': 10585, 'scanlon': 12593, 'hoarse': 7235, 'pastor': 10810, 'wiped': 16021, 'xxxxxxx': 16270, 'plebs': 11173, 'thennn': 14481, '72': 464, '65': 426, 'knack': 8334, 'melted': 9362, 'speaker': 13526, 'rings': 12245, 'oatmeal': 10319, 'gmail': 6456, 'altanta': 1249, 'flying': 5898, '_crow': 606, 'granted': 6617, 'poltergeist': 11248, 'neeeeeeeed': 10012, 'tedtalks': 14330, 'foul': 6014, 'dialed': 4520, 'payed': 10847, 'gameplay': 6247, 'eleminis': 5139, 'dhlq5t': 4514, 'screamm': 12660, '613': 420, 'installments': 7737, 'lottery': 8915, 'combo': 3580, 'wayyy': 15750, 'lvoe': 9025, 'triumph': 14934, 'luc': 8975, 'bourdon': 2515, 'ck': 3390, 'eztv': 5524, 'torrent': 14779, 'movieee': 9767, 'donkey': 4769, 'smokey': 13291, 'jemi': 7996, 'cramples': 3928, 'bursting': 2778, 'nab': 9908, 'larenz': 8492, 'fineass': 5779, 'tate': 14284, 'popularity': 11286, 'clubhouse': 3481, 'wl': 16065, 'spaces': 13505, 'aniya666': 1358, 'suspicion': 14103, 'standard': 13676, 'ourselves': 10579, 'remeber': 12038, 'dolphin': 4755, 'peers': 10882, 'creeper': 3974, 'swensens': 14146, 'woody': 16104, 'bullseye': 2741, 'crime': 3989, 'ooc': 10464, 'yearbook': 16334, 'narnia': 9949, 'blurb': 2377, 'aslan': 1616, 'skandar': 13157, 'hosp': 7352, 'pedicure': 10873, 'embarrassed': 5172, 'irate': 7839, 'callers': 2879, 'arre': 1563, 'followin': 5927, 'booooored': 2455, 'lindsay': 8709, 'fansite': 5596, 'ultimatelohan': 15193, 'rolled': 12313, '4we51': 349, 'bugzy': 2723, 'tinkered': 14643, 'virtualbox': 15531, 'numan': 10280, 'remixes': 12051, 'pipers': 11082, 'molars': 9631, '33333333333': 249, 'cooool': 3817, 'lovve': 8962, 'ro': 12269, 'shoooow': 12963, 'tmwr': 14680, 'uncomfortable': 15219, 'rss': 12388, 'borred': 2488, '400mb': 288, 'onpeak': 10458, 'downloads': 4819, 'appointments': 1484, 'farrah': 5611, 'cashflow': 3018, 'forecasts': 5965, 'myka': 9885, 'yeahs': 16332, 'softshock': 13369, 'jager': 7933, 'licked': 8660, 'trashed': 14882, 'jalapeno': 7939, 'chaparros': 3167, 'del': 4375, 'ee': 5081, 'minits': 9514, 'leff': 8594, 'librefm': 8658, 'audacious': 1690, 'combination': 3578, 'wink': 16012, 'matte': 9259, 'lcd': 8558, 'hongkong': 7297, 'international': 7776, 'ignor': 7544, 'lookd': 8869, 'familar': 5578, 'appropriate': 1494, 'seduced': 12715, 'homee': 7274, 'okiee': 10405, 'angels': 1342, 'makingfun': 9119, 'inlove': 7698, 'oooooooooo': 10479, 'dancin': 4201, 'steals': 13737, 'jumpstart': 8134, 'wotd': 16166, 'jape': 7958, 'qq': 11670, 'comppetitive': 3663, 'overcompetitive': 10612, 'responses': 12136, 'painfully': 10692, 'pange': 10732, 'ahahahahaha': 1121, '_truong': 824, 'hb': 7010, 'encore': 5205, 'snsd': 13334, '_85': 551, 'aaaargh': 866, 'conspiracy': 3742, 'lizzi': 8775, 'environment': 5268, 'tweeet': 15032, 'shareeee': 12859, 'monthly': 9677, '5days': 390, 'gangsterrr': 6259, 'unit': 15273, 'flordia': 5878, 'shrits': 13016, 'cinnamon': 3375, 'rolls': 12317, '4wp8l': 358, 'chilly': 3284, 'oracle': 10522, 'sock': 13356, 'summit': 14000, 'spew': 13560, 'withb': 16042, 'alrer': 1239, 'addictive': 998, 'slowed': 13243, 'quickly': 11704, 'creatur': 3967, 'dne': 4715, 'unbelievably': 15216, 'firing': 5806, 'districtlines': 4682, 'revertfashion': 12184, 'didntb': 4537, 'cy_k': 4132, 'cymk': 4141, 'legend': 8599, 'inotia': 7707, '31': 239, 'immediately': 7596, 'funnest': 6173, 'righ': 12238, 'moab': 9603, 'phil': 10971, 'fullest': 6153, 'rih': 12241, '4jhp8': 323, '67kb6': 446, 'youl': 16419, 'cert': 3112, 'excursion': 5444, 'mixer': 9583, 'reminders': 12046, 'hummmmm': 7439, 'thar': 14450, 'jayk': 7972, 'handmade': 6891, 'menudo': 9392, 'permanently': 10930, 'weathers': 15769, 'dazzle': 4276, 'widddd': 15962, 'als': 1245, 'peoplebrowsr': 10901, 'urge': 15349, 'sofas': 13364, 'tiling': 14624, 'shock': 12951, 'bonding': 2420, 'autumn': 1722, 'sheeeeit': 12882, 'claude': 3415, 'summahkayy': 13994, 'effin': 5104, 'overwhelming': 10633, 'birfdayy': 2245, 'soupy': 13480, 'heinz': 7099, 'bbl': 2015, 'win1': 15996, 'mito': 9579, 'stealing': 13736, 'tiesto': 14609, 'evicted': 5400, 'suspend': 14101, 'fewer': 5717, 'mem': 9364, 'leaks': 8570, 'replied': 12082, 'custom': 4108, 'designed': 4455, 'superfresh': 14046, 'somebodies': 13394, 'rails': 11753, 'sup': 14041, 'sits': 13138, 'summat': 13996, 'ju': 8111, 'witty': 16054, 'situational': 13144, 'equations': 5289, 'graphs': 6624, 'motivating': 9746, 'emotionally': 5189, '_rose': 787, 'follows': 5929, 'nites': 10150, 'haribo': 6943, 'relatively': 12007, 'vespa': 15477, 'deena': 4344, 'paranoif': 10758, 'lolz': 8844, 'retweets': 12177, 'bid': 2205, 'leaders': 8563, 'theatlantic': 14463, 'facebookhumor': 5538, 'mhtml': 9437, 'w12th': 15603, 'kaiboshed': 8163, 'wtg': 16205, 'dvsca': 4976, 'bh': 2195, 'technologically': 14327, 'challenged': 3137, 'tweetb4ueat': 15041, 'positivity': 11313, 'received': 11878, 'standin': 13679, 'maddest': 9066, 'cofffeeeeeee': 3532, 'gossiping': 6555, 'bia': 2200, 'lion': 8728, 'problematic': 11473, 'maps': 9167, 'snapped': 13308, 'katy': 8195, 'perry': 10934, 'soundtrack': 13478, 'arr': 1559, 'sowbur': 13491, 'promiss': 11526, 'styoopid': 13910, 'hazzunt': 7009, 'kum': 8395, 'owt': 10650, 'agane': 1091, 'itt': 7892, 'hyding': 7482, 'sumware': 14008, 'larffing': 8493, 'att': 1666, 'rambly': 11773, 'cathylo': 3045, 'fran': 6030, 'bestfriend': 2158, 'aubrey': 1687, 'pearl': 10869, 'uninteresting': 15266, 'snuggled': 13338, 'beneath': 2138, 'duvet': 4973, 'vibe': 15487, 'downed': 4810, 'barrio': 1953, 'records': 11911, 'cmyk': 3495, 'madrid': 9073, 'choke': 3312, 'py3': 11652, '_boo': 589, 'unfortunatley': 15255, 'amazed': 1276, 'perform': 10917, 'halu': 6865, 'yï': 16476, 'tï': 15154, 'tinh': 14641, 'thï': 14589, '½ng': 16542, 'sanh': 12538, 'mï': 9903, 'chï': 3354, 'pineforest': 11067, 'ojugsb': 10397, 'conjunctivitis': 3726, 'vedanta': 15442, 'vibrations': 15490, 'remembering': 12042, 'mazembe': 9280, 'lamps': 8463, 'offended': 10362, 'lovelies': 8945, 'okami': 10400, 'jose': 8082, 'noooooooooo': 10206, 'pointy': 11225, 'enw': 5271, 'kaput': 8178, 'swords': 14162, 'reallly': 11855, 'horrid': 7340, 'dealin': 4298, 'impatiently': 7603, 'hor': 7332, 'heartbreaks': 7053, 'rescue': 12118, 'bootcamp': 2461, 'knoxville': 8358, 'tabs': 14197, 'attics': 1681, 'eden': 5061, '½9': 16528, '½13': 16523, 'ct': 4056, 'steadily': 13731, 'pns': 11203, 'availble': 1727, 'tent': 14385, 'mosh': 9729, 'edgefesssssst': 5064, 'boyzone': 2536, 'shoesshoesshoes': 12957, 'yayyayyay': 16310, 'advance': 1044, 'sponsor': 13594, 'bea': 2031, 'eithe': 5122, 'appears': 1469, 'signs': 13074, '4jax3': 314, 'ableton': 903, 'chars': 3194, 'tarte': 14270, 'dde2v6': 4285, 'newsletter': 10077, '2for1': 195, 'awesomeupdater': 1772, 'loney': 8853, 'pimm': 11058, 'tattered': 14287, 'hoovering': 7315, 'warcraft': 15679, 'lazzzy': 8554, 'indonesia': 7667, 'stating': 13717, 'thatd': 14454, 'campus': 2910, 'cov': 3894, 'producers': 11490, 'mustard': 9864, 'veges': 15446, 'constructivist': 3749, 'isnï': 7867, 'hahahahahahahahahahahaha': 6826, 'cravings': 3942, 'crispy': 3997, 'skyping': 13186, 'allison': 1206, 'mooned': 9687, 'liverpool': 8768, '_brads': 590, 'curling': 4093, 'ribbon': 12213, '_de_baillon': 615, 'antibiotics': 1401, 'gnight': 6464, 'squirted': 13637, 'irrelevant': 7853, 'innocently': 7705, 'rda2009cla': 11819, 'labels': 8419, 'needy': 10008, '29': 185, 'guiness': 6750, 'bidding': 2206, 'dropping': 4897, 'urgent': 15350, 'persist': 10936, 'blossom': 2358, 'almonds': 1223, 'incorporate': 7649, '_girl': 653, 'notebook': 10235, 'wrinkles': 16188, 'canadians': 2915, 'tur': 15007, 'drumset': 4911, 'whaaaaaaaahhhh': 15857, 'sighs': 13063, '4wn9q': 357, 'nerding': 10039, 'andrew': 1330, 'yolanda': 16402, 'equivalent': 5291, 'overheardinlondon': 10614, 'runaway': 12417, 'weatherrrrr': 15768, 'knotts': 8349, 'mannnn': 9159, 'wagon': 15624, 'jogged': 8057, 'turbines': 15008, 'atlanits': 1658, 'bryan': 2679, 'wingnuts': 16009, 'ironic': 7846, 'auditions': 1699, 'virtues': 15534, 'roc': 12289, 'chu': 3343, 'creeped': 3973, 'dodgy': 4732, 'neighbourhood': 10025, 'lovesu': 8953, 'ababa': 887, 'rutledge': 12441, 'jbcg': 7977, 'magners': 9083, 'themselves': 14479, 'shell': 12894, 'slave': 13197, 'command': 3602, 'lifts': 8678, 'boo0o0o0o00oring': 2429, 'dollars': 4752, 'crazi': 3948, 'mustv': 9866, 'thunderstorming': 14582, 'listened': 8743, 'domination': 4759, 'grooveshark': 6686, 'tinysong': 14646, '36pz': 260, 'cahnge': 2855, 'pw': 11647, 'ufff': 15163, 'dc101': 4280, 'ant': 1396, 'djing': 4705, 'boooooo': 2452, 'ditto': 4688, 'wrapped': 16177, 'outtamyleague': 10599, 'pardon': 10760, 'relapse': 12002, 'explained': 5482, 'iloveyouuu': 7578, 'bamboozle': 1904, '_2008': 545, 'sadie': 12474, 'included': 7640, 'mental': 9383, 'ov': 10605, '13f5m0': 67, 'disbelief': 4621, 'alternatives': 1254, 'posited': 11309, 'tweepsland': 15038, 'shaved': 12873, 'beards': 2041, 'minority': 9521, 'phillies': 10974, 'gamee': 6246, 'oyy': 10659, 'geas': 6303, 'ts': 14971, 'crikey': 3988, 'vineri': 15518, 'nimic': 10134, 'sau': 12569, 'probabil': 11468, 'alt': 1247, 'pierdut': 11036, 'ease': 5019, 'syncs': 14180, 'appealing': 1464, 'clunky': 3486, 'foll0w': 5919, 'friidays': 6108, 'f0llowers': 5525, 'fjgkfld': 5834, 'sdh': 12683, 'hometown': 7284, 'turnon': 15013, 'dillematic': 4572, 'damp': 4190, 'adopted': 1028, 'ub40': 15156, '7af72': 477, 'russia': 12434, 'cracking': 3921, 'economics': 5049, 'tommorow': 14721, 'retaking': 12157, 'hva': 7474, 'shw': 13032, 'scifi': 12632, 'flake': 5842, 'jacks': 7928, 'metros': 9422, 'nextday': 10082, 'uprooted': 15334, '5jg6f': 398, 'bri': 2596, 'alreadi': 1235, 'craigslist': 3923, 'blouse': 2359, 'unconditonally': 15220, 'cupboards': 4082, 'terminal': 14392, 'ignorance': 7545, 'controls': 3784, 'marrying': 9210, 'lovable': 8928, 'guuud': 6774, 'wilshire': 15991, 'stalkerishly': 13669, 'middlesbrough': 9456, 'dreaded': 4853, 'katieheidie': 8193, 'sperm': 13559, 'clone': 3457, 'heater': 7062, 'suddenly': 13958, 'chelsea': 3239, 'hurtig': 7464, 'vaccines': 15418, 'todayyy': 14696, 'loveeya': 8943, '4wk9i': 354, 'pose': 11306, 'ericsson': 5303, 'disturb': 4683, 'maaate': 9045, 'grooovin': 6684, 'chain': 3129, 'sluzzaa': 13255, 'gumbo': 6754, 'tommorrow': 14722, 'description': 4447, 'picturisation': 11029, 'whom': 15937, 'picks': 11022, 'blury': 2379, 'tweep': 15034, 'businesses': 2787, 'sinking': 13117, 'youregreat': 16426, 'mirrors': 9535, 'thermal': 14492, '75c': 470, 'gpu': 6575, 'vin': 15515, 'greeting': 6659, 'subtl': 13929, 'prima': 11440, 'catchyy': 3041, 'clogging': 3456, 'clocked': 3453, 'comforting': 3593, 'savechuck': 12574, 'takin': 14223, 'pwned': 11650, 'shottie': 12989, 'exahausted': 5417, 'tallebudgera': 14242, '5pm': 403, 'bumped': 2751, 'shelaaaaaaaa': 12890, 'numero': 10284, 'sarcastic': 12551, 'toughest': 14803, 'firth': 5810, 'groupie': 6694, 'slovakian': 13241, 'siccck': 13041, 'redbull': 11926, 'toilets': 14708, 'toasting': 14688, 'waffles': 15621, 'crampsss': 3930, 'workmen': 16138, 'extending': 5505, '45am': 297, 'racket': 11740, 'birth': 2247, 'misshu': 9557, 'cal': 2862, 'irvine': 7855, 'tolerate': 14714, 'iemoticons': 7537, 'appstore': 1498, '79ï': 475, 'basketball': 1975, 'iron': 7844, 'curls': 4094, 'mason': 9231, '_b_10': 577, 'sed': 12713, 'zguqp': 16499, 'memorizing': 9372, 'prologue': 11518, 'canterbury': 2937, 'tales': 14230, 'juhs': 8121, 'florists': 5882, 'cassadee': 3025, 'fricken': 6088, 'florist': 5881, 'noob': 10193, 'wooooo': 16113, 'yankees': 16294, 'enlightening': 5241, 'masseusse': 9237, 'buttocks': 2805, 'quarter': 11682, 'inch': 7635, 'joyologist': 8098, 'freak': 6046, 'chipping': 3296, 'barks': 1947, 'strict': 13859, 'lonley': 8863, 'boyzzzz': 2537, 'gahd': 6229, 'wearin': 15764, 'brudder': 2664, 'prosper': 11541, 'worr': 16153, 'detention': 4483, '_gun': 660, '89': 501, '_b': 576, 'comming': 3614, 'outdated': 10583, 'bigoted': 2218, 'patronising': 10825, 'imperialist': 7604, 'monoculturalist': 9664, 'righteous': 12240, 'helloooo': 7113, 'hairsss': 6846, 'actinggg': 975, 'noooooooooooo': 10208, 'awesomely': 1768, 'slices': 13221, 'cheddar': 3217, '3000': 225, 'wafting': 15622, 'papaw': 10743, 'smoker': 13289, 'trop': 14943, 'awa': 1744, 'lofnotc': 8816, 'jordan': 8078, 'jeffs': 7991, 'seventeen': 12813, 'fascination': 5619, 'pretend': 11421, 'shakedown': 12841, 'tomm': 14720, 'seafood': 12687, 'chow': 3331, 'hall': 6851, 'noooowww': 10211, 'tomrrow': 14732, 'amisha': 1306, 'patel': 10814, '1984': 111, 'yesssssir': 16368, 'pampering': 10721, 'fem': 5695, 'kool': 8370, 'tink': 14642, 'senior': 12751, 'recognition': 11894, 'carpet': 2998, 'booze': 2466, 'vouchers': 15588, 'meaningful': 9310, 'carlton': 2988, 'benz': 2147, 'serviced': 12794, 'jennah': 7999, 'amaaaazing': 1264, 'kis': 8312, '2am': 189, 'loveeeeeeee': 8940, 'weho': 15810, 'brit': 2621, 'beyonce': 2181, 'uhmygawddd': 15186, 'trivia': 14935, 'braggin': 2546, 'einstein': 5120, 'yesss': 16366, 'aaaaaaaaaaa': 854, 'rogers': 12307, 'travesty': 14890, 'stressing': 13853, 'associated': 1633, 'global': 6444, 'agencies': 1096, 'viva': 15554, 'juicy': 8123, 'testers': 14409, 'asap': 1596, 'sacred': 12461, 'yoursel': 16430, 'bastard': 1979, 'inerne': 7673, 'gtg': 6727, 'santi': 12545, 'interlock': 7772, 'calculus': 2866, 'derivative': 4443, 'identity': 7522, 'asaran': 1597, '_shaw': 793, 'tekenen': 14346, 'awfully': 1777, 'coogan': 3799, 'moran': 9701, 'uncontrolable': 15221, 'covering': 3898, 'scarf': 12600, 'washington': 15703, 'calfornia': 2873, 'greet': 6658, 'against': 1090, 'bsb': 2683, 'loooooong': 8883, 'shutting': 13031, 'reprezent': 12098, 'zen': 16492, 'pag': 10682, 'ugghhh': 15170, 'views': 15509, 'estk': 5336, 'nemonem': 10034, 'hateee': 6970, 'glamorous': 6433, 'insanely': 7715, 'unbearable': 15213, 'probly': 11476, 'rlich': 12265, 'doogie': 4779, 'howser': 7389, 'captain': 2950, 'pius': 11102, 'pep': 10903, 'rally': 11770, 'concerns': 3679, 'cupie': 4085, 'mariage': 9184, 'ants': 1409, 'lollipop': 8836, 'posse': 11314, 'vacant': 15413, 'dynamic': 4985, '_it_good': 692, 'elite': 5148, 'epl': 5284, 'derby': 4439, 'upp': 15330, 'handsom': 6894, 'motorcycle': 9751, 'nudged': 10273, 'diane': 4523, 'relized': 12029, 'blech': 2310, 'idiom': 7525, 'chopped': 3320, 'liver': 8767, 'unravel': 15299, 'bambi': 1902, 'kansas': 8176, 'carthage': 3008, 'compliments': 3659, 'shontelle': 12959, 'layne': 8549, 'kingston': 8306, 'illuminated': 7569, 'dreaaaming': 4851, 'membership': 9367, 'grrrreat': 6707, 'railsbridge': 11754, 'picat': 11016, 'jeeeez': 7987, 'inbound': 7630, 'lincoln': 8707, 'tunnel': 15005, 'selfishness': 12736, 'pwns': 11651, 'workdone': 16131, 'redtape': 11940, '44': 294, '8k': 506, 'btween': 2689, 'comedyqueen': 3587, 'fundraiser': 6163, 'percent': 10910, 'platium': 11141, 'package': 10671, 'umma': 15203, 'wannabe': 15665, 'njoy': 10152, 'shuting': 13028, 'creative': 3963, 'tweetage': 15040, 'amongst': 1311, 'bags': 1863, 'obbsessed': 10321, 'thirsty': 14515, 'zulu': 16517, 'meth': 9417, '1million': 121, 'typical': 15144, 'thatwouldmake1of': 14457, 'myfriends': 9883, 'xxxxx': 16268, '5z4p7': 411, 'geoff': 6336, 'tenerife': 14377, 'b25651': 1805, 'digg': 4561, 'cyberstalking': 4135, 'privacy': 11461, 'prettie': 11424, 'esspensive': 5331, 'nao': 9942, 'haff': 6806, 'bearfoot': 2042, 'jolly': 8071, 'pirated': 11084, 'usefu': 15367, 'benn': 2144, 'beb': 2064, 'innn': 7703, 'chemical': 3243, 'sodahead': 13361, 'mychemicalromance': 9882, '_writer': 842, 'dissapeared': 4667, 'iknow': 7558, 'tutle': 15019, 'leiiin': 8611, 'introoo': 7794, 'varnishing': 15436, 'engine': 5223, 'bikes': 2222, '167': 93, 'weep': 15805, 'sos': 13465, 'accompanied': 938, 'gliss': 6441, 'brandi': 2558, 'carlile': 2984, 'gigwise': 6383, 'fillings': 5760, 'gates': 6287, 'charles': 3184, '6jwjmy': 454, 'mn': 9600, 'flicker': 5862, 'stephane': 13745, 'sympathy': 14173, 'suwweeeeet': 14110, 'fondont': 5931, 'howeva': 7384, 'mk': 9586, 'pam': 10720, 'exs': 5498, 'growl': 6701, 'tshirt': 14973, 'yoooouuuu': 16412, 'hooooommmeeee': 7309, 'tattooed': 14289, 'sizzling': 13154, '250e': 170, 'crazzyyyy': 3953, 'ci': 3355, 'rd': 11818, 'achan': 955, 'designia': 4458, 'tweaking': 15029, 'pleb': 11172, 'freezer': 6067, 'gprof': 6573, 'trailhead': 14855, 'sched': 12616, 'bazillionz': 2010, 'supprtin': 14069, 'jovani': 8095, 'asks': 1615, 'tiredd': 14656, 'mentaly': 9386, 'debit': 4309, 'dey': 4503, 'ina': 7627, 'gown': 6569, 'morrow': 9724, 'cramp': 3926, 'layout': 8550, 'funnel': 6172, 'mag': 9075, 'aches': 957, 'broompark': 2650, 'destroys': 4477, 'looove': 8891, 'cristal': 3998, 'bleeding': 2313, 'owwiee': 10652, 'deflated': 4364, '2345': 160, 'tissue': 14665, 'melrose': 9359, 'cashier': 3019, 'registrar': 11980, 'unfun': 15258, 'angles': 1346, 'hotness': 7363, 'personified': 10942, 'yayyy': 16311, 'haaaa': 6788, 'hoe': 7239, 'aero': 1057, 'louder': 8917, '_i_am_jes': 678, 'looooved': 8890, 'midnite': 9458, 'jumpy': 8135, 'brightly': 2612, 'darkest': 4223, 'stakc': 13665, 'pulaski': 11598, 'barre': 1952, 'chord': 3322, 'mixed': 9582, 'chocolatey': 3308, 'yoooo': 16407, 'rocstar': 12302, 'exhusband': 5456, 'submitted': 13920, 'repaired': 12072, 'xpvt7': 16261, 'mistakes': 9570, 'lettin': 8639, 'kettle': 8241, 'equation': 5288, 'nicest': 10102, 'basten': 1981, 'harrump': 6956, 'gagging': 6226, 'calexico': 2872, 'hanna': 6906, 'keeper': 8215, 'ile': 7561, 'unimpressed': 15264, 'ame': 1291, 'refused': 11968, 'shorten': 12978, 'luvd': 9017, 'cupcake': 4083, 'comfy': 3595, 'judgement': 8114, 'wgn': 15854, 'airing': 1152, 'programing': 11505, 'shouting': 12999, 'kacie': 8159, '_galore': 650, 'aaaw': 875, 'instincts': 7742, 'frustration': 6135, 'furniture': 6187, 'yeaah': 16324, 'fooood': 5947, 'confirm': 3702, 'signups': 13076, '67zgz': 451, '14m': 75, 'exctied': 5443, 'whispering': 15917, 'redmango': 11934, 'folding': 5915, 'shaking': 12843, 'prerecorded': 11402, 'jj': 8036, 'admk': 1023, '914044621160': 520, 'hyderabad': 7481, 'connections': 3732, 'slash': 13195, 'tyres': 15150, 'cuold': 4078, 'archetype': 1516, 'studied': 13886, 'archetypes': 1517, 'guts': 6769, 'hounslow': 7371, 'nw': 10303, 'kimbeommie': 8294, 'mainly': 9101, 'twi': 15064, 'duckraces': 4934, 'angus': 1350, '10jsep': 37, 'nicc': 10096, 'bon': 2419, 'iver': 7904, 'gents': 6333, 'retiring': 12164, 'beefin': 2090, 'gerbil': 6342, 'procrastinate': 11483, 'luking': 8992, '6500km': 427, 'gis': 6414, 'queries': 11692, 'goddam': 6476, 'gauge': 6290, 'dixon': 4700, 'gps': 6574, 'bttr': 2687, 'bulk': 2734, 'lipstick': 8732, 'glory': 6449, 'eternally': 5341, 'talkshow': 14239, 'larry': 8497, 'cage': 2854, 'watered': 15722, 'agents': 1099, 'wham': 15866, 'disproves': 4665, 'holland': 7259, '930': 523, 'intriguing': 7792, '__jazz__': 560, '8weeks': 510, 'greece': 6648, 'differently': 4551, 'shopped': 12968, 'uff': 15162, 'fusterated': 6194, 'vintage': 15520, 'topgear': 14764, 'freeeeeeeeeeeeeeeezing': 6062, 'soooooooon': 13432, 'yehhaaaaaaa': 16352, 'noe': 10172, 'hmmmmmmm': 7229, 'dvds': 4975, 'snuggles': 13339, 'dongggg': 4768, 'breakie': 2574, 'cisco': 3380, 'solve': 13386, 'whose': 15948, 'eewwwww': 5097, 'sergi': 12781, 'riverside': 12262, 'sosad': 13466, 'delightful': 4388, 'sgb': 12828, 'scrape': 12650, 'ville': 15513, 'shardup': 12856, 'admeeet': 1016, 'sadder': 12469, 'sweetdreams': 14138, 'landlord': 8471, 'peeing': 10877, 'sensation': 12754, 'suuuks': 14107, 'louisa': 8922, 'warden': 15680, 'dahh': 4170, 'thirty': 14516, 'daddddd': 4163, 'cousins': 3892, 'disembarking': 4645, 'tswassen': 14976, 'whales': 15865, 'pointing': 11221, 'memes': 9369, 'twitterring': 15109, 'myspac': 9893, 'bullshitting': 2742, '144': 73, 'geeta': 6311, 'plague': 11118, 'blending': 2318, 'newbie': 10067, 'bitched': 2258, 'particular': 10781, 'markets': 9199, 'decline': 4329, 'macarena': 9050, 'gota': 6558, 'alter': 1250, 'criminal': 3990, 'artificial': 1584, 'oofm': 10467, 'rihanna': 12242, 'arizzard': 1543, 'twhirl': 15063, 'expresso': 5497, 'weights': 15815, 'atlanta': 1659, 'commented': 3608, 'pixar': 11104, 'laavly': 8415, 'villains': 15512, 'chew': 3254, 'aerobics': 1058, 'tickled': 14599, 'dada': 4162, 'beachwood': 2035, '_missrachel': 745, 'sofa': 13363, 'pirating': 11085, 'clair': 3399, '_defcon1': 616, 'gather': 6288, 'continued': 3772, 'bunchh': 2756, '_dinasadik': 620, 'forensic': 5969, 'mein': 9353, 'kya': 8407, 'bas': 1959, 'jaao': 7918, 'micro': 9448, 'diagnosis': 4518, 'highland': 7178, 'screws': 12672, 'assfuck': 1626, 'dosriosrestaurant': 4794, 'telemarketers': 14350, 'perfection': 10914, 'renegades': 12059, 'closest': 3464, 'greenock': 6655, 'kilmacolm': 8292, 'awesomeily': 1767, 'obession': 10322, 'ugliest': 15180, 'uv': 15404, 'prisnor': 11458, 'trans': 14864, 'engage': 5221, '_griffin': 658, 'vodka': 15565, 'yessssss': 16369, 'planting': 11135, 'grandaddy': 6605, 'whisked': 15912, 'googled': 6523, 'blowout': 2364, 'permanent': 10929, 'earthquake': 5017, 'submarine': 13919, 'fiber': 5726, 'optics': 10515, 'damaged': 4178, 'bonnie': 2426, 'oster': 10559, 'alltel': 1219, 'welcom': 15828, 'zemote': 16491, 'litter': 8756, 'azalea': 1803, 'bunnies': 2757, 'heyyyyy': 7159, 'nancy': 9939, 'moves': 9765, 'laters': 8511, 'ux': 15408, 'criticisms': 4000, 'bccg': 2023, 'geeez': 6305, 'immobilizer': 7598, 'recliner': 11892, 'dhq': 4515, 'longgggg': 8859, 'mimis': 9496, 'tiredddd': 14658, 'odeeee': 10350, '_ryan': 790, 'carol': 2994, 'wildomar': 15981, 'towing': 14820, 'ape': 1449, 'javascript': 7968, 'mootools': 9696, 'odqwgh': 10353, 'responding': 12134, 'ganda': 6253, 'mga': 9431, 'scenes': 12609, 'svkch': 14114, 'downloadfestival': 4817, 'lineup': 8715, 'festivals': 5707, 'dl': 4709, 'uzbekistan': 15409, 'convenient': 3786, 'darlin': 4226, 'easties': 5027, 'silent': 13082, 'treatment': 14897, 'torture': 14784, 'method': 9419, 'psprint': 11571, 'karaoke': 8180, 'doze': 4826, 'snappy': 13312, 'gooodmorning': 6526, 'badminton': 1857, 'cooperate': 3821, 'itd': 7881, 'miranda': 9533, 'apply': 1482, 'sonics': 13415, 'bred': 2585, 'blu': 2367, 'decently': 4319, 'priced': 11432, 'jkin': 8039, '_nobel': 758, 'tapes': 14259, 'separate': 12769, 'desks': 4462, 'dividers': 4694, 'ceiling': 3083, 'scratching': 12654, 'kelli': 8222, 'edits': 5072, 'sombody': 13391, 'inpu': 7708, 'occasion': 10335, 'socal': 13348, 'concer': 3676, 'chilee': 3274, 'sleeptime': 13215, 'lannen': 8481, 'sass': 12555, 'scenie': 12610, 'denied': 4409, 'desperately': 4467, 'wahts': 15628, 'coffeclub': 3528, 'passes': 10800, 'wookiee': 16107, 'increased': 7650, 'suiva': 13982, 'transform': 14869, '74': 466, 'cinnamin': 3374, 'crunch': 4028, 'debussy': 4312, 'homeboy': 7273, 'clownin': 3477, 'flooded': 5873, 'htown': 7405, 'april': 1500, '_angel': 568, 'mms': 9599, 'waer': 15620, 'nexxt': 10083, 'feckin': 5670, 'bumps': 2752, 'festivus': 5710, 'mud': 9815, 'wrestling': 16186, 'spilling': 13570, 'wonderfully': 16092, 'harrassment': 6955, 'panera': 10731, '4j8yk': 312, 'thelma': 14473, 'rebeca': 11866, 'fernanda': 5701, 'symonds': 14169, 'cordova': 3836, 'lightning': 8684, 'scarededededed': 12598, 'gingg': 6396, 'cleanse': 3427, 'regarding': 11973, 'sista': 13130, 'chutzpah': 3351, 'kudai': 8392, 'chile': 3273, 'npr': 10265, 'choreographing': 3324, 'discover': 4633, 'ditch': 4686, 'myspaces': 9895, 'wwww': 16218, 'fiftyfivethreads': 5738, 'clothing': 3472, 'yoko': 16401, 'ono': 10455, 'gmorning': 6457, 'terrib': 14399, 'amazes': 1278, 'salt': 12506, 'grrrrr': 6708, 'twittered': 15094, 'hooome': 7307, 'expense': 5469, 'corrected': 3848, 'mobiles': 9606, 'slaves': 13198, 'advantages': 1047, 'imissu': 7592, 'pug': 11591, 'ight': 7543, 'bugg': 2715, 'shawna': 12875, 'damon': 4189, 'referring': 11953, 'castlebar': 3031, 'galway': 6242, 'adoptive': 1031, 'mommies': 9643, 'isint': 7859, 'ahhahahaha': 1126, 'ariyan': 1542, 'hamish': 6872, 'podcas': 11209, 'calorie': 2888, 'phishing': 10981, 'succeed': 13935, 'janessa': 7951, 'cinncinatti': 3376, 'adobe': 1026, 'registered': 11979, 'salaried': 12497, 'territory': 14404, 'freeeeeeee': 6061, 'britneys': 2627, 'spears': 13530, 'disgust': 4647, 'ovie': 10637, 'highlights': 7180, 'toke': 14710, '1thing': 125, 'led2': 8591, 'wants2': 15673, 'madison': 9070, 'deliveries': 4393, 'instructors': 7745, 'suprised': 14070, 'ion': 7823, 'itx': 7898, 'earliest': 5004, 'bikeshed': 2223, 'provolone': 11560, 'beefsteak': 2091, 'tomatoes': 14719, 'divine': 4695, 'leo': 8623, 'carillo': 2979, 'lhq8': 8651, 'microsoft': 9451, 'candidate': 2930, 'elora': 5157, 'danan': 4195, 'nsw': 10269, 'melbs': 9358, 'sorors': 13450, 'toni': 14736, 'gng': 6462, 'portable': 11295, 'sais': 12493, 'excist': 5431, 'photographer': 10997, '57': 385, 'trackpad': 14837, 'pak': 10706, '64': 424, 'boise': 2411, 'debate': 4305, 'waxed': 15744, 'okok': 10406, '_indian': 690, 'tlc': 14674, '_in_tx': 689, '30am': 229, 'yow': 16444, 'attal': 1671, 'bulls': 2740, 'alrite': 1244, 'omfgggg': 10428, 'suckkkkkk': 13950, 'reque': 12100, 'similar': 13089, 'soichi': 13372, 'negishi': 10019, 'amai': 1267, 'koibito': 8366, 'wonderfu': 16089, '20mins': 145, 'hibernate': 7164, 'unsettled': 15305, 'transfer': 14867, 'suspect': 14099, 'offend': 10361, 'nay': 9976, '41': 290, 'migraines': 9466, '_yours13': 848, 'cont': 3753, 'stale': 13666, 'musicistheheartofoursoul': 9855, 'niptuck': 10142, 's5': 12452, '_hope': 676, 'coordinate': 3823, 'fetti': 5713, 'applies': 1481, 'willblok': 15983, 'ne6twc': 9987, 'muay': 9810, 'romantic': 12322, 'suzy': 14113, 'uptown': 15344, 'coo': 3798, 'ghd': 6362, 'straightener': 13818, 'subponea': 13923, '80s': 487, 'dismal': 4658, 'disapointing': 4611, 'audioo': 1694, 'newborn': 10068, 'kubbur': 8391, 'applaud': 1473, 'accepting': 929, 'rels': 12032, 'chloes': 3303, 'trike': 14922, 'thingo': 14506, 'girraffe': 6410, 'probobly': 11477, 'yelled': 16354, 'mushy': 9849, 'ammmmazing': 1308, 'crib': 3985, 'fedex': 5672, 'recommending': 11904, '123': 53, 'kamei': 8171, 'sensei': 12756, 'kuso': 8399, 'baaaaaaaad': 1813, 'payment': 10849, 'publishers': 11584, 'janeiro': 7949, 'babelfish': 1816, 'physical': 11010, 'milage': 9473, 'apologised': 1457, 'tweefight': 15033, 'opp': 10508, 'hangy': 6904, 'hapy': 6928, 'sowwwy': 13494, 'unsurprised': 15308, 'healed': 7037, 'injuries': 7693, 'hp': 7391, 'sucking': 13948, 'lunching': 9004, 'visitng': 15547, 'boyet': 2529, 'famm': 5582, 'mtn': 9802, 'praising': 11372, '8gb8r': 505, 'material': 9247, 'bsame': 2682, 'encourage': 5206, 'twitwoo': 15123, 'blowin': 2361, 'immboredddd': 7595, 'cba': 3067, 'rec': 11874, 'anto': 1405, 'ngh': 10087, 'condolence': 3691, 'crutches': 4035, 'iflowers': 7541, 'trough': 14948, 'kwnx': 8404, 'cla': 3394, 'amost': 1314, 'ohwell': 10392, 'innabit': 7699, 'misery': 9545, 'omr': 10442, 'wy': 16224, 'lhr': 8652, '911': 518, 'carbonara': 2961, 'tutoring': 15022, 'overr': 10620, 'terrorizing': 14405, 'snipurl': 13324, 'hbp3g': 7011, 'canalway': 2917, 'cavalcade': 3058, 'venice': 15458, 'warwick': 15697, 'invert': 7804, '_toni': 822, 'gallore': 6241, 'kayleigh': 8201, 'coatandkay': 3507, 'soompiradio': 13421, 'itch': 7877, 'fools': 5945, 'butteflies': 2799, 'tippers': 14649, 'mt4opc': 9796, 'mule': 9824, 'dumped': 4952, 'holmbury': 7267, 'absolute': 910, 'bueno': 2708, 'salon': 12505, 'trim': 14923, 'eyebrows': 5519, 'una': 15207, 'aussies': 1711, 'todo': 14700, '_starla': 807, 'cringe': 3992, 'worthy': 16163, 'delcious': 4379, 'commercially': 3612, 'viable': 15486, 'officejet': 10372, 'j4550': 7916, 'ink': 7696, 'windows7': 16003, 'fax': 5654, 'racism': 11738, 'closeness': 3462, 'resume': 12153, 'scrub': 12676, 'ustre': 15382, '2uhs': 219, 'ebtg': 5040, 'bett': 2167, 'weeping': 15807, 'grumble': 6718, 'yaayy': 16284, 'saynow': 12586, 'jbs': 7978, 'landline': 8470, 'whoshere': 15949, 'artesia': 1580, 'cerritos': 3111, 'quest': 11697, 'guna': 6759, 'dsaa09': 4918, 'horrendous': 7336, 'emmm': 5185, 'geee': 6304, 'uup': 15398, 'lub': 8973, 'aaaaaaaahhhhhhhh': 856, 'widgets': 15965, 'practically': 11367, 'lolll': 8837, 'buzy': 2816, 'eatin': 5035, 'outage': 10582, 'surveys': 14091, 'genre': 6330, 'paving': 10838, 'intelligent': 7758, 'adopting': 1029, 'exotic': 5462, '9100': 517, 'tres': 14910, 'victor': 15498, 'weatler': 15770, 'lo0l': 8793, 'brummie': 2671, 'jennnnnn': 8001, 'richhhh': 12218, 'wast': 15709, 'winding': 15999, 'rode': 12303, 'duc': 4930, 'pops': 11284, 'adrenaline': 1035, 'wears': 15766, 'godtalk': 6484, 'segment': 12728, 'intern': 7774, 'dangit': 4210, 'sheriff': 12903, 'goooooooooooooooooooooooood': 6542, 'prank': 11373, 'aobut': 1436, 'han': 6880, 'awh': 1778, 'greattttt': 6645, 'reeeejuvinated': 11945, 'outright': 10592, 'deny': 4418, 'accusations': 948, 'depeche': 4423, 'stoooopit': 13796, 'stooopit': 13798, 'rendition': 12058, 'selenagomezlast': 12733, 'pleeeeeassseee': 11175, 'athens': 1654, 'shoulve': 12997, 'rm': 12268, 'yacht': 16285, 'joshstore': 8086, 'joshmobile': 8085, 'pft': 10958, 'economic': 5048, '5ghz': 391, 'freezes': 6068, 'sweats': 14127, 'bebe': 2065, 'vegetable': 15447, 'honesty': 7292, 'straightening': 13819, 'overrr': 10623, 'rught': 12401, 'carlos': 2986, 'interactive': 7765, 'orals': 10524, 'calicut': 2875, '4wfeo': 351, 'shudnt': 13022, 'mowgli': 9773, 'kealie': 8209, 'kia': 8264, '_ci': 601, 'reasonable': 11863, '35am': 254, 'unfortuantley': 15252, 'picker': 11019, 'grabbed': 6581, 'cherry': 3250, 'soprano': 13443, 'yardhouse': 16299, 'waikiki': 15629, 'dany': 4216, 'sorcha': 13447, 'yer': 16359, 'ladybug': 8436, 'borders': 2473, 'visa': 15539, 'gehts': 6313, 'abi': 898, 'delusional': 4396, 'errg': 5311, '___': 553, '90th': 516, 'downer': 4811, 'leslie': 8628, 'siiiick': 13078, 'paintballin': 10695, 'battle': 1997, 'mnet': 9601, 'vocals': 15561, 'ad': 989, 'freehugs': 6063, 'derek': 4441, 'iwish': 7906, 'criado': 3984, 'snapping': 13310, 'cna': 3496, '½17': 16524, '½27': 16526, 'lappie': 8486, 'whatta': 15879, 'doo': 4777, 'duncan': 4955, 'orders': 10532, 'leaves': 8584, 'iraq': 7838, 'waching': 15616, 'antomy': 1406, 'kq47ah': 8378, 'dachshund': 4160, 'breed': 2586, 'crucial': 4019, 'gums': 6757, 'earpiece': 5013, 'erica': 5300, 'bean': 2037, 'rinse': 12247, 'conditoner': 3690, 'harding': 6939, 'pitty': 11100, 'arnd': 1553, 'wld': 16067, 'rlly': 12266, 'inspire': 7731, 'thankyooooou': 14444, 'feasts': 5664, 'tomoo': 14725, 'dedicating': 4340, '300th': 226, 'eventually': 5372, 'lobby': 8802, 'exited': 5460, '22nd': 157, 'helen': 7104, 'kayla': 8200, 'nightly': 10120, 'routine': 12367, 'gahh': 6230, 'bahama': 1868, 'fij': 5747, 'cambs': 2896, 'recharger': 11885, 'pratchett': 11374, 'thingie': 14505, 'laterz': 8512, '4wsst': 362, 'stacey': 13654, 'whys': 15955, 'esquire': 5326, 'fantasize': 5599, '4jgro': 322, 'reader': 11832, 'newb': 10066, 'lolzz': 8846, 'bannished': 1930, 'readyyy': 11837, 'fright': 6105, 'regal': 11972, 'slab': 13188, 'hahaaaa': 6814, 'grief': 6671, '_of_the_dead': 762, '_staack': 806, 'aaggh': 879, 'gash': 6281, 'advertisement': 1051, 'munched': 9835, 'fanatic': 5585, 'favourites': 5651, 'caramel': 2957, 'muchachomalo': 9812, 'bffs': 2188, 'q100': 11654, 'cooker': 3803, 'combined': 3579, 'disastrously': 4619, 'carville': 3011, 'bald': 1886, 'tactical': 14201, 'belinda': 2124, 'jensen': 8003, '3647': 259, 'subbed': 13915, 'pika': 11047, 'nichi': 10103, 'heeeee': 7078, 'coined': 3537, 'hottie': 7367, 'switchfoot': 14160, 'hsg': 7399, 'zappa': 16484, 'inflating': 7680, 'kellan': 8221, 'lutz': 9015, 'safer': 12483, '_double': 623, 'fkl': 5838, 'flirt': 5868, 'contributors': 3782, 'batty': 2004, 'homesick': 7282, 'mornig': 9710, 'everone': 5377, 'diplo': 4591, 'kathryn': 8191, 'smartbar': 13265, 'pleaase': 11158, 'dating': 4247, 'gar': 6263, 'relight': 12025, 'budapest': 2701, 'samee': 12512, 'obv': 10330, 'herman': 7138, 'underpriveledged': 15231, 'li': 8653, 'ridin': 12234, 'cb': 3066, 'gaga': 6225, 'timee': 14629, 'guitars': 6753, 'hut': 7471, 'crunchy': 4029, 'veggie': 15450, 'technically': 14323, 'chingo': 3289, 'busting': 2792, 'lucia': 8978, 'waaaaay': 15610, 'halloween': 6856, 'carnt': 2992, 'biggs': 2215, '53am': 377, 'changedd': 3154, 'picnik': 11024, 'bastos': 1982, 'sixteenth': 13147, 'acceptable': 926, 'golfing': 6495, 'frolic': 6117, 'consummate': 3751, 'hogging': 7243, 'hice': 7166, 'maruchan': 9218, 'willy': 15990, 'valdez': 15422, 'muchly': 9813, 'blackird72': 2281, 'masekela': 9226, 'pix': 11103, 'contern': 3764, 't71': 14189, 'clada': 3396, 'lu': 8972, 'naturally7': 9968, 'curb': 4088, 'maid': 9092, 'cplt7p': 3910, 'aaaah': 864, 'portuguese': 11304, 'national': 9962, '1995': 113, 'tx': 15133, 'anyones': 1423, '_von_abe': 832, 'flyer': 5895, 'notary': 10233, 'retrograde': 12169, 'rachellovespeace': 11736, 'noiiiiice': 10180, 'bmth': 2383, 'youknowimsofreshtilldeath': 16418, 'smiled': 13280, 'degenerate': 4369, 'occupants': 10338, 'reprehensibles': 12094, 'minethatbird': 9505, 'servings': 12796, '88db': 499, 'mixing': 9584, '808': 486, 'providing': 11558, 'geelong': 6309, 'toyota': 14823, 'landcruiser': 8467, '1996': 114, 'hcc': 7013, '_thomas': 818, 'fangs': 5592, '10p': 39, 'dig': 4558, 'selfridges': 12737, 'jfk': 8021, 'tasted': 14277, 'hurdle': 7456, 'gooooodnight': 6532, 'cuss': 4105, '_louise': 729, 'ingredients': 7687, 'seperate': 12770, 'depressi': 4433, 'grimestopper': 6677, 'drowned': 4900, 'pi': 11013, 'stricken': 13858, 'omaha': 10422, 'calvin': 2890, 'kleins': 8330, 'yearsssssss': 16337, 'expires': 5480, 'parker': 10769, 'batcave': 1984, 'startup': 13704, 'motorbikes': 9749, 'biafra': 2201, 'momotlv': 9647, 'iva09': 7902, 'trumps': 14957, 'minor': 9520, 'heal': 7036, 'furbabies': 6184, 'hog': 7242, 'retweeted': 12176, 'gryffindor': 6722, 'ching': 3288, 'chong': 3317, 'wing': 16008, 'wong': 16097, 'pong': 11254, 'dong': 4767, 'tabloids': 14196, 'aaa': 852, 'hiii': 7187, 'bara': 1936, 'closets': 3466, 'aggghhhh': 1101, 'muak': 9809, 'mcm': 9295, 'sudah': 13956, 'cripple': 3993, 'twizzler': 15124, 'rolynn719': 12318, 'awayyy': 1754, 'moooorning': 9692, 'hellloooo': 7111, 'g4p': 6213, 'rockstars': 12300, 'havnt': 6992, 'answerd': 1390, 'replys': 12087, '_gant': 651, 'trendy': 14908, 'hysterical': 7492, 'deffinately': 4355, 'bookbag': 2436, 'linoleum': 8723, 'carving': 3012, 'aweesome': 1758, 'errors': 5313, 'thankss': 14441, 'farming': 5610, 'catty': 3047, 'omigoodness': 10436, 'orthodontist': 10555, 'and_jay': 1327, 'har': 6929, '4jb4o': 315, 'santana': 12544, 'wraith': 16175, '7a10a': 476, '1386am': 66, 'muttering': 9869, 'arseholes': 1576, 'rickroll': 12224, 'collabs': 3551, 'aroun': 1557, 'tweettttt': 15058, 'leads': 8565, 'uce': 15160, 'pgce': 10960, 'leicester': 8609, 'hairdo': 6839, '250gb': 171, 'nia': 10092, 'dayton': 4268, 'cincinnati': 3365, 'wiz': 16057, 'ebtter': 5041, 'stopping': 13801, 'adn': 1024, 'shortstack': 12985, 'friggin': 6104, 'bbye': 2020, 'biochem': 2237, 'waaaaahhhhhh': 15609, 'wxizo': 16220, 'booking': 2439, 'brave': 2563, 'belive': 2125, 'yous': 16432, '333333333': 248, 'chats': 3199, 'cyrus': 4143, 'runday': 12418, 'conan': 3670, 'forsure': 5996, 'yvr': 16473, 'conflicts': 3707, 'acid': 963, 'reflux': 11957, 'appending': 1470, 'verticalchinese': 15474, 'damjust': 4181, 'jaoo': 7955, 'damper': 4192, 'georgetown': 6340, 'invoices': 7816, 'upside': 15340, 'wt': 16203, 'yaer': 16286, 'moshing': 9732, 'confetti': 3698, 'whatev': 15873, 'carlsbad': 2987, 'spilled': 13569, 'involve': 7817, 'ning': 10138, 'reg': 11971, 'fff': 5721, 'naturalismo': 9966, 'elliott10': 5154, 'uterus': 15390, 'saddo': 12472, 'jonaswebcast': 8074, 'misadventures': 9537, 'sociology': 13355, 'worldsbestpornmovies': 16147, 'faithmov3': 5562, 'htm': 7403, 'whores': 15946, 'umbrella': 15198, 'carpenters': 2997, 'topical': 14766, 'okayy': 10402, 'burped': 2774, 'lmfa0': 8785, 'recordings': 11910, '_stiller': 811, 'adios': 1013, 'vanilla': 15433, 'oxoxoteambreezy': 10657, 'rez': 12199, 'reboot': 11870, 'osx': 10561, 'supported': 14063, 'tampa': 14245, 'gtgs': 6728, 'decongestant': 4333, '_laiter': 718, 'discussed': 4640, 'yeaaaaaah': 16322, 'chichis': 3260, 'grande': 6607, '66of2': 432, 'polly': 11246, 'arrested': 1565, '4uy8l': 335, 'drumming': 4909, 'twin': 15074, 'gazing': 6295, 'webkit': 15781, '_flora': 642, 'optimized': 10517, 'caching': 2847, 'deploy': 4426, 'siad': 13038, 'fav5': 5642, 'rex': 12195, 'wonderfull': 16091, 'dins': 4585, 'epicentre': 5279, 'wheelock': 15890, '250': 168, 'units': 15275, 'tourists': 14808, '_bizzle': 585, 'speakers': 13527, 'polaroid': 11233, 'cleavage': 3433, 'yorkshire': 16414, 'survived': 14093, 'kuya': 8402, 'owns': 10649, 'wating': 15731, 'wereld': 15844, '_steph': 809, 'alo': 1226, 'wowza': 16172, '0f': 18, 'fried': 6094, 'offensive': 10364, 'earlierrr': 5003, 'tooz': 14762, 'misbehaved': 9538, 'thelovelybones': 14474, 'spy': 13627, 'premire': 11397, 'xbb1qyx0e': 16242, 'guttah': 6770, 'happydance': 6926, 'piercing': 11034, 'tryst': 14970, 'fishys': 5820, 'happymothersday': 6927, '50p': 370, 'o_o': 10318, 'ivs': 7905, 'pikachu': 11048, '104': 32, 'buckonellen': 2698, 'livingroom': 8774, 'bfe': 2185, 'donating': 4764, 'cj': 3389, 'wc': 15756, 't20': 14187, 'benefiting': 2141, 'caps': 2949, 'barbs': 1939, 'dsl': 4920, 'realising': 11845, 'sheltered': 12896, 'upbringing': 15316, 'commitments': 3615, 'pfftt': 10957, 'pinched': 11062, 'unpleasant': 15295, 'mindblowing': 9500, 'nahh': 9916, 'sowwieee': 13493, 'git': 6415, 'dez': 4504, 'sneakerz': 13314, 'aghhh': 1104, '2dives': 194, 'dive': 4691, '10m': 38, 'colours': 3575, 'dynamite': 4986, 'reefs': 11948, 'edwin': 5080, 'batch': 1985, 'eveyone': 5398, 'cloggin': 3455, 'tweetup': 15059, 'pushit': 11638, 'sundress': 14022, 'bogus': 2404, 'txts': 15137, 'weirdness': 15822, 'lmgtfy': 8789, '49am': 304, 'su': 13911, 'goodb': 6504, 'heap': 7043, 'bitchy': 2259, 'knoww': 8356, '_renee': 783, 'shay': 12876, 'shlda': 12948, 'knwn': 8360, 'whn': 15926, 'crd': 3954, 'scorpian': 12641, 'bwl': 2822, 'aud': 1689, 'timestamp': 14635, 'chantellie': 3164, 'colorful': 3570, 'atmosphere': 1664, 'mozert': 9779, 'covina': 3899, 'goooo': 6528, 'veryy': 15476, 'el_rumi': 5127, 'fii': 5746, 'mushkila': 9846, 'router': 12366, 'apnea': 1454, 'causing': 3055, '4w9zb': 344, 'dimple': 4575, 'cave': 3059, 'adt': 1039, 'princelple': 11445, 'plate': 11139, 'adtï': 1040, '½000': 16522, 'nyhmz': 10311, 'dionusia': 4588, 'congratuations': 3722, 'tyson': 15152, 'kitteh': 8319, '1980s': 110, 'combed': 3577, 'poofy': 11260, '30yo': 237, 'anywayz': 1432, 'rye': 12446, 'speedway': 13546, 'utd': 15389, 'horrific': 7341, 'wack': 15617, 'swimsuit': 14152, 'dd7': 4284, 'treating': 14896, 'ds9': 4917, 'casino': 3023, 'unpacking': 15293, 'restful': 12143, 'alternatively': 1253, 'sprinkle': 13620, 'doubtful': 4802, 'biscuit': 2252, '4wtom': 363, 'appetite': 1471, 'disorders': 4662, 'vomit': 15579, 'amazeeeeeee': 1277, '527': 375, 'quesadillas': 11695, 'ai': 1139, 'stephen': 13746, 'sho': 12950, 'oreo': 10535, 'milkskake': 9485, '_cook': 605, 'gran': 6601, 'drift': 4875, 'seaworld': 12704, 'wid': 15961, 'phaket': 10962, 'anneliese': 1369, '_wi_no_name': 840, 'someones': 13399, 'runner': 12421, 'greatt': 6644, 'roaming': 12272, '24hrs': 166, 'tfa': 14423, 'x2rgl': 16236, 'sliders': 13223, 'barjohnnys': 1945, 'omelette': 10425, 'dim': 4574, 'feast': 5662, 'mariah': 9185, 'shhhweeeet': 12908, 'storming': 13808, 'starship': 13697, 'garnier': 6276, 'roasted': 12275, 'drugstore': 4906, 'rember': 12037, 'riped': 12251, 'mags': 9087, 'beated': 2047, 'heartburn': 7055, 'bitch': 2257, 'showss': 13013, 'trials': 14913, 'puppies': 11619, 'solved': 13387, 'oreos': 10536, 'owls': 10643, 'slurred': 13252, 'keyla': 8254, 'janice': 7952, 'getttin': 6355, '_meirizka': 741, 'ladyhawke': 8438, 'rutger': 12439, 'hauer': 6977, '4jbzv': 316, 'richie': 12219, 'fuzzball': 6202, '0a7v3j': 17, 'waaay': 15613, 'cruisey': 4023, 'relaxed': 12012, 'sporting': 13602, 'tm': 14675, '_lightmare': 720, 'yeeet': 16347, 'longggg': 8858, '007': 3, 'cooler': 3810, 'hawk': 6994, 'virgin': 15527, 'openhacklondon': 10495, 'tty': 14981, 'barnes': 1950, 'noble': 10166, 'comforts': 3594, 'custody': 4107, 'settled': 12807, 'skool': 13178, 'excite': 5433, 'wohoo': 16075, 'wis': 16026, '4w52z': 339, 'withdrawal': 16043, 'cingular': 3373, 'jr': 8106, 'cockatiels': 3513, 'mishaneedschapstick': 9549, 'beads': 2036, 'ipswitch': 7833, 'shannon': 12854, 'moral': 9700, 'likewise': 8692, 'copyed': 3831, 'chicks': 3265, 'yesssss': 16367, 'sue': 13961, 'banned': 1928, 'blt': 2366, 'nï': 10316, '½mme': 16539, 'equipment': 5290, 'telescopic': 14354, 'trekking': 14905, 'poles': 11235, 'boots': 2465, 'coolmax': 3813, 'fontanas': 5936, 'summmmmerrrrr': 14001, 'yeahhhhhyaaaaaa': 16331, 'organic': 10539, 'proto': 11545, 'furries': 6188, 'acen': 951, 'bubba': 2693, 'rubs': 12396, 'hairdresser': 6840, 'cutted': 4120, 'laaame': 8414, 'gays': 6293, 'bais': 1874, '_are_fire': 570, 'wowzers': 16173, 'addicting': 996, 'kalahari': 8166, 'biggie': 2214, 'puked': 11597, 'conditioning': 3688, 'bret': 2591, 'scarce': 12595, 'bestest': 2157, 'xams': 16240, 'roman': 12319, 'lq': 8968, 'agreeing': 1110, 'yogurt': 16399, '8am': 502, 'hughesy': 7425, 'rafferty': 11746, 'investigate': 7805, 'ye': 16317, 'operational': 10503, 'types': 15143, 'lmaoz': 8783, 'con': 3669, 'streamline': 13839, 'halfway': 6850, 'hoooray': 7310, 'bahamas': 1869, 'sealers': 12688, '_kimbalicious': 708, 'slippery': 13232, 'peep': 10879, 'singles': 13111, 'ipsohot': 7832, '_hong': 675, 'cosplay': 3857, 'marathons': 9171, 'affect': 1063, 'dreadweave': 4857, 'holds': 7250, 'chloe': 3302, 'pitiful': 11099, 'gre': 6634, 'floyd': 5887, 'busssssss': 2789, 'stickler': 13764, 'dpi': 4828, 'export': 5491, 'value': 15426, 'morga': 9706, 'mander': 9145, 'goooooooooooood': 6541, 'morrrrrrrrning': 9726, 'josie': 8088, 'pandora': 10728, 'birthdaaaay': 2248, 'unsalvageable': 15304, 'vibrating': 15489, 'et': 5338, 'genitals': 6327, 'glowing': 6452, 'babydoll': 1821, 'spaghetti': 13506, 'strap': 13828, 'smokinggg': 13293, 'nto': 10271, 'gaskarth': 6283, 'wraps': 16179, 'cofo': 3534, 'buyer': 2812, 'wifes': 15970, 'poke': 11226, 'quirky': 11711, 'concerto': 3681, 'reallllyyy': 11854, 'defend': 4351, '_ashley': 572, 'graduated': 6595, 'rentaphone': 12067, 'yesy': 16374, 'endometriosis': 5214, 'sez': 12824, 'quadriceps': 11676, '4we4j': 348, 'screens': 12666, 'somthing': 13408, 'dreamland': 4862, 'fruity': 6130, 'pebbles': 10871, 'charms': 3193, 'tiiiiiiired': 14617, 'fucckinggg': 6143, 'flaky': 5844, 'vrbo': 15592, 'tuff': 14991, '88th': 500, 'buffet': 2712, 'fabulously': 5533, 'satz': 12568, 'blend': 2317, 'ks': 8387, 'preheatin': 11394, 'indeedy': 7656, 'dios': 4589, 'mio': 9531, 'mailbox': 9095, 'loic': 8830, 'scheduler': 12619, 'halo3': 6861, 'freeze': 6066, 'halt': 6864, '79l0': 474, 'packages': 10672, 'wathing': 15730, 'dollhouse': 4753, 'taquito': 14263, 'employed': 5196, 'clint': 3448, 'haiku': 6834, 'yayschoolisout': 16308, 'sarcasm': 12550, 'eassy': 5023, 'wo': 16072, 'possessed': 11315, 'honking': 7298, 'minibar': 9509, 'starups': 13705, 'bridget': 2605, 'farted': 5615, 'broody': 2645, 'predicting': 11389, 'kindergarten': 8298, 'lof': 8815, 'malamang': 9121, 'chef': 3237, 'neeeeed': 10011, 'melting': 9363, 'trackflashback': 14834, 'darkness': 4224, 'mah': 9088, 'rotation': 12354, 'prowse': 11561, 'factory': 5544, 'yesh': 16363, 'howdyyy': 7383, 'answering': 1393, 'islip': 7864, 'benefit': 2140, 'customers': 4110, 'adulthood': 1042, 'surprisingly': 14087, 'mallorca': 9130, 'mypict': 9889, 'fuz': 6200, 'meadowbank': 9303, 'schuhz': 12630, 'arabyrd': 1510, 'seeyuhhh': 12727, '3hrs': 271, 'lightly': 8683, 'misted': 9571, 'ignoring': 7549, 'frowns': 6124, 'sympathise': 14171, '4ws8w': 361, '56': 383, 'sulumits': 13988, 'retsambew': 12170, 'billion': 2228, 'adds': 1007, 'technician': 14324, '_0': 537, 'whatthefuck': 15881, 'suns': 14033, 'tossa': 14788, 'hubz': 7416, 'organising': 10541, 'robins': 12285, 'collared': 3556, 'psst': 11573, 'latvian': 8521, 'beaurocracy': 2058, 'resident': 12127, 'publish': 11582, 'braille': 2548, 'highway': 7184, 'farsi': 5614, 'hopeflly': 7319, 'socialily': 13352, 'naiv': 9924, 'lsat': 8970, 'wallpapers': 15657, '_okeefe': 765, 'celeb': 3084, 'mattcutts': 9258, 'umzug': 15205, 'und': 15223, 'neues': 10058, 'redir': 11932, 'if2b': 7540, 'sorted': 13462, 'empathise': 5191, 'luvvie': 9021, 'tartan': 14269, 'dodgers': 4731, 'giants': 6370, 'quizfarm': 11717, 'quizrunner': 11718, 'dies': 4543, 'nvm': 10301, 'retreat': 12166, 'nike': 10129, 'clingy': 3447, 'purty': 11633, 'qa': 11656, 'frehley': 6070, 'coherent': 3535, 'fortunate': 6001, 'meditating': 9331, 'partial': 10778, 'sobre': 13347, 'maxed': 9266, 'concentration': 3673, 'smoothness': 13298, 'ilycecily': 7580, 'bampa': 1906, 'disdrict': 4643, 'advised': 1054, 'ridden': 12228, 'eppy': 5285, 'bats': 1994, 'tht': 14572, 'puffy': 11590, 'actresses': 984, '_jayytee': 698, 'community': 3623, 'moderators': 9619, 'niandra': 10094, 'cracked': 3917, 'daaaang': 4156, 'regards': 11974, 'archive': 1521, 'postings': 11330, '_stokoe': 812, 'sor': 13445, 'garcia': 6268, 'bleah': 2309, 'maddies': 9067, 'cuteset': 4115, 'looooooove': 8886, '_kill_boy': 707, 'yummmm': 16462, 'selected': 12730, 'fortunately': 6002, 'nkorea': 10155, 'abbey': 890, 'whatï': 15884, 'latter': 8520, 'op': 10488, 'paraded': 10752, 'rejection': 12000, 'coulnd': 3873, 'marry': 9209, 'dana': 4194, 'x29ss': 16231, 'boohoohoo': 2434, 'fotoreportage': 6011, 'mashup': 9229, 'hysteria': 7491, 'blows': 2365, 'faillllllllll': 5552, 'amma': 1307, 'demistylesource': 4402, 'fought': 6013, 'jered': 8005, 'chop': 3319, 'tomarrow': 14717, 'foreword': 5974, 'adidas': 1012, 'denyer': 4419, 'xoxoxoxoxoxo': 16259, 'rogue': 12308, 'x2dvj': 16233, 'boverd': 2519, 'keynote': 8255, 'screencastsonline': 12662, 'jobros': 8049, 'gentoo': 6332, 'reinstallation': 11996, 'conor': 3734, 'quarantined': 11680, 'pinkeye': 11073, 'bej': 2113, 'semanggi': 12741, 'siggghh': 13060, 'futile': 6195, 'conditioner': 3687, '_peer': 772, 'wishful': 16033, 'wuv': 16210, 'reserve': 12125, 'lowest': 8966, '664': 430, '_whacker': 838, 'skypeeeeee': 13185, 'oy': 10658, 'writin': 16193, 'lik': 8686, 'venti': 15461, 'prettier': 11425, 'spendin': 13556, 'haley': 6848, 'leyton': 8648, 'dgtcj2': 4509, 'neeed': 10009, 'seed': 12717, 'regionals': 11976, 'plugs': 11192, 'patent': 10815, 'earplugs': 5014, 'benedryl': 2139, 'addressed': 1005, 'lols': 8843, 'junction': 8136, 'ollies': 10419, '_hayes': 668, 'sharkeez': 12863, 'graphic': 6622, 'designers': 4457, 'flushed': 5893, 'collapses': 3554, 'nup': 10286, 'zeros': 16497, 'crest': 3981, 'plaid': 11119, 'mashed': 9228, 'limb': 8700, 'searched': 12693, 'yahh': 16288, 'wxmt': 16221, 'grim': 6676, 'fandango': 5589, 'xbla': 16244, 'psn': 11569, 'plana': 11122, 'strippers': 13865, 'stripey': 13863, '_hayward': 669, 'marche': 9174, 'rosti': 12352, 'crepes': 3980, '2nit': 214, 'reporter': 12090, 'forreal': 5991, 'rowing': 12372, 'sounders': 13475, 'cartoon': 3009, 'cgi': 3121, 'volunteer': 15577, 'fkkk': 5837, 'nurses': 10289, 'mores': 9704, 'cait': 2857, 'realis': 11841, '_sunshine': 813, 'yiiiit': 16388, 'jarn': 7960, '8c36ej': 503, 'artomatic': 1588, 'gawd': 6292, 'ughhhhhhhhh': 15178, 'triste': 14932, 'brlliant': 2632, 'amanzimtoti': 1270, 'quaver': 11686, 'sandwiches': 12533, 'muji': 9822, 'yipee': 16390, 'bsg': 2684, 'disks': 4654, 'gabe': 6221, 'afterparty': 1080, 'iq': 7835, 'wheelchair': 15889, 'buzzing': 2819, 'homebound': 7272, 'yeahhhhh': 16329, 'kn3mp': 8333, 'dann': 4213, 'misconnected': 9541, 'midway': 9460, 'undies': 15241, 'bestttt': 2162, 'gem': 6317, 'anddd': 1328, 'angrily': 1347, 'extemely': 5502, 'abusive': 917, 'mentally': 9385, 'nightss': 10125, 'bastards': 1980, 'congradts': 3717, 'phoning': 10993, 'lovebank': 8933, 'audit': 1696, 'portrait': 11300, 'unbecominglily': 15214, 'announcing': 1380, 'ecomonday': 5047, '_fischer': 641, 'recommendations': 11902, 'programmer': 11508, 'mcflurry': 9290, 'aaaaahhhh': 860, 'foking': 5913, 'eilish': 5117, 'competiton': 3644, 'pineapples': 11066, '4bckp': 307, 'moy': 9775, 'vs2003': 15595, 'generates': 6322, 'uninstall': 15265, 'guesss': 6739, 'duo': 4961, 'matilda': 9255, 'tabletop': 14194, 'programming': 11509, 'heidi': 7097, 'stiff': 13767, 'aundy': 1703, 'lamee': 8456, 'compatible': 3637, 'satisfaction': 12560, 'afraidiowe': 1072, 'bordatella': 2470, 'harbor': 6931, 'fallout': 5571, 'ammo': 1309, 'pvajlm': 11646, 'rome': 12323, 'elegance': 5136, 'tulane': 14992, 'unique': 15271, 'keren': 8234, 'hairdressers': 6841, 'fuschia': 6190, 'sheath': 12879, '_nicole': 756, 'lambastes': 8452, 'bankers': 1923, 'insurers': 7752, '½greed': 16532, '½stupidityï': 16549, 'wyab': 16225, 'amercia': 1293, 'jetsetter': 8016, 'dinghy': 4580, 'tossed': 14789, 'strategic': 13830, 'waterfall': 15723, 'redirects': 11933, 'item': 7883, 'pulish': 11599, 'proving': 11559, 'sympathies': 14170, 'loadsa': 8799, 'shizze': 12945, 'farrells': 5612, 'parlor': 10772, 'staging': 13661, '5ygpg': 406, 'gazillion': 6294, '_benson': 584, 'bawkmarked': 2005, 'condition': 3686, 'ironpython': 7849, 'compact': 3629, 'reflection': 11955, 'emit': 5182, 'amason': 1272, 'gos': 6551, 'rounding': 12361, 'bases': 1965, 'relays': 12016, 'purate': 11623, 'odyssey': 10354, 'rrod': 12385, 'odour': 10352, 'unemployment': 15244, 'spasy': 13521, 'lovage': 8929, 'essex': 5330, '4jkes': 326, '36': 257, 'clea': 3421, 'lold': 8833, 'releasd': 12018, '_molecule': 746, 'sackiroth': 12459, 'mcflys': 9292, 'halla': 6852, 'haloom': 6862, 'eeem': 5091, 'rationale': 11800, 'eyaseer': 5517, 'inshallah': 7720, 'dolidh': 4749, 'mister': 9572, 'rhiannon': 12205, 'evidence': 5401, 'nooobody': 10199, 'tada': 14203, 'prada': 11370, 'fragrance': 6026, '_nye': 759, 'arabic': 1508, 'suprisingly': 14072, 'nighter': 10119, 'suggesting': 13974, 'restarting': 12139, 'tribute': 14916, 'chrissy': 3333, 'laaaaaaaaave': 8413, 'horrors': 7344, 'sthlm': 13758, 'pizzahut': 11108, 'irratated': 7851, 'dw': 4977, 'twitterr': 15108, 'impostor': 7614, 'booster': 2458, 'woork': 16119, 'opposed': 10510, 'distract': 4677, 'fristy': 6111, 'polo': 11247, 'wl9yl': 16066, 'diggnation': 4564, 'prays': 11379, 'canon': 2934, 'i900d': 7495, 'fisheye': 5816, 'exp': 5463, 'nims': 10135, 'annie': 1370, 'downnnn': 4820, 'depresses': 4432, 'goodbey': 6505, 'pressents': 11418, 'mobypicture': 9607, 'uqi0h2': 15346, 'mtml': 9801, '2500': 169, 'hmmpph': 7230, 'linz': 8727, 'lsats': 8971, 'cissbury': 3381, 'hired': 7207, 'monkeys': 9662, 'izzy': 7914, 'eastbay': 5025, 'unlike': 15284, 'suspended': 14102, 'aaaa': 853, 'pagee': 10684, 'twittername': 15105, 'hosted': 7357, 'twitterfridge': 15099, 'sinned': 13118, 'univ': 15277, 'strauss': 13832, 'terence': 14390, 'cao': 2942, 'dose': 4790, 'fainting': 5555, '66shw': 434, 'davey': 4254, 'flygroups': 5897, 'bells': 2128, 'refunds': 11965, 'foreboding': 5963, 'shave': 12872, 'byebye': 2826, 'fudge': 6149, 'grahmcracker': 6598, 'nc': 9980, 'hobby': 7236, 'fuckyoumonday': 6148, 'cocktails': 3515, 'reduculous': 11941, 'uduhn': 15161, 'lun': 8997, 'mustangs': 9863, 'homesssssskooooler': 7283, 'everton': 5381, 'chanclas': 3149, 'diy': 4701, '4jccd': 317, 'reeboks': 11943, 'cornerstone': 3842, 'eur': 5353, 'jpy': 8105, '132': 63, '130': 60, '131': 62, 'executed': 5446, 'yesssssssssss': 16371, 'rocket': 12295, 'oral': 10523, 'welllll': 15834, 'pending': 10890, 'proofing': 11531, 'sessions': 12801, 'ankile': 1359, 'healthified': 7041, 'streusel': 13857, 'sleeper': 13207, 'bootleg': 2464, 'pharos': 10964, 'weighed': 15813, 'highest': 7177, 'proven': 11551, 'powerblog': 11353, 'followe': 5921, 'speeding': 13542, 'sufficient': 13968, 'satisfying': 12563, 'hayfever': 7002, 'piriton': 11086, 'croatian': 4002, 'puncture': 11612, 'sheena': 12884, 'othman': 10569, 'productivity': 11494, 'shelves': 12898, 'diva': 4690, 'elk': 5151, 'grove': 6697, 'cass': 3024, 'nes': 10046, 'pussycat': 11639, 'degeneres': 4370, 'hxlfm': 7480, 'satan': 12558, 'hamptons': 6878, 'relaxation': 12011, 'westt': 15851, 'stint': 13779, 'live360': 8761, 'baylee': 2008, 'alreadt': 1236, 'ilovemymommy': 7573, 'yeeeeah': 16345, 'empireonline': 5194, 'harold': 6952, 'distracting': 4679, 'stil': 13768, '4w70j': 341, '_iain': 680, '_mcfly': 737, '_saurus': 791, 'surprises': 14085, 'damnnn': 4188, 'timeee': 14630, 'unrelated': 15302, 'himym': 7199, 'fukin': 6151, 'constructions': 3748, 'poping': 11279, 'interface': 7770, 'teething': 14338, 'ringing': 12244, 'asbos': 1599, 'supper': 14058, 'kurumi': 8398, 'direction': 4597, 'mailed': 9096, 'rhinitis': 12206, 'sukked': 13983, 'dats': 4248, 'groundbreaking': 6691, 'production': 11492, 'zul': 16516, 'jin': 8033, 'crumbles': 4025, 'bonfires': 2423, '_idiots': 683, 'alrighty': 1242, 'nicley': 10107, 'palmdale': 10717, 'mspacers': 9795, 'kewl': 8245, 'missmickey': 9564, 'vast': 15438, 'stereos': 13752, 'colleague': 3557, '539': 376, 'dms': 4712, 'acw': 988, 'dazzleglasses': 4277, 'poker': 11229, 'padestrian': 10681, 'skanking': 13158, 'salmon': 12504, 'sashimi': 12554, 'aaaaaw': 862, '14mph': 76, 'intelligence': 7757, 'hike': 7188, 'everyonee': 5390, 'itll': 7886, '_gyrl': 663, 'penalty': 10887, '_zwitschert': 850, 'sso': 13646, 'luckyyyyyyy': 8987, 'enduring': 5218, 'boba': 2394, 'xoxox': 16258, 'sims2': 13100, 'minutee': 9528, 'destroyed': 4476, 'pf': 10953, 'changs': 3158, 'yogurtland': 16400, 'livenation': 8766, 'boooooooored': 2454, 'iat': 7499, 'rivercenter': 12261, 'pokemon': 11228, 'handed': 6884, 'personalit': 10939, 'blunt': 2376, 'tos': 14786, '11pm': 48, 'workk': 16136, 'dandy': 4203, 'darnit': 4229, 'interwebs': 7788, 'wor': 16123, '_com': 602, '8830': 497, 'pistons': 11094, 'yooooooo': 16408, 'robe': 12281, 'trnds3trs': 14938, 'pleaseyour': 11170, 'muffins': 9816, 'vocie': 15562, 'adele': 1010, 'collage': 3552, 'feminism': 5698, 'arnie': 1554, 'cities': 3384, 'rescued': 12119, 'luxurious': 9024, 'ba115': 1810, 'enlisted': 5242, 'crs': 4018, 'conflict': 3706, 'labo': 8420, 'wrigley': 16187, 'dims': 4576, '_matta': 736, 'yday': 16316, 'feds': 5673, 'topping': 14771, 'adem': 1011, 'uit': 15188, 'contemplating': 3760, 'somethings': 13403, 'tellin': 14360, 'bishop': 2254, 'pugged': 11592, 'bombard': 2417, 'pleeeeeeeassseeeeeee': 11176, 'mel': 9355, 'siento': 13059, 'dint': 4586, 'bapang': 1932, 'dito': 4687, 'addiction': 997, 'sameee': 12513, 'v0': 15410, '99pb5': 530, 'commodores': 3617, 'achieving': 960, 'xml': 16253, 'cumbersome': 4076, 'ciao': 3357, 'packin': 10675, 'poems': 11213, 'known': 8354, 'haahaha': 6793, 'minging': 9506, 'blogher09': 2345, 'squee': 13633, '_kay': 704, 'named': 9932, 'neaby': 9988, 'grans': 6616, 'grandkids': 6609, 'amt': 1318, '_n': 750, 'ooze': 10487, 'wiping': 16022, 'janis': 7953, 'ibood': 7502, 'ladiez': 8434, 'boffert': 2402, 'pumkpin': 11605, '_20': 544, 'fakes': 5564, '_370': 549, 'ohmygod': 10389, 'flattened': 5855, 'cast': 3029, 'raid': 11752, 'excepting': 5427, 'liquor': 8735, 'lexi': 8646, '_dave': 613, 'meany': 9316, 'haz': 7006, 'obnoxiously': 10325, 'cl': 3393, '66': 429, 'blaaaqhhh': 2276, 'givers': 6421, 'takers': 14221, 'talaga': 14226, 'chux': 3352, 'witnessing': 16053, 'historical': 7211, 'cote': 3865, 'fuuuuuuck': 6199, 'youuuu': 16440, 'hamster': 6879, 'strangely': 13824, 'allllllllright': 1214, 'exposed': 5492, 'milkkk': 9483, 'brits': 2628, 'esther': 5333, 'rhymes': 12207, 'investor': 7809, 'tester': 14408, 'jester': 8013, 'pester': 10947, 'polyester': 11249, 'sylvester': 14168, 'requester': 12104, 'occured': 10339, 'typin': 15146, 'sidekick': 13056, 'similarity': 13090, 'alexander': 1182, 'mylan': 9886, '60hrs': 418, 'compensate': 3638, 'vzerohost': 15600, 'hosting': 7358, 'drawings': 4849, '674p1': 438, 'itto': 7893, 'entertain': 5252, 'oldest': 10410, 'celebration': 3088, 'passive': 10803, 'agressive': 1113, 'owners': 10648, 'owne': 10646, 'limitation': 8703, 'shakespeare': 12842, 'garrulous': 6278, 'scribe': 12673, 'tps': 14827, 'schoolbooks': 12626, 'confusions': 3714, 'understanding': 15236, 'require': 12108, 'paragraph': 10754, 'para5': 10749, 'opting': 10518, '9ftuv3xmrn0': 534, 'purrrrs': 11631, 'ooooh': 10475, 'orchid': 10528, 'zak': 16481, 'wuld': 16208, 'x2qkb': 16235, 'cluedo': 3484, 'queue': 11701, 'deranged': 4438, 'anticipation': 1402, 'bullwinkle': 2743, 'fractured': 6025, 'fairy': 5560, 'stations': 13719, 'struck': 13873, 'lllooovvveee': 8779, 'boldly': 2413, 'teng': 14378, 'blake': 2292, 'sandbox': 12526, 'objects': 10324, 'autoreturn': 1721, '5z05g': 407, 'runny': 12425, 'unfort': 15251, 'abbreviation': 892, 'crowntown': 4017, 'hyperventilating': 7490, 'changin': 3156, 'johny': 8063, 'khayyam': 8260, 'wakil': 15641, 'scotts': 12644, 'tanya': 14256, 'microwave': 9452, 'larin': 8496, 'itv': 7897, '18mos': 105, 'nickname': 10105, 'mtaby': 9797, 'uniqname': 15270, 'punkin': 11616, 'appartment': 1462, 'cds': 3076, 'wooohooo': 16111, 'evacuating': 5361, 'coldddd': 3544, 'barked': 1946, 'cigarette': 3363, 'g00d': 6209, 'calms': 2887, 'embrace': 5177, 'deficiency': 4357, 'pitching': 11098, 'lackluster': 8427, 'naa': 9906, 'nm': 10159, '4get': 311, 'gina': 6393, 'corrections': 3850, 'officer': 10373, 'misplaced': 9551, 'waterguns': 15725, 'neighbourss': 10027, 'heya': 7152, 'famine': 5581, 'moleskineï': 9634, 'notebooks': 10236, 'worldwide': 16148, 'heeels': 7082, 'cutee': 4114, 'lullaby': 8993, 'failfriday': 5550, 'okaaaay': 10399, '4jj43': 324, 'ari': 1540, 'funniset': 6177, 'staples': 13684, 'fucktards': 6147, 'dread': 4852, '_kookie': 711, 'forrealll': 5992, '48hours': 302, 'cryed': 4039, 'mentality': 9384, 'evangelizing': 5363, 'cubicle': 4062, 'p4': 10666, 'brantley': 2561, 'tnite': 14682, 'sawn': 12581, 'misss': 9565, 'youhhhhhhh': 16417, 'touche': 14798, 'paparazzi': 10742, 'avian': 1735, 'risking': 12258, 'fisnihsed': 5821, 'yesert': 16362, 'admitted': 1021, 'discharge': 4623, 'henry': 7130, 'gunner': 6762, 'wwdc': 16214, 'exit': 5459, 'wayne': 15747, '½jï': 16536, 'vu': 15597, 'gq': 6576, 'stooopid': 13797, 'reconnect': 11905, 'interent': 7766, 'sads': 12479, 'screenings': 12665, 'troubleshooting': 14947, 'clusters': 3487, 'availability': 1725, 'horatio': 7333, 'caine': 2856, 'effed': 5102, 'graandma': 6579, 'houseee': 7375, 'havee': 6983, 'spreading': 13614, 'hayley': 7003, 'battlegrounds': 2000, 'battleground': 1999, 'tpc': 14826, 'boxing': 2527, 'phenomenon': 10969, 'superbad': 14045, 'invalid': 7798, 'euggh': 5349, 'hawt': 6997, 'motherland': 9739, 'caro': 2993, 'boak': 2387, 'madness': 9072, 'torts': 14783, 'anoher': 1386, 'grates': 6627, 'collide': 3564, 'imposed': 7611, 'outlets': 10589, 'neemah': 10013, 'philos': 10978, 'ophy': 10506, 'mat': 9241, 'intersubjectively': 7782, 'liar': 8656, 'recovery': 11920, 'firms': 5807, 'chabibi': 3125, 'recollecting': 11897, 'unkown': 15281, 'toasties': 14687, 'behin': 2109, 'thumping': 14578, 'podcasters': 11211, 'emporium': 5199, 'needa': 10001, 'stastics': 13710, 'waved': 15738, 'eddie': 5058, 'izzard': 7912, 'theese': 14469, 'noticing': 10245, '81': 489, 'jury': 8145, 'removed': 12055, 'recite': 11888, '_diesel': 619, '_web_desig': 836, 'sammie': 12515, 'omggg': 10432, 'cobras': 3512, 'wantewd': 15670, 'noting': 10247, 'meteor': 9416, 'yooo': 16406, 'kb': 8204, 'df': 4505, 'layenn': 8547, 'uughh': 15397, 'dunt': 4960, 'caffine': 2852, 'nicola': 10109, 'naisee': 9923, 'flares': 5849, 'arond': 1556, 'scurred': 12681, 'stillll': 13771, 'civics': 3387, 'wiff': 15971, 'whitby': 15920, 'lill': 8697, 'betta': 2168, 'fantasy': 5602, 'zenjar': 16494, 'mit': 9574, 'courseware': 3888, 'ygt5': 16382, 'disclaimer': 4624, 'external': 5507, 'coffees': 3531, 'ripping': 12253, 'path': 10816, 'alenka': 1179, 'chicky': 3266, 'energized': 5219, 'ceasar': 3078, 'sacrilege': 12462, 'traumatizing': 14884, 'vacay': 15417, 'spares': 13517, 'liesgirlstell': 8669, 'threads': 14550, 'relationships': 12005, 'twitaddicted': 15082, 'ooommmmggggg': 10473, 'technical': 14322, 'agenda': 1097, 'evan': 5362, 'longoria': 8862, 'cheesey': 3233, 'orignal': 10549, 'pissssssing': 11092, 'kitties': 8323, 'banana': 1908, 'yaaayyy': 16281, 'kd': 8207, '880': 496, 'derham': 4442, '3yeem': 284, 'crocodile': 4005, 'ballarat': 1895, '03': 5, 'welcomed': 15830, 'endorsement': 5215, 'rover': 12370, 'spencer': 13554, 'elevated': 5143, 'streamkeys': 13838, 'migration': 9469, '_uppercut': 827, 'kazim': 8203, 'becky': 2075, 'chorleywood': 3326, 'cambridge': 2895, 'caled': 2869, '_gov': 656, '_sims': 798, 'scoop': 12636, 'kwod': 8405, 'zoomed': 16511, 'trace': 14830, 'tireddd': 14657, 'rosalie': 12343, 'motorcades': 9750, 'clinton': 3449, 'overwhelmi': 10632, 'louise': 8923, 'rennison': 12062, 'tradition': 14843, 'crawfish': 3943, 'khichadi': 8261, 'andrews': 1331, 'sands': 12529, '4jerc': 320, 'morocco': 9718, 'straits': 13821, 'gibraltar': 6373, 'europa': 5355, 'gib': 6371, 'dirteeh': 4604, 'coffeeclub': 3530, 'lee': 8592, '_nj': 757, 'underbelly': 15226, 'bootay': 2460, 'caca': 2845, 'metreon': 9420, 'matchmaker': 9245, 'noice': 10178, 'amazake': 1274, 'powder': 11351, 'agave': 1092, 'jeep': 7988, 'strength': 13844, 'momsen': 9649, 'iowa': 7825, 'creamy': 3958, 'mushrooms': 9848, 'ktfbr': 8389, 'desert': 4448, 'universal': 15278, 'celebrities': 3090, 'carr': 2999, 'threee': 14552, '__hell': 559, 'beeeeaaaaatooooo': 2087, 'purposely': 11629, 'schade': 12615, 'tv_addict': 15025, 'aweee': 1757, 'showwwww': 13014, 'sadifying': 12475, 'oj': 10395, 'soldiers': 13377, 'concepts': 3675, 'mozconcept': 9778, 'thash': 14452, 'manics': 9154, 'speedy': 13547, 'vinny': 15519, 'opposite': 10511, 'hahahahahahahahahah': 6825, 'guility': 6746, 'nds': 9985, 'mettallica': 9423, 'christine': 3337, 'fones': 5934, 'treadmill': 14893, 'penetration': 10892, 'rabbits': 11731, 'recommendation': 11901, 'legalisation': 8598, 'sunshineeeeeee': 14039, 'microplaza': 9449, 'abrjp': 907, 'actions': 977, 'volumes': 15576, 'inspiring': 7733, 'girlie': 6403, 'lester': 8632, 'sox': 13497, 'sturday': 13907, 'discretion': 4636, 'lovah': 8930, 'charter': 3195, 'rohan': 12309, 'sowy': 13496, 'tripping': 14930, 'p2l88x': 10664, 'impatient': 7602, 'gratitude': 6629, 'bymyself': 2830, 'thnk': 14524, 'bruno': 2673, 'trafffffffic': 14846, 'suckssss': 13954, 'headahce': 7022, 'okiebud': 10404, 'quad': 11675, 'ssd': 13645, 'arrives': 1571, 'enable': 5203, 'plugin': 11191, 'duped': 4963, 'notifications': 10246, 'werent': 15846, 'yeaa': 16320, 'performances': 10919, 'silverstone': 13087, 'iracing': 7837, 'scanned': 12594, 'vado': 15420, 'bookmarks': 2442, 'ahold': 1135, 'fuzz': 6201, 'magnet': 9084, 'supplies': 14059, 'gla': 6427, 'valley': 15425, 'forseeable': 5995, 'spaceports': 13504, 'quarantine': 11679, 'clubs': 3482, 'summers': 13999, 'cheescake': 3228, '_jackson': 695, 'cabbage': 2842, 'freeballing': 6056, 'peer': 10881, 'trained': 14857, 'partners': 10786, 'kp': 8375, 'contributed': 3780, 'stimulus': 13772, 'replacements': 12078, 'promiscuous': 11521, 'surfing': 14080, 'chaos': 3165, 'ewww': 5412, 'upppppp': 15333, 'calmin': 2885, 'thristy': 14557, 'gwenyth': 6779, 'scarlett': 12604, 'grumpiness': 6720, 'uniservity': 15272, 'tutorials': 15021, 'motorbike': 9748, 'roundtrip': 12364, 'hapee': 6910, 'hada': 6803, 'retro': 12168, 'southridge': 13489, 'cardiff': 2967, 'girllll': 6405, 'horsing': 7348, 'hubb': 7412, 'naming': 9936, 'importantly': 7609, 'lure': 9008, 'fluids': 5891, 'buwieser': 2810, 'philadelphia': 10972, 'shucks': 13019, 'signings': 13073, 'meka': 9354, 'raspberry': 11793, 'sherbert': 12902, 'frankly': 6036, 'fixing': 5832, 'laptops': 8490, 'diseases': 4644, 'bubblewrap': 2695, 'busyy': 2794, 'astronomy': 1641, '_addict': 564, 'sweeney': 14135, '67f8o': 440, 'treck': 14900, 'lafayette': 8439, 'batonrouge': 1993, 'ashminov': 1606, 'deplurk': 4427, 'buhbyeee': 2725, 'rp3ir': 12375, 'spok': 13591, 'como': 3626, 'surfers': 14078, 'rats': 11801, 'aggressive': 1102, 'sketches': 13161, 'yeeeehaaa': 16346, 'stupidly': 13905, 'clearing': 3431, 'antonio': 1407, 'waitressing': 15635, 'haveto': 6987, 'claire': 3400, 'dayuuum': 4270, '5o': 402, 'edinburghac': 5066, 'pllleeeaaasse': 11182, '_eclectic': 627, 'illogical': 7568, '701': 462, 'inspirative': 7730, 'porto': 11299, 'alegre': 1177, 'fixd': 5830, 'suspicious': 14104, 'dabbling': 4158, 'intros': 7795, 'probability': 11469, 'otherdad': 10566, 'mischief': 9540, 'disgraced': 4646, 'perfectionist': 10915, 'luckkkk': 8983, 'yeehah': 16348, 'loudly': 8918, 'coworker': 3904, 'klemm': 8331, '_0_tronic': 541, 'dyededed': 4981, 'pocketwit': 11207, 'twikini': 15071, 'annapolis': 1365, 'froyo': 6125, 'handsome': 6895, 'colbert': 3542, 'boringgg': 2486, 'lampions': 8462, 'clay': 3418, 'aiken': 1143, 'autograph': 1718, 'blackpool': 2283, 'amandas': 1269, 'crystal': 4042, 'calendars': 2871, 'frat': 6042, 'slooowww': 13237, 'chays': 3204, 'dfizzy': 4507, 'stuffs': 13895, 'arvo': 1593, 'skl': 13177, 'ships': 12932, 'dfs7fy': 4508, 'cavitie': 3062, 'gdit': 6299, 'ranger': 11782, 'yj': 16392, 'hulk': 7429, '88': 495, 'whyyyyyy': 15958, 'loveyoufletch': 8955, 'organize': 10542, 'guarantee': 6733, 'oldies': 10411, 'engulfed': 5229, 'shooooes': 12962, 'maany': 9047, 'annoyingly': 1384, 'annas': 1366, 'sytycd': 14186, 'izzayyy': 7913, 'mwan': 9877, 'carwash': 3013, '48hoursnz': 303, 'goy': 6570, 'naughty': 9970, 'gab': 6219, 'footer': 5952, 'silvera': 13085, 'jenny': 8002, 'greeat': 6647, '8d': 504, 'rolland': 12312, 'garros': 6277, 'amazning': 1284, 'bgn': 2193, '_j9': 694, 'weirdly': 15821, 'greenbelt': 6653, 'dfb': 4506, 'werder': 15842, 'suckiest': 13947, 'tiasha': 14593, 'yiha': 16387, 'feta': 5711, 'cups': 4087, 'armani': 1548, 'pantone': 10739, '109': 35, 'mug': 9818, 'scarfed': 12601, 'feastfriday': 5663, 'requests': 12105, 'nkkairplay': 10154, 'smo': 13287, 'mpg': 9784, 'hasa': 6961, 'casserole': 3027, 'sundaes': 14018, 'dumpster': 4953, '371': 262, 'specilist': 13536, 'ectopic': 5054, 'uritors': 15353, 'ultrasound': 15195, 'surgury': 14082, 'humane': 7433, 'gel': 6315, 'snacks': 13303, 'momol': 9646, 'rplpr': 12377, 'womp': 16086, 'woooomp': 16112, '_yavanna': 847, 'sunrays': 14031, 'acoustic': 967, 'caleb': 2868, 'gump': 6756, 'marcus': 9177, 'macrina': 9061, 'lawyer': 8543, 'trial': 14912, '3to7': 280, 'chief': 3267, 'exhaaaausted': 5450, '_i_girl': 679, 'twitterbff': 15092, 'luch': 8977, 'poetry': 11215, 'catullus': 3048, 'ovid': 10636, 'extract': 5510, 'aeneid': 1056, 'addicts': 999, 'poll': 11244, 'yolonda': 16403, 'pedicures': 10874, 'recouperating': 11915, 'natalies': 9957, 'faceeee': 5539, 'positioned': 11311, 'poof': 11259, 'ringtones': 12246, 'iphones': 7829, 'alicia': 1192, 'yaaawn': 16279, 'pho': 10984, 'harpers': 6953, 'whatsup': 15878, 'peanuts': 10867, 'pounces': 11341, 'thanksss': 14442, 'tania': 14250, 'halved': 6866, 'workload': 16137, 'mtb': 9798, '4wry2': 360, 'rosemary': 12348, 'camerabag': 2902, 'lolo': 8841, 'grandad': 6604, 'estimation': 5335, 'wonders': 16095, 'marshmellows': 9211, 'cramcrackers': 3924, 'mckenna': 9294, 'diets': 4545, 'lathargic': 8515, 'sqeaky': 13628, '_0robertpatt': 542, 'fruit': 6127, 'sparring': 13520, 'rerecordings': 12112, 'cs': 4046, 'fmlllll': 5901, 'beatiful': 2051, 'kiwi': 8326, 'artis': 1585, 'overheat': 10615, 'eabeauty': 4993, 'restricted': 12146, 'priviledges': 11463, 'landon': 8473, 'announces': 1379, 'yell': 16353, 'monroe': 9665, 'trx': 14963, 'ropes': 12341, '30sec': 235, 'swings': 14157, 'windmills': 16000, 'wve': 16212, 'aquats': 1505, 'walker': 15650, 'dismissed': 4659, 'tonyt': 14747, 'frend': 6077, 'l8': 8409, 'wernt': 15847, 'letin': 8635, 'aplyin': 1452, 'agen': 1095, 'trips': 14931, 'enjoyyitverymu': 5237, 'giggles': 6381, 'interupted': 7783, 'ao': 1435, 'gadget': 6222, 'yhere': 16384, 'hickups': 7168, 'exposure': 5493, 'lonesome': 8852, 'nervou': 10043, 'havelunch': 6984, 'lily': 8699, 'allen': 1201, 'oof': 10466, 'moronmonday': 9719, 'aam': 882, 'sunbeam': 14012, 'cafï': 2853, 'grinder': 6679, '198': 109, 'primavera': 11441, 'lorenzo': 8901, 'jarvis': 7962, 'novacaine': 10256, 'pancake': 10723, 'damnation': 4185, 'yon': 16404, 'nearest': 9991, 'walgreens': 15647, 'proverbs': 11552, 'coins': 3538, 'tossin': 14791, 'turnin': 15011, 'awwwwwwwwwwe': 1796, 'wizard': 16058, 'waverly': 15739, 'meantime': 9314, 'packs': 10677, 'liter': 8753, 'kiddo': 8277, 'transcribing': 14866, 'tenth': 14386, 'snore': 13330, 'recos': 11913, 'bles': 2320, 'radishes': 11745, 'blooms': 2356, 'enthused': 5257, 'erock': 5308, 'enjoys': 5236, 'ooh_bell': 10469, 'whatchu': 15872, 'monsterpalooza': 9667, 'contestant': 3766, 'x0': 16228, 'meanie': 9308, '67rt8': 448, '_mounce': 749, '_ofoz': 763, 'cheered': 3223, 'hospice': 7353, 'gpa': 6571, 'compose': 3660, 'architect': 1519, 'tar': 14264, 'promenade': 11520, 'injustice': 7695, 'pox': 11358, 'syphilis': 14183, 'neeeddd': 10010, 'foooddd': 5946, 'lewishhh': 8645, 'mlb': 9588, 'mlbn': 9589, 'televising': 14355, 'det': 4479, 'bal': 1882, 'wieters': 15967, 'chainsaw': 3131, 'sssnoring': 13648, 'wrecked': 16181, 'yessssssir': 16370, 'cancerfree': 2927, 'revisingg': 12189, 'friending': 6097, 'zoozoo': 16512, 'voda': 15564, 'ilike': 7563, 'qljyb': 11664, '_newnew': 754, '_in_nh': 688, '0zywwj': 21, 'ladybug602': 8437, 'nun': 10285, '__cullen_': 556, 'emmett': 5184, 'jorge': 8080, 'tgxzu': 14425, 'pcvs': 10858, 'cider': 3361, 'forgiving': 5978, '_go': 655, 'hwy': 7479, 'cwack': 4130, 'od': 10347, 'pe': 10862, 'fil': 5749, 'breakfasted': 2573, 'playoff': 11153, 'helio': 7106, 'greentea': 6656, 'gettt': 6354, 'clickin': 3440, 'poets': 11216, 'medici': 9327, 'talkedabout': 14234, 'sting': 13773, 'aracheologist': 1511, 'thooo': 14532, 'comeeeeee': 3588, 'whaat': 15860, 'scale': 12590, 'misshimalready': 9556, 'blurryness': 2378, 'monumental': 9681, 'rural': 12428, '2630': 177, 'interviews': 7787, 'rented': 12068, 'roomy': 12335, 'welp': 15836, 'oopsie': 10485, 'origami': 10545, 'partyyyyy': 10794, 'luketic': 8991, 'heathfox': 7064, 'rhythms': 12209, 'sinc': 13102, 'inutero': 7796, 'trainer': 14858, 'babycham': 1820, 'ouuuuuuuuuchhhhhhhh': 10604, 'barca': 1940, 'spanking': 13515, 'impersonal': 7605, 'aaarrrgggghhh': 873, 'decribe': 4337, 'funnily': 6176, 'scarey': 12599, '_kid': 706, 'jobless': 8048, 'cordoning': 3835, 'fantabulous': 5598, '2hrs': 201, 'tanner': 14252, 'avocado': 1737, 'goodgirl': 6509, 'malay': 9123, 'warrior': 15695, 'workaholic': 16130, 'lifestyle': 8675, '55hq2o': 382, 'kristen': 8381, 'venom': 15459, 'styles': 13909, 'scooby': 12635, 'clot': 3469, 'clutches': 3488, 'depaul': 4422, 'origins': 10548, 'spit': 13579, 'ciber': 3358, 'sabip': 12456, 'fp': 6021, 'char': 3172, 'outlook': 10590, 'selective': 12732, 'opacity': 10489, 'wingstop': 16011, '4jken': 325, 'noida': 10179, 'venues': 15465, 'crashes': 3937, 'cmd': 3493, 'qood': 11668, 'morninq': 9716, 'aunty': 1708, 'buenos': 2709, 'dias': 4528, 'mundo': 9837, 'orphanage': 10553, 'combonations': 3581, 'worcester': 16124, 'clive': 3451, 'precise': 11385, 'inconsiderate': 7646, 'yout': 16435, 'swearing': 14123, 'harmed': 6947, 'procuts': 11487, 'weeekend': 15796, 'outsidee': 10594, 'revisionn': 12191, 'kiddnation': 8276, 'puppyy': 11621, 'squirells': 13636, '_sweethearts': 815, 'hydra': 7483, 'wisely': 16029, 'connector': 3733, 'yes2': 16361, 'carefully': 2975, 'deserv': 4449, 'singz': 13114, 'girrrlfriend': 6411, 'cm': 3492, 'aced': 950, 'defense': 4353, 'plumber': 11193, 'chelseavantol': 3240, '_bennett': 583, 'njoying': 10153, 'riveting': 12263, 'britta': 2629, 'absolutley': 912, 'balling': 1897, 'witnessed': 16052, 'fickleness': 5729, 'urm': 15357, 'soonest': 13424, 'fields': 5735, '13pdrmj': 68, 'overstressed': 10627, 'absolves': 914, 'suckd': 13944, 'challenger': 3138, '_exp': 636, 'virtualkiss': 15532, 'alwas': 1258, 'brilliantly': 2615, 'ox': 10655, 'beckett': 2074, 'hiyaaaaaa': 7223, 'defying': 4368, 'alll': 1207, '52': 374, 'tooshers': 14759, 'invitw': 7815, 'omw': 10443, 'kaggra': 8162, 'mistaken': 9569, '1k': 120, 'gastos': 6285, 'namen': 9933, 'warblers': 15678, 'gnatcatcher': 6461, 'awesoome': 1774, 'allllllllll': 1211, 'thrilling': 14556, 'passport': 10804, 'serena': 12777, 'darrian': 4232, 'confess': 3697, 'centro': 3105, 'transfered': 14868, 'sadpanda': 12478, 'ek': 5125, 'makati': 9111, 'stnt0': 13783, 'ahd': 1123, 'parental': 10764, 'halp': 6863, 'longgggggggg': 8860, 'coooolest': 3818, 'offiacial': 10370, 'express': 5495, 'lane': 8475, 'liquid': 8734, 'mmmmmmmmmmmm': 9597, 'euhm': 5351, 'handles': 6890, 'ufr1u': 15164, '121908inlove': 52, 'biddy': 2207, 'bops': 2468, 'kaotic': 8177, 'gossipy': 6556, 'likable': 8687, 'hahahahahahaha': 6823, 'abuzz': 918, 'advertising': 1052, 'qi': 11660, 'davis': 4256, 'tortoiseshell': 14782, 'indoor': 7668, 'colette': 3548, 'shoesless': 12956, 'ardillaaaaaaaaaaaa': 1524, 'whuahahhaha': 15951, 'forrea': 5990, 'jamlegend': 7945, 'petey': 10950, 'rollerskate': 12315, 'graveyard': 6631, 'atltweet': 1662, 'utrecht': 15392, 'marg': 9180, 'cloth': 3470, 'pradas': 11371, 'dunks': 4957, 'getonu2': 6350, 'bethhh': 2165, 'cuong': 4079, 'brokey': 2643, 'borat': 2469, 'izkab': 7910, 'stickam': 13760, 'awwwwww': 1792, 'photowalkingutah': 11003, 'calendar': 2870, 'photowalks': 11004, 'lunches': 9002, 'rp': 12374, '55am': 380, 'iloveyopu': 7574, 'greshamblake': 6664, 'sicken': 13045, 'pav': 10836, 'bhaaji': 2196, 'finely': 5781, 'desappointed': 4445, '_0407': 540, 'havn': 6990, 'sumptuous': 14004, '_2229': 547, 'chased': 3196, '_fan': 638, 'trumping': 14956, 'omlette': 10438, 'omnomlette': 10439, 'ecw': 5055, 'urk': 15354, 'glammyyy': 6432, 'crocker': 4004, 'kaushik': 8196, 'sucka': 13943, 'booker': 2438, 'specified': 13535, 'creator': 3965, '2005': 132, 'buggered': 2717, 'succesful': 13936, 'enroll': 5246, 'successfully': 13939, '_xo': 845, 'colds': 3546, 'assedly': 1622, 'ginniejean': 6397, 'mojo': 9628, 'expedited': 5468, 'geeksonaplane': 6307, 'web1': 15773, 'web2': 15774, 'automated': 1719, 'downloader': 4816, 'youporn': 16423, 'strongest': 13870, 'cvs': 4128, 'wandering': 15662, 'motorola': 9752, 'naini': 9922, '2my': 209, 'hv': 7473, 'snt': 13335, 'multimedia': 9826, 'subscription': 13926, 'repost': 12093, 'deceiving': 4315, 'proclamation': 11482, 'rebellioustwitwhoknowsacoolcatcook': 11869, 'hypertrophy': 7488, 'shoulders': 12993, 'defrag': 4366, 'reassembled': 11865, 'lodging': 8814, 'sportv': 13605, 'connect': 3727, 'rang': 11780, 'glimpse': 6440, 'opps': 10512, 'robbed': 12279, 'approaching': 1493, 'ckas': 3391, 'foster': 6008, 'chubba': 3344, 'winston': 16018, 'inherent': 7689, 'humility': 7437, 'eater': 5034, 'ttc': 14977, 'schmoo': 12623, 'moisturize': 9626, 'inclined': 7638, 'rephael': 12076, '_herdman': 673, '_peek': 771, 'mkay': 9587, 'bfgurelgbsr': 2189, 'orthodontisssttt': 10554, 'uwian': 15406, 'phplurk': 11007, 'sun82': 14010, 'drs': 4903, 'howre': 7387, 'princeton': 11449, 'goodwill': 6519, 'awessomee': 1775, '3lqux': 275, 'harassing': 6930, 'lance': 8465, 'thks': 14520, 'bettiye': 2174, 'riqht': 12255, 'prioritizing': 11455, 'kew': 8244, '_of_chas': 761, 'puter': 11641, 'whee': 15887, 'rebootiness': 11871, 'lorry': 8904, 'signaling': 13070, 'friench': 6095, '4wg12': 352, 'muh': 9820, 'samantha': 12510, 'rustling': 12438, 'pane': 10729, 'shipwrecked': 12933, 'risk': 12257, 'overreaction': 10622, 'yeahhhhhhhhhhhhh': 16330, 'phne': 10983, 'unsociable': 15306, 'earful': 5000, 'psychopath': 11577, 'creep': 3972, 'inconvenient': 7648, '½sseldorf': 16547, 'deffo': 4356, 'waz': 15752, '518': 373, 'joss': 8089, 'whedon': 15886, 'animation': 1355, 'leonardo': 8624, 'dicaprio': 4530, 'jennifer': 8000, 'davisson': 4257, 'killoran': 8290, 'cynthia': 4142, 'coffin': 3533, 'drinkers': 4878, 'mort': 9727, 'subite': 13917, 'luvin': 9018, 'fishfingers': 5817, 'tiday': 14601, 'celli': 3096, 'murphys': 9842, 'tatami': 14282, 'practicing': 11369, 'haaaate': 6790, '67tp9': 449, 'fjeam': 5833, 'reli': 12022, 'buh': 2724, 'balistic': 1892, 'dayuumm': 4269, 'alexis': 1184, 'bledel': 2311, 'ick': 7513, 'winks': 16013, 'heckitty': 7072, '____________': 555, '15yyid': 86, 'twitterin': 15102, 'frustrade': 6131, 'tropical': 14944, 'depression': 4435, 'mrsal65': 9790, 'hurricane': 7460, 'subs': 13924, 'sleeeeeppyyyyyy': 13202, '128': 54, '_reid': 782, 'dissing': 4672, 'octopus': 10346, 'redeems': 11927, 'rescuing': 12121, 'anymoree': 1421, 'zune': 16519, 'bible': 2203, 'consist': 3741, 'realitychec': 11848, 'sprite': 13623, '20somethin': 146, 'bkk': 2273, 'doug': 4805, '_roy': 789, 'fretful': 6086, 'mumbling': 9830, 'muffled': 9817, 'breesaholic': 2588, 'bullhorn': 2738, 'onnnnnnnnnnnn': 10454, 'strangle': 13827, 'salesman': 12501, 'shamwow': 12852, 'relise': 12028, 'respite': 12132, 'yayay': 16306, 'slopes': 13238, 'aarrgghh': 884, 'hose': 7351, 'balearic': 1889, 'sins': 13121, 'hhaha': 7161, 'mhmhmh': 9436, 'biking': 2224, 'sprawled': 13611, 'owww': 10653, 'tempat': 14366, 'apa': 1440, 'yang': 16293, 'paling': 10713, 'cocok': 3518, 'jupiter': 8143, 'seru': 12789, 'juga': 8119, 'vanessa': 15432, 'vfactory': 15481, 'namerebecca': 9934, 'chroma': 3340, 'cumulus': 4077, 'seminars': 12745, 'dagnamit': 4169, 'clickable': 3438, 'maitreya': 9106, 'bobby': 2396, 'produced': 11489, 'machineeeee': 9058, 'extremly': 5515, 'pointlesss': 11223, 'negghead': 10018, 'cantt': 2938, 'sleepp': 13212, 'omq': 10441, 'yuppie': 16469, 'opens': 10497, 'vampire': 15427, 'p274b': 10663, 'wango': 15663, 'elementary': 5138, 'zion': 16502, 'reporting': 12091, 'hayward': 7005, 'summary': 13995, 'harmful': 6948, 'crab': 3913, '5jeu7': 395, 'homemade': 7278, 'lasagna': 8499, 'supwp': 14073, 'tisk': 14664, 'upto': 15343, 'waxing': 15745, 'mustache': 9861, 'cecilia': 3082, 'eddplant': 5059, 'uugghh': 15396, 'storyyyyy': 13813, 'erasure': 5296, 'aaaaaah': 857, 'isabelle': 7857, 'erase': 5294, 'architecture': 1520, 'idgaf': 7523, 'arbiter': 1513, 'judith': 8116, 'volcom': 15573, 'choregoraphy': 3323, 'shaked': 12840, 'fists': 5823, 'abuse': 916, 'dpressed': 4829, 'ust': 15381, 'oriole': 10551, 'suet': 13962, 'feeder': 5677, 'thistle': 14519, '_44': 550, 'reeesee': 11946, 'pulleaase': 11601, 'saimee': 12491, 'underage': 15225, 'roasting': 12277, 'postcard': 11323, 'century': 3107, 'farewells': 5607, 'divorce': 4696, 'acess': 953, 'acceptance': 927, 'tommy': 14723, 'maintanance': 9104, 'womens': 16085, 'alternate': 1252, 'seeley': 12720, 'myoh': 9888, 'tarsier': 14268, 'pins': 11079, 'agrees': 1112, 'dontlike': 4774, 'stalin': 13667, '16ï': 95, '½c': 16530, '25ï': 175, '30ï': 238, 'reaped': 11859, 'nutty': 10298, 'newsire': 10076, 'twitterfeed': 15097, 'bodyshop': 2401, 'takeaway': 14218, 'kardashian': 8181, 'droids': 4892, '_monstr': 748, 'prism': 11457, 'hs': 7396, 'hou': 7369, 'retired': 12162, 'compan': 3630, 'reference': 11951, 'eldorado': 5128, 'aliante': 1189, 'torchwood': 14773, 'lillestrï': 8698, 'struggles': 13876, 'lassetti': 8502, 'fame': 5577, 'unfuzzy': 15259, 'lux': 9023, 'dentists': 4415, 'brainfreeze': 2551, 'mediation': 9324, 'pornstar': 11293, 'dessert': 4470, 'bordom': 2474, 'fatigue': 5635, 'settling': 12808, 'laundering': 8530, 'constructed': 3746, 'possession': 11316, 'rs': 12387, 'counteract': 3876, 'discouraging': 4632, 'tempe': 14367, 'marketplace': 9198, 'filipno': 5755, 'eff': 5098, 'shannen': 12853, 'improves': 7623, 'lashes': 8501, 'tined': 14640, 'moisturiser': 9625, 'glossary': 6450, 'brunt': 2675, 'ratt': 11802, 'pearcy': 10868, 'freed': 6058, 'carefree': 2973, 'feeeeeet': 5679, 'newsbites': 10075, 'wyb4h': 16226, 'fau': 5638, 'steet': 13742, 'slowmotion': 13247, 'hollykins': 7264, 'soproudofyo': 13444, 'multi': 9825, 'taskin': 14273, 'n95': 9904, 'opda': 10491, 'palmade': 10716, 'yeaaaah': 16323, 'livejournal': 8764, 'hedge': 7075, 'dee': 4341, 'btr': 2686, 'lautner': 8535, 'cameron': 2904, 'darts': 4234, 'brass': 2562, 'bmap': 2382, '2134': 148, 'sameway': 12514, '2006': 133, 'joyride': 8099, 'roadtrip': 12271, 'pai': 10687, 'paulo': 10833, '_fr': 644, 'digestif': 4560, 'swimmer': 14150, '291km': 186, '3k': 272, '170th': 98, 'stressiest': 13851, '_chick': 600, 'phantom': 10963, 'stitch': 13781, 'racing': 11737, 'creased': 3959, 'goljwp': 6496, 'roundabouts': 12360, 'proximately': 11562, 'tmw': 14679, 'smartest': 13268, 'newss': 10079, 'honeymoon': 7294, 'bask': 1973, '_dreamer': 624, 'greaaaaat': 6636, 'allies': 1205, 'holga': 7253, 'tomoz': 14731, 'bludge': 2368, '_team': 816, 'etb8h': 5339, 'thu': 14574, 'surface': 14077, 'afro': 1074, 'certificate': 3115, 'sn': 13301, 'cincy': 3366, 'collabro': 3550, 'jphlip': 8104, 'tweethearts': 15049, 'traveling': 14886, '302': 227, 'faires': 5558, 'hub': 7411, 'plugged': 11189, '_hearts': 671, 'cannae': 2932, 'hen': 7125, 'nae': 9911, 'livi': 8771, 'xxxx': 16267, 'twitskies': 15086, 'r20': 11727, '2l': 203, 'webcam': 15775, 'begining': 2100, 'tighten': 14615, 'bolts': 2414, 'lazying': 8553, 'annabelle': 1364, 'thxx': 14588, 'rhcp': 12203, 'electronic': 5134, 'keyhole': 8253, 'belgrade': 2118, 'nyayhahahah': 10308, 'assignemtn': 1628, 'remmebered': 12052, 'umbrellaï': 15200, '2aa0m': 188, 'grrrrrrrr': 6712, '67hac': 443, '67ha': 442, 'plantin': 11134, 'soil': 13373, 'populated': 11287, 'tegan': 14341, 'hazy': 7008, 'gh3': 6360, 'myweakness': 9902, 'besos': 2155, 'grandes': 6608, 'guapo': 6732, 'mileey': 9477, 'voe': 15566, 'mv': 9873, 'eeecontrol': 5083, '1500rpm': 80, '55c': 381, '0rpm': 19, 'twittersucks': 15113, 'noghty': 10177, 'twitterific': 15101, 'painf': 10690, 'applescript': 1477, 'sears': 12695, 'minh': 9507, 'sauna': 12571, 'mgcotd': 9432, 'maxxie': 9271, 'anwar': 1411, 'stressfree': 13849, 'appetizing': 1472, 'rugrats': 12402, 'anndd': 1367, 'lawl': 8539, 'greetings': 6660, 'lighten': 8680, 'spiced': 13561, 'yummmmmyyyy': 16463, 'herbs': 7132, 'shared': 12858, 'wealth': 15761, 'awesomeeeee': 1765, 'yeg': 16350, 'desserts': 4471, 'rethinking': 12160, 'toggling': 14706, 'ganesh': 6256, 'jaju': 7937, 'clicking': 3441, '_ife': 684, 'omnomnom': 10440, '69': 452, 'backlog': 1838, 'lxjd': 9026, 'shades': 12835, 'afireinside687': 1070, 'publicist': 11581, 'learnin': 8575, 'speechless': 13540, 'avalina': 1728, 'dud': 4936, 'humous': 7441, 'dorito': 4787, 'num': 10279, 'sharsies': 12867, 'renew': 12060, 'highways': 7185, '_j': 693, '_logos': 725, 'supply': 14060, 'useing': 15369, 'timothy': 14638, 'ceramic': 3108, 'shatter': 12870, 'singers': 13108, 'pooorr': 11268, '_what_': 839, 'skipped': 13173, 'pinkish': 11074, 'sweeter': 14139, 'seuss': 12810, '½you': 16554, 'accounting': 944, 'hedache': 7074, 'policy': 11237, 'puzzle': 11644, 'aber': 896, 'keanu': 8211, 'eagles': 4997, 'pooof': 11265, 'nirvana': 10144, 'tdl': 14303, 'sleepiness': 13209, 'zelda': 16490, 'pand_i': 10726, 'minnish': 9519, 'd7jvop': 4152, '1800': 101, 'lucien': 8979, 'kerk': 8235, 'vocalists': 15560, 'survey': 14090}\n\n\n\n# let us check the first tweet in the train set\n[invert_voc[k] for k in sorted([e for e in X[0,:].nonzero()[1]])]\n\n['after',\n 'at',\n 'but',\n 'chillin',\n 'continually',\n 'door',\n 'in',\n 'is',\n 'just',\n 'knocking',\n 'long',\n 'my',\n 'pjs',\n 'short',\n 'someone',\n 'week',\n 'why']\n\n\n\n# compare with original tweet\ndf_train['selected_text'].iloc[0]\n\n'Just chillin` in pjs after a short, but long week - why is someone continually knocking at my door?'\n\n\n\nConvert occurrencies to frequencies. Make another version with tf-idf.\n\n\n# check form the sklearn tutorial\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=False).fit(X)\nX_tf = tf_transformer.fit_transform(X)\nX_tf.shape\n\n(24732, 16556)\n\n\n\n# Let's check on the first document:\nfor e in X_tf[0,:]: print(e)\n\n  (0, 1075) 0.24253562503633297\n  (0, 1650) 0.24253562503633297\n  (0, 2795) 0.24253562503633297\n  (0, 3282) 0.24253562503633297\n  (0, 3770) 0.24253562503633297\n  (0, 4783) 0.24253562503633297\n  (0, 7626) 0.24253562503633297\n  (0, 7856) 0.24253562503633297\n  (0, 8148) 0.24253562503633297\n  (0, 8347) 0.24253562503633297\n  (0, 8854) 0.24253562503633297\n  (0, 9880) 0.24253562503633297\n  (0, 11111)    0.24253562503633297\n  (0, 12974)    0.24253562503633297\n  (0, 13398)    0.24253562503633297\n  (0, 15797)    0.24253562503633297\n  (0, 15953)    0.24253562503633297\n\n\n\n# do the same with inverse fequencies\nfrom sklearn.feature_extraction.text import TfidfTransformer\nitf_transformer = TfidfTransformer(use_idf=True).fit(X)\nX_itf = itf_transformer.fit_transform(X)\nX_itf.shape\n\n(24732, 16556)\n\n\n\n# Let's check on the first document:\nfor e in X_itf[0,:]: print(e)\n\n  (0, 15953)    0.20354703308587963\n  (0, 15797)    0.20898321420927882\n  (0, 13398)    0.2373398838636749\n  (0, 12974)    0.27287194297865613\n  (0, 11111)    0.3628915357919444\n  (0, 9880) 0.12289245253150947\n  (0, 8854) 0.22165871192301748\n  (0, 8347) 0.3628915357919444\n  (0, 8148) 0.15137551849249775\n  (0, 7856) 0.13049471067513022\n  (0, 7626) 0.13373857958694677\n  (0, 4783) 0.3000529254510452\n  (0, 3770) 0.3628915357919444\n  (0, 3282) 0.28329367170798353\n  (0, 2795) 0.14697601887809775\n  (0, 1650) 0.15791012471766455\n  (0, 1075) 0.2239315055018362\n\n\n\nChoose a classifier to predict the sentiment on the validation set. Compute the confusion matrix.\n\nWe are now ready to use the features to build a classifier. A common choice of a classifier (used for spam-detection among oters) is a Naive Bayes.\n\n# we can define and train the classifier in one single line\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_itf, df_train['sentiment'])\n\nTo use the new classifier on the test set, we need first to compute the features on the test set. We need to use exactly the same procedure as for the train set.\n\nimport numpy as np\n\n\n# note that we are using `transform`, not `fit_transform` as we are not recomputing the f\nX_test = count_vect.transform(df_test['selected_text'])\nX_itf_test = itf_transformer.transform(X_test)\n\n\npredictions = clf.predict(X_itf_test)\n\n\n# we can now use ready-made functions to compute statistics\nfrom sklearn import metrics\nprint(metrics.classification_report(df_test['sentiment'], predictions))\n\n\n# or start from the confusion matrix\n\n\nmetrics.confusion_matrix(df_test['sentiment'], predictions)\n\narray([[ 458,  317,   24],\n       [  14, 1055,   33],\n       [  12,  206,  629]])\n\n\n\n# comments...\n\n(2748,)"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis_correction.html#lending-club-dataset",
    "href": "tutorials/session_8/sentiment_analysis_correction.html#lending-club-dataset",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The following code processes the Lending Club Dataset from https://www.kaggle.com/datasets/mariiagusarova/preprocessed-lending-club-dataset-v2.\nRun and comment the following instructions (fix them if needed). Inspect he dataframe?\n\n#\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\nimport numpy as np\n\n\n# \nloan = pd.read_csv('loans.csv', low_memory=True)\n\n\nloan.head(2)\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\n0\n3600.0\n1.0\n24.0\n10.0\n55000.0\n0.0\n5.91\n4.0\n29.7\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20000.0\n2.0\n14.0\n10.0\n63000.0\n0.0\n10.78\n10.0\n56.2\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 66 columns\n\n\n\n\nloan.describe()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\ncount\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n...\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n848454.000000\n\n\nmean\n14685.697457\n1.268249\n21.268733\n6.145857\n72400.496752\n0.207527\n18.575478\n6.705665\n52.912851\n8.344402\n...\n0.002113\n0.015782\n0.085755\n0.007478\n0.027221\n0.002114\n0.022196\n0.013875\n0.003502\n0.002349\n\n\nstd\n8280.771514\n0.443048\n12.763837\n3.662421\n25589.248556\n0.405536\n8.199811\n5.820888\n23.374794\n4.340655\n...\n0.045922\n0.124630\n0.280002\n0.086153\n0.162728\n0.045934\n0.147320\n0.116971\n0.059071\n0.048409\n\n\nmin\n1000.000000\n1.000000\n1.000000\n0.000000\n35000.990000\n0.000000\n-1.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n8000.000000\n1.000000\n12.000000\n3.000000\n51500.000000\n0.000000\n12.510000\n2.000000\n35.700000\n5.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13000.000000\n1.000000\n21.000000\n7.000000\n67600.000000\n0.000000\n18.130000\n5.000000\n53.200000\n8.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20000.000000\n2.000000\n31.000000\n10.000000\n90000.000000\n0.000000\n24.360000\n10.000000\n70.700000\n11.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n40000.000000\n2.000000\n65.000000\n10.000000\n144999.000000\n1.000000\n45.000000\n25.000000\n148.000000\n35.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 66 columns\n\n\n\nThe dataset contains many information about borrowers from an unspecified instutitution. We can observe that categorical data (like add_state) has already been encoded using dummy variables for each value. As a result the number of regressors is fairly large (66). On the other hands, the number of observations is also very large.\nRun and comment the following instructions (fix them if needed). What kind of modeling exercise is performed?\n\nloan['loan_status'].value_counts()\n\nloan_status\n0.0    672377\n1.0    176077\nName: count, dtype: int64\n\n\nThe objective here is to build a model to predict which loans will default (loan_status=1). It is a classification exercise. Since the number of regressors is fairly large it is natural to look for a machine learning approcah.\n\n#\nfeatures = loan.columns.to_list()\nfeatures.remove('loan_status')\n\n\n#\ndf_train, df_test = train_test_split(loan, test_size=0.25, random_state=42)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n#\nclf = LogisticRegression()\nclf.fit(df_train[features], df_train['loan_status'])\n\n\n#\ny_pred = clf.predict(df_test[features])\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(df_test[features], df_test['loan_status'])))\n\nAccuracy of logistic regression classifier on test set: 0.79\n\n\n\n#\ncal = sklearn.metrics.confusion_matrix(df_test['loan_status'], y_pred, labels=clf.classes_)\nprint(cal)\n\n[[165146   2918]\n [ 41439   2611]]\n\n\n\n#\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cal, display_labels=clf.classes_)\ndisp.plot()\n\n\n\n\n\n\n\n\nThe database contains information about many clients.\nFor the confusion matrix that was just computed compute accuracy, precision, recall and f1 score (lookup the definitions if needed).\n\n# copmute the different statistics (by hand or programmatically)\n\nComment on the model validity."
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis_correction.html#the-dataset",
    "href": "tutorials/session_8/sentiment_analysis_correction.html#the-dataset",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "We use the News Sentiment Dataset from Kaggle.\n\nImport Dataset as a pandas dataframe. Remove rows where selected_text is not available.\n\n\n# the following command checks the current working directory \n# it should end with session_8\n%pwd\n\n'/home/pablo/Teaching/escp/dbe/tutorials/session_8'\n\n\n\nimport pandas\ndf = pandas.read_csv(\"Tweets.csv\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n\n\n\n\n\n\n# we can count the number of non available values for `text`\nsum(df['selected_text'].isna())\n\n1\n\n\n\ndf = df[df['selected_text'].notna()]\n\n\n# check it worked\nsum(df['selected_text'].isna())\n\n0\n\n\n\nDescribe Dataset (text and graphs). What is the distribution of the various sentiment values?\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\ncount\n27480\n27480\n27480\n27480\n\n\nunique\n27480\n27480\n22463\n3\n\n\ntop\ncb774db0d1\nI`d have responded, if I were going\ngood\nneutral\n\n\nfreq\n1\n1\n199\n11117\n\n\n\n\n\n\n\nThere are 2780 tweets. Only the tweets, not any context information.\n\n# information about the distrbution of sentiments\nsentiments = df['sentiment']\n\n\ncounts = sentiments.value_counts()\ncounts\n\nsentiment\nneutral     11117\npositive     8582\nnegative     7781\nName: count, dtype: int64\n\n\n\n# we can convert in percentages...\ncounts / sum(counts) * 100 \n\nsentiment\nneutral     40.454876\npositive    31.229985\nnegative    28.315138\nName: count, dtype: float64\n\n\n\n# ... or use the normalize option\nsentiments.value_counts(normalize=True)\n\nsentiment\nneutral     0.404549\npositive    0.312300\nnegative    0.283151\nName: proportion, dtype: float64\n\n\nThe distribution of tweet sentiment is rather well balanced: majority of tweets are neutreal with about 30% positive and 30% negative tweets.\n\nimport seaborn as sns\nsns.histplot(sentiments)\n\n\n\n\n\n\n\n\n\nCount the number of tweets mentionng trump.\n\n\n# this is a series where a line is True only if df['text'] contains \"trump\"\nind = df['text'].str.contains(\"trump\", na=False) | df['text'].str.contains(\"Trump\", na=False) \nind\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n27476    False\n27477    False\n27478    False\n27479    False\n27480    False\nName: text, Length: 27480, dtype: bool\n\n\n\n# which tweets contain word \"trump\"\ndf[ind]\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n1589\n6c5505a37c\nEnjoy! Family trumps everything\nEnjoy! Family trumps everything\npositive\n\n\n6235\n234a562dd9\nLOL I love my MacBook too. Oh and my iMac. Ca...\nlove\npositive\n\n\n14005\n464eafe267\nHow to get a $40 trumpet book - get caught in ...\ncaught\nnegative\n\n\n14615\nfc16472bdf\nSick kid trumps advance planning. Bummer\nSick\nnegative\n\n\n14671\n3314e0f981\nGrrr, I can`t even practice Trumpet or vocals ...\nneck hurt to\nnegative\n\n\n22230\n2762b6624b\n_fan Been busy trumping your cheese omlette wi...\n_fan Been busy trumping your cheese omlette wi...\nneutral\n\n\n\n\n\n\n\n\n# how many tweets contain word \"trump\"\nsum(ind)\n\n6\n\n\n\nSplit Dataset into training, and test set.\n\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n\nimport sklearn\n# here the data set is very big, we can choose a relatively small test set\ndf_train, df_test  = sklearn.model_selection.train_test_split(df, test_size=0.1)\n\n\ndf\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n...\n...\n...\n...\n...\n\n\n27476\n4eac33d1c0\nwish we could come see u on Denver husband l...\nd lost\nnegative\n\n\n27477\n4f4c4fc327\nI`ve wondered about rake to. The client has ...\n, don`t force\nnegative\n\n\n27478\nf67aae2310\nYay good for both of you. Enjoy the break - y...\nYay good for both of you.\npositive\n\n\n27479\ned167662a5\nBut it was worth it ****.\nBut it was worth it ****.\npositive\n\n\n27480\n6f7127d9d7\nAll this flirting going on - The ATG smiles...\nAll this flirting going on - The ATG smiles. Y...\nneutral\n\n\n\n\n27480 rows × 4 columns\n\n\n\n\nsum((df['selected_text'].isna()))\n\n0\n\n\n\ndf_train.shape\n\n(24732, 4)"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis_correction.html#classifying-tweets",
    "href": "tutorials/session_8/sentiment_analysis_correction.html#classifying-tweets",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The goal is now to to build a tweet classifier to predict a tweet sentiment, without any human input.\n\nExtract features from the training dataset. What do you do with non-words / punctuation?\n\n(hint: check the CountVectorizer function and the tutorial on sklearn webpage.)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n\n\nX = count_vect.fit_transform(df_train['selected_text'])\n\n\n# X is a matrix of \"features\", each one corresponding to the counts of specific tokens\nX.shape\n\n(24732, 16556)\n\n\n\n# the matrix is sparse: only non zero entries are stored\n# check for the first entry\nX[0,:]\n\n&lt;1x16556 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 17 stored elements in Compressed Sparse Row format&gt;\n\n\n\nfor e in X[0,:]: print(e)\n\n  (0, 8148) 1\n  (0, 3282) 1\n  (0, 7626) 1\n  (0, 11111)    1\n  (0, 1075) 1\n  (0, 12974)    1\n  (0, 2795) 1\n  (0, 8854) 1\n  (0, 15797)    1\n  (0, 15953)    1\n  (0, 7856) 1\n  (0, 13398)    1\n  (0, 3770) 1\n  (0, 8347) 1\n  (0, 1650) 1\n  (0, 9880) 1\n  (0, 4783) 1\n\n\n\n# we can actually print the processed tokens\nprint(count_vect.vocabulary_)\ninvert_voc = {v:k for k,v in count_vect.vocabulary_.items()}\n\n{'just': 8148, 'chillin': 3282, 'in': 7626, 'pjs': 11111, 'after': 1075, 'short': 12974, 'but': 2795, 'long': 8854, 'week': 15797, 'why': 15953, 'is': 7856, 'someone': 13398, 'continually': 3770, 'knocking': 8347, 'at': 1650, 'my': 9880, 'door': 4783, 'ouch': 10572, 'back': 1831, 'sick': 13043, 'http': 7406, 'twitpic': 15084, 'com': 3576, '4w6lf': 340, 'bbq': 2018, 'time': 14628, 'again': 1088, 'baby': 1819, 'alex': 1181, 'miss': 9552, 'you': 16416, 'ily': 7579, 'good': 6503, 'night': 10117, 'ah': 1114, 'not': 10232, 'doing': 4747, 'cbeebies': 3069, 'epg': 5277, 'by': 2824, 'any': 1414, 'chance': 3147, 'are': 1525, 'hehe': 7090, 'need': 9999, 'pinkyponk': 11077, 'crash': 3935, 'warnings': 15690, 'your': 16424, 'journey': 8094, 'home': 7271, 'hope': 7317, 'fl': 5840, 'omg': 10429, 'disco': 4625, 'packed': 10674, 'allergies': 1203, 'must': 9860, 'be': 2029, 'sleeping': 13210, 'walking': 15651, 'work': 16129, 'wasn': 15704, 'nice': 10097, 'cof': 3526, 'hate': 6968, 'electronics': 5135, 'making': 9118, 'an': 1322, 'imovie': 7601, 'of': 10356, 'college': 3563, 'im': 7581, 'school': 12625, 'right': 12239, 'now': 10260, 'fav': 5641, 'everyone': 5389, 'here': 7135, 'we': 15759, 're': 11822, 'off': 10360, 'to': 14684, 'party': 10788, 'have': 6982, 'myself': 9891, 'barely': 1943, 'hrs': 7395, 'sleep': 13205, 'and': 1326, 'the': 14461, 'lord': 8900, 'blessed': 2322, 'me': 9301, 'with': 16041, 'what': 15869, 'feels': 5685, 'like': 8688, 'it': 7873, 'came': 2899, 'sore': 13448, 'throat': 14559, 'pumpin': 11607, 'out': 10580, 'this': 14517, 'paper': 10744, 'gift': 6375, 'love': 8932, 'that': 14453, 'quiet': 11706, 'office': 10371, 'haha': 6812, 'totally': 14794, 'more': 9702, 'efficient': 5103, 'pic': 11015, 'donnie': 4770, 'one': 10446, 'siouxsinner': 13124, 'took': 14751, 'last': 8503, 'launch': 8528, 'word': 16125, 'has': 6960, 'he': 7015, 'got': 6557, 'solo': 13383, 'album': 1172, 'comin': 3598, 'too': 14748, 'yeaaa': 16321, 'so': 13340, 'classy': 3414, 'economy': 5051, 'depressing': 4434, 'fave': 5643, 'tired': 14655, 'over': 10608, 'worked': 16132, 'lol': 8832, 'wish': 16030, 'had': 6802, 'book': 2435, 'dirty': 4605, 'sad': 12464, 'sorry': 13458, 'offline': 10377, 'for': 5956, 'll': 8777, 'tweet': 15039, 'later': 8510, 'jeff': 7989, 'will': 15982, 'hard': 6932, 'from': 6118, '42nd': 292, 'pats': 10827, 'philly': 10977, 'am': 1263, 'mmm': 9592, 'cheesesteak': 3232, 'boyfriend': 2531, 'vacation': 15414, 'happy': 6925, 'mother': 9738, 'day': 4263, 'know': 8350, 'maybe': 9274, 'then': 14480, 'forgotten': 5980, 'about': 904, 'christmas': 3338, 'july': 8128, 'homey': 7286, 'missed': 9554, 'da': 4153, 'bus': 2781, 'writing': 16194, 'some': 13392, 'mild': 9475, 'wild': 15979, 'articles': 1583, 'want': 15667, 'go': 6467, 'peru': 10944, 'summer': 13998, 'ahhhhhhh': 1132, 'hopefully': 7322, 'yesyesyes': 16375, 'there': 14488, 'hey': 7151, 'guys': 6776, 'should': 12990, 'invite': 7812, 'moody': 9685, 'only': 10452, 'friend': 6096, 'these': 14494, 'does': 4735, 'hurt': 7462, 'much': 9811, 'would': 16167, 'ride': 12230, 'superman': 14047, 'cute': 4113, 'horrible': 7337, 'prefer': 11390, '80': 483, 'singstar': 13113, 'all': 1199, 'words': 16127, 'watched': 15716, 'final': 5767, 'break': 2571, 'prison': 11459, 'episode': 5281, 'was': 15698, 'great': 6641, 'farewell': 5606, 'dearly': 4302, 'follow': 5920, 'friday': 6091, 'following': 5928, 'people': 10900, 'followers': 5924, 'woot': 16121, 'followfriday': 5925, 'feeling': 5683, 'really': 11856, 'cool': 3809, 'oh': 10383, 'forgot': 5979, 'ferry': 5703, 'turns': 15015, 'around': 1558, 'before': 2098, 'going': 6488, 'sitting': 13141, 'sun': 14009, 'other': 10565, 'side': 13055, 'deck': 4327, 'closed': 3460, 'how': 7381, 'dare': 4219, 'apologize': 1458, 'uk': 15189, 'being': 2112, 'gone': 6500, 'while': 15905, 'don': 4762, 'america': 1295, 'never': 10062, 'touring': 14806, 'awesome': 1763, 'lmao': 8782, 'ha': 6787, 'bummed': 2748, 'aw': 1743, 'nick': 10104, 'his': 7208, 'heart': 7050, 'broken': 2641, 'poor': 11272, 'pleasant': 11159, 'surprise': 14083, 'still': 13770, 'working': 16135, 'on': 10444, 'music': 9850, 'grades': 6592, 'outside': 10593, 'behind': 2110, 'big': 2211, 'gray': 6633, 'cloud': 3473, 'thanks': 14440, 'thank': 14434, 'fkn': 5839, 'ugly': 15181, 'matches': 9244, 'mood': 9682, 'unfortunatly': 15256, 'punchy': 11611, 'due': 4939, 'two': 15125, 'early': 5005, 'morning': 9712, 'pager': 10685, 'events': 5371, 'ashleighhh': 1604, 'get': 6348, 'newww': 10080, 'cd': 3074, 'funky': 6170, 'reel': 11949, 'awful': 1776, 'pretty': 11426, 'icky': 7514, 'getting': 6353, 'ready': 11835, 'think': 14509, 'catching': 3039, 'cold': 3543, 'yay': 16305, 'gotta': 6564, 'pull': 11600, 'hour': 7372, 'shift': 12912, 'holla': 7257, 'ya': 16273, 'sometime': 13404, 'tomorrow': 14729, 'peace': 10863, 'new': 10064, 'born': 2487, 'single': 13110, 'our': 10577, 'little': 8758, 'puppy': 11620, 'basset': 1978, 'hound': 7370, 'actress': 983, 'cryin': 4040, 'could': 3870, 'bed': 2080, 'nine': 10137, 'once': 10445, 'make': 9112, 'sleepdeprived': 13206, 'luck': 8980, 'half': 6849, 'term': 14391, 'isn': 7865, 'enough': 5244, 'bless': 2321, 'her': 7131, 'dropped': 4896, 'lei': 8607, 'cemetary': 3100, 'least': 8579, 'wont': 16098, 'hav': 6980, 'put': 11640, 'up': 15314, 'wiv': 16055, 'him': 7197, 'no': 10161, 'problemo': 11474, 'poooooooor': 11267, 'sheeeep': 12883, '20': 128, 'john': 8058, 'drop': 4894, 'something': 13402, 'man': 9137, 'can': 2911, 'hang': 6898, 'instead': 7741, 'macaroni': 9052, 'cheese': 3229, 'please': 11161, 'cubbies': 4060, 'even': 5365, '500': 366, 'else': 5159, 'matter': 9260, 'beat': 2046, 'dem': 4398, 'bums': 2753, 'chicago': 3259, 'baseball': 1962, 'ore': 10533, 'play': 11143, 'audition': 1697, 'anna': 1362, 'dont': 4773, 'into': 7789, 'feel': 5680, 'bad': 1850, 'kids': 8280, 'disappointment': 4617, 'roof': 12327, 'decal': 4313, 'saving': 12576, 'gaahhhh': 6218, 'rock': 12293, 'honored': 7300, 'le': 8559, 'mont': 9669, 'they': 14496, 'rich': 12217, 'passed': 10799, 'pairung': 10702, 'as': 1594, 'well': 15833, '10': 22, 'minutes': 9529, 'class': 3408, 'its': 7888, 'kinda': 8296, 'cause': 3053, 'fun': 6156, 'adore': 1033, 'went': 15841, 'vending': 15453, 'maching': 9060, 'bugles': 2721, 'perhaps': 10924, 'sign': 13068, 'snack': 13302, 'anyway': 1429, 'probably': 11471, 'same': 12511, 'thing': 14504, 'wrong': 16199, 'paul': 10831, 'scheuring': 12621, 'series': 12784, 'finale': 5768, 'sucked': 13945, 'bank': 1922, 'teller': 14358, 'definitely': 4361, 'hitting': 7219, 'interested': 7768, 'dear': 4301, 'dance': 4197, 'tonight': 14738, 'look': 8868, 'quick': 11703, 'post': 11321, 'dashboard': 4238, 'pleasure': 11171, 'apparently': 1461, 'tv': 15024, 'fixed': 5831, 'plus': 11196, 'refunded': 11964, 'cost': 3858, 'buy': 2811, 'downside': 4822, 'tomato': 14718, 'soup': 13479, 'tastes': 14278, 'red': 11925, 'peppers': 10907, 'stressed': 13848, 'pugsly': 11595, 'missing': 9559, 'invisible': 7810, 'car': 2955, 'helps': 7123, 'boost': 2457, 'recycling': 11923, 'honest': 7290, 'gonna': 6501, 'ats': 1665, 'charmingly': 3192, 'where': 15896, 'did': 4532, 'leave': 8582, 'citeh': 3382, 'jersey': 8007, 'connection': 3731, 'gw': 6777, 'dutch': 4969, 'might': 9463, 'do': 4719, 'art': 1578, 'children': 3271, 'promised': 11523, 'birthday': 2249, 'brekkie': 2590, 'problem': 11472, 'bugger': 2716, 'food': 5940, 'house': 7374, 'stuff': 13893, 'precariously': 11382, 'close': 3459, 'ricotta': 12225, 'didn': 4535, 'help': 7118, 'fx': 6205, 'windows': 16002, '1000x': 26, 'better': 2170, 'than': 14433, 'vista': 15551, 'far': 5604, 'win': 15995, 'reborn': 11872, 'perfect': 10913, 'waking': 15642, 'star': 13685, 'wars': 15696, 'end': 5208, 'title': 14667, 'song': 13411, 'sorta': 13461, 'also': 1246, 'creepy': 3977, 'fine': 5778, 'shrug': 13018, 'send': 12747, 'loves': 8951, 'revenge': 12183, 'possibility': 11318, 'come': 3583, 'friends': 6100, 'takes': 14222, 'error': 5312, 'page': 10683, 'yall': 16291, 'ni99as': 10091, 'loopjazz': 8893, 'yess': 16364, 'ez': 5523, 'most': 9735, 'bejï': 2114, '½n': 16540, 'taylor': 14297, 'swift': 14148, 'joe': 8053, 'jonas': 8073, 'looked': 8870, 'together': 14704, 'broke': 2640, 'able': 902, 'recovering': 11919, 'italian': 7875, 'cruise': 4022, 'mediterenean': 9332, 'finished': 5787, 'weeding': 15791, 'flower': 5884, 'weeds': 15792, 'grow': 6699, 'legitimate': 8604, 'plants': 11136, 'rubbish': 12394, 'eh': 5113, 'sure': 14074, 'daw': 4258, 'guesting': 6741, 'ni': 10090, 'cha': 3123, 'monday': 9652, 'pendulum': 10891, 'goodbyes': 6508, 'suck': 13942, 'shut': 13027, 'plz': 11199, 'toy': 14822, 'story': 13810, 'jb': 7976, '3d': 268, 'movie': 9766, '2moro': 206, 'aaaand': 865, 'funny': 6180, 'actually': 987, 'were': 15843, 'decisive': 4326, 'nah': 9915, 'next': 10081, 'honey': 7293, 'ma': 9038, 'fault': 5639, 'confusin': 3711, 'nobody': 10167, 'write': 16190, 'wrk': 16196, 'gtta': 6729, 'tonite': 14742, 'mobile': 9605, 'advert': 1050, 'trafalger': 14845, 'square': 13631, 'looks': 8874, 'lot': 8911, 'except': 5426, 'if': 7539, 'load': 8794, 'pigeons': 11041, 'best': 2156, 'bring': 2616, 'show': 13002, 'denial': 4408, 'very': 15475, 'powerful': 11355, 'thinking': 14512, 'having': 6989, 'lunch': 8999, 'soon': 13422, 'quite': 11713, 'such': 13941, 'shame': 12848, 've': 15441, 'coz': 3906, 'different': 4550, 'aren': 1527, 'left': 8595, 'knee': 8338, 'somehow': 13397, 'hurts': 7468, 'walk': 15648, 'supposed': 14068, 'sometimes': 13405, 'busy': 2793, 'recognize': 11895, 'always': 1259, 'family': 5580, 'god': 6473, 'vegas': 15445, 'without': 16049, '_war_mt': 834, 'longer': 8855, 'blue': 2369, 'fantastic': 5600, 'likes': 8691, 'face': 5535, 'battery': 1996, 'die': 4539, 'daddy': 4165, 'see': 12716, 'driving': 4890, 'insane': 7714, 'talk': 14232, 'hips': 7205, 'lovely': 8948, 'sunniest': 14026, 'ages': 1100, 'exams': 5419, 'yes': 16360, 'sir': 13126, 'ok': 10398, 'hungry': 7449, 'fat': 5628, 'aha': 1115, 'jake': 7938, 'thomas': 14530, 'precious': 11384, 'when': 15892, 'say': 12582, 'white': 15921, 'light': 8679, 'every': 5382, 'ghost': 6366, 'whisperer': 15915, 'failed': 5549, 'miserably': 9544, 'cuties': 4118, 'tried': 14921, 'ftp': 6139, 's3': 12451, 'use': 15365, 'fireftp': 5803, 'firefox': 5802, 'browser': 2659, 'addin': 1000, 'mention': 9387, 'support': 14062, 'their': 14471, 'site': 13135, 'though': 14540, 'meetings': 9346, 'afternoon': 1078, 'started': 13700, 'fill': 5756, 'golden': 6492, 'times': 14634, 'hmmm': 7227, 'skip': 13172, 'grocery': 6682, 'store': 13804, 'head': 7017, 'walmart': 15658, 'potentially': 11337, 'direct': 4595, 'message': 9403, 'hell': 7107, 'complaining': 3648, '27': 180, 'answer': 1389, 'ask': 1611, 'ew': 5409, 'tongue': 14735, 'kiss': 8314, 'waiting': 15633, 'front': 6119, 'dps': 4830, 'julian': 8125, 'take': 14217, 'test': 14407, 'loneliness': 8848, 'bah': 1865, 'fb': 5655, 'fan': 5584, '7500': 469, 'diiinner': 4568, 'sooo': 13426, 'thunderrrr': 14580, 'cuddling': 4068, 'blanky': 2300, 'during': 4965, 'storm': 13807, 'whats': 15876, 'voice': 15569, 'shot': 12987, 'luv': 9016, 'true': 14954, 'forward': 6005, 'ugh': 15172, 'goes': 6485, 'life': 8670, 'cardboard': 2965, 'box': 2524, 'ch': 3122, 'appreciat': 1487, 'anything': 1427, 'gd': 6298, 'excited': 5434, 'made': 9068, '100x': 28, 'chocolates': 3307, 'stuck': 13882, 'yum': 16458, 'lucky': 8986, 'swimming': 14151, 'tanning': 14253, 'heaven': 7065, 'playing': 11151, 'infamous': 7676, 'hear': 7045, 'unfortunately': 15254, 'yeahhh': 16327, 'wasnt': 15705, 'thereeeeeeeeeee': 14490, 'woooooo': 16114, 'seeing': 12718, 'yo': 16395, 'reply': 12084, 'booooo': 2451, 'us': 15361, 'lmfaoooo': 8788, 'boring': 2485, 'account': 943, 'local': 8804, 'comic': 3596, 'ran': 11775, '4k': 328, 'gosh': 6552, 'happend': 6914, 'yesterday': 16373, 'license': 8659, 'hated': 6969, 'sounds': 13477, 'scared': 12597, 'cannot': 2933, 'wait': 15630, 'glad': 6428, 'copy': 3830, 'shipped': 12930, 'shows': 13012, 'today': 14693, 'tune': 15000, 'plans': 11129, 'cancelled': 2924, 'hmv': 7233, 'raises': 11767, 'hand': 6881, 'caffeine': 2850, 'although': 1256, 'caveat': 3060, 'normally': 10218, 'read': 11831, 'non': 10187, 'fiction': 5730, 'recent': 11880, 'years': 16336, 'novels': 10257, 'woke': 16076, 'wanna': 15664, 'stay': 13726, 'walked': 15649, 'room': 12329, 'bug': 2714, 'flew': 5860, 'eye': 5518, 'change': 3152, 'name': 9931, 'under': 15224, 'settings': 12805, 'pain': 10689, 'sky': 13181, 'until': 15309, 'awww': 1787, 'wanted': 15669, 'greg': 6661, 'watching': 15719, 'milk': 9482, 'decent': 4318, 'australian': 1715, 'find': 5774, 'trust': 14960, 'american': 1296, 'nerrrvous': 10040, 'yummy': 16464, 'thx': 14586, 'accidents': 937, 'boo': 2428, 'soggy': 13371, 'wished': 16031, 'cockermouth': 3514, '45': 296, 'jesus': 8014, 'christ': 3334, 'sunny': 14028, 'muuch': 9871, 'shall': 12844, 'web': 15772, 'won': 16087, 'let': 8633, 'reads': 11834, 'tweets': 15054, 'confident': 3700, 'she': 12878, 'pretenders': 11422, 'listening': 8746, 'favourite': 5650, 'year': 16333, 'weather': 15767, 'widget': 15964, 'imac': 7583, 'iphone': 7828, 'predict': 11386, 'rainy': 11764, 'season': 12697, 'comes': 3589, 'tuna': 14998, 'noodles': 10194, 'smashed': 13269, 'potato': 11334, 'cakes': 2861, 'winksy': 16014, 'says': 12587, 'dissappointed': 4671, 'past': 10807, 'few': 5716, 'days': 4267, 'st': 13650, 'killing': 8289, 'jean': 7984, 'possible': 11319, 'case': 3015, 'h1n1': 6785, 'ft': 6138, 'knox': 8357, 'ky': 8406, 'tiny': 14644, 'cc': 3070, 'gnyq7': 6466, 'note': 10234, 'info': 7683, 'purposes': 11630, 'panic': 10733, 'jï': 8156, 'bday': 2028, 'beautiful': 2059, 'aunt': 1704, 's2': 12450, 'mess': 9402, 'sensible': 12759, 'airport': 1155, 'wdw': 15758, 'trip': 14925, 'worse': 16158, 'fingered': 5784, 'coke': 3539, 'machine': 9057, 'drinking': 4880, 'diet': 4544, 'pepsi': 10908, 'dr': 4831, 'pepper': 10905, 'used': 15366, 'correction': 3849, 'legal': 8597, 'wooowww': 16116, 'full': 6152, 'idea': 7518, 'found': 6015, 'tinyurl': 14647, 'qlrcec': 11665, 'frenchies': 6076, 'set': 12802, 'precedent': 11383, 'loading': 8797, 'bought': 2507, 'plastic': 11138, 'personal': 10938, 'wine': 16007, 'bottles': 2504, 'jewel': 8018, 'rooftop': 12328, 'boozin': 2467, 'been': 2093, 'beating': 2052, 'irregularly': 7852, 'ever': 5373, 'since': 13103, 'explode': 5484, 'or': 10521, 'implode': 7606, 'yeah': 16325, 'done': 4766, 'several': 12815, 'dumerils': 4948, 'boa': 2385, 'prob': 11467, 'wonder': 16088, 'hognose': 7244, 'needs': 10007, 'bigger': 2212, 'blah': 2287, 'yea': 16319, 'max': 9265, 'trivun': 14936, 'poked': 11227, 'undertanding': 15238, 'none': 10188, 'watch': 15715, 'point': 11219, 'hd': 7014, 'stace': 13653, 'wants': 15672, 'hack': 6797, 'directions': 4598, 'multiple': 9827, 'social': 13351, 'net': 10050, 'sites': 13137, 'add': 992, 'chose': 3328, 'mu': 9804, 'hilarious': 7191, 'mine': 9503, 'gross': 6687, '_vegetable': 829, 'mean': 9306, 'yu': 16451, 'doctor': 4726, 'tuesday': 14990, 'yup': 16467, 'everything': 5395, 'loved': 8935, 'mom': 9636, 'rul': 12407, 'amazing': 1281, 'marks': 9201, 'roommate': 12333, 'era': 5293, 'wontons': 16099, 'sent': 12761, 'abrams': 906, 'surrounded': 14089, 'meee': 9338, 'tooooooo': 14758, 'bored': 2476, 'shinning': 12925, 'ate': 1652, 'chocolate': 3306, 'honour': 7301, 'cheering': 3224, 'talks': 14238, '_dunk': 626, 'partyends': 10789, 'blog': 2342, '1654': 92, 'release': 12019, 'stubb': 13879, 'dude': 4937, 'hax0r': 6999, 'cut': 4111, 'pro': 11466, 'tell': 14357, 'stable': 13652, 'checked': 3212, 'hubster': 7415, 'pass': 10798, 'lazy': 8552, 'caravan': 2959, 'running': 12424, 'clever': 3436, 'seems': 12724, 'call': 2877, 'ur': 15347, 'shattered': 12871, 'tre': 14892, 'hood': 7303, 'claim': 3397, 'thats': 14455, 'atl': 1657, 'theme': 14476, 'which': 15904, 'aint': 1149, 'youtube': 16437, 'vid': 15500, 'posted': 11324, 'lay': 8545, 'swineflu': 14154, 'illegal': 7565, 'law': 8538, 'abiding': 899, 'citizens': 3385, 'didnt': 4536, 'cry': 4038, 'er': 5292, 'tummy': 14995, 'ache': 956, 'taste': 14276, 'enjoy': 5231, '_gaba': 648, 'evrytime': 5408, 'listen': 8742, 'tat': 14280, 'plce': 11157, 'reminded': 12044, 'mojojojo': 9629, 'dexter': 4502, 'lab': 8416, 'stopped': 13800, 'coffee': 3529, 'maryland': 9223, 'rest': 12137, 'area': 1526, 'five': 5828, 'entire': 5260, 'middle': 9455, 'schools': 12627, 'heck': 7071, 'audrey': 1700, 'bestie': 2160, 'loving': 8958, 'may': 9272, 'holiday': 7254, 'foot': 5948, 'gym': 6782, 'study': 13889, 'ooooo': 10478, 'using': 15378, 'version': 15472, 'twidget': 15069, 'mac': 9049, 'heading': 7027, 'downtown': 4825, 'drinks': 4881, 'dancing': 4202, 'goin': 6487, 'brothers': 2655, 'live': 8760, 'rocking': 12297, 'sneezing': 13317, 'frequently': 6081, 'manager': 9143, 'switched': 14159, 'schedule': 12617, 'closing': 3467, 'pre': 11381, 'twitter': 15089, 'formal': 5983, 'invitation': 7811, 'speak': 13525, 'edu2': 5075, 'yaoo': 16297, 'sonnnn': 13416, 'andy': 1335, 'fonzie': 5938, 'gomez': 6497, 'sry': 13643, 'figure': 5743, 'works': 16141, 'finally': 5769, 'setting': 12804, 'switch': 14158, 'care': 2970, 'fans': 5595, 'hero': 7142, 'rocks': 12298, 'socks': 13358, 'cartoons': 3010, 'haven': 6985, 'eating': 5036, 'breakfast': 2572, 'programs': 11510, 'changed': 3153, 'viewing': 15508, 'evening': 5366, 'jay': 7969, 'way': 15746, 'cardinal': 2968, 'fallen': 5568, 'nest': 10049, 'attacked': 1669, 'bird': 2241, 'dog': 4739, 'leaving': 8586, 'everyday': 5386, 'fact': 5542, 'city': 3386, 'earlier': 5002, 'mondays': 9653, 'sleepy': 13216, 'awake': 1746, 'tweetdeck': 15043, 'keeps': 8218, 'crashing': 3938, 'nj': 10151, 'avoiding': 1742, 'nascar': 9952, 'ima': 7582, 'habits': 6796, 'incredibly': 7654, 'sweeeet': 14133, 'brake': 2552, 'nothing': 10240, 'stress': 13846, 'dream': 4858, 'loooong': 8881, 'weekend': 15799, 'inlawing': 7697, 'movies': 9768, 'trek': 14903, 'flick': 5861, 'top': 14763, 'molar': 9630, 'extracted': 5511, 'swollen': 14161, 'numb': 10281, 'toothache': 14761, 'sinus': 13123, 'suppose': 14067, '4th': 333, 'waited': 15631, 'wind': 15998, 'blowing': 2362, 'through': 14563, 'tumbleweed': 14993, 'old': 10408, 'crackerack': 3919, 'couldnt': 3872, 'southend': 13486, '2nd': 211, 'tickets': 14597, 'ill': 7564, 'tour': 14804, 'bg': 2191, 'emo': 5186, 'jesse': 8009, 'mccartney': 9286, 'tends': 14374, 'internet': 7777, 'sweetie': 14142, 'aliens': 1194, 'boobie': 2430, 'craig': 3922, 'rise': 12256, '11': 43, 'bedtime': 2085, 'grown': 6702, 'ups': 15335, 'because': 2071, 'couldn': 3871, 'access': 930, 'properly': 11535, 'shiny': 12926, 'studying': 13891, 'computing': 3668, 'feck': 5669, 'changing': 3157, 'plant': 11130, 'science': 12631, 'confused': 3710, 'girl': 6399, 'sit': 13134, 'double': 4798, 'decker': 4328, 'neck': 9995, 'down': 4809, 'ppl': 11362, 'who': 15927, 'level': 8642, 'above': 905, 'imagine': 7588, '30': 223, 'okay': 10401, 'sane': 12535, 'population': 11288, 'world': 16145, 'strawberry': 13835, 'margaritas': 9181, 'baked': 1878, 'stuffed': 13894, 'shrimp': 13015, 'effects': 5101, 'worth': 16161, 'wear': 15763, 'jacket': 7924, 'cos': 3855, 'notice': 10242, 'mark': 9193, 'shirt': 12934, 'drunk': 4912, 'specifically': 13534, 'i36': 7494, 'tinypic': 14645, 'mwz6uo': 9878, 'jpg': 8103, 'outfit': 10586, 'q63obq': 11655, 'ahhh': 1127, 'soooo': 13427, 'kill': 8283, 'gilbert': 6386, 'sullivan': 13986, 'cheer': 3222, 'th': 14426, 'serenading': 12779, 'expensive': 5471, 'miserable': 9543, 'smile': 13279, 'fed': 5671, 'happily': 6921, 'stupid': 13904, 'mane': 9148, 'um': 15196, 'gave': 6291, 'nope': 10212, 'intended': 7759, 'start': 13699, 'habit': 6795, 'fast': 5624, 'ends': 5216, 'meet': 9343, 'sports': 13603, 'three': 14551, 'legged': 8601, 'race': 11732, 'mr': 9787, 'pettigrew': 10952, 'said': 12488, 'aloud': 1232, 'grandpa': 6612, 'telling': 14361, 'human': 7432, 'bodies': 2399, 'med': 9320, 'hotter': 7365, 'wearing': 15765, 'shorts': 12984, 'snl': 13327, 'justin': 8151, 'timberlake': 14627, 'dreamt': 4864, 'committed': 3616, 'suicide': 13977, 'drip': 4882, 'stand': 13675, 'freaked': 6047, 'testing': 14410, 'dawn': 4261, 'carlingford': 2985, 'lough': 8919, 'place': 11113, 'involves': 7819, 'tho': 14528, 'picture': 11027, 'sucks': 13951, 'lacking': 8426, 'trousers': 14949, 'thunder': 14579, 'thighs': 14502, 'daughtry': 4251, 'upload': 15326, 'pulled': 11602, 'muscle': 9843, 'twitterbugs': 15093, 'loooooooooong': 8884, 'lie': 8663, 'spent': 13558, 'wife': 15969, 'tangled': 14248, 'wheels': 15891, 'daughter': 4249, 'hit': 7213, 'dislocation': 4657, 'fracture': 6024, 'resulted': 12151, 'failure': 5554, 'phone': 10990, 'catch': 3037, 'aww': 1785, 'cant': 2935, 'till': 14625, 'sydney': 14167, 'soooooo': 13429, 'sleeeeepy': 13203, 'already': 1237, 'wow': 16170, 'hurtling': 7467, 'assembling': 1623, 'figuring': 5745, 'alarm': 1168, 'loud': 8916, 'blew': 2324, 'ear': 4998, 'drum': 4907, 'josette': 8083, 'across': 970, 'pond': 11251, 'nowhere': 10262, 'exciting': 5438, 'goddamnit': 6478, 'row': 12371, 'run': 12416, 'corrupted': 3853, 'singing': 13109, 'away': 1753, 'fml': 5900, 'dad': 4161, 'moneyz': 9656, 'gets': 6351, 'finish': 5786, 'welcome': 15829, 'easy': 5030, 'offer': 10365, 'lost': 8910, 'both': 2497, 'hates': 6974, 'sound': 13473, 'played': 11147, 'shaw': 12874, 'computer': 3666, 'bum': 2745, 'ultimate': 15192, 'exicted': 5457, 'radiator': 11743, 'boiled': 2407, '000': 1, 'depressed': 4431, 'boarding': 2389, 'uss': 15380, 'enterprise': 5251, 'warp': 15691, 'speed': 13541, 'ahead': 1124, '4x4s': 364, 'shizz': 12944, '2gether': 199, 'yr': 16445, 'realized': 11850, 'shout': 12998, 'player': 11148, 'hahahah': 6818, 'cooperating': 3822, 'mad': 9063, 'flash': 5850, 'awoke': 1783, '12': 50, 'crazy': 3952, 'nervous': 10044, 'entering': 5250, 'pogue': 11217, 'according': 942, 'yet': 16376, 'seem': 12721, 'tearrrrrrrr': 14315, 'serendipity': 12780, 'hours': 7373, 'lets': 8636, 'happier': 6919, 'sadly': 12476, 'fever': 5714, 'lemonade': 8615, 'hairspray': 6845, 'nutritious': 10293, 'delayed': 4377, 'malibu': 9127, 'doesn': 4736, 'text': 14416, 'silly': 13084, 'ring': 12243, 'hugs': 7426, 'sacramento': 12460, 'continue': 3771, 'server': 12792, '2008': 135, 'r2': 11726, 'unleashed': 15282, 'sigh': 13061, 'window': 16001, 'ladies': 8433, 'suffer': 13963, 'scenario': 12607, 'men': 9375, 'experience': 5473, 'felt': 5694, 'pee': 10875, 'cried': 3986, 'jonathon': 8076, 'ross': 12351, 'audience': 1691, 'anthem': 1397, 'high': 7175, 'dry': 4914, 'blip': 2330, 'fm': 5899, '79fcr': 473, 'actual': 986, 'heartbreak': 7052, 'purple': 11627, 'converse': 3790, 'game': 6244, 'sorrry': 13456, 'jackass': 7922, 'congrats': 3719, 'drink': 4876, 'hahaha': 6817, 'pleased': 11162, 'wit': 16038, 'tha': 14427, 'turnout': 15014, 'common': 3618, 'remember': 12040, 'nic': 10095, 'wouldn': 16169, 'online': 10451, 'clearly': 3432, 'kept': 8233, 'sunburned': 14015, 'volcano': 15572, 'nd': 9984, 'nyt': 10312, 'sux': 14111, 'burned': 2766, 'bit': 2255, 'news': 10074, 'truth': 14962, 'hiding': 7173, 'eyes': 5521, 'paramore': 10756, 'decode': 4332, 'butterfly': 2802, 'dead': 4289, 'late': 8508, 'congratulations': 3724, 'leah': 8568, 'mothers': 9740, 'duuuude': 4972, 'tim': 14626, 'vs': 15594, 'animated': 1354, 'either': 5123, 'adult': 1041, 'swim': 14149, 'g4': 6212, 'neither': 10029, 'suffering': 13965, 'ish': 7858, 'loveee': 8937, 'breakin': 2575, 'laughed': 8524, 'update': 15318, 'crying': 4041, 'spineless': 13573, 'makes': 9115, 'downloaded': 4815, 'parnoid': 10774, 'dnt': 4717, 'cuddle': 4067, 'visiting': 15546, 'friendster': 6102, 'facebook': 5537, 'poorly': 11274, 'ree': 11942, 'heally': 7039, 'hoping': 7329, 'rain': 11756, 'stops': 13802, 'sunburnt': 14017, 'arms': 1551, 'itching': 7879, 'hello': 7112, 'worry': 16156, 'soooooooooo': 13434, 'palm': 10715, 'niggas': 10112, 'talkin': 14236, 'bout': 2516, 'bm': 2381, '745': 467, 'poop': 11269, 'nite': 10148, 'weird': 15817, 'synced': 14178, 'research': 12122, 'guilt': 6747, 'yah': 16287, 'ordered': 10530, 'asus': 1643, 'eee': 5082, 'pc': 10854, 'gon': 6498, 'ignore': 7546, 'txt': 15134, 'unless': 15283, '6th': 458, 'street': 13840, 'wedding': 15786, 'eventful': 5369, 'third': 14514, 'freelesson': 6064, 'freistunde': 6072, 'surprised': 14084, 'choice': 3310, 'project': 11513, 'believe': 2119, 'kind': 8295, 'relationship': 12004, 'thought': 14542, '50': 365, 'sigjeans': 13066, 'fugees': 6150, 'keeping': 8217, 'company': 3633, 'troubles': 14946, 'ain': 1148, 'grandparents': 6613, 'ilocos': 7570, 'moved': 9763, 'nueva': 10274, 'ecija': 5045, 'mate': 9246, 'pills': 11055, 'cuz': 4126, 'nuts': 10294, 'whatever': 15874, 'pung': 11614, 'jk': 8038, 'retarded': 12158, '16th': 94, 'learned': 8574, 'lesson': 8630, 'nyc': 10309, 'huh': 7428, 'nearly': 9992, 'causes': 3054, 'mic': 9441, 'problems': 11475, 'euruko': 5358, 'hold': 7247, 'bottom': 2505, 'brother': 2654, 'knows': 8355, 'haters': 6973, 'taking': 14224, 'sang': 12537, 'xbox': 16245, 'lips': 8730, 'beer': 2094, 'karla': 8183, 'leno': 8619, 'mourning': 9758, 'hon': 7289, 'discriminating': 4637, 'wierd': 15966, 'jus': 8146, 'warm': 15684, 'cozy': 3907, 'inside': 7721, 'nanay': 9938, 'reading': 11833, 'card': 2964, 'hugged': 7420, 'knew': 8340, 'babies': 1818, 'booo': 2448, 'tgif': 14424, 'finger': 5783, 'traffic': 14847, 'jam': 7940, 'bet': 2163, 'fall': 5567, 'asleep': 1617, 'sorries': 13452, 'afford': 1067, 'raining': 11761, 'cats': 3046, 'dogs': 4744, 'mysore': 9892, 'thankfully': 14436, 'pigs': 11046, 'swines': 14155, 'comeback': 3584, 'bob': 2393, 'proctor': 11486, 'advice': 1053, 'freaks': 6050, 'funnnn': 6179, 'talinda': 14231, 'tyler': 15139, 'slept': 13219, 'nephew': 10035, 'yrs': 16446, 'naps': 9946, 'age': 1093, 'concur': 3685, 'check': 3211, 'ones': 10448, 'tod': 14690, 'showed': 13004, 'fluid': 5890, 'decreasing': 4336, 'slightly': 13226, 'doesnt': 4737, 'anytime': 1428, 'ive': 7903, 'uploading': 15328, 'photos': 11000, 'cell': 3095, 'nothin': 10238, 'candid': 2929, 'ness': 10048, 'bitbetter': 2256, 'share': 12857, 'bartender': 1957, 'ly': 9027, 'ba3nf': 1811, 'disappointed': 4615, 'hopes': 7326, 'lovin': 8957, 'itsucks': 7891, 'sensors': 12760, 'vocï': 15563, 'que': 11687, 'sumiu': 13991, 'forever': 5972, 'msn': 9794, 'fiftythousand': 5739, 'hugh': 7423, 'laurie': 8534, 'needle': 10004, 'haystack': 7004, 'whole': 15933, 'somethign': 13400, '200': 129, 'updates': 15320, 'alotment': 1231, 'hangers': 6899, 'closet': 3465, '_ricardo': 784, 'road': 12270, 'indies': 7663, 'rule': 12408, 'curious': 4091, 'benjamin': 2143, 'button': 2806, 'another': 1388, 'block': 2335, 'cross': 4010, 'fro': 6114, 'bloody': 2354, 'boiling': 2409, 'hardcoded': 6933, '650mb': 428, 'brain': 2549, '90s': 515, 'despite': 4469, 'amount': 1315, 'forgive': 5977, 'politicians': 11241, 'pics': 11025, 'lots': 8913, 'stories': 13806, 'goodies': 6510, 'memories': 9371, 'concrete': 3684, 'landfill': 8469, 'recycler': 11922, 'fuckkk': 6145, 'sleepppppppp': 13213, 'mummy': 9831, 'symphonic': 14174, 'pawing': 10839, 'thru': 14571, 'elle': 5152, 'saw': 12580, 'innit': 7702, 'safely': 12482, 'euro': 5354, 'winner': 16015, 'approve': 1495, 'heartedly': 7056, 'priceless': 11433, 'falls': 5574, '1pm': 122, 'replying': 12086, 'sweet': 14137, 'booth': 2463, 'newport': 10073, 'heb': 7070, 'thanxs': 14447, 'mooooooooornin': 9690, 'legs': 8606, 'whenever': 15893, 'rains': 11762, 'motivated': 9745, 'thinks': 14513, '66sbz': 433, 'adorable': 1032, 'ooh': 10468, 'driver': 4886, 'gloomy': 6446, 'sulking': 13985, 'tumblr': 14994, 'gru3some': 6714, 'save': 12573, 'xd51qx88b': 16248, 'beyond': 2182, 'cheap': 3205, 'yoooouuu': 16411, 'confsuing': 3708, 'yearling': 16335, 'pet': 10948, 'died': 4540, 'exhausted': 5451, 'nvr': 10302, 'bidor': 2208, 'wantan': 15668, 'mee': 9337, 'drinkin': 4879, 'cham': 3140, 'ping': 11068, 'puts': 11642, 'stole': 13789, 'occasionally': 10337, 'edward': 5079, 'cullen': 4073, 'obama': 10320, 'mail': 9094, 'ordering': 10531, 'aswered': 1646, 'calls': 2882, 'beatwittyparty': 2056, 'relaxing': 12014, 'uminaa': 15201, 'pink': 11071, 'hardcore': 6934, 'jamie': 7943, 'everyoneeeee': 5391, 'xxxxxxloser': 16269, 'holy': 7269, 'se': 12685, 'beats': 2054, 'heat': 7060, 'guava': 6734, 'juice': 8122, 'handi': 6888, 'water': 15721, 'beach': 2033, 'tubes': 14987, '9th': 536, '10th': 41, 'june': 8137, 'venue': 15464, 'washed': 15700, 'sleeve': 13217, 'keep': 8214, 'tea': 14305, 'alas': 1169, '_o': 760, 'uh': 15182, 'visit': 15544, 'austin': 1712, 'gigantic': 6379, 'moving': 9771, 'decision': 4324, 'closer': 3463, 'louis': 8921, 'portland': 11298, 'register': 11978, 'presale': 11404, 'saturday': 12565, 'mayhem': 9277, 'wake': 15637, 'comm': 3601, 'thanxxx': 14449, 'gods': 6483, 'sake': 12494, 'ik': 7556, 'opted': 10514, 'strike': 13860, '12th': 58, 'altaf': 1248, 'bhai': 2197, 'unveil': 15313, 'happened': 6915, 'karachi': 8179, 'open': 10492, 'experiement': 5472, 'remembered': 12041, 'pentecost': 10898, 'uselessly': 15371, 'complicated': 3656, 'marty': 9217, 'mcflyy': 9293, 'agaaaaaaiiiin': 1086, 'maaaam': 9042, 'chickened': 3264, 'interesting': 7769, 'beauty': 2062, 'massacre': 9233, 'australia': 1714, 'florida': 5880, 'misses': 9555, 'perfectly': 10916, 'goo': 6502, 'twitterverse': 15114, 'entertained': 5253, 'boredd': 2477, 'forrest': 5993, 'baron': 1951, 'fpu': 6022, 'thn': 14522, 'drugs': 4905, 'turn': 15009, 'pumpkin': 11608, 'mornings': 9714, 'exist': 5458, 'weren': 15845, 'hot': 7360, 'faith': 5561, 'picked': 11018, 'taco': 14199, 'guitar': 6752, 'agree': 1107, 'usb': 15364, 'key': 8247, 'fancy': 5588, 'packaging': 10673, 'fire': 5797, 'flames': 5847, 'dragonforce': 4838, '102': 29, 'kiis': 8281, 'ryan': 12445, 'meowmie': 9394, 'money': 9654, 'pay': 10842, 'them': 14475, 'real': 11839, 'seventh': 12814, 'gorilla': 6550, 'pod': 11208, 'lego': 8605, 'knight': 8342, 'helmet': 7117, 'whoishonorsociety': 15932, 'pajama': 10705, 'pants': 10740, 'official': 10374, 'twilight': 15072, 'saga': 12486, 'ended': 5210, 'empty': 5200, 'lulu': 8994, 'slanted': 13192, 'farmer': 5609, 'market': 9196, 'trying': 14967, 'cat': 3034, 'figured': 5744, 'sink': 13116, 'cupboard': 4081, 'target': 14265, 'safety': 12485, 'locks': 8812, 'heavy': 7069, 'part': 10777, 'reason': 11862, 'aladdin': 1167, 'eclipse': 5046, 'mints': 9524, 'soo': 13420, 'sunshine': 14038, 'headaches': 7021, 'cleaned': 3423, 'list': 8740, 'blocked': 2337, 'girls': 6406, 'spammer': 13510, 'di': 4516, 'bella': 2127, 'roasters': 12276, 'asked': 1612, 'job': 8046, 'person': 10937, 'needed': 10002, 'horrifying': 7342, 'skateboard': 13160, 'candy': 2931, 'nvd': 10300, 'prize': 11465, 'concert': 3680, 'ar': 1507, '15': 78, 'qualifies': 11677, 'gun': 6758, '21': 147, 'parts': 10787, 'ake': 1164, 'lazing': 8551, 'congratss': 3721, 'btw': 2688, 'nathanfillion': 9961, 'flight': 5865, 'control': 3783, 'score': 12637, 'each': 4994, 'darn': 4228, 'angry': 1349, 'random': 11777, 'asian': 1609, 'paws': 10841, 'air': 1150, 'sushi': 14098, 'chinese': 3287, 'restaurants': 12141, 'apple': 1474, 'katie': 8192, 'cheery': 3227, 'free': 6055, 'fieldnotes': 5734, 'travel': 14885, 'north': 10220, '3rd': 279, 'own': 10645, 'purchases': 11625, 'yard': 16298, 'sales': 12500, 'large': 8494, 'crates': 3940, 'refrigerator': 11961, '35': 253, 'ment': 9382, 'disappointing': 4616, 'report': 12089, 'updating': 15321, 'supertarget': 14053, 'squeeky': 13634, 'hereeeeee': 7136, 'imma': 7594, 'georgia': 6341, 'la': 8412, 'grrrrrr': 6710, 'shower': 13005, 'brb': 2568, 'mins': 9522, 'awwwwwwww': 1794, 'dried': 4874, 'wats': 15733, 'inning': 7701, 'mum': 9829, 'bacon': 1847, 'smoke': 13288, 'stars': 13695, 'ja': 7917, 'binks': 2234, 'crappy': 3933, 'motivation': 9747, 'revise': 12187, 'odd': 10348, 'mitchel': 9576, 'musso': 9858, 'dosent': 4792, 'pumped': 11606, 'mind': 9498, 'reckon': 11890, 'stop': 13799, 'hair': 6836, 'greying': 6668, 'retire': 12161, 'gill': 6387, 'dark': 4222, '_c': 591, 'taken': 14219, 'twatter': 15028, 'lately': 8509, 'replies': 12083, 'superb': 14044, 'rehearsal': 11990, 'rob': 12278, 'delucca': 4395, 'thingy': 14508, 'month': 9676, 'learning': 8576, 'songs': 13413, 'minute': 9527, 'delicious': 4386, 'losing': 8908, 'neighbors': 10024, 'give': 6416, 'brazil': 2565, 'presentation': 11411, 'shouldve': 12996, 'given': 6420, 'wed': 15784, 'overtime': 10629, 'bgt': 2194, 'awkward': 1781, 'moment': 9637, 'loose': 8895, 'faster': 5625, 'move': 9761, 'feet': 5686, 'correct': 3847, 'misconstrued': 9542, 'smell': 13271, 'pancakes': 10724, 'toast': 14686, 'cooking': 3807, 'awwww': 1789, 'slow': 13242, 'certain': 3113, 'ahhhh': 1129, 'smart': 13264, 'avoid': 1739, 'happening': 6916, 'super': 14043, 'toured': 14805, 'philippines': 10973, 'nashville': 9953, 'employee': 5197, 'forbid': 5957, 'usually': 15385, 'along': 1229, 'commenting': 3609, 'nachos': 9910, 'himself': 7198, 'clear': 3429, 'birds': 2243, 'docks': 4724, 'shopping': 12969, 'recontinue': 11906, 'breathless': 2584, 'scent': 12611, 'huge': 7419, 'hits': 7217, 'bummer': 2749, 'officially': 10375, 'dust': 4966, 'grounded': 6692, 'nicer': 10101, 'drive': 4883, 'careful': 2974, 'blackberry': 2279, 'explain': 5481, 'intrigued': 7791, 'calling': 2881, 'shitshow': 12940, 'lying': 9029, 'looooooooves': 8885, '_fred6': 645, 'guess': 6736, 'option': 10519, 'mhm': 9435, 'om': 10421, 'nom': 10186, 'gutted': 6771, 'beside': 2153, 'pool': 11261, 'spending': 13557, 'lonely': 8849, 'dressed': 4869, 'granny': 6614, 'twitting': 15118, 'familiar': 5579, 'goodnight': 6514, 'hating': 6976, 'prom': 11519, 'p9': 10667, 'danica': 4211, 'team': 14311, 'spongebob': 13593, 'inspirational': 7729, 'gots': 6562, 'beathing': 2049, 'suit': 13978, 'victoia': 15497, 'secret': 12708, 'exchange': 5430, 'bottoms': 2506, 'weeks': 15803, 'revision': 12190, 'biology': 2238, 'chemistry': 3246, 'plenty': 11179, 'gold': 6491, 'willing': 15987, 'jumping': 8132, 'trampoline': 14863, 'thud': 14575, 'shouldn': 12994, 'eaten': 5033, 'cookie': 3805, 'myloc': 9887, '1xiz': 127, 'migawd': 9462, 'surf': 14076, 'anymore': 1420, 'modem': 9618, 'withdrawals': 16044, 'tweeple': 15035, 'recommend': 11899, 'www': 16217, 'audiomicro': 1693, 'tr': 14829, 'gwoy': 6780, 'gwpx': 6781, 'looking': 8872, 'betterrrrrrrr': 2172, 'rocked': 12294, 'strapped': 13829, 'ohio': 10388, 'california': 2876, 'parks': 10771, 'nmr7pc': 10160, 'hi': 7163, 'frenchie': 6075, 'bullet': 2737, 'train': 14856, 'tokyo': 14711, 'gf': 6356, 'japan': 7956, 'thursday': 14584, 'sightseeing': 13065, 'gaijin': 6232, 'bulky': 2735, 'idol': 7531, 'exspecially': 5500, 'mommy': 9644, 'christoph': 3339, 'fridge': 6093, 'those': 14537, 'south': 13485, 'mowing': 9774, 'coco': 3516, 'blame': 2293, 'goose': 6544, 'gotcha': 6559, 'feelin': 5682, 'tron': 14941, 'zone': 16507, 'michelle': 9445, 'luke': 8990, 'quote': 11721, 'goodluck': 6511, 'dreading': 4856, 'trouble': 14945, 'voting': 15586, 'thankgod': 14437, 'crackberry': 3916, 'pray': 11375, 'zac': 16477, 'brown': 2657, 'virginia': 15528, 'nova': 10255, '_gutter': 661, 'trent': 14909, 'wore': 16128, 'bouta': 2517, 'donate': 4763, 'blood': 2353, 'ahhhhh': 1130, 'margs': 9182, 'brightens': 2610, 'lobster': 8803, 'dinner': 4583, 'mba': 9284, 'folks': 5918, 'terasse': 14388, 'pouring': 11346, 'mtl': 9800, 'eat': 5032, 'sandwhich': 12530, 'meeting': 9345, 'brag': 2544, 'itchy': 7880, 'sorrow': 13453, 'linkin': 8721, 'park': 10767, 'spend': 13555, 'student': 13883, 'burbank': 2760, 'fly': 5894, 'dc': 4279, 'enthusiasm': 5258, 'meeeee': 9341, 'annoyed': 1382, 'phew': 10970, 'anyone': 1422, 'runs': 12426, 'issueï': 7871, 'juz': 8154, 'gadgetopia': 6223, 'dm': 4710, 'email': 5165, 'basquash': 1976, '07': 10, 'sengoku': 12750, 'basara': 1960, 'valkyria': 15424, 'chronicles': 3342, 'requiemforthephantom': 12107, 'edenoftheeast': 5062, 'simpsons': 13098, 'nodding': 10171, 'weir': 15816, 'lady': 8435, 'plurk': 11195, 'rq5ru': 12380, 'viral': 15525, 'danny': 4214, 'xoxo': 16257, 'loooooove': 8887, 'revising': 12188, 'exam': 5418, 'website': 15782, 'c7yojg': 2837, 'idk': 7529, 'loopy': 8894, 'indeed': 7655, 'pigment': 11045, 'imagination': 7587, 'ncis': 9982, 'complete': 3650, 'first': 5809, 'jealous': 7981, 'tom': 14716, 'germany': 6344, 'fas': 5617, 'booted': 2462, 'tvs': 15026, 'maaaaan': 9040, 'aswell': 1645, 'joke': 8068, 'surely': 14075, 'bothered': 2499, 'sony': 13419, 'awe': 1756, 'inventing': 7801, 'melatonin': 9356, 'enthu': 5256, '_anshul': 569, 'speaks': 13529, 'asylum': 1649, 'insomnia': 7723, 'kickin': 8269, 'butt': 2798, 'contribute': 3779, 'destruction': 4478, 'language': 8477, 'finishing': 5788, 'magazine': 9076, 'article': 1582, 'cap': 2943, 'almost': 1225, 'sholders': 12958, 'enjoying': 5235, 'duber': 4928, 'merlin': 9400, 'bt': 2685, 'course': 3886, 'es': 5316, 'starwars': 13707, 'rocio': 12292, 'ian': 7497, 'important': 7608, 'polish': 11238, 'nail': 9918, 'handle': 6889, 'murdered': 9841, 'filipino': 5754, 'prof': 11495, 'gorayeb': 6546, 'migranes': 9467, 'practice': 11368, 'pools': 11263, 'threw': 14554, 'normal': 10217, 'boston': 2493, 'daft': 4168, 'recieve': 11886, 'million': 9489, 'invites': 7814, 'derny': 4444, 'sofie': 13365, 'cindy': 3368, 'eric': 5299, 'coloured': 3573, 'leg': 8596, 'filled': 5757, 'colour': 3572, 'truly': 14955, 'integrity': 7755, 'comment': 3605, 'ipod': 7831, 'touch': 14797, 'literally': 8754, 'falling': 5570, 'apart': 1444, 'junk': 8141, 'reluctant': 12033, 'finals': 5770, 'unfollowed': 15249, 'purpose': 11628, 'fishy': 5819, 'bouncing': 2509, 'rush': 12430, 'nauseous': 9973, 'revel': 12182, 'ago': 1106, 'homeless': 7276, 'afterjune': 1077, '1st': 123, 'nighttt': 10126, 'xxo': 16265, 'gurl': 6764, 'face2face': 5536, 'palisades': 10714, 'try': 14964, 'pei': 10884, 'wei': 15811, 'spelt': 13553, 'deadly': 4293, 'pure': 11626, 'escapism': 5319, 'achieve': 958, 'strange': 13823, 'taxi': 14296, 'bits': 2264, 'titanic': 14666, 'couple': 3884, 'iya': 7907, 'starting': 13702, 'krn': 8385, 'mitzy': 9580, 'baru': 1958, 'ber': 2149, 'tweeter': 15046, 'jg': 8022, 'nit': 10146, 'badoptus': 1858, '60k': 419, 'oscar': 10557, 'de': 4288, 'renta': 12066, 'east': 5024, 'village': 15511, 'shops': 12971, 'booked': 2437, 'museum': 9845, 'ooopps': 10480, '10am': 36, 'shizzle': 12946, 'gear': 6301, 'hospital': 7354, 'studio': 13888, 'x2wsw': 16237, 'desktop': 4463, 'mouse': 9759, 'ketboard': 8239, 'ot': 10562, 'second': 12706, 'monitor': 9659, 'completely': 3652, 'wii': 15973, 'vacuum': 15419, 'currently': 4098, 'posts': 11332, 'business': 2786, 'venturefile': 15463, 'alright': 1240, 'salad': 12495, 'swag': 14117, 'chili': 3275, 'minneapolis': 9516, 'refuse': 11967, 'masala': 9225, 'chaas': 3124, 'london': 8847, 'tourist': 14807, 'managed': 9141, 'purchase': 11624, 'ovi': 10635, 'constant': 3743, 'wasup': 15713, 'givin': 6423, 'lil': 8695, 'odee': 10349, 'qot': 11669, 'some1': 13393, 'heard': 7046, 'lonq': 8866, 'crawling': 3946, 'wrist': 16189, 'massive': 9238, 'headache': 7020, 'blown': 2363, 'ahaha': 1118, 'dunno': 4958, 'freakin': 6048, 'parents': 10765, 'island': 7861, 'flowers': 5885, 'pouch': 11340, 'stealth': 13738, 'present': 11410, 'mall': 9129, 'hide': 7171, 'churro': 3350, 'ut': 15387, 'coolness': 3814, 'conversations': 3789, 'ybd0': 16315, 'thunderstorm': 14581, 'track': 14833, 'licnse': 8662, 'plates': 11140, 'renewed': 12061, 'mechanic': 9319, 'inspections': 7727, 'town': 14821, 'promise': 11522, 'broiler': 2639, 'burger': 2762, 'king': 8303, 'burgers': 2763, 'chicken': 3263, 'fries': 6103, 'argentina': 1532, 'otra': 10570, 'vez': 15480, 'tshwane': 14974, 'rates': 11797, 'service': 12793, 'endless': 5212, 'loop': 8892, 'fail': 5547, 'spammers': 13511, 'anychance': 1418, 'matthew': 9262, 'sarah': 12549, 'ghosts': 6367, 'girlfriends': 6402, 'brought': 2656, 'states': 13715, 'learn': 8573, 'talking': 14237, 'library': 8657, 'nevermind': 10063, 'twitteraddict': 15090, 'likely': 8690, 'canada': 2913, 'nikkie': 10132, 'payne': 10850, 'pervs': 10946, 'teens': 14336, 'brand': 2556, 'bo': 2384, 'alcohol': 1174, 'band': 1910, 'idiot': 7526, 'leisure': 8612, 'bay': 2006, 'uploaded': 15327, 'grrr': 6705, 'games': 6249, 'wishes': 16032, 'sowwie': 13492, 'paint': 10694, 'chin': 3285, 'convo': 3797, 'victim': 15495, 'smells': 13274, 'gorgeous': 6549, 'everybody': 5384, 'indo': 7666, 'thai': 14431, 'traditional': 14844, 'clothes': 3471, 'object': 10323, 'wat': 15714, 'arun': 1592, 'rele': 12017, 'lfpa': 8649, 'cop': 3825, 'theater': 14462, 'drawning': 4850, 'emails': 5167, 'input': 7709, 'rough': 12356, 'sounded': 13474, 'agitated': 1105, 'knowledge': 8353, 'neuroanatomy': 10060, 'special': 13532, 'icarly': 7506, 'damage': 4177, 'guides': 6744, 'many': 9164, 'lifesaver': 8673, 'cud': 4065, 'entertainment': 5255, 'romania': 12321, 'mtv': 9803, 'sunday': 14019, 'motherssss': 9742, 'authority': 1716, '2day': 193, 'stranger': 13825, 'beblessed': 2066, 'names': 9935, 'ff': 5719, 'feline': 5688, 'anyways': 1430, 'pictures': 11028, 'caturday': 3049, 'earth': 5016, 'led': 8590, 'bittt': 2266, 'wendy': 15840, 'french': 6074, 'begins': 2103, 'bouquets': 2513, 'em': 5164, 'arranged': 1562, 'momma': 9640, 'proud': 11547, 'shoutout': 13000, 'moms': 9648, 'appreciate': 1488, 'cares': 2977, 'ohhhh': 10386, 'plain': 11120, 'cheers': 3226, 'downloading': 4818, 'tf2': 14422, 'mere': 9398, '3g': 269, 'instantly': 7740, '500kb': 368, 'brighter': 2611, 'sandwiched': 12532, 'between': 2175, 'ham': 6867, 'months': 9678, 'ta': 14190, 'wking': 16061, 'wkend': 16060, 'funniest': 6175, 'blow': 2360, 'harder': 6937, 'thankyou': 14445, 'log': 8818, 'details': 4481, 'adam': 991, 'lambert': 8453, 'count': 3874, 'sheep': 12885, 'fairly': 5559, 'flat': 5853, 'route': 12365, 'available': 1726, 'fully': 6154, 'functional': 6158, 'bike': 2220, 'yikes': 16389, 'false': 5575, 'jst': 8109, 'scratch': 12651, 'wht': 15950, 'nonsense': 10191, 'perfs': 10922, 'reyah': 12197, 'fellow': 5692, 'homegirl': 7275, 'situations': 13145, 'push': 11634, 'happens': 6918, '_jayy': 697, 'history': 7212, 'essay': 5327, 'housemate': 7376, 'angel': 1339, 'demons': 4405, 'reminise': 12050, 'qfsag': 11659, 'shots': 12988, 'espresso': 5324, 'aaaaand': 861, 'harmless': 6949, '_vip': 831, 'bye': 2825, 'beard': 2040, 'artwork': 1590, '3am': 266, 'portfolio': 11297, 'meca': 9318, 'wednesday': 15788, 'milonzzi': 9494, 'review': 12185, 'ball': 1894, 'coached': 3503, 'youngster': 16422, 'fundamentals': 6161, 'steak': 13733, 'understands': 15237, 'cabin': 2843, 'stocked': 13785, '140': 72, 'pages': 10686, 'serious': 12785, 'react': 11827, 'worst': 16160, 'date': 4243, 'miles': 9478, 'bew': 2178, 'gtalk': 6726, 'pls': 11187, '600': 415, 'ruth': 12440, 'paid': 10688, 'bills': 2229, 'electricity': 5132, 'liquour': 8736, 'knife': 8341, 'straight': 13816, 'curse': 4099, 'women': 16084, 'unavailable': 15212, 'until10': 15310, 'reinstalled': 11997, 'ubuntu': 15159, 'laptop': 8489, 'ext4': 5501, 'filesystem': 5753, 'system': 14184, 'boot': 2459, 'grub': 6715, 'operability': 10499, 'seconds': 12707, 'shudve': 13023, 'told': 14712, 'happen': 6913, 'nigh': 10116, 'lovess': 8952, 'bliss': 2331, 'jpeg': 8102, 'format': 5984, 'board': 2388, 'terminology': 14394, 'related': 12003, 'twitters': 15111, 'cuba': 4059, 'gona': 6499, 'headin': 7026, 'chilis': 3276, 'presidente': 11416, 'callin': 2880, 'ice': 7507, 'wonderful': 16090, 'yeaup': 16339, 'allowed': 1218, 'nap': 9943, 'teachers': 14308, 'johnathan': 8059, 'marly': 9204, 'meds': 9336, 'addicted': 994, 'disspointed': 4673, 'tonked': 14743, '40': 285, 'overs': 10624, 'war': 15674, 'begin': 2099, 'fiuuhh': 5827, 'bath': 1987, 'twits': 15085, 'lili': 8696, 'homie': 7287, 'black': 2277, '75': 468, 'exact': 5415, 'empanadas': 5190, 'ending': 5211, 'antony': 1408, 'johnsons': 8062, 'woohoo': 16106, 'wreck': 16180, 'nutella': 10291, 'mochi': 9610, 'waste': 15710, 'sunburn': 14014, 'knees': 8339, 'yield': 16386, 'webkinz': 15780, 'club': 3479, 'horse': 7345, 'called': 2878, 'dollar': 4751, 'ded': 4338, 'eeeeeee': 5085, 'binder': 2232, 'pa': 10668, 'starts': 13703, 'briefing': 2607, 'coast': 3505, 'acct': 946, 'mornin': 9711, 'decided': 4321, 'whateverworks': 15875, 'buisiness': 2730, 'fridays': 6092, 'algebra': 1186, 'cereal': 3109, 'toys': 14824, 'thinkin': 14511, 'buyin': 2813, 'kinds': 8302, 'mo': 9602, 'golf': 6494, 'vp': 15591, 'fight': 5740, 'argh': 1534, '_d': 609, 'zwarte': 16520, 'maillot': 9097, 'enjoyed': 5234, '_marguerite': 734, 'dianne': 4524, 'claudia': 3416, 'tweetville': 15060, 'active': 978, 'abbie': 891, 'round': 12359, 'things': 14507, 'podcast': 11210, 'listenening': 8744, 'def': 4348, 'peolple': 10899, 'wallace': 15654, 'gromit': 6683, 'bbc1': 2013, 'chevy': 3253, 'dealership': 4297, 'utah': 15388, 'van': 15428, 'fr': 6023, 'angrrry': 1348, 'sportsmens': 13604, 'warehouse': 15682, 'field': 5733, 'points': 11224, 'arrows': 1574, 'noo': 10192, 'zoo': 16509, 'ru': 12391, '2night': 213, 'skype': 13184, 'ssshh': 13647, 'playlist': 11152, 'leaning': 8572, 'toward': 14816, 'serani': 12776, 'detalis': 4482, 'gruesome': 6717, 'pom': 11250, 'chi': 3257, '4wdgr': 347, 'blonde': 2351, 'girly': 6407, 'hella': 7108, 'sausage': 12572, 'mcmuffin': 9298, 'gt': 6724, 'pill': 11052, 'display': 4664, 'functioning': 6159, 'alzheimer': 1262, 'patients': 10821, 'noise': 10181, 'greenville': 6657, 'sport': 13601, 'charlotte': 3188, '_lisa': 723, 'rip': 12250, 'gaming': 6252, '13': 59, 'session': 12800, 'absolutely': 911, 'badu': 1859, '_peanut_': 769, 'liked': 8689, 'commute': 3624, 'pmsl': 11202, 'everywhere': 5397, 'wool': 16108, 'stash': 13709, '_josa': 701, '_diaz': 618, 'stores': 13805, 'sat': 12556, 'church': 3348, 'sing': 13105, 'choir': 3311, 'services': 12795, 'lacey': 8424, 'granulation': 6618, 'omgggg': 10433, 'hawaii': 6993, 'future': 6196, 'queens': 11690, 'kendra': 8228, 'cancel': 2920, 'rumbo': 12413, 'bachilleres': 1830, 'sunstroke': 14040, 'kickable': 8266, 'starbucks': 13687, 'discount': 4629, 'code': 3522, 'youstinkatrespondingtotexts': 16434, 'trailers': 14854, 'hannah': 6907, 'montana': 9671, 'trailer': 14853, 'grease': 6639, 'conaway': 3671, 'guy': 6775, 'kenickie': 8229, 'celebrity': 3091, 'rehab': 11989, 'apologies': 1456, 'fix': 5829, 'feelings': 5684, 'kev': 8242, 'issue': 7869, 'oxm': 10656, 'link': 8718, 'wholeheartedly': 15934, 'asprin': 1619, 'excellent': 5425, 'vote': 15583, 'votes': 15585, 'category': 3043, 'bee': 2086, 'keemie': 8212, 'guus': 6773, '2moz': 208, 'sooooooooo': 13433, 'wh': 15855, 'beginnings': 2102, 'grabe': 6583, 'norms': 10219, 'researched': 12123, 'dami': 4179, 'pala': 10709, 'flamenco': 5846, 'forms': 5987, 'sub': 13914, 'kaloka': 8170, 'attempted': 1673, 'solea': 13379, 'ang': 1337, 'rockin': 12296, 'river': 12260, 'waitin': 15632, 'boy': 2528, 'app': 1460, 'development': 4496, 'easywriter': 5031, 'electro': 5133, 'bahah': 1866, 'gareths': 6275, 'abroad': 908, 'niamh': 10093, 'coming': 3599, 'gareth': 6274, 'frown': 6123, 'aussie': 1710, 'muah': 9806, 'din': 4577, 'amy': 1321, 'lori': 8902, 'pricey': 11435, 'dress': 4868, 'pissed': 11088, 'twitterena': 15095, 'boiler': 2408, 'tubas': 14985, 'upsets': 15337, 'becasue': 2070, 'imm': 7593, 'uset': 15377, 'happiest': 6920, 'itl': 7885, '_m': 731, 'nature': 9969, 'hockey': 7238, 'ideas': 7520, 'places': 11117, 'member': 9365, 'sooooo': 13428, 'writers': 16192, 'foisting': 5912, 'pipe': 11081, 'dreams': 4863, 'onto': 10461, 'typically': 15145, 'bristol': 2620, '_shutupandsmile': 797, 'ow': 10638, 'avatar': 1729, 'tear': 14313, 'co': 3501, 'workers': 16133, 'bars': 1956, 'britain': 2622, 'shabby': 12831, 'nerdfriends': 10038, 'rochester': 12291, 'sunbathin': 14011, 'clicked': 3439, 'woops': 16118, 'dayem': 4264, 'twpp': 15127, 'coughing': 3869, 'impossible': 7613, 'herts': 7149, 'cheerios': 3225, 'scones': 12634, 'saying': 12585, 'goodbye': 6506, 'fantastically': 5601, 'drives': 4888, 'others': 10567, 'distracted': 4678, 'easier': 5020, 'doc': 4722, 'douchenozzle': 4804, '_hawt': 667, 'fitness': 5825, 'regime': 11975, 'hehehehe': 7093, 'wishing': 16036, 'realize': 11849, 'fac': 5534, 'resist': 12128, '__': 552, 'unfair': 15246, 'wa': 15604, 'graduation': 6597, 'cp': 3908, 'gl': 6426, 'awsome': 1784, 'youre': 16425, 'mis': 9536, 'users': 15375, 'google': 6522, 'chrome': 3341, 'teaching': 14310, 'messing': 9408, 'gaah': 6217, 'fish': 5814, 'fingers': 5785, 'croquettes': 4009, 'beans': 2038, 'tend': 14373, 'namaskar': 9928, 'namaste': 9929, 'marathi': 9169, 'naaaah': 9907, 'thro': 14558, 'mama': 9131, 'kfc': 8258, 'lake': 8446, 'ahahaha': 1120, 'deal': 4295, 'snuff': 13336, 'hermione': 7140, 'action': 976, 'm8zfx': 9037, 'pole': 11234, 'stunningly': 13903, 'piece': 11031, 'puffffy': 11588, 'leavinggggggg': 8587, 'nooooooooo': 10205, '105': 33, '18': 100, '24': 165, 'dk': 4708, 'profiles': 11499, 'range': 11781, 'roots': 12339, 'pinic': 11070, 'mcdonalds': 9289, 'smiles': 13281, 'slush': 13253, 'six': 13146, 'flags': 5841, 'wicked': 15960, '17': 96, 'popular': 11285, 'vaca': 15412, 'cayman': 3065, 'islands': 7862, 'cnt': 3499, 'iloveyou': 7575, 'fresh': 6082, 'blanco': 2295, '100': 23, 'puerto': 11587, 'rican': 12215, 'mami': 9133, 'muy': 9872, 'amor': 1312, 'yi': 16385, 'tu': 14984, 'crushed': 4031, 'upset': 15336, 'soothing': 13440, 'nursing': 10290, 'etc': 5340, 'nights': 10123, 'request': 12102, 'guests': 6742, 'apartment': 1445, 'downfall': 4812, 'upgraded': 15323, 'spotify': 13608, 'premium': 11398, 'exceeded': 5423, 'threshold': 14553, 'awesomeness': 1769, 'itunes': 7895, 'bipolar': 2240, 'lame': 8455, 'holding': 7249, 'candic': 2928, 'soft': 13366, 'sighh': 13062, 'walks': 15652, '_diva': 621, 'tracks': 14838, 'dancefloor': 4198, 'thanxx': 14448, 'iï': 7915, '½ï': 16555, '½m': 16537, '_emily': 629, 'carl': 2981, '10yr': 42, 'slowest': 13245, 'starving': 13706, 'interessanter': 7767, 'issues': 7870, 'tough': 14802, 'number': 10282, 'rung': 12420, 'wales': 15646, 'court': 3890, 'juss': 8147, 'lavender': 8537, 'camera': 2901, 'celebreting': 3089, 'aweso': 1761, 'sufferin': 13964, 'personally': 10941, 'decide': 4320, 'election': 5130, 'north40': 10221, 'reagan': 11838, '08': 14, 'tjefferson': 14671, 'troosevelt': 14942, 'hott': 7364, 'africa': 1073, 'woo': 16100, 'finding': 5776, 'nemo': 10033, 'bws': 2823, 'scents': 12612, 'learnt': 8577, 'manual': 9162, 'enj': 5230, 'besides': 2154, 'cleaning': 3426, 'packing': 10676, 'msg': 9792, 'comedy': 3586, 'central': 3103, 'endure': 5217, 'brief': 2606, 'scrubs': 12678, 'steps': 13750, 'checking': 3214, 'hearing': 7048, 'colleagues': 3558, 'lists': 8750, 'samsam': 12519, 'sexy': 12823, 'crooked': 4006, 'body': 2400, 'drained': 4840, 'classic': 3410, 'jt': 8110, 'willkommen': 15988, 'twitterland': 15104, 'flu': 5888, 'cough': 3868, 'promises': 11524, 'anywhere': 1433, 'snapppp': 13311, 'lollll': 8838, 'fool': 5943, 'tesco': 14406, 'nearby': 9990, 'lj': 8776, 'sware': 14120, 'en': 5202, 'mady': 9074, 'truck': 14952, 'palin': 10712, 'haircut': 6838, 'grey': 6667, 'surrender': 14088, 'd5mjyj': 4151, 'scan': 12592, 'photo': 10995, 'shop': 12966, 'holly': 7263, 'gabbie': 6220, 'haahaaa': 6792, 'jellybeaniesss': 7994, 'comfortable': 3592, 'tweeting': 15053, '5rylt': 404, 'stage': 13660, 'theem': 14468, 'dangerous': 4207, 'avid': 1736, 'main': 9098, 'lack': 8425, 'folk': 5916, 'nina': 10136, 'lastnight': 8505, 'answerr': 1394, 'destrey': 4473, 'videos': 15502, 'bound': 2510, 'stressful': 13850, 'wrote': 16200, 'cmf': 3494, 'ads': 1037, 'march': 9173, 'deleted': 4382, 'wtf': 16204, 'arrived': 1570, '150': 79, 'fellows': 5693, 'pin': 11059, 'hole': 7251, 'todaaaaay': 14692, 'film': 5762, 'cw8wp2': 4129, 'utter': 15393, 'major': 9107, 'texts': 14421, 'citibank': 3383, 'grannys': 6615, 'burp': 2773, 'frog': 6115, 'banged': 1918, 'chips': 3297, 'sale': 12499, 'low': 8963, 'budget': 2705, 'video': 15501, 'annoying': 1383, 'hands': 6892, 'headed': 7025, 'search': 12692, 'c4237j': 2835, 'montday': 9673, 'award': 1747, 'ceremony': 3110, '4jkvl': 327, 'earning': 5010, 'cash': 3017, 'coboyf': 3510, 'paypal': 10851, 'cashouts': 3022, 'minimum': 9512, 'companion': 3632, 'bff': 2186, 'alreadyyyy': 1238, 'realy': 11858, 'sounding': 13476, 'lurgy': 9010, 'rope': 12340, 'bro': 2634, 'proudly': 11548, 'representing': 12097, 'father': 5632, 'badly': 1855, 'gr8': 6578, 'airbrushed': 1151, 'messed': 9405, 'eerrrr': 5096, 'seen': 12725, 'bcoz': 2025, 'george': 6339, 'however': 7385, 'loveeee': 8938, 'dcd': 4281, 'seniors': 12752, 'character': 3173, 'designs': 4459, 'sneak': 13313, 'peak': 10865, 'oooh': 10471, 'regular': 11988, 'wave': 15737, 'exactly': 5416, 'northern': 10223, 'emirates': 5181, 'richter': 12221, 'lotsa': 8914, 'seriously': 12786, 'motherï': 9743, '½s': 16546, 'yyyyuck': 16474, '_autism': 575, 'bright': 2609, 'refuses': 11969, 'wires': 16024, 'sort': 13460, 'graph': 6621, 'botega': 2496, 'branded': 2557, 'public': 11580, 'menace': 9376, 'state': 13712, 'paychecks': 10845, 'fair': 5556, 'worried': 16154, 'printer': 11452, 'newest': 10070, 'glade': 6429, 'debian': 4308, 'onoir': 10457, 'montreal': 9680, 'anxiety': 1412, 'denmark': 4410, 'yukky': 16455, 'collecting': 3560, 'tacos': 14200, 'pasadena': 10796, 'huaaahhhhh': 7410, 'jupaa': 8142, 'resaaaa': 12115, 'awas': 1752, 'kaliaaannn': 8169, 'buttfuck': 2804, 'yoooooooou': 16410, 'task': 14271, 'concern': 3677, 'quotess': 11723, 'afraid': 1071, 'comments': 3610, 'mp': 9781, 'expenses': 5470, 'hopelessly': 7325, 'average': 1733, 'salary': 12498, 'insensitive': 7717, 'bag': 1861, 'bathrooms': 1990, 'cent': 3101, 'mmmm': 9593, 'granada': 6602, 'bowl': 2520, 'crew': 3982, 'bowling': 2522, 'fisch': 5812, '4wn29': 356, 'gm': 6455, 'bankruptcy': 1926, 'loss': 8909, 'offset': 10378, 'gains': 6237, 'tax': 14294, 'wise': 16028, 'lates': 8513, 'pfff': 10954, 'sucksss': 13953, 'myrtle': 9890, 'sc': 12589, 'woeiwoeiwoei': 16074, 'yep': 16357, 'fic': 5728, 'unopened': 15292, 'pack': 10670, 'goat': 6470, 'starin': 13692, 'til': 14621, 'abt': 915, 'cr': 3912, 'chan': 3146, 'sentiments': 12765, 'sophomore': 13442, 'junior': 8139, 'bringing': 2617, 'esgiq': 5320, 'stayin': 13728, '630': 422, '100th': 27, 'matters': 9261, 'anybody': 1417, 'win7': 15997, 'sp2': 13501, 'haiiii': 6833, 'sankq': 12541, 'fineee': 5780, 'js': 8108, 'checkup': 3216, 'rib': 12212, 'fakin': 5565, 'kisha': 8313, 'dood': 4778, 'locked': 8810, 'vimeo': 15514, 'signup': 13075, 'dojo': 4748, 'workshop': 16143, 'munich': 9838, 'ct83ub': 4057, 'hurry': 7461, 'nico': 10108, 'landed': 8468, 'addy': 1008, 'fwd': 6204, 'stat': 13711, 'priority': 11456, 'praying': 11378, 'humble': 7434, 'gives': 6422, 'grace': 6585, 'onscreen': 10459, 'keyboard': 8248, 'dammit': 4183, 'hahahahahahhah': 6827, 'gratiss': 6628, 'milo': 9493, 'saddest': 12470, 'bother': 2498, 'ermm': 5307, 'upper': 15331, 'completley': 3654, 'flo': 5870, 'rida': 12227, '_erincharde': 634, 'afterthought': 1081, 'spurs': 13625, 'champs': 3145, 'kitty': 8324, 'sling': 13227, 'hanging': 6901, 'killed': 8284, 'horde': 7335, 'gears': 6302, 'sittin': 13140, 'lb': 8555, 'dummy': 4949, 'drag': 4833, 'feb': 5668, '09': 16, '56pm': 384, 'gmt': 6458, '805': 485, 'including': 7642, 'somewhere': 13407, 'twice': 15068, 'outvoted': 10602, 'screwed': 12670, 'san': 12520, 'diego': 4541, 'whether': 15900, 'holes': 7252, 'shoes': 12955, 'harley': 6945, 'december': 4316, 'onoger': 10456, 'providence': 11555, 'handy': 6896, 'joy': 8096, 'serial': 12783, 'fraudster': 6043, 'retriever': 12167, 'living': 8773, 'keen': 8213, 'ears': 5015, 'ashley': 1605, 'wee': 15789, 'cafe': 2848, 'latteeeeeeeeeee': 8519, '_gm': 654, 'plan': 11121, 'vicious': 15493, 'circl': 3377, 'dramatic': 4844, 'wassup': 15708, 'dvd': 4974, 'dawson': 4262, 'creek': 3971, 'trash': 14881, 'repops': 12088, 'michigan': 9446, 'hahha': 6829, 'leacing': 8560, 'wi': 15959, 'envy': 5270, 'instant': 7739, 'marketing': 9197, 'empire': 5193, 'bonus': 2427, 'recoup': 11914, 'investment': 7808, 'less': 8629, 'vur': 15598, 'megainternetwealth': 9349, 'megaredpacket': 9351, 'stoked': 13788, 'paisley': 10704, 'cone': 3694, '_y_tony': 846, 'pad': 10680, 'ragoons': 11750, 'yumb': 16459, 'david': 4255, '_from_hell': 646, 'difficult': 4555, 'thankies': 14438, 'scotland': 12642, 'lived': 8763, 'europe': 5356, 'cranking': 3932, 'spreadsheets': 13615, 'postieeee': 11327, 'andshehopes': 1333, 'blogspot': 2347, '2009': 136, '05': 7, 'kewpie': 8246, 'html': 7404, 'shining': 12923, 'valid': 15423, 'excuse': 5445, 'forth': 5999, 'kidding': 8275, 'quoting': 11724, 'delight': 4387, 'amber': 1287, 'backround': 1842, 'design': 4453, 'scroll': 12675, 'bottem': 2502, 'click': 3437, 'image': 7584, 'slapton': 13194, 'borin': 2484, 'woman': 16083, 'dar': 4218, 'iloveyoumoreeee': 7576, 'hellotxt': 7115, 'booming': 2446, 'hoes': 7241, 'rarely': 11791, 'dye': 4979, 'aaaagggessss': 863, 'todays': 14694, 'deucie': 4489, 'sitter': 13139, 'fab': 5530, 'beautifuul': 2061, 'planning': 11128, 'hgtv': 7160, '236am': 161, 'decor': 4334, 'amazi': 1279, '_pe': 768, 'phillip': 10975, 'hasn': 6963, 'scored': 12638, 'mojito': 9627, 'unhappy': 15261, 'ambulance': 1289, 'temporary': 14369, 'address': 1004, 'm8': 9036, 'domain': 4757, 'isss': 7868, 'boredddd': 2478, 'morningg': 9713, 'goodness': 6513, 'create': 3960, 'tons': 14744, 'drama': 4842, 'babes': 1817, 'isnt': 7866, 'mikey': 9472, 'cousin': 3891, 'caught': 3051, 'bouquet': 2512, 'geesh': 6310, 'nside': 10267, 'joint': 8067, 'bf': 2183, 'confuzzled': 3715, 'shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit': 12916, 'explore': 5486, 'snake': 13305, 'garden': 6269, 'imposible': 7612, 'moodle': 9683, 'intro': 7793, 'training': 14859, 'bookings': 2440, 'squashed': 13632, 'hq': 7392, 'scoundrels': 12645, 'aching': 961, 'honestly': 7291, 'hug': 7418, 'consider': 3737, 'yultron': 16457, 'meanest': 9307, 'extinction': 5508, 'frogs': 6116, 'portsmouth': 11302, 'terrified': 14403, 'pensioners': 10897, 'expect': 5465, 'suprises': 14071, 'screw': 12669, 'solutions': 13385, 'badmicrosoft': 1856, 'hopefuly': 7323, 'sisters': 13133, 'timeline': 14633, 'loveliness': 8946, 'wondering': 16094, 'colossal': 3571, 'spiritualsmarts': 13578, '1560': 81, 'published': 11583, 'digest': 4559, 'weekly': 15802, 'virge': 15526, 'dying': 4983, 'obvious': 10331, 'aidan': 1141, 'judges': 8115, 'fourth': 6019, 'spelled': 13549, 'metallica': 9413, 'amused': 1319, 'reviews': 12186, 'canadian': 2914, 'incident': 7636, 'near': 9989, 'relying': 12034, 'lacks': 8428, 'reception': 11882, 'giving': 6424, 'rad': 11742, 'michele': 9444, 'teehee': 14332, 'halo': 6860, 'eww': 5411, 'atm': 1663, 'roast': 12274, 'unappealing': 15211, 'tattoo': 14288, 'owwwwwww': 10654, 'download': 4814, 'rofl': 12306, 'kristin': 8382, 'bone': 2421, 'join': 8064, 'draw': 4847, 'neuro': 10059, 'fxs3l': 6206, 'ton': 14733, 'homework': 7285, '4wrrp': 359, 'pg': 10959, '237': 162, 'laying': 8548, 'wash': 15699, 'mommas': 9641, 'callum': 2883, 'someday': 13396, 'salivary': 12502, 'gland': 6434, 'frm': 6113, 'flies': 5864, 'alison': 1196, 'queen': 11689, 'growing': 6700, 'ashleigh': 1603, 'teacher': 14307, 'myspace': 9894, 'janedurkin': 7948, 'tease': 14317, 'lessons': 8631, 'patiently': 10820, 'mega': 9348, 'redpacket': 11936, 'epic': 5278, 'lappytop': 8488, 'baterrry': 1986, 'construction': 3747, 'smh': 13278, 'lo': 8792, 'hence': 7126, 'smiley': 13282, 'hong': 7296, 'monsters': 9668, 'regret': 11984, 'boooo': 2450, 'strained': 13820, 'cup': 4080, 'yfrog': 16380, 'ehhmyj': 5115, 'success': 13937, 'sooooooooooooooooo': 13436, 'bs': 2681, '4real': 331, 'deadass': 4290, 'herrreeeee': 7145, 'lisa': 8738, 'usa': 15362, 'oowweee': 10486, 'china': 3286, 'wuz': 16211, 'poppin': 11281, 'lipstic': 8731, 'pumps': 11609, 'effect': 5099, 'met': 9410, 'shortest': 12982, 'line': 8711, 'fcb': 5656, 'tunapuna': 14999, 'tellers': 14359, 'disappeared': 4612, '14': 71, 'whoop': 15944, 'daaaaang': 4155, 'cobra': 3511, 'cam': 2892, 'dragged': 4834, 'craving': 3941, 'satisfy': 12562, 'meant': 9313, 'sentimental': 12764, 'ental': 5247, 'fren': 6073, 'lorraine': 8903, 'climb': 3445, 'shoulder': 12992, 'burnt': 2772, 'hangover': 6902, 'ol': 10407, 'english': 5227, 'fry': 6136, 'wives': 16056, 'tips': 14651, 'cya': 4133, 'warning': 15689, 'signal': 13069, 'lyndon': 9031, '4pm': 330, 'via': 15485, 'tasmania': 14275, 'wweeeeooo': 16216, 'downhill': 4813, 'welcomes': 15832, 'christian': 3336, 'grimmy': 6678, 'cars': 3006, 'deed': 4342, 'deserves': 4452, 'somatic': 13390, 'drawing': 4848, 'lanham': 8480, 'leopard': 8625, 'breeding': 2587, 'ground': 6690, 'montanna': 9672, 'shes': 12904, 'lonelyyyy': 8850, 'oooo': 10474, 'ughh': 15173, '2stop': 218, 'killin': 8288, 'ra': 11729, 'neighbor': 10022, 'paved': 10837, 'forced': 5960, 'zeke': 16489, 'astroturf': 1642, 'drain': 4839, 'spray': 13612, 'green': 6652, 'dreadful': 4854, 'fruits': 6129, 'visible': 15542, 'magically': 9082, 'ry': 12443, 'bebo': 2067, 'putting': 11643, 'edinburgh': 5065, 'haggis': 6810, 'hes': 7150, 'xox': 16256, 'slower': 13244, 'mexico': 9426, 'aeropuerto': 1059, 'shocked': 12952, 'departed': 4421, 'relaxin': 12013, 'looooong': 8882, 'bhr': 2198, 'hosts': 7359, 'rqpl7': 12382, 'small': 13261, 'doses': 4793, 'commercial': 3611, 'shoot': 12964, 'dirt': 4603, 'vacations': 15416, 'woken': 16077, 'targeted': 14266, 'uses': 15376, 'ie6': 7536, 'core': 3837, 'default': 4349, 'trend': 14906, 'ruilen': 12403, 'panthers': 10738, 'saints': 12492, 'crossed': 4012, 'qld': 11663, 'teams': 14312, 'math': 9251, 'lunchhhhhhhh': 9003, 'fifteen': 5736, 'awwwwweeee': 1791, 'rotate': 12353, 'outsider': 10595, 'pub': 11579, 'likey': 8693, 'grateful': 6626, 'differe': 4547, 'role': 12310, 'suggestion': 13975, 'convince': 3795, 'aweful': 1759, 'taltal': 14243, 'aol': 1437, '96': 527, 'added': 993, 'coincidence': 3536, 'tal': 14225, 'chocked': 3305, 'arm': 1547, 'ahh': 1125, 'amounts': 1316, 'msgs': 9793, 'ack': 965, 'order': 10529, 'press': 11417, '107': 34, 'no0o0o0o': 10162, 'waldo': 15645, 'ckc': 3392, 'chilling': 3283, 'accident': 933, 'pr': 11366, 'pacific': 10669, 'tomorow': 14728, 'besties': 2161, 'mcdo': 9287, 'mains': 9102, '2nite': 215, 'debating': 4306, 'harry': 6957, 'potter': 11338, 'bwahahahaha': 2821, 'totes': 14796, 'giggle': 6380, 'howmany': 7386, 'sorryyyyyy': 13459, 'chapter': 3171, 'kiyosaki': 8327, 'hire': 7206, 'cleaner': 3424, 'lov': 8926, 'skills': 13166, 'voted': 15584, 'yours': 16429, 'hiss': 7210, 'arik': 1541, 'popped': 11280, 'reall': 11852, 'hiccups': 7165, 'relief': 12023, 'nooooooooooo': 10207, 'sf': 12825, 'ahahahahahahahaha': 1122, 'jeans': 7985, 'emergency': 5180, 'plez': 11180, '4us': 334, '630p': 423, 'sprints': 13622, 'impressed': 7615, 'keys': 8257, 'looooove': 8888, 'shortcakefai': 12975, 'dang': 4204, 'dave': 4252, 'whining': 15910, 'heyy': 7155, 'yuup': 16471, 'hows': 7388, 'enjoyable': 5232, 'dizzy': 4702, 'calories': 2889, 'balk': 1893, 'disappoint': 4614, 'boys': 2533, '__d': 557, 'awayyyyy': 1755, 'attended': 1678, 'innovation': 7706, 'seminar': 12744, 'picnic': 11023, '2007': 134, 'rumored': 12415, 'screen': 12661, 'kindle': 8300, 'wyfi': 16227, 'k12': 8157, 'textbooks': 14417, 'amorsote': 1313, 'iced': 7510, 'sluggish': 13249, 'soak': 13341, '30mins': 231, 'wall': 15653, 'sharing': 12861, 'migraine': 9465, 'coffe': 3527, 'strong': 13868, 'hahah': 6816, 'boonies': 2447, 'map': 9165, 'transition': 14871, 'bkite': 2272, '07kjr': 13, 'nooooooo': 10203, 'dumb': 4946, 'scheduled': 12618, 'exhausting': 5452, 'qt': 11673, 'mow': 9772, 'lawn': 8540, 'marco': 9176, 'smoking': 13292, 'weed': 15790, 'philosophical': 10979, 'ohh': 10384, 'attack': 1668, '4d': 308, 'cassie': 3028, 'wimbledon': 15993, 'imax': 7590, 'successful': 13938, 'death': 4303, 'evil': 5403, 'homely': 7277, 'freddie': 6054, 'grilled': 6674, 'breast': 2580, 'reefried': 11947, 'gnite': 6465, 'twitties': 15117, 'freshman': 6083, 'alllllll': 1210, 'cqc': 3911, 'brazilian': 2566, 'humor': 7440, 'program': 11504, 'aston': 1639, 'spanish': 13513, 'min': 9497, 'outstanding': 10597, 'tc': 14300, 'r6rfc': 11728, 'tool': 14752, 'smilin': 13283, '1000': 24, '__diamond': 558, 'period': 10926, 'stain': 13662, 'blues': 2372, '_henrie': 672, 'bathroom': 1989, 'weirdest': 15819, 'wateva': 15729, 'michael': 9443, 'scofiled': 12633, 'noooooo': 10202, 'writer': 16191, 'tooth': 14760, 'tucking': 14988, 'mogwai': 9621, 'repeat': 12073, 'eeeeeeeeeee': 5086, 'luddite': 8989, 'theatre': 14464, 'younger': 16421, 'sis': 13127, 'mary': 9221, 'poppins': 11283, 'judge': 8113, 'pillow': 11053, 'nanna': 9940, 'bubble': 2694, 'baths': 1991, 'ahahah': 1119, 'holyyyyyyy': 7270, 'lumberjack': 8996, 'lipton': 8733, 'sparkling': 13518, 'telecom': 14348, 'sister': 13132, 'tonigh': 14737, 'monkey': 9661, 'russian': 12435, 'roulette': 12358, 'fabulous': 5532, 'interracial': 7780, 'ba': 1809, 'gphone': 6572, 'signed': 13071, 'io': 7822, 'sticking': 13763, 'clients': 3443, 'record': 11907, 'locations': 8808, 'young': 16420, 'sum': 13989, 'attending': 1679, 'cic': 3359, 'orientation': 10544, 'hearin': 7047, 'loggade': 8820, 'visst': 15550, '400e': 287, 'cache': 2846, 'lï': 9034, '½rdags': 16544, 'alone': 1228, 'mid': 9453, 'detox': 4487, 'piss': 11087, 'bruce': 2663, 'healthy': 7042, 'scary': 12606, 'maggots': 9079, 'rat': 11794, 'avoidance': 1740, 'continuous': 3774, 'flow': 5883, 'trick': 14917, 'loosen': 8896, 'gremlins': 6663, 'talented': 14229, 'omj': 10437, 'tonights': 14739, 'yanks': 16295, 'lakers': 8448, 'accounts': 945, 'county': 3882, 'raly': 11771, 'tak': 14216, 'quotes': 11722, 'pieces': 11032, 'ady': 1055, 'yoga': 16397, 'smith': 13285, 'amherst': 1301, 'mini': 9508, 'reunion': 12178, 'built': 2729, 'bridge': 2604, 'boards': 2390, 'nails': 9920, 'starwarsday': 13708, 'flickr': 5863, 'safe': 12481, 'returned': 12172, 'huntsville': 7454, 'sam': 12509, 'houston': 7379, 'grave': 6630, 'dipped': 4592, 'strawberries': 13834, 'considering': 3740, 'yarn': 16301, 'recently': 11881, 'necessary': 9994, 'angelina': 1341, 'spam': 13509, 'checks': 3215, 'spooky': 13597, 'pork': 11291, 'stir': 13780, 'rice': 12216, 'breaks': 2578, 'tip': 14648, 'bottle': 2503, 'securely': 12711, 'tightly': 14616, 'knw': 8359, 'maan': 9046, 'become': 2077, 'omelettes': 10426, 'chapathis': 3168, 'busted': 2791, 'lip': 8729, 'nurse': 10288, 'rid': 12226, 'event': 5368, 'management': 9142, 'prerequisites': 11403, 'dis': 4606, 'charger': 3177, 'verizon': 15469, 'fone': 5933, 'semester': 12742, 'pity': 11101, 'liesboystell': 8668, 'henna': 7128, 'redhead': 11929, 'lookin': 8871, 'boat': 2391, 'whew': 15902, 'kicking': 8270, 'holidays': 7255, 'whiles': 15906, 'power': 11352, 'cutting': 4121, 'aaaawww': 868, 'alcoholic': 1175, 'beverage': 2177, '_skies': 800, 'grrrrri': 6709, 'justice': 8149, 'cantttt': 2939, 'tormented': 14775, 'showing': 13009, '22': 153, 'especially': 5322, 'loads': 8798, 'jobs': 8050, 'themed': 14477, 'brickman': 2601, 'atleast': 1661, 'joined': 8065, 'invent': 7799, 'flavour': 5858, 'competion': 3640, 'entry': 5264, 'recognised': 11893, 'sweetpotatoe': 14143, 'seasalt': 12696, 'robert': 12282, 'pattinson': 10829, 'conchords': 3683, 'limo': 8705, 'yayy': 16309, 'fo': 5902, 'battlestar': 2002, 'galactica': 6239, 'channel': 3159, 'rebellion': 11868, 'became': 2068, 'gaeta': 6224, 'elephant': 5140, 'painting': 10697, 'shortly': 12983, 'west': 15848, 'apartments': 1446, 'ponying': 11255, 'minds': 9501, 'pushing': 11637, 'heels': 7085, 'livelovesing': 8765, 'embarassed': 5169, 'result': 12150, 'gettin': 6352, 'ak': 1161, 'flop': 5875, '2k6': 202, 'raised': 11766, '25': 167, 'rag': 11748, 'bets': 2166, 'raise': 11765, 'managaed': 9139, 'eels': 5094, 'donuts': 4775, 'comfort': 3591, 'fu': 6142, 'pleeez': 11177, 'bamf': 1905, 'ing': 7685, 'robsessedpattinson': 12288, 'biggest': 2213, 'burrito': 2775, 'asada': 1595, 'ooo': 10470, 'blahh': 2288, 'stomachace': 13792, 'girlys': 6408, 'wayy': 15749, 'kirsty': 8311, 'manually': 9163, 'oooiifull': 10472, 'mp4': 9783, 'shifts': 12914, 'orphan': 10552, 'lambs': 8454, 'cheated': 3209, 'zoombezi': 16510, 'buuuut': 2809, 'muahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 9808, 'dati': 4246, 'lian': 8655, 'eina': 5118, 'gelli': 6316, 'zero': 16496, 'francis': 6032, 'fit': 5824, 'implying': 7607, 'subtle': 13930, 'usual': 15384, 'remote': 12053, 'fighting': 5741, 'heerlen': 7086, 'shouts': 13001, 'peoples': 10902, 'twitterville': 15115, 'attention': 1680, 'length': 8617, 'type': 15142, 'mans': 9161, 'amazon': 1285, 'ce': 3077, 'grade': 6589, 'mock': 9611, 'lovel': 8944, 'ave': 1731, 'sweetest': 14140, 'obviously': 10332, 'kudo': 8393, 'concept': 3674, 'adjusting': 1014, 'obvs': 10333, 'tryna': 14969, 'noooooooo': 10204, 'seasons': 12698, 'files': 5752, 'hurray': 7459, 'phillll': 10976, 'wazzuppppp': 15753, 'semi': 12743, 'older': 10409, 'stack': 13656, 'talent': 14228, 'altered': 1251, 'peeps': 10880, 'twitterworld': 15116, 'jackson': 7929, 'rathbone': 11798, 'russians': 12436, 'quotation': 11720, 'rug': 12399, 'tied': 14606, 'unny': 15289, 'blatently': 2304, 'rather': 11799, 'thoughts': 14544, 'loz': 8967, 'alive': 1198, 'gandhi': 6255, 'books': 2443, 'charged': 3176, 'sean': 12690, 'teasing': 14320, 'wings': 16010, 'font': 5935, 'fonts': 5937, 'qw': 11725, 'makers': 9114, 'laugh': 8523, 'sees': 12726, 'amaze': 1275, 'annnnnnddd': 1375, 'bites': 2261, 'unfixable': 15248, 'forget': 5975, 'sunglasses': 14024, 'thanx': 14446, 'filthy': 5766, 'jimmy': 8032, 'lasted': 8504, 'soaked': 13342, 'hiking': 7189, 'wand': 15661, 'hawthorn': 6998, 'kraken': 8379, 'heartstrings': 7059, 'whoaa': 15929, 'toledo': 14713, 'pizza': 11107, 'enlightened': 5240, 'inspired': 7732, 'letters': 8638, 'hill': 7193, 'piano': 11014, 'lemme': 8613, 'mates': 9249, 'millsy': 9492, 'yahoo': 16289, 'yups': 16470, 'pruning': 11564, 'unfollowers': 15250, 'omgogmgo': 10434, 'wakeup': 15640, 'noooo': 10200, 'banquet': 1931, 'sb': 12588, '239k': 164, 'players': 11149, 'brick': 2599, 'kick': 8265, 'a9': 851, '_precious06': 775, 'talked': 14233, 'grandma': 6610, 'hindi': 7200, 'manged': 9149, 'software': 13370, 'install': 7734, 'willl': 15989, 'nighty': 10127, 'favorit': 5646, 'mommys': 9645, 'afternoons': 1079, 'happiness': 6922, 'milan': 9474, 'bfoyf': 2190, 'geneva': 6326, 'fam': 5576, 'kirk': 8309, 'id': 7517, 'clubbing': 3480, 'holdin': 7248, 'imo': 7600, 'icant': 7505, 'sober': 13346, 'professional': 11497, 'interns': 7779, 'sukks': 13984, 'telenovelas': 14351, 'braid': 2547, 'brewer': 2593, 'jordie': 8079, 'disliking': 4656, 'legit': 8603, 'aaaaaaaaaahhhhhhhh': 855, 'melt': 9360, 'brrrrrr': 2662, 'cleared': 3430, 'lah': 8443, 'linda': 8708, '27th': 181, '5th': 405, 'gangstarr': 6258, 'exgirl': 5449, 'nas': 9950, 'texas': 14415, 'eeekkkk': 5090, 'goofin': 6521, 'knowing': 8351, 'mouth': 9760, 'sissy': 13129, 'subway': 13934, 'letting': 8640, 'hw': 7478, 'tia': 14592, 'rose': 12345, 'sh': 12830, 'prove': 11549, 'cream': 3955, 'vancouver': 15430, 'throwing': 14569, 'canucks': 2940, 'polite': 11239, 'chosen': 3329, 'jab': 7919, 'knitting': 8343, 'needles': 10005, 'patrick': 10823, 'urgh': 15352, '_bambu': 579, 'mending': 9379, 'accomplish': 940, 'congratulatory': 3725, 'thankful': 14435, 'thnkn': 14525, 'virtus': 15535, 'treviso': 14911, 'futurshow': 6197, 'forza': 6007, 'ragazzi': 11749, 'erased': 5295, 'pfft': 10956, 'helicopters': 7105, 'imaginary': 7586, 'penis': 10893, 'drake': 4841, 'lovliest': 8961, 'ruined': 12405, 'doctors': 4727, 'inability': 7628, 'warn': 15687, 'tests': 14411, 'congrat': 3718, 'bach': 1829, 'theres': 14491, 'assignment': 1629, 'havent': 6986, 'concentrate': 3672, 'statue': 13722, 'venetian': 15455, 'screamed': 12657, 'p2v': 10665, 'migrates': 9468, 'vmware': 15558, 'vsp': 15596, 'magento': 9078, 'config': 3701, 'lemon': 8614, 'ranch': 11776, 'yself': 16449, 'heroism': 7144, 'badges': 1853, 'achievement': 959, 'favorite': 5647, 'hopeless': 7324, 'desk': 4461, 'lifespan': 8674, 'chel': 3238, 'nicole': 10110, 'indianapolis': 7661, 'buzzed': 2818, 'awhile': 1780, 'niceee': 10098, 'stickersss': 13762, 'woop': 16117, 'itself': 7889, 'country': 3879, 'preschool': 11405, 'kiddies': 8273, 'dunkin': 4956, 'hee': 7076, 'group': 6693, 'ripped': 12252, 'dentention': 4413, 'anxious': 1413, 'boreedd': 2481, 'cwpm': 4131, 'society': 13354, 'crimson': 3991, 'tide': 14602, 'sauce': 12570, 'tuner': 15002, 'deserve': 4450, 'posh': 11307, 'tatler': 14286, 'kk': 8328, 'challenge': 3136, 'sail': 12489, 'grad': 6588, 'eventhough': 5370, 'cook': 3801, 'sculpting': 12680, 'koi': 8365, 'dangerously': 4208, 'fell': 5689, 'paris': 10766, 'logs': 8829, 'resturants': 12149, 'predictable': 11387, 'cloudy': 3475, 'washing': 15702, 'messages': 9404, 'quilt': 11708, 'laid': 8444, 'finds': 5777, 'hardest': 6938, 'gained': 6235, 'kgs': 8259, 'shite': 12938, 'fornicating': 5989, 'nose': 10229, 'gaaay': 6216, 'accidentaly': 935, 'slammed': 13191, 'trunk': 14958, 'delete': 4381, 'network': 10056, 'cyber': 4134, 'kidnap': 8278, 'festival': 5706, 'stockholm': 13786, 'relay': 12015, 'honor': 7299, '06': 8, 'uncle': 15218, 'howard': 7382, 'firefly': 5801, 'nathan': 9960, 'fillion': 5761, 'tnx': 14683, 'progressive': 11512, 'smiling': 13284, 'piggies': 11042, 'hmpf': 7231, '6am': 453, 'dmp': 4711, 'definately': 4358, 'pm': 11201, 'circle': 3378, 'ab': 886, 'zum': 16518, 'spooohort': 13600, 'buying': 2814, 'twitterparty': 15107, 'laundry': 8531, 'sparkly': 13519, 'awwwwwwwwww': 1795, 'awwwww': 1790, 'groove': 6685, 'heeelllppppp': 7081, 'meeeeeeee': 9342, '_pearson': 770, 'fallow': 5572, 'babysitting': 1824, 'paionks': 10699, 'obsessed': 10327, 'addison': 1002, 'exclamation': 5439, 'throw': 14564, 'twitterrr': 15110, 'cervical': 3116, 'cancer': 2926, 'recession': 11883, 'clue': 3483, 'fafsa': 5546, 'form': 5982, 'ann': 1361, 'arbor': 1514, 'detroit': 4488, 'metro': 9421, 'computers': 3667, 'massages': 9235, 'son': 13409, 'grab': 6580, 'cover': 3895, 'realised': 11844, 'sending': 12749, 'staying': 13729, 'hve': 7475, 'docent': 4723, 'hvng': 7477, 'zombie': 16505, 'aftrn': 1084, 'shld': 12947, 'lebron': 8588, 'mvp': 9874, 'nba': 9978, 'lifetime': 8676, 'talktalk': 14240, 'broadband': 2636, 'workin': 16134, 'ova': 10606, 'sum1': 13990, 'dice': 4531, 'heroes': 7143, 'classics': 3411, 'timeless': 14632, 'numbers': 10283, 'performed': 10920, 'positive': 11312, 'tennessee': 14382, 'alot': 1230, 'tragic': 14850, 'mysterious': 9898, 'lights': 8685, 'messy': 9409, 'ponytail': 11256, 'preschoolers': 11407, 'hundred': 7443, 'weigh': 15812, 'lbs': 8556, 'gta4': 6725, 'meh': 9352, 'djandyw': 4704, 'coursework': 3889, 'mileyyyyyy': 9481, 'calm': 2884, 'breath': 2581, 'dragonball': 4837, 'listing': 8748, 'slide': 13222, 'hungover': 7447, 'sickening': 13046, 'regression': 11983, 'process': 11479, 'motion': 9744, 'possibe': 11317, 'gathering': 6289, 'corner': 3841, 'regretting': 11987, 'collection': 3561, 'ironclad': 7845, 'determination': 4484, 'uttered': 15394, 'ical': 7504, 'farrrrr': 5613, 'pdhpe': 10860, 'flats': 5854, 'helping': 7121, 'often': 10381, 'dummyhead': 4950, 'natsmith88': 9965, 'prepare': 11400, 'yourself': 16431, 'meditate': 9330, 'musical': 9851, 'actor': 981, 'neighbours': 10026, 'quitting': 11715, 'private': 11462, 'beta': 2164, 'blackberries': 2278, 'sucky': 13955, 'ticks': 14600, 'pakistan': 10707, '07kbq': 12, 'heeeeey': 7079, '38': 264, 'sickkkk': 13050, 'previous': 11429, 'understand': 15234, 'defo': 4365, 'uggo': 15171, 'lapit': 8485, 'na': 9905, 'ko': 8361, 'magout': 9086, 'fated': 5631, 'rrj4e': 12384, 'yt': 16450, 'partner': 10785, 'stubbed': 13880, 'giddy': 6374, 'invention': 7802, 'television': 14356, 'influence': 7682, '_ernman': 635, 'hej': 7101, 'malena': 9126, 'lycka': 9028, 'eurovision': 5357, 'united': 15274, 'kingdom': 8304, 'belly': 2129, 'assistant': 1632, 'focus': 5906, 'kid': 8272, 'particularly': 10782, 'pcd': 10855, 'painful': 10691, 'inflammed': 7679, 'aiming': 1147, 'asking': 1614, 'linked': 8719, 'developing': 4495, 'patient': 10819, 'homeschooling': 7281, 'dd': 4283, 'pitcher': 11097, 'roar': 12273, 'poorer': 11273, 'mexican': 9425, 'mx': 9879, 'automatically': 1720, 'caramels': 2958, 'crack': 3915, 'deadline': 4291, 'hectic': 7073, 'promo': 11527, 'orbicule': 10526, 'translation': 14874, 'norwegian': 10226, 'icecream': 7509, 'chris': 3332, 'pine': 11064, 'nudge': 10272, 'highlight': 7179, 'sold': 13376, 'rl': 12264, 'photoshop': 11002, 'nkotb': 10156, 'tish': 14663, 'crv': 4037, 'pick': 11017, 'duckie': 4933, 'match': 9242, 'handball': 6883, 'profile': 11498, 'loaded': 8795, 'variety': 15434, 'daily': 4171, 'comics': 3597, 'freshner': 6084, 'horriblel': 7338, 'grossed': 6688, 'ethan': 5343, 'coooooooool': 3819, 'dooooooooown': 4782, 'patience': 10818, 'virtue': 15533, 'attempt': 1672, 'alochol': 1227, 'thrown': 14570, 'teenagers': 14335, 'encouragement': 5207, 'bluetooth': 2373, 'device': 4497, 'greedy': 6649, 'adults': 1043, 'burn': 2765, 'irony': 7850, 'inventor': 7803, 'ford': 5962, 'mustang': 9862, 'hahaaaha': 6815, 'necklace': 9996, 'weekends': 15800, 'injured': 7692, 'passing': 10801, 'germs': 6345, 'surviving': 14094, 'rachael': 11734, 'classes': 3409, 'ohhh': 10385, 'wowwww': 16171, 'empitome': 5195, 'spirit': 13575, 'cake': 2860, 'rooting': 12338, 'treat': 14895, 'chem': 3242, 'physics': 11012, 'freaaaaaaak': 6045, 'compliment': 3658, 'endearing': 5209, 'completed': 3651, 'hunt': 7452, 'betting': 2173, 'slip': 13229, 'tenner': 14381, 'newcastle': 10069, 'relegated': 12021, 'tw': 15027, 'lovers': 8950, 'hills': 7194, 'index': 7657, 'lover': 8949, 'trudy': 14953, 'burnet': 2767, 'hasnt': 6964, 'occasional': 10336, 'reruns': 12114, 'bury': 2780, 'ours': 10578, 'fourteen': 6018, 'ehhh': 5114, 'dentist': 4414, 'root': 12337, 'canal': 2916, 'host': 7355, '_w': 833, 'interview': 7785, 'chat': 3198, 'laura': 8532, 'wna': 16069, 'tan': 14246, 'chilled': 3280, 'leavin': 8585, 'stripper': 13864, 'suppoort': 14061, 'freezing': 6069, 'damnit': 4187, 'stp': 13815, 'step': 13743, 'vhs': 15483, 'connecting': 3730, 'harddrive': 6935, 'mon': 9650, 'oven': 10607, 'dam': 4176, 'zach': 16478, 'canceled': 2921, 'spring': 13617, 'emoticons': 5187, 'reno': 12063, 'sabi': 12455, 'nga': 10086, 'lighting': 8682, 'hazardous': 7007, 'health': 7040, 'happ': 6911, 'mmot': 9598, 'wishin': 16035, '00': 0, 'highschool': 7183, '57am': 386, 'promising': 11525, 'october': 10345, 'rc': 11815, 'sa': 12453, 'western': 15849, 'digital': 4565, 'caviar': 3061, '1tb': 124, 'sata': 12557, '300': 224, 'ncq': 9983, '32mb': 243, 'appreciated': 1489, 'reward': 12193, 'weak': 15760, 'participated': 10780, 'lit': 8751, 'foa': 5903, 'diff': 4546, 'storys': 13811, 'neva': 10061, 'tools': 14754, 'darling': 4227, 'edit': 5067, 'suite': 13980, 'prince': 11443, 'persia': 10935, 'recogns': 11896, 'assignments': 1630, 'installing': 7736, 'redo': 11935, 'claimed': 3398, 'media': 9323, 'stats': 13721, 'tch': 14301, 'mm': 9590, 'jeffree': 7990, 'rawks': 11808, 'difficulties': 4556, 'stepped': 13749, 'addresses': 1006, 'ne': 9986, 'miley': 9480, 'cse': 4049, 'rfid': 12202, 'guinea': 6749, 'extension': 5506, 'modern': 9620, 'studies': 13887, 'subject': 13918, 'guessing': 6738, 'supertramp': 14054, '5jucn': 400, 'jess': 8008, 'props': 11540, 'paying': 10848, 'heartbroken': 7054, 'burnsy': 2771, 'comparison': 3636, 'rocky': 12301, 'mountains': 9755, 'enlgland': 5239, '_k': 703, 'question': 11698, 'storms': 13809, 'partyyy': 10792, 'religious': 12027, 'supernatural': 14049, 'busiest': 2785, 'boss': 2491, 'prep': 11399, 'gifts': 6377, 'bfast': 2184, 'tires': 14659, 'inspection': 7726, 'sticker': 13761, '590': 388, 'repair': 12071, 'partying': 10791, '_baby': 578, 'obsession': 10328, 'editing': 5069, 'whoa': 15928, 'aaahaha': 870, 'unread': 15300, 'ditzy': 4689, 'skits': 13176, 'sadness': 12477, 'awards': 1749, '_idance19': 682, 'thiss': 14518, 'blastinggg': 2303, 'minnie': 9518, 'overjoyed': 10618, 'jleno': 8040, 'minus': 9526, 'vita': 15553, 'rickbaker24': 12223, 'posting': 11329, 'tehe': 14343, 'spread': 13613, 'vibes': 15488, 'outty': 10601, 'couuuurse': 3893, 'fika': 5748, 'thes': 14493, 'hurtin': 7465, '_carter': 595, 'blind': 2326, 'mums': 9833, 'fiona': 5795, 'enought': 5245, 'megan': 9350, 'suitcase': 13979, 'pisses': 11089, 'amara': 1271, 'qqe7b': 11671, 'heres': 7137, 'goodmorning': 6512, 'baptized': 1934, 'graduate': 6594, 'dal': 4174, 'den': 4407, 'officials': 10376, 'twiiter': 15070, 'erm': 5305, 'minibus': 9510, 'useless': 15370, 'laodicean': 8483, 'kavya': 8198, 'spelling': 13552, 'crown': 4016, 'spell': 13548, 'transformers': 14870, 'colombia': 3566, 'ms': 9791, 'yayz': 16314, 'x3': 16238, 'horribly': 7339, 'jackie': 7926, 'embraced': 5178, 'appeal': 1463, 'quality': 11678, 'audio': 1692, 'artists': 1587, 'grizzly': 6681, 'bears': 2043, 'adventures': 1049, 'swine': 14153, 'scaring': 12603, 'thnx': 14527, 'tweeten': 15045, 'tix': 14669, 'deals': 4299, '28': 182, '43': 293, 'blerg': 2319, 'glasses': 6438, 'color': 3567, 'carded': 2966, 'gamestop': 6250, 'ego': 5112, 'refusal': 11966, 'upon': 15329, 'realisation': 11842, 'morrissey': 9723, 'gig': 6378, 'brixton': 2631, 'postponed': 11331, 'monthsish': 9679, 'tto': 14980, 'tafe': 14204, 'tooo': 14755, 'characters': 3174, 'gambit': 6243, 'fold': 5914, 'dayy': 4271, 'travelled': 14887, 'grew': 6666, 'genoese': 6329, 'pasta': 10808, 'sgt': 12829, 'hughes': 7424, 'bacteria': 1849, 'meningitis': 9380, 'disinfected': 4653, 'female': 5696, 'latrine': 8517, 'ski': 13162, 'heh': 7089, 'differences': 4549, 'b3': 1807, 'b4': 1808, 'smelly': 13275, 'vixon': 15556, 'kadi': 8160, 'amazinq': 1283, 'qirls': 11661, 'niqht': 10143, 'quess': 11696, 'qreat': 11672, 'dresses': 4870, 'neglected': 10021, 'surgery': 14081, 'mostley': 9736, 'yellow': 16356, '_read': 779, 'background': 1834, 'apps': 1497, 'stressin': 13852, '_waters': 835, 'hahahahah': 6820, 'indian': 7659, 'cowboy': 3901, 'carnival': 2990, 'costco': 3860, 'print': 11451, 'photobook': 10996, 'weee': 15793, 'blinds': 2328, 'glare': 6435, 'style': 13908, 'dual': 4924, 'reject': 11998, 'mfm7tl': 9429, 'defending': 4352, 'bleach': 2307, 'doggie': 4740, 'forest': 5970, 'programmed': 11507, 'fotos': 6012, 'showr': 13011, 'tidy': 14604, 'hse': 7398, 'stamp': 13674, 'skin': 13168, 'medicine': 9328, 'laughing': 8525, 'laughter': 8527, 'gud': 6735, 'noticed': 10243, '_shediddy': 795, 'phped': 11006, 'successfuly': 13940, 'ems': 5201, 'anywho': 1434, 'roooooooooom': 12336, 'dsi': 4919, 'aim': 1146, '_is_here': 691, 'hun': 7442, 'edna': 5074, 'goldfish': 6493, 'spain': 13508, 'tiff': 14610, 'fuzzy': 6203, '_shep': 796, 'relax': 12009, 'translated': 14873, 'swedish': 14131, 'screaming': 12659, 'noes': 10173, 'standby': 13678, 'plug': 11188, 'fuse': 6191, 'duty': 4971, 'students': 13885, 'union': 15269, 'punters': 11617, 'members': 9366, 'clueles': 3485, 'qantas': 11657, 'lounge': 8924, 'flyertalk': 5896, 'bloggers': 2343, 'panel': 10730, 'listed': 8741, '2pm': 217, '1e15': 118, 'bea09': 2032, 'freaking': 6049, 'spank': 13514, 'cavs': 3064, 'anniversary': 1372, 'stanley': 13683, 'steemer': 13741, '800': 484, 'seattle': 12703, 'matt': 9257, 'spots': 13610, 'memphis': 9374, 'visited': 15545, 'icon': 7515, 'restaurant': 12140, 'settle': 12806, 'hamburger': 6869, 'godawful': 6475, 'mizmind': 9585, 'highness': 7182, 'crystalmariedontluvspiteanymore': 4043, 'appreciation': 1490, '4w8l1': 343, 'haaha': 6791, 'bangs': 1920, 'oops': 10483, 'thaank': 14429, 'lies': 8667, 'embedded': 5175, 'hampstead': 6877, 'constituency': 3745, 'apples': 1476, 'rio': 12248, 'bravo': 2564, 'hunting': 7453, 'sense': 12755, 'aparently': 1443, 'sight': 13064, 'sampler': 12518, 'snot': 13332, 'hubby': 7413, 'ticketttttss': 14598, 'tahong': 14210, 'x24ke': 16230, 'partied': 10783, 'updated': 15319, 'skins': 13170, 'nightmare': 10121, '82k': 490, 'css': 4054, 'woe': 16073, 'mulching': 9823, 'bunny': 2758, 'survive': 14092, 'struggle': 13875, 'screwing': 12671, 'lovee': 8936, 'phogs': 10989, 'edited': 5068, 'docs': 4725, 'appt': 1499, 'hallooo': 6855, 'bayern': 2007, 'stau': 13725, '_l': 714, 'exhibit': 5453, 'factor': 5543, 'child': 3268, 'droped': 4895, '120': 51, 'smarted': 13266, 'bussiness': 2788, 'breaky': 2579, 'russtle': 12437, '_prototype09': 776, 'americanidolislove': 1298, 'ughhhhhh': 15176, 'youu': 16438, 'cuute': 4122, 'goitn': 6490, 'cleanin': 3425, 'somethin': 13401, 'totem': 14795, 'phd': 10967, 'taunting': 14293, '_ii': 685, 'foggy': 5911, 'tiger': 14611, 'claws': 3417, 'nutz': 10299, 'skint': 13171, 'backup': 1845, 'renae': 12056, 'abducted': 895, 'pregnant': 11392, 'changes': 3155, 'sesh': 12797, 'lined': 8712, 'c4s': 2836, 'otherwise': 10568, 'emailed': 5166, 'zuccini': 16515, 'heads': 7034, 'awwwh': 1788, 'directory': 4601, 'toll': 14715, 'highly': 7181, 'correctly': 3851, 'attacking': 1670, 'tei': 14344, 'sency': 12746, 'ghina': 6365, 'tricked': 14918, 'differing': 4553, 'mandy': 9147, 'robbie': 12280, 'monk': 9660, 'jumps': 8133, 'developed': 4492, 'asda': 1600, 'tweetstats': 15056, 'confirmed': 3704, 'suspected': 14100, 'maccy': 9055, 'extra': 5509, 'sudden': 13957, 'evr': 5406, 'brngs': 2633, 'starbux': 13689, 'ten': 14372, 'bucks': 2699, 'boulder': 2508, 'ruled': 12410, 'travelling': 14888, 'ahhhhhh': 1131, 'realx': 11857, 'hamdemic': 6870, 'aporkalypse': 1459, 'parmageddon': 10773, 'rules': 12411, 'tension': 14384, '_cheryl': 598, 'teleport': 14353, 'nonetheless': 10189, 'sunnybank': 14029, 'sep': 12768, 'austira': 1713, 'wrap': 16176, 'ashamed': 1601, 'timid': 14636, 'osocute': 10558, 'bashfulness': 1967, 'quit': 11712, 'chzlw': 3353, '_n_nouns': 751, 'ikr': 7559, 'boystown': 2535, 'eastwood': 5029, '3hours': 270, 'cambie': 2893, 'feedback': 5676, 'bogged': 2403, 'eugh': 5350, 'britains': 2623, 'meagan': 9304, 'mindy': 9502, 'blanket': 2297, 'shed': 12880, 'lint': 8724, 'skirt': 13175, 'roll': 12311, 'stone': 13794, 'charity': 3181, 'cystic': 4144, 'fibrosis': 5727, 'thrilled': 14555, 'teen': 14333, 'sikaflex': 13079, 'caulk': 3052, 'ich': 7511, 'auch': 1688, 'zu': 16514, 'pinkpop': 11076, 'mitchell': 9577, 'tryed': 14965, 'twitterfon': 15098, 'inconclusive': 7645, 'rays': 11810, 'moments': 9638, 'intense': 7760, 'genius': 6328, 'sheer': 12886, 'insanity': 7716, 'sanity': 12539, 'seats': 12702, 'powers': 11357, 'withdrawl': 16045, 'symptons': 14176, 'darrius': 4233, 'symptoms': 14175, 'minne': 9515, 'syamptoms': 14165, 'scores': 12640, 'no1': 10163, 'slot': 13239, '306': 228, 'spectacular': 13537, 'nutt': 10295, 'collar': 3555, 'leash': 8578, 'matched': 9243, 'winter': 16019, 'ps': 11565, 'paranoid': 10757, 'arriving': 1572, 'drat': 4846, 'means': 9312, 'wolftrap': 16081, 'thedailyshow': 14466, 'nooooo': 10201, 'shud': 13020, 'throooooooooooooo': 14561, 'twitts': 15120, 'respond': 12133, 'diver': 4692, 'theory': 14484, '70': 460, 'questions': 11700, '35mins': 255, 'ea': 4991, 'grouchy': 6689, 'rants': 11786, 'ace': 949, 'hardly': 6940, 'waves': 15740, 'shape': 12855, 'sms': 13299, 'solution': 13384, 'promotion': 11529, 'itagg': 7874, '½6': 16527, 'smoky': 13294, 'missus': 9566, 'drizzle': 4891, 'bruised': 2667, 'german': 6343, 'follower': 5923, 'hermine': 7139, 'sudetenland': 13959, 'sweden': 14130, 'refugee': 11963, '1948': 108, 'whitsun': 15925, 'tube': 14986, 'deeeesearted': 4343, 'ns2l55': 10266, 'aceness': 952, 'extreme': 5513, 'cheesy': 3234, 'victoria': 15499, 'slippers': 13231, 'chauncey': 3203, 'sac': 12457, 'speech': 13538, 'careless': 2976, 'dreaming': 4861, 'funerals': 6166, 'education': 5077, 'circus': 3379, 'offtopic': 10380, 'inappropriate': 7629, 'bacardi': 1827, 'phuket': 11009, 'thailand': 14432, 'singapore': 13106, 'sooner': 13423, 'moses': 9728, 'girlfriend': 6401, 'placed': 11114, 'dos': 4789, 'screenshot': 12667, 'qlzp2': 11666, 'kiran': 8308, 'land': 8466, 'jd': 7980, 'yaaaaay': 16277, 'colored': 3569, 'volleyball': 15574, 'everyones': 5393, 'kojikun': 8367, 'seastar': 12699, 'brian': 2597, 'sug': 13970, 'mistake': 9568, 'skipping': 13174, 'carleigh': 2983, 'barley': 1948, 'hr': 7393, 'bby': 2019, 'kitten': 8320, 'babe': 1815, 'nxt': 10305, 'wk': 16059, 'fallin': 5569, 'fatty': 5637, 'cali': 2874, 'filming': 5763, 'musicans': 9852, 'legion': 8602, 'dan': 4193, 'leader': 8562, 'festive': 5708, 'projection': 11514, 'complaints': 3649, '4jcfg': 318, 'cycle': 4136, 'within': 16048, 'quarry': 11681, 'shipley': 12928, 'glen': 6439, 'baildon': 1872, 'moor': 9694, 'woods': 16102, '_x': 844, 'hah': 6811, 'tory': 14785, 'watty': 15736, 'louie': 8920, 'thq': 14548, 'billed': 2226, 'rpg': 12376, 'keyed': 8252, 'shibuya': 12909, 'contact': 3754, 'lenses': 8621, 'shinjuku': 12924, 'stick': 13759, 'hawks': 6996, 'mighty': 9464, 'blackhawks': 2280, 'friendss': 6101, 'aka': 1162, 'tidied': 14603, 'hoovered': 7314, 'out2': 10581, 'bakery': 1880, 'followed': 5922, 'mea': 9302, 'daaay': 4157, 'stung': 13900, 'stinging': 13774, 'nettles': 10055, 'shin': 12920, 'cable': 2844, 'soz': 13499, 'geography': 6337, 'cz': 4146, 'whitney': 15924, 'dangggg': 4209, 'celebrate': 3085, 'rained': 11759, 'devon': 4500, 'harde': 6936, 'nat': 9956, 'goon': 6524, 'subo': 13922, 'britians': 2624, 'contacts': 3757, 'naw': 9975, 'tame': 14244, 'costume': 3864, 'voyager': 15589, 'medical': 9325, 'uni': 15263, 'welcomeee': 15831, 'backkkkk': 1837, 'standing': 13680, 'treatments': 14898, 'script': 12674, 'amaaazing': 1265, 'ladder': 8431, 'collapse': 3553, 'hittin': 7218, 'ole': 10413, 'dusty': 4968, 'trail': 14852, 'morgan': 9707, 'efforts': 5107, 'gentleman': 6331, 'agreement': 1111, 'whoever': 15931, 'wins': 16017, 'pays': 10852, 'sample': 12517, 'frustrated': 6133, 'scare': 12596, 'imaging': 7589, 'staring': 13693, 'faces': 5541, 'basically': 1969, 'insurance': 7751, 'bugs': 2722, 'shine': 12921, 'tad': 14202, 'selfish': 12735, 'rewrite': 12194, 'bosses': 2492, 'summy': 14002, 'cocoa': 3517, 'crispies': 3995, 'chemo': 3247, 'recover': 11916, 'bumbed': 2746, 'laker': 8447, 'welchs': 15827, 'grape': 6619, 'toronto': 14778, '48': 301, 'bluedart': 2371, 'shipment': 12929, 'deliver': 4391, 'databases': 4242, 'ph': 10961, 'macaron': 9051, 'smackdown': 13259, 'aware': 1750, 'pride': 11437, 'blank': 2296, 'sweeet': 14134, 'headlining': 7031, 'doubt': 4801, 'scratched': 12652, 'intentando': 7762, 'intentarlo': 7763, 'daddio': 4164, 'wolverine': 16082, 'ooaf': 10463, 'jackman': 7927, 'sctrahc': 12679, 'creammm': 3956, 'pair': 10700, 'txting': 15136, 'resurrect': 12154, 'replace': 12077, 'tekzilla': 14347, 'ginger': 6395, 'biscuits': 2253, 'urgently': 15351, 'weight': 15814, 'scales': 12591, 'wknd': 16062, 'prayin': 11377, '4wlgi': 355, 'drems': 4867, 'x2eb3': 16234, 'frappuccino': 6039, 'mc': 9285, 'sim': 13088, 'posit': 11308, 'softees': 13368, 'stranded': 13822, 'warren': 15694, 'tech': 14321, 'spec': 13531, 'tempted': 14370, 'leica': 8608, 'palawan': 10710, 'karen': 8182, 'donation': 4765, 'banner': 1929, 'productive': 11493, 'keychain': 8250, 'user': 15372, 'differs': 4554, 'keychains': 8251, '501': 369, 'owner': 10647, 'excitement': 5436, 'kiddin': 8274, 'shy': 13036, 'describe': 4446, 'whispergifts': 15916, 'bridal': 2602, 'registry': 11982, 'workplace': 16140, 'sleeved': 13218, 'grecia': 6646, 'fershure': 5704, 'gaggles': 6227, 'commuters': 3625, 'squished': 13638, 'sellout': 12740, 'fluke': 5892, 'projector': 11515, 'heavens': 7068, 'certainly': 3114, 'warmed': 15685, 'dressing': 4871, 'twtvite': 15129, '3koyqo': 273, 'aptw': 1503, 'engagements': 5222, 'bold': 2412, 'borriing': 2489, 'stuffy': 13897, 'disagree': 4608, 'balancing': 1884, 'chair': 3132, 'sixth': 13148, 'sneers': 13315, 'hung': 7444, 'balls': 1899, 'butttt': 2808, 'august': 1702, '7th': 482, 'feelers': 5681, 'harrassed': 6954, 'twit': 15081, 'knda': 8337, 'hallucinating': 6858, 'boohoo': 2433, 'embarrassing': 5173, 'phonograph': 10994, 'industry': 7672, 'apm': 1453, 'berry': 2151, 'rabbit': 11730, 'ribena': 12214, '_vet': 830, 'von': 15581, 'jo': 8043, 'exsausted': 5499, 'sweaty': 14129, 'chalky': 3134, 'iusedtobescaredof': 7900, 'cooolooorss': 3816, 'composition': 3662, 'tumor': 14996, 'grandas': 6606, 'probable': 11470, 'kisses': 8315, '33': 244, 'omgg': 10431, 'jello': 7992, 'delirious': 4389, 'jimmie': 8031, 'cg': 3120, 'admit': 1020, 'smooth': 13295, 'nor': 10213, 'cajun': 2859, 'refreshed': 11959, 'gotten': 6565, 'lead': 8561, 'sadd': 12465, 'hols': 7268, 'chomp': 3316, 'terribly': 14401, 'chill': 3277, 'bylaurenluke': 2829, 'cudamo': 4066, 'biotch': 2239, 'feed': 5675, '0ut': 20, 'somebody': 13395, 'lappy': 8487, 'norm': 10215, 'attempts': 1675, 'extend': 5503, 'inner': 7700, 'closure': 3468, 'ref': 11950, 'pare': 10761, 'portraits': 11301, 'bangbang': 1917, 'streak': 13836, 'lng': 8791, '13pqtw': 69, 'foolish': 5944, 'iiight': 7553, 'yuck': 16452, 'maids': 9093, 'lc': 8557, '18hrs': 104, '22hrs': 156, 'reminds': 12048, 'catche': 3038, 'anti': 1400, 'gravity': 6632, 'chamber': 3141, '_cheshire_cat_': 599, 'idiotat': 7527, 'tove_liden': 14814, 'nks': 10157, 'tove': 14813, 'ddoodm': 4286, 'shares': 12860, 'czhzb3': 4148, 'draft': 4832, 'rpzmx': 12379, 'moring': 9708, 'stomach': 13791, 'invaders': 7797, 'base': 1961, 'kickass': 8267, 'phast': 10966, 'thick': 14499, '_h786': 665, 'restrictions': 12147, 'suffers': 13966, 'jetsons': 8017, 'finltstones': 5790, 'thumb': 14576, 'pot': 11333, '970': 528, 'shidduch': 12910, 'anythgin': 1424, 'fastsmallballbuster': 5627, 'weirdherout': 15820, 'visionboard': 15543, 'superpower': 14050, 'whalen': 15864, 'peter': 10949, 'gunna': 6761, 'smallville': 13263, 'incredible': 7653, 'reach': 11825, 'injury': 7694, '_ohh': 764, 'thread': 14549, 'results': 12152, 'announced': 1377, 'prd': 11380, 'recommendatiion': 11900, 'lecture': 8589, 'tank': 14251, 'guild': 6745, 'omelets': 10424, 'butter': 2800, 'olive': 10415, 'oil': 10394, 'spits': 13581, 'napkin': 9944, 'jet': 8015, 'carbon': 2960, 'footprint': 5953, '_guy': 662, 'xmen': 16251, 'continues': 3773, 'mazie': 9281, 'kristina': 8383, 'ericka': 5301, 'robin': 12283, 'liers': 8666, 'metabolism': 9411, 'choose': 3318, 'digs': 4566, 'extremely': 5514, 'yummm': 16461, 'hangin': 6900, 'ashington': 1602, 'sept': 12771, 'northumberland': 10224, 'academy': 920, 'lancaster': 8464, 'hel': 7102, 'rosary': 12344, 'mucking': 9814, 'rig': 12237, 'demo': 4403, 'envious': 5267, 'chang': 3151, 'thay': 14460, 'anh': 1351, 'gi': 6368, 'ca': 2839, 'amp': 1317, 'heated': 7061, 'dyed': 4980, 'indie': 7662, 'need2learn': 10000, 'tunes': 15003, '1hour': 119, 'fuunn': 6198, 'az': 1802, 'reques': 12101, 'james': 7942, '_west': 837, 'hubs': 7414, 'missions': 9563, 'brings': 2618, 'daze': 4275, 'canberra': 2919, 'lotion': 8912, 'mohawk': 9622, 'dhcp': 4513, 'bacoor': 1848, 'wahaha': 15626, 'wifi': 15972, 'toinks': 14709, 'wiih': 15974, 'camp': 2905, 'fires': 5805, 'charlene': 3183, 'yeay': 16340, 'inglewood': 7686, '2ndary': 212, 'friendly': 6098, 'ozzy': 10661, 'forgottten': 5981, 'meaning': 9309, 'insonmia': 7725, 'maaaaaan': 9039, 'oui': 10575, 'brushing': 2677, 'bonjour': 2424, 'seriuosly': 12787, 'backache': 1832, 'psh': 11567, 'brilliant': 2614, 'sloooooooooowlyyyyyyyyyyyyy': 13236, 'popcornss': 11277, 'whasup': 15868, 'jen': 7997, 'caption': 2951, 'animal': 1352, 'planet': 11125, 'tiime': 14619, 'fascinating': 5618, 'bedroom': 2082, 'ignored': 7547, 'showin': 13008, 'genes': 6325, 'doente': 4734, 'teniece': 14380, 'pop': 11275, 'taskbar': 14272, 'corrupt': 3852, 'chkdisk': 3301, 'anita': 1357, 'pankraz': 10736, 'liek': 8665, 'famous': 5583, 'luckkyy': 8984, 'trix': 14937, 'engines': 5224, 'mana': 9138, 'accel': 923, 'gooooood': 6535, 'pshh': 11568, 'redskins': 11938, 'jansen': 7954, 'twitterberry': 15091, 'faults': 5640, 'ducked': 4932, 'ga': 6215, 'pleasantly': 11160, 'hhahaa': 7162, 'ahah': 1117, 'lolllllyyyyyyyy': 8839, '2026': 142, '2164': 149, '6790': 439, '9128': 519, 'credit': 3969, 'cue': 4070, 'kenny': 8231, 'underrated': 15232, 'porridge': 11294, 'gloopy': 6447, 'kicked': 8268, 'beckyyy': 2076, 'chackin': 3126, 'discuss': 4639, '87': 494, 'degrees': 4373, 'hagg': 6809, 'bomb': 2416, 'jack': 7920, 'whataburger': 15870, 'cabanaaaaaa': 2841, 'lm': 8781, 'ny': 10306, '2ft': 196, 'laud': 8522, '4a': 305, 'weddin': 15785, 'spike': 13567, 'mahn': 9090, '700': 461, 'compared': 3635, 'ppls': 11364, 'robots': 12286, 'ttfn': 14978, 'waitt': 15636, 'scratchy': 12655, 'biased': 2202, 'hopeful': 7320, 'tweetbeaks': 15042, 'rblpnqte': 11814, 'sheesh': 12887, 'rb': 11812, 'delux': 4397, '5z36j': 409, 'offf': 10368, 'ws': 16202, 'oeiras': 10355, 'portugal': 11303, 'hoorah': 7312, 'e3': 4989, 'g4tv': 6214, 'coverage': 3896, 'comcast': 3582, 'basic': 1968, 'ways': 15748, 'brooke': 2646, 'skate': 13159, 'seshion': 12798, 'arrands': 1560, 'faq': 5603, 'languages': 8478, 'resting': 12144, 'pune': 11613, 'enjoye': 5233, 'nanny': 9941, 'iater': 7500, 'pudding': 11586, 'tis': 14661, 'col': 3540, 'polka': 11243, 'dots': 4797, 'glam': 6431, 'badluck': 1854, 'sidewalk': 13058, 'harvard': 6959, 'cobblestones': 3509, 'buddy': 2704, 'teased': 14318, 'puggy': 11593, 'pughug': 11594, 'ac': 919, 'tmr': 14677, 'foodland': 5941, 'heaps': 7044, 'spa': 13502, 'fort': 5997, 'collins': 3565, 'casa': 3014, 'kent': 8232, 'ale': 1176, 'reached': 11826, 'nasal': 9951, 'trimmer': 14924, 'query': 11693, 'carla': 2982, 'illness': 7567, 'belfast': 2116, 'clean': 3422, 'building': 2728, 'sayed': 12583, 'hassan': 6965, 'looool': 8880, 'ana': 1323, '7ag': 478, 'ele': 5129, 'ye6le3ni': 16318, 'taqa3od': 14262, 'memory': 9373, 'rap': 11787, 'loser': 8907, 'aight': 1142, 'pau': 10830, 'au': 1686, 'blaisdell': 2291, 'arena': 1528, 'cheehee': 3218, 'awwrrite': 1786, 'pocket': 11206, 'suggestions': 13976, 'replacing': 12079, 'ken': 8226, 'onion': 10450, 'chive': 3300, 'self': 12734, 'whoops': 15945, 'lifeline': 8672, 'neat': 9993, 'gary': 6279, 'ticker': 14594, 'morrrning': 9725, 'ti': 14590, '_ianne': 681, 'terrance': 14397, 'center': 3102, 'admitting': 1022, 'plots': 11185, 'romance': 12320, 'feminist': 5699, 'whaaaaaat': 15858, 'waah': 15614, 'aunts': 1707, 'hopfully': 7327, 'wisdom': 16027, 'teeth': 14337, 'startin': 13701, 'goto': 6561, 'lonovala': 8865, 'frenz': 6079, 'transport': 14877, 'arrngmnts': 1573, 'wrkg': 16197, 'wxnwa': 16222, 'yeee': 16342, 'venessa': 15454, 'internal': 7775, 'spare': 13516, 'loft': 8817, '4ward': 345, 'industrial': 7671, 'estate': 5332, 'pirate': 11083, 'ffancy': 5720, 'mile': 9476, 'understandable': 15235, 'chillaxin': 3278, 'bankholiday': 1924, 'everbody': 5374, 'plane': 11123, 'fanboy': 5586, 'jason': 7964, 'mraz': 9788, 'morrison': 9721, 'jealousy': 7983, 'tangible': 14247, 'safesex': 12484, 'topic': 14765, 'gossip': 6554, 'blessings': 2323, 'metal': 9412, 'bat': 1983, 'socked': 13357, 'reckless': 11889, 'kuwait': 8401, 'suv': 14109, 'ticket': 14595, 'function': 6157, 'grrh': 6704, 'kut3': 8400, 'pit': 11095, 'witch': 16039, 'upstate': 15342, 'hick': 7167, 'ethnic': 5345, '_grubb': 659, 'kevin': 8243, 'joining': 8066, 'carrying': 3005, 'backpack': 1840, 'ontario': 10460, 'saddens': 12468, 'delegate': 4380, 'vogue': 15568, 'model': 9615, 'rachel': 11735, 'merh': 9399, 'wasted': 15711, 'impromptu': 7620, 'costa': 3859, 'deciding': 4323, 'workout': 16139, 'du': 4923, 'jour': 8090, '000th': 2, 'stayed': 13727, 'annivarsary': 1371, 'hanson': 6908, 'niceeeee': 10099, 'convinced': 3796, 'les': 8627, 'spat': 13522, 'rite': 12259, 'flor': 5877, 'grets': 6665, 'easports': 5022, 'madden': 9065, 'funeral': 6165, 'aye': 1800, 'ffwd': 5723, 'aac': 878, 'markers': 9195, 'tyring': 15151, 'aruba': 1591, 'soul': 13469, 'fest': 5705, 'bam': 1901, 'ouuhh': 10603, 'tight': 14614, 'bithday': 2262, 'pierre': 11037, 'bl': 2275, 'gogol': 6486, 'bordello': 2472, 'par': 10748, 'prolly': 11517, 'gb': 6296, 'fashioned': 5621, 'civilization': 3388, 'ii': 7552, 'every1': 5383, 'laser': 8500, 'tagging': 14208, 'dominos': 4761, 'martabak': 9213, 'tempting': 14371, 'fattening': 5636, 'comedian': 3585, 'ada': 990, 'acara': 921, 'menarik': 9377, 'lain': 8445, 'vip': 15523, 'ttg': 14979, 'yg': 16381, 'dikasih': 4570, 'tasks': 14274, 'approach': 1492, 'strangers': 13826, 'crowd': 4014, 'hmm': 7226, 'twittter': 15121, 'boredom': 2480, 'split': 13585, 'duties': 4970, 'osu': 10560, 'ath': 1653, 'trng': 14939, 'rcption': 11817, 'chatting': 3201, 'rummage': 12414, 'mcfly': 9291, 'shee': 12881, 'applied': 1480, 'mercedez': 9395, 'snitchsneeker': 13325, 'chest': 3251, 'yew': 16377, 'prooved': 11532, 'bumme': 2747, 'marking': 9200, '135': 64, 'layed': 8546, '23': 158, 'jobfield': 8047, 'unlucky': 15287, 'attempting': 1674, 'return': 12171, 'dayyy': 4272, 'ng': 10085, '_magazine': 732, 'reality': 11847, 'cs4': 4048, 'wishlist': 16037, 'whered': 15897, 'funnier': 6174, 'theyve': 14498, 'gooood': 6529, 'fusion': 6192, 'directly': 4599, 'banking': 1925, 'pal': 10708, 'witless': 16050, 'episodes': 5282, 'nbc': 9979, 'theofficenbc': 14483, 'gervais': 6347, 'skillz': 13167, '_shawn': 794, 'ikea': 7557, 'rooms': 12334, 'twitterfriends': 15100, 'headbutt': 7023, 'football': 5950, 'refreshing': 11960, 'fuming': 6155, 'ebay': 5038, 'agai': 1087, 'wolfram': 16080, 'alpha': 1233, 'cuil': 4072, 'bud': 2700, 'lime': 8701, 'ep': 5274, 'hassn': 6966, 'recommended': 11903, 'harney': 6951, 'sons': 13418, 'steady': 13732, 'income': 7644, 'cab': 2840, 'mastered': 9240, 'assume': 1635, 'heffer': 7088, 'morris': 9720, 'leopold': 8626, 'mapped': 9166, 'served': 12791, 'transparency': 14876, 'opaqueness': 10490, 'birdy': 2244, 'lap': 8484, 'eve': 5364, 'al': 1166, 'hers': 7147, 'twins': 15076, 'asthma': 1638, 'bot': 2494, 'nauseas': 9971, 'chick': 3261, 'fila': 5750, 'crampin': 3927, 'birdies': 2242, 'decrease': 4335, 'sinhalenfoss': 13115, 'ep22': 5275, 'kpor': 8377, 'blocking': 2339, 'cops': 3829, 'tori': 14774, 'remains': 12035, 'latest': 8514, 'channels': 3160, 'view': 15506, 'makin': 9117, 'lenny': 8618, 'bulgaria': 2733, 'maroon': 9205, 'jane': 7947, 'albums': 1173, 'aaaaw': 867, 'tennis': 14383, 'emotional': 5188, 'kan': 8173, 'smfh': 13277, 'reactin': 11828, 'pilots': 11057, 'airline': 1153, 'unpaid': 15294, 'd53dmn': 4150, 'parody': 10775, 'moyles': 9776, 'booomb': 2449, 'mocking': 9612, '12hr': 55, 'ugg': 15166, 'kaylen': 8202, 'finna': 5791, 'uuurgg': 15400, 'marley': 9202, 'earphones': 5012, 'wnna': 16070, 'fkin': 5836, 'costly': 3861, 'jewelry': 8020, 'designer': 4456, '1600th': 89, 'contacting': 3756, 'peaceful': 10864, 'everythin': 5394, 'rushing': 12431, '103f': 31, 'fevered': 5715, 'preschooler': 11406, 'havin': 6988, 'corona': 3845, 'woulda': 16168, 'buddies': 2703, 'pero': 10932, 'yata': 16303, 'boyssss': 2534, 'rafting': 11747, 'tripics': 14926, 'ap': 1439, 'gov': 6567, 'total': 14793, 'nerd': 10037, 'cases': 3016, 'beasted': 2045, 'bctiny': 2026, 'po4me': 11205, 'milestones': 9479, 'projects': 11516, 'contest': 3765, 'groupies': 6695, 'stalkers': 13670, 'buzz': 2817, 'reunited': 12179, 'satisfied': 12561, 'hurting': 7466, 'nellie': 10032, 'blisters': 2334, 'bu': 2690, 'blast': 2301, 'arghhhh': 1536, 'clutters': 3490, 'shhh': 12907, 'litle': 8755, 'yumm': 16460, 'unstoppable': 15307, 'refusn': 11970, 'dissapointment': 4670, 'np': 10264, 'svm3r': 14115, 'fucken': 6144, 'recorded': 11908, 'telly': 14363, 'sesion': 12799, 'sorce': 13446, 'sincerely': 13104, 'yonkers': 16405, 'newspaper': 10078, 'furloughed': 6186, 'noooootttttttt': 10210, 'ls': 8969, 'taker': 14220, 'splodge': 13586, 'ketchup': 8240, 'arrival': 1568, 'ship': 12927, '_lou27': 728, 'glasgow': 6436, 'doh': 4745, 'exclusive': 5440, 'cruel': 4021, 'stunk': 13901, 'headphones': 7032, 'buggy': 2720, 'staff': 13659, 'patrons': 10826, 'blueberry': 2370, '_supernatural_': 814, '4w8cw': 342, 'mishaaaaaaaa': 9548, 'campaign': 2906, 'held': 7103, 'essential': 5329, 'psps': 11572, 'il': 7560, 'delays': 4378, 'cancellations': 2923, 'bite': 2260, 'sympathize': 14172, 'ganna': 6260, '_erica': 633, 'lonnngg': 8864, '26': 176, 'beatrice': 2053, 'guide': 6743, 'hallway': 6859, 'worries': 16155, 'ambers': 1288, '4am': 306, 'cooked': 3802, 'todayy': 14695, 'bounds': 2511, 'whyd': 15954, 'bailey': 1873, 'clipped': 3450, 'billing': 2227, 'fee': 5674, 'countries': 3878, 'setup': 12809, '5000': 367, 'mamma': 9135, 'bore': 2475, 'hat': 6967, 'mir': 9532, 'echt': 5044, 'angetan': 1343, '90210': 512, '½sst': 16548, 'grï': 6723, '½en': 16531, 'maine': 9099, 'effing': 5105, 'torta': 14780, 'queer': 11691, 'conscience': 3736, 'efteling': 5109, 'expected': 5466, 'silverlight': 13086, 'content': 3761, 'vine': 15516, 'aspx': 1620, 'tony': 14746, 'santa': 12543, 'cruz': 4036, 'jamba': 7941, 'gah': 6228, 'roseburg': 12347, 'dreamin': 4860, 'hook': 7305, 'marlon': 9203, 'brando': 2559, 'godfather': 6482, 'browsing': 2660, 'terrible': 14400, 'simplicity': 13095, 'useful': 15368, 'watchin': 15718, 'gilmore': 6389, 'girlz': 6409, 'accomplished': 941, 'boi': 2405, 'jammin': 7946, 'stephens': 13747, 'almond': 1222, 'festivities': 5709, 'earl': 5001, 'bev': 2176, '_zol': 849, 'password': 10805, 'medium': 9334, 'heffas': 7087, 'jasmin': 7963, 'beginning': 2101, 'nita': 10147, 'excit': 5432, '2me': 205, 'u2': 15155, 'bettering': 2171, 'ach': 954, 'tire': 14654, 'reaaaally': 11824, 'batman': 1992, 'loove': 8898, 'chores': 3325, 'apt': 1501, 'hallo': 6854, 'sheffield': 12889, 'romeo': 12324, 'juliet': 8126, 'bryant': 2680, 'wallinwood': 15656, 'montagues': 9670, 'capulets': 2953, '_miss': 744, 'vh1': 15482, 'preview': 11428, 'keypad': 8256, 'sundays': 14020, '½ve': 16551, 'sj': 13155, '45pm': 298, 'cinelux': 3369, 'almaden': 1220, 'contractions': 3776, 'wxidk': 16219, 'psp': 11570, 'fathers': 5633, 'userid': 15373, 'vishnupsp': 15540, 'heylo': 7154, 'johnn': 8060, 'awesomee': 1764, 'cuzz': 4127, 'webcast': 15776, 'logged': 8821, 'jars': 7961, 'duper': 4964, 'kelly': 8224, 'tao': 14257, 'inxs': 7821, 'bass': 1977, 'yelling': 16355, 'tet': 14412, 'outing': 10588, 'marwell': 9220, 'marines': 9190, 'protect': 11542, 'serve': 12790, 'bif': 2210, 'eom': 5272, 'dishes': 4651, 'rep': 12070, 'cramps': 3929, 'november': 10258, 'maintenance': 9105, 'despair': 4465, 'manors': 9160, 'proved': 11550, 'psyched': 11574, 'gkr': 6425, 'logical': 8824, 'hollered': 7260, '3x': 282, 'dnw': 4718, 'roses': 12349, 'ins': 7713, 'daa': 4154, 'crewww': 3983, 'malaria': 9122, 'affects': 1065, 'bell': 2126, 'wormy': 16151, 'labyrinth': 8423, 'tomorrowland': 14730, 'confirmation': 3703, 'tours': 14811, 'tww': 15132, 'solving': 13388, 'logic': 8823, 'puzzles': 11645, 'englands': 5226, 'becoming': 2079, 'lamer': 8459, 'crosse': 4011, 'discrimination': 4638, 'frustrating': 6134, 'advanced': 1045, 'outdoor': 10584, 'shelter': 12895, 'glove': 6451, 'helpline': 7122, 'meann': 9311, 'medisoft': 9329, 'hammock': 6874, 'tree': 14901, 'husband': 7469, 'mf88dz': 9428, 'nt': 10270, 'mayb': 9273, 'networking': 10057, 'kennel': 8230, 'goblet': 6472, 'menu': 9391, 's0ulja': 12449, 'b0y': 1804, 'te': 14304, 'trending': 14907, 'soulja': 13471, 'itsjeff': 7890, 'garbo': 6267, 'fake': 5563, '_2gnt': 548, 'skill': 13165, 'bio': 2235, 'suckkkk': 13949, 'bahaha': 1867, 'tribbles': 14914, 'tracking': 14835, 'devices': 4498, 'htc': 7401, 'costs': 3862, 'jazzercise': 7974, 'mrs': 9789, 'underwood': 15239, 'chaperone': 3170, 'agent': 1098, 'indoors': 7669, 'buffett': 2713, 'ftw': 6141, 'appear': 1465, 'perfume': 10923, 'tarot': 14267, 'cards': 2969, 'storysize': 13812, 'liner': 8713, 'glue': 6453, 'poster': 11325, 'earnt': 5011, 'financial': 5772, 'grand': 6603, 'wheres': 15898, 'nfg': 10084, 'cow': 3900, 'epicfail': 5280, 'goals': 6469, 'knock': 8345, 'charla': 3182, 'stevie': 13756, 'reschedule': 12116, 'competition': 3641, 'sims': 13099, 'luna': 8998, '6pm': 456, 'grovelled': 6698, '9am': 532, 'mann': 9156, 'das': 4236, 'ist': 7872, 'lustig': 9014, 'armer': 1549, 'macs': 9062, 'charging': 3179, 'garantiefall': 6265, 'cleanup': 3428, 'teratoma': 14389, 'cavity': 3063, 'experiencing': 5475, 'male': 9125, 'pattern': 10828, 'baldness': 1887, '4jg09': 321, 'gh': 6359, 'attend': 1676, 'battling': 2003, 'moviesss': 9769, 'sex': 12820, 'howz': 7390, 'estrella05azul': 5337, 'wordpress': 16126, 'impressive': 7618, 'macbook': 9053, 'blink': 2329, 'billy': 2230, 'bragg': 2545, 'colin': 3549, 'blunstone': 2375, 'dump': 4951, 'hacked': 6798, 'pressure': 11419, 'subside': 13928, 'seizures': 12729, 'tumors': 14997, 'niggling': 10113, 'adoarble': 1025, 'booboo': 2431, 'md': 9300, 'noises': 10182, 'arguing': 1538, 'entertaining': 5254, 'cholesterol': 3314, 'lunchbreak': 9000, 'lactose': 8429, 'oopse': 10484, 'earnest': 5009, '40404': 289, '32665': 242, 'commentary': 3607, 'daves': 4253, 'shatner': 12868, 'frosties': 6121, 'centre': 3104, 'knackers': 8336, 'sickkkkk': 13051, 'emma': 5183, 'diversity': 4693, 'cherokee': 3249, 'ability': 900, 'grammar': 6600, 'nazi': 9977, 'elsewhere': 5161, 'intentionally': 7764, 'simply': 13096, 'malakas': 9120, 'ulan': 15190, 'behave': 2107, 'thaw': 14459, 'nicely': 10100, 'liking': 8694, 'alice': 1191, 'ramei': 11774, 'excellant': 5424, 'scene': 12608, 'translate': 14872, 'xmind': 16252, 'pie': 11030, 'buttercup': 2801, 'married': 9208, 'hehehehehe': 7095, 'includes': 7641, 'shoe': 12954, 'trading': 14842, 'songgoeswrongs': 13412, '400': 286, 'sumone': 14003, 'sweetheart': 14141, 'gahhh': 6231, 'dunnooooooo': 4959, 'confuzzzledd': 3716, '_aid16': 567, 'rub': 12392, 'cuuuute': 4124, 'gfail': 6357, 'battles': 2001, 'amazingly': 1282, 'fathom': 5634, 'ignorent': 7548, 'aholes': 1136, 'overturn': 10630, 'prop': 11533, '92': 522, 'turning': 15012, 'roo': 12326, 'oop': 10482, 'returning': 12173, 'floor': 5874, 'calcutta': 2867, 'delhi': 4383, 'lucknow': 8985, 'absence': 909, 'hur': 7455, 'spirituality': 13577, 'realizing': 11851, 'potty': 11339, 'knocked': 8346, 'spot': 13607, 'wefollow': 15809, 'perth': 10943, 'eleven': 5145, 'barbeque': 1938, 'station': 13718, 'stink': 13776, 'birthdays': 2250, 'bra': 2539, 'anyhow': 1419, 'spite': 13580, 'outta': 10598, 'space': 13503, 'xo': 16255, 'https': 7407, 'bb': 2011, 'spicy': 13562, 'lentil': 8622, 'gap': 6262, 'aaahh': 871, 'spose': 13606, 'weeek': 15795, 'fasho': 5623, 'shelters': 12897, 'natalyy': 9958, 'rotten': 12355, 'averages': 1734, 'facepanda': 5540, 'iis': 7555, 'geordanos': 6338, 'thin': 14503, 'crust': 4032, 'olives': 10417, 'peperoni': 10904, 'mushroom': 9847, 'maria': 9183, 'accidentally': 934, 'disconnected': 4627, 'catalog': 3035, 'jitsu': 8034, 'appearing': 1468, 'photoshoot': 11001, '116': 44, 'amadeus': 1266, 'mozart': 9777, 'chorus': 3327, 'ey': 5516, 'speedo': 13543, 'kudos': 8394, 'disgusting': 4649, 'goddamn': 6477, 'loooove': 8889, 'godddd': 6480, 'dothebouncy': 4796, 'smf': 13276, 'shameless': 12850, 'plugging': 11190, 'rangers': 11783, 'forum': 6003, 'bk': 2271, 'sumtime': 14007, 'soaps': 13344, 'rents': 12069, 'ahve': 1138, 'yessum': 16372, 'oct': 10344, 'speeds': 13545, 'unlimited': 15285, 'further': 6189, '34': 252, 'naman': 9927, 'films': 5764, 'insufficient': 7747, 'fundage': 6160, 'watchmen': 15720, 'keyboards': 8249, 'pohaku': 11218, '12st': 57, 'spinning': 13574, 'shack': 12832, 'localgovcamp': 8805, 'letter': 8637, 'contents': 3763, 'mybrute': 9881, 'yhana09': 16383, 'completing': 3653, 'quizzes': 11719, 'nigguh': 10114, 'bali': 1890, 'afterwards': 1082, 'smack': 13258, 'energy': 5220, 'hip': 7203, 'kicks': 8271, 'grrrr': 6706, 'bull': 2736, 'arthritus': 1581, 'hobo': 7237, 'cuy43t': 4125, 'shoo': 12960, 'woosoo': 16120, 'gant': 6261, 'driveing': 4884, 'nutts': 10297, 'goddaughters': 6479, 'categories': 3042, 'remove': 12054, 'heineken': 7098, 'yucky': 16453, 'adopt': 1027, 'slack': 13189, 'heartless': 7057, 'huwwts': 7472, 'texted': 14418, 'crawl': 3944, 'act': 973, 'lmfaoo': 8787, 'ht': 7400, 'monster': 9666, 'brush': 2676, 'possibly': 11320, 'noah': 10164, 'whahahah': 15862, 'definition': 4362, 'crackers': 3920, 'whale': 15863, 'visitor': 15548, 'reinstall': 11995, 'easily': 5021, 'elusive': 5163, 'deff': 4354, 'weekened': 15801, 'instrumentals': 7746, 'shelby': 12891, 'twp': 15126, 'yeh': 16351, 'breaking': 2576, '_sml': 802, 'elitecamp': 5149, 'beaten': 2048, 'pulp': 11604, 'impression': 7616, 'topify': 14768, 'mong': 9657, '__marie': 561, 'sticks': 13765, 'hc': 7012, 'trade': 14839, 'racoons': 11741, 'skunks': 13180, 'livin': 8772, 'shooeessss': 12961, 'logging': 8822, '200km': 138, 'errands': 5310, 'avoided': 1741, 'cinder': 3367, 'blocks': 2341, 'dantas': 4215, 'porky': 11292, 'beavs': 2063, 'rconp': 11816, 'pulling': 11603, 'vegetarian': 15449, 'soda': 13360, 'majorly': 9108, 'cutback': 4112, 'carbs': 2963, 'yummyyy': 16465, 'proposed': 11538, '15th': 85, 'conversating': 3787, 'nintendogs': 10141, 'upsetting': 15338, 'roundtable': 12363, 'hamburg': 6868, 'detlev': 4485, 'fischer': 5813, 'accessibility': 931, 'bitv': 2267, 'xing': 16249, '333622': 250, 'awesom': 1762, 'snuggle': 13337, 'wut': 16209, 'harsh': 6958, 'archuleta': 1522, 'turned': 15010, '_finn_': 640, 'ding': 4579, 'beautifullll': 2060, 'eu': 5348, 'probs': 11478, 'satellite': 12559, 'provider': 11556, 'volunteering': 15578, 'syndrome': 14181, 'indiana': 7660, 'hu': 7408, '_newamerykah_': 753, 'alllllllllllllllllllllllllllll': 1213, 'wakenbake': 15638, 'ocean': 10341, 'glass': 6437, 'experts': 5478, 'exercise': 5447, '90': 511, 'jillian': 8028, 'optimistic': 10516, 'hunny': 7451, 'nasty': 9955, 'jig': 8025, 'shakalohana': 12837, 'wavez': 15741, 'surfin': 14079, 'trak': 14862, 'hoilday': 7245, 'zebra': 16487, 'maar': 9048, 'lurk': 9011, 'mode': 9614, 'lock': 8809, 'poured': 11345, 'whiskey': 15914, 'joan': 8044, '5jbp3': 394, 'nsty': 10268, 'remind': 12043, 'blooming': 2355, 'clematis': 3434, 'princess': 11447, 'lingerie': 8716, 'parties': 10784, 'paycheck': 10844, '_langley': 719, 'igbaras': 7542, 'cheeto': 3235, 'puffs': 11589, 'tiring': 14660, '77': 471, 'faves': 5644, 'unproductive': 15298, 'grandmother': 6611, '7am': 479, 'plotting': 11186, 'muahahaha': 9807, 'jackets': 7925, 'headach': 7019, 'bust': 2790, 'accompany': 939, 'cement': 3099, 'rows': 12373, 'pixels': 11105, 'sewing': 12819, 'divorcing': 4697, 'bear': 2039, 'hank': 6905, 'thompson': 14531, '5jboh': 393, 'stumpy': 13899, 'chemics': 3244, 'smelling': 13273, 'repellant': 12074, 'stinks': 13777, 'marykay': 9222, 'sendai': 12748, 'hopped': 7330, 'swwwaaaaggg': 14163, 'oooonnnn': 10477, 'england': 5225, '_uk': 826, 'myst': 9896, 'toooo': 14756, 'isis': 7860, 'tapeworm': 14260, 'wanting': 15671, 'chaper': 3169, 'beover': 2148, 'huggles': 7422, 'hovering': 7380, 'pissy': 11093, 'decisions': 4325, 'chauffer': 3202, 'tmrw': 14678, 'ned': 9998, 'beathroom': 2050, 'outsie': 10596, 'daylight': 4265, 'deadlines': 4292, 'cuppa': 4086, '_starr': 808, 'hawkesbury': 6995, 'windsor': 16005, 'dj': 4703, 'fcs': 5658, 'suggest': 13972, 'algae': 1185, 'antics': 1403, 'parella': 10762, 'marcia': 9175, 'twitte': 15088, 'celebrating': 3087, 'generations': 6324, 'bleeeeah': 2315, 'stairs': 13664, 'toe': 14701, 'wasssup': 15707, 'cocoliciousness': 3519, 'nhl': 10089, 'pens': 10896, 'kings': 8305, '2010': 140, 'errr': 5314, 'espressos': 5325, 'latte': 8518, 'noon': 10195, 'mish': 9546, 'ladie': 8432, 'jungle': 8138, 'hotel': 7361, '_mcflyy': 738, 'finaly': 5771, 'mwahaha': 9875, 'podcasting': 11212, 'snap': 13307, 'nod': 10170, 'thxs': 14587, 'whine': 15908, 'slo': 13235, 'intellectually': 7756, 'severing': 12817, 'disagreement': 4609, 'peppermint': 10906, 'mochas': 9609, 'frappachinos': 6038, 'addicti': 995, 'hoo': 7302, 'crossing': 4013, 'hacks': 6800, 'humbug': 7435, 'nh': 10088, 'bï': 2832, 'vï': 15601, '½o': 16543, 'kho': 8262, 'hypervenilating': 7489, 'keith': 8219, 'urban': 15348, 'belt': 2134, 'easactive': 5018, 'paused': 10835, 'resistance': 12129, 'torn': 14776, 'morn': 9709, 'mirror': 9534, 'luvvv': 9022, 'aquestion': 1506, 'moon': 9686, 'recommen': 11898, 'bes': 2152, 'rockstar': 12299, 'claps': 3403, 'forwar': 6004, 'kidney': 8279, 'tonsils': 14745, 'hilariously': 7192, 'champagne': 3142, 'slingalink': 13228, 'evice1': 5399, 'daughters': 4250, 'kindergarden': 8297, 'xd': 16247, 'viggo': 15510, 'miami': 9440, 'boyle': 2532, 'ridding': 12229, 'dangero': 4206, 'waaaaaaa': 15607, 'throbbing': 14560, 'nooooothing': 10209, 'studyhall': 13890, '_h': 664, 'daisy': 4173, 'amazin': 1280, 'bunch': 2755, 'homes': 7280, 'alrightttt': 1241, 'netball': 10051, 'nowww': 10263, '600th': 417, 'byeeeee': 2828, 'soothes': 13439, 'coldplay': 3545, 'twittering': 15103, 'jag': 7932, 'alternator': 1255, 'clock': 3452, 'frd': 6044, 'mission': 9561, 'yung': 16466, 'scariest': 12602, 'rick': 12222, 'thurs': 14583, 'nikeplus': 10130, 'arrive': 1569, 'acting': 974, 'rear': 11860, 'marilyn': 9188, 'seria': 12782, 'asses': 1625, 'overwhelmed': 10631, '_emily_young_': 630, 'okey': 10403, 'gaining': 6236, 'bread': 2570, 'peanut': 10866, 'dragging': 4835, 'lied': 8664, 'baddd': 1851, 'slap': 13193, 'stretch': 13855, 'meme': 9368, 'hop': 7316, 'por': 11290, 'favor': 5645, 'dzcpg3': 4987, 'chained': 3130, 'webcom': 15777, 'yul': 16456, 'chai': 3128, 'checkin': 3213, 'hottest': 7366, 'assing': 1631, 'g1': 6210, 'biznesssss': 2269, 'paniniii': 10735, 'ange': 1338, '_phillips': 773, 'stock': 13784, 'hoped': 7318, 'cheaptweet': 3207, 'syopvd': 14182, 'sin': 13101, 'shamed': 12849, 'unemployed': 15243, 'meself': 9401, 'whack': 15861, 'tweethug': 15050, '21u': 152, 'elephants': 5141, 'magnificent': 9085, 'pleease': 11174, 'sonetime': 13410, 'flavor': 5856, 'belgian': 2117, 'considered': 3739, 'truss': 14959, 'failing': 5551, 'strip': 13862, 'mikee': 9471, 'mike': 9470, 'miffed': 9461, 'tbh': 14299, 'vic': 15491, '_da_': 611, '60': 414, 'mangoes': 9151, 'baka': 1876, 'naglilihi': 9914, 'ka': 8158, 'lang': 8476, 'carmen': 2989, 'respect': 12131, 'dryer': 4915, 'kno': 8344, 'shineeeee': 12922, 'fk': 5835, 'replenished': 12081, 'bathing': 1988, 'suits': 13981, 'anthony': 1398, 'rapp': 11790, 'defective': 4350, 'skid': 13163, 'zoning': 16508, 'shul': 13025, 'jon': 8072, 'kate': 8187, 'drove': 4899, 'mazda': 9279, 'rx8': 12442, 'angela': 1340, 'utterly': 15395, 'yaaaay': 16278, 'tila': 14622, 'tierd': 14607, 'delivery': 4394, 'fringe': 6110, 'comix': 3600, 'radio': 11744, 'disney': 4660, 'guilty': 6748, 'yogulicious': 16398, 'sour': 13481, 'sally': 12503, 'competitor': 3645, 'chances': 3148, 'minimal': 9511, 'po': 11204, 'tail': 14212, '_01': 538, 'snow': 13333, 'solitaire': 13382, 'fiddle': 5731, 'playable': 11144, 'joker': 8069, 'ps3': 11566, 'exclusivity': 5441, 'arkham': 1545, 'twistory': 15080, 'diapers': 4525, 'feeding': 5678, 'iam': 7496, 'greaaat': 6637, 'adress': 1036, 'loco_crime_1st': 8813, 'wha': 15856, 'hacky': 6801, 'sack': 12458, 'sans': 12542, 'fuckn': 6146, 'drew': 4873, 'macdonalds': 9056, 'mango': 9150, 'medley': 9335, 'dammm': 4184, 'considerably': 3738, 'sorr': 13451, 'babysit': 1823, 'nare': 9948, 'nightall': 10118, 'frankie': 6035, 'claires': 3401, '_lord': 727, 'overcast': 10610, 'ohhhhh': 10387, 'brunch': 2672, 'wide': 15963, 'zimmer': 16501, 'frame': 6028, '_hall': 666, 'homies': 7288, 'answered': 1391, 'postal': 11322, 'upcoming': 15317, 'documents': 4729, 'jog': 8056, 'pat': 10813, 'darren': 4230, 'linkedin': 8720, 'darrenmonroe': 4231, 'tucson': 14989, '18th': 106, 'finest': 5782, 'vol': 15571, 'lookn': 8873, 'colorado': 3568, 'sunrise': 14032, 'caity': 2858, 'wossy': 16164, 'brum': 2670, 'controversial': 3785, 'estimate': 5334, 'timeframe': 14631, 'colab': 3541, 'sw': 14116, 'hopefull': 7321, 'hooray': 7313, 'healing': 7038, 'matey': 9250, 'ssssssssssmack': 13649, 'ww': 16213, 'digusted': 4567, 'wroclaw': 16198, 'poland': 11231, 'kosmo': 8374, 'eovvn': 5273, 'buried': 2764, 'entourage': 5262, 'bittersweet': 2265, 'bleaching': 2308, 'yeahhhh': 16328, 'ali': 1188, 'justwatched': 8152, 'escaped': 5318, 'discovery': 4635, 'imprisoned': 7619, 'newlyweds': 10072, 'loosing': 8897, 'bbbbrrrrrrrr': 2012, 'tweeters': 15047, '_rocks': 785, 'spurted': 13626, 'fanta': 5597, 'uno': 15290, 'rprl0': 12378, 'midnight': 9457, 'fri': 6087, 'lower': 8965, 'stings': 13775, 'sugar': 13971, 'havnet': 6991, 'butlers': 2797, 'hispanic': 7209, 'hendrix': 7127, 'cosmos': 3856, 'ebm': 5039, 'waaaay': 15611, 'katherine': 8189, 'zachy': 16479, 'triple': 14927, 'stocktwits': 13787, 'clutter': 3489, 'twittersphere': 15112, 'goody': 6520, 'iptv': 7834, 'application': 1478, 'compiled': 3646, 'slowly': 13246, 'sorting': 13463, 'musiq': 9857, 'soulchild': 13470, 'hamilton': 6871, 'ugggghhhh': 15167, 'begun': 2105, 'documentation': 4728, 'viewers': 15507, 'eagle': 4996, 'ustream': 15383, 'funnyy': 6181, 'original': 10546, 'obu': 10329, 'ryaaaaaaaaaaaaan': 12444, 'snjen': 13326, 'bride': 2603, 'pinch': 11061, 'foreigner': 5967, '2xjoc': 222, 'cloooseee': 3458, 'mocha': 9608, 'frapp': 6037, 'simple': 13094, 'safari': 12480, 'toolbar': 14753, 'calculator': 2865, 'calcs': 2864, 'havaianas': 6981, 'molded': 9632, 'watchers': 15717, 'shiggity': 12915, 'shwa': 13033, 'coda': 3521, '99usd': 531, 'sunscreen': 14034, 'simpson': 13097, 'shezza': 12905, 'paula': 10832, 'ohshnapsss': 10391, 'blair': 2289, 'yeeeah': 16343, 'bake': 1877, 'cookies': 3806, 'bck': 2024, 'thts': 14573, 'tricks': 14919, '_kearley': 705, 'egg': 5110, 'whites': 15922, 'grain': 6599, 'interviewed': 7786, 'wil': 15977, 'gardeners': 6270, 'statistic': 13720, 'playback': 11145, 'comet': 3590, 'sensex': 12757, 'travis': 14891, 'clark': 3406, 'hilaaaaarious': 7190, 'subscriptions': 13927, 'nzz': 10315, 'economist': 5050, 'costsavings': 3863, 'luckily': 8982, 'versions': 15473, 'hiya': 7222, 'exhilerating': 5455, 'william': 15985, 'kong': 8368, 'airplane': 1154, 'payday': 10846, 'fishes': 5815, 'kindest': 8299, 'roomies': 12332, 'clwn': 3491, 'celebrated': 3086, 'softball': 13367, 'robina': 12284, 'favourtie': 5652, 'axsujx': 1798, 'awes': 1760, 'gooooooooood': 6540, 'texting': 14420, 'virus': 15536, 'tweeps': 15037, 'worksheet': 16142, 'wb': 15754, 'sniffle': 13321, 'cornish': 3843, 'countryside': 3880, 'british': 2625, 'hol': 7246, 'ailun': 1145, '6rww': 457, 'reeeally': 11944, 'stumble': 13898, 'weeee': 15794, 'coordinating': 3824, 'lovebug': 8934, 'weepies': 15806, 'goodnights': 6515, 'agh': 1103, 'pleaser': 11169, 'foreigners': 5968, 'pretty_mess': 11427, 'psychology': 11576, 'statements': 13714, 'parallel': 10755, 'devastated': 4490, 'seat': 12700, 'dreamwidth': 4865, 'martin': 9214, 'japanese': 7957, 'rochelle': 12290, 'cater': 3044, 'produce': 11488, 'houses': 7377, 'smelled': 13272, 'alley': 1204, 'maaad': 9044, 'basement': 1964, 'whitley': 15923, 'diwali': 4698, 'saddi': 12471, 'dilli': 4573, 'disorder': 4661, 'jjj': 8037, 'gal': 6238, 'pals': 10719, 'phoenix': 10987, 'editions': 5071, 'greatest': 6642, 'purse': 11632, 'soulmate': 13472, 'frustraded': 6132, '_bob7': 587, 'vet': 15479, 'xrays': 16262, 'enter': 5248, 'giveaway': 6417, 'elmo': 5156, 'finshed': 5793, 'annnd': 1373, '44am': 295, 'sleeps': 13214, 'lift': 8677, 'splints': 13584, 'fey': 5718, 'slays': 13199, '30rock': 233, 'inbruges': 7632, 'hitman': 7216, 'tale': 14227, 'pale': 10711, 'shirtless': 12935, 'footballers': 5951, 'attractive': 1685, 'tweetie': 15051, 'ghetto': 6363, 'seo': 12766, 'voodoo': 15582, 'noarchive': 10165, 'hides': 7172, 'beast': 2044, 'riding': 12235, '_holden': 674, 'herself': 7148, 'decides': 4322, 'client': 3442, 'embracing': 5179, 'tears': 14316, 'shallow': 12845, 'airy': 1157, 'ttyl': 14982, 'moods': 9684, 'frickin': 6089, 'slithering': 13233, 'snakes': 13306, 'frightening': 6107, 'detail': 4480, 'preparing': 11401, 'sermons': 12788, 'powerpoint': 11356, 'slides': 13224, 'ed': 5057, 'speedracher': 13544, 'strongly': 13871, '12seconds': 56, 'spock': 13587, 'prototype': 11546, 'mentor': 9390, 'heyya': 7156, 'formulas': 5988, 'sudoku': 13960, 'theirs': 14472, 'giant': 6369, 'wet': 15852, 'stinky': 13778, 'loaner': 8801, 'bein': 2111, '7day': 480, 'moro': 9717, 'kimba': 8293, 'diaries': 4526, 'nicky': 10106, 'bette': 2169, 'blumenthal': 2374, 'toodaayy': 14749, 'researching': 12124, 'dislike': 4655, 'dublin': 4929, 'whatcha': 15871, 'originally': 10547, 'thankujesus': 14443, 'beyeblessed': 2180, 'traces': 14831, 'rdj': 11820, 'dreadfully': 4855, 'twitterology': 15106, 'offshore': 10379, 'behalf': 2106, 'cet0': 3117, 'searching': 12694, 'wavy': 15743, 'hairstyle': 6847, 'fondue': 5932, 'lava': 8536, '10pm': 40, 'rate': 11795, 'inputting': 7710, 'excitin': 5437, 'shade': 12834, 'native': 9964, 'wildflowers': 15980, 'notion': 10248, '_monk': 747, 'elp': 5158, 'medication': 9326, 'twilightguy': 15073, 'kalebnation': 8167, 'spamming': 13512, 'cashis': 3021, 'jenkins': 7998, 'shitttt': 12942, 'blockbuster': 2336, 'zealand': 16486, 'coraline': 3834, 'puke': 11596, 'ahaa': 1116, 'techno': 14326, 'backs': 1843, 'ui': 15187, 'snappier': 13309, 'bugging': 2719, 'launched': 8529, 'below': 2133, 'hadfr': 6804, 'smacked': 13260, '_in_forks': 687, 'static': 13716, 'bar': 1935, 'shatranjanpoli': 12869, '26498457': 178, 'andheri': 1329, '26733333': 179, 'ki': 8263, 'jai': 7934, 'ho': 7234, 'saddened': 12467, 'husker': 7470, '66st1': 435, 'freee': 6060, 'loongerrr': 8877, 'sadface': 12473, 'sob': 13345, 'girrrrrrrrlll': 6413, 'jeeeeez': 7986, 'misha': 9547, 'directing': 4596, 'whywhywhy': 15956, 'essays': 5328, 'killer': 8286, 'manage': 9140, 'champions': 3144, 'suffolk': 13969, 'mb': 9283, 'neil': 10028, 'jodi': 8051, 'terminator': 14393, 'trs': 14950, 'wikipedia': 15976, 'brighton': 2613, 'esp': 5321, 'iiii': 7554, 'handedly': 6885, 'crane': 3931, 'hairbrush': 6837, 'disinfect': 4652, 'beware': 2179, 'visialvoicemail': 15541, 'sync': 14177, 'seamless': 12689, 'toro': 14777, 'quebec': 11688, 'tripped': 14928, 'awarded': 1748, 'dork': 4788, 'appointment': 1483, 'earned': 5008, '_devil1': 617, 'quiz': 11716, 'wooden': 16101, 'couch': 3867, 'guessed': 6737, 'phoenixfm': 10988, 'php': 11005, 'herniated': 7141, 'disc': 4622, 'lluuvv': 8780, 'evenn': 5367, 'moreee': 9703, 'antidisestablishmentarianism': 1404, 'excruciating': 5442, 'heeder': 7077, 'irma': 7843, 'vep': 15467, 'rehearsals': 11991, 'moveable': 9762, 'fox': 6020, 'refresh': 11958, 'mimcy': 9495, 'mornining': 9715, 'easton': 5028, 'loner': 8851, 'difference': 4548, 'whatsoever': 15877, 'tpt': 14828, 'absolutly': 913, 'stressors': 13854, 'force': 5959, 'becase': 2069, 'foods': 5942, 'viruses': 15537, 'luvly': 9019, 'recory': 11912, 'disconnects': 4628, 'uber': 15157, 'steam': 13739, 'willdo': 15984, 'anythig': 1425, 'acknowledge': 966, 'inevitable': 7675, 'opendns': 10493, 'dns': 4716, 'goooooodmorning': 6537, 'sleeeeeeepz': 13201, 'myhouse': 9884, 'gusying': 6767, 'bourbon': 2514, 'branch': 2554, 'zeitgeist': 16488, 'shh': 12906, 'snob': 13328, 'scarred': 12605, 'amie': 1302, 'cody': 3525, 'ddyyd6': 4287, 'boooooooo': 2453, 'lopatcong': 8899, '4w1s0': 336, 'carey': 2978, '_bloggerific': 586, 'innocent': 7704, 'sell': 12738, 'profitable': 11501, 'graders': 6591, 'tigers': 14613, 'omfg': 10427, 'cleveland': 3435, 'grass': 6625, 'majors': 9109, 'rejoice': 12001, 'ultra': 15194, 'firmware': 5808, 'installed': 7735, 'ard': 1523, 'earn': 5007, 'dissapear': 4666, 'pastors': 10811, 'bris': 2619, 'per': 10909, 'distant': 4675, 'relative': 12006, 'heir': 7100, '5k': 401, '250k': 172, 'clouds': 3474, 'sleepover': 13211, 'generation': 6323, 'shudder': 13021, 'reclaim': 11891, 'funty': 6182, 'september': 12772, 'ridiculous': 12232, '16': 87, 'hungraaaaaaaaay': 7448, 'shorter': 12981, 'borgata': 2483, 'nyappy': 10307, 'boating': 2392, 'songwriting': 13414, 'cape': 2945, 'cod': 3520, 'gooooooood': 6539, 'tomoroo': 14727, 'giid': 6384, 'gni': 6463, 'linuxoutlaws': 8726, '4jcjj': 319, 'boagsie': 2386, 'folkestone': 5917, 'cinema': 3370, 'escape': 5317, 'longg': 8857, 'neighborhood': 10023, 'runners': 12422, 'ty': 15138, 'auntiegail': 1706, 'picking': 11020, 'vis': 15538, 'vests': 15478, 'auntie': 1705, 'gails': 6233, 'childminding': 3270, 'sic': 13040, 'attendances': 1677, 'boingo': 2410, 'connected': 3728, 'fuss': 6193, 'snail': 13304, 'minday': 9499, 'arlington': 1546, 'zeta': 16498, '5z4uq': 412, 'retardeddddddd': 12159, '1300': 61, 'sicky': 13054, 'headline': 7029, 'susan': 14096, 'quits': 11714, 'josh': 8084, 'lured': 9009, 'memorial': 9370, 'hurdles': 7457, 'currency': 4096, 'ironpoodonia': 7848, 'uuu': 15399, 'nou': 10253, 'yeey': 16349, 'epenis': 5276, 'gas': 6280, 'pissing': 11090, 'weiters': 15825, 'rf': 12201, 'eggs': 5111, 'stove': 13814, 'boil': 2406, 'worthless': 16162, 'jorx': 8081, 'jail': 7935, 'stalker': 13668, 'belongs': 2131, 'minnesota': 9517, '4more': 329, 'infection': 7678, 'higher': 7176, 'bing': 2233, 'smarter': 13267, 'classier': 3412, 'fillin': 5758, 'commence': 3604, 'pouting': 11348, 'returns': 12174, 'readysetgo': 11836, 'yeahh': 16326, 'uhhh': 15184, 'mia': 9439, 'evie': 5402, 'capture': 2952, 'violently': 15522, 'phrase': 11008, 'hater': 6972, 'yoda': 16396, 'psyching': 11575, 'deodorant': 4420, 'lungs': 9007, 'bleed': 2312, 'inhaled': 7688, 'constantly': 3744, 'females': 5697, '_rankin': 777, 'meat': 9317, '_tina': 821, '_adriii': 565, 'lindt': 8710, 'cafes': 2849, 'offhand': 10369, 'crowdsourcing': 4015, 'graphics': 6623, 'backgroung': 1835, 'ppicture': 11361, 'mostly': 9737, 'todd': 14698, 'mgmt': 9433, 'atbfm': 1651, 'solidsyn': 13381, 'realise': 11843, 'organiser': 10540, 'dun': 4954, 'performing': 10921, 'artist': 1586, 'djs': 4706, 'malaysian': 9124, 'laws': 8542, 'dolls': 4754, 'sailor': 12490, 'webdu': 15779, 'conf': 3695, 'depends': 4425, 'owe': 10639, 'jump': 8131, 'pout': 11347, 'maccies': 9054, 'difficulty': 4557, 'shiit': 12919, 'pig': 11039, 'saturdays': 12566, 'colouring': 3574, 'gymnastics': 6783, 'gils': 6390, 'eeee': 5084, 'agreed': 1109, 'brookings': 2647, 'geek': 6306, 'gender': 6319, 'mens': 9381, 'cryyy': 4045, 'katies': 8194, 'bronchitis': 2644, 'bangalore': 1916, 'ballerina': 1896, 'chucking': 3345, 'edumedia09': 5078, 'goooood': 6530, 'sg': 12827, 'sana': 12521, 'jurong': 8144, 'guttered': 6772, 'ruby': 12397, 'eliminated': 5147, 'nzntm': 10314, 'supporting': 14066, 'locals': 8806, 'hosanna': 7350, 'bleh': 2316, 'proper': 11534, 'coding': 3524, 'gimme': 6391, 'situation': 13143, 'mandriva': 9146, 'unloved': 15286, 'bin': 2231, 'stressd': 13847, 'ako': 1165, 'tayo': 14298, 'toasty': 14689, 'montel': 9675, 'mommie': 9642, 'uuuugh': 15401, '_era': 632, 'powerdvd': 11354, 'daisies': 4172, 'screenies': 12663, 'icons': 7516, 'protection': 11543, 'bonkers': 2425, 'invited': 7813, 'togethers': 14705, 'playin': 11150, 'security': 12712, 'wallet': 15655, 'goooosh': 6543, 'rt': 12390, 'lar': 8491, 'mainland': 9100, 'police': 11236, 'representatives': 12096, 'owm': 10644, 'tryinh': 14968, 'o2': 10317, 'goooooood': 6538, 'prejudice': 11396, 'gr': 6577, 'frank': 6034, 'pixies': 11106, 'vomiting': 15580, 'sniffly': 13323, 'bens': 2145, 'nightshift': 10124, 'soooooooooooooooooooooooooooooooooooo': 13437, 'txtin': 15135, 'audiotistic': 1695, 'soooory': 13438, 'rested': 12142, 'russel': 12433, 'ponderland': 11252, 'timing': 14637, 'exploring': 5487, 'atlas': 1660, 'hercules': 7133, 'craters': 3939, 'uma': 15197, 'oph': 10505, 'ser': 12775, 'caput': 2954, 'saturn': 12567, 'fogged': 5910, 'eyepiece': 5520, '2a': 187, 'alllllllllllllllll': 1212, '_recordings': 780, 'occuring': 10340, '8th': 509, 'hm': 7225, '_hypnotic': 677, 'ilost': 7571, 'iappreciate': 7498, 'supermodel': 14048, 'python': 11653, 'conversion': 3791, 'extranet': 5512, 'promote': 11528, 'jcdecaux': 7979, 'counts': 3881, 'sinner': 13119, 'femme': 5700, 'fatale': 5629, 'photographers': 10998, 'greastest': 6640, 'gode': 6481, 'ould': 10576, 'recording': 11909, 'blogtv': 2348, 'overnight': 10619, 'printing': 11453, 'shipping': 12931, 'urs': 15358, 'tomo': 14724, 'awareness': 1751, 'principle': 11450, 'mourners': 9757, 'arggghhhhhhhhhhh': 1533, 'fortuna': 6000, 'prints': 11454, 'goodbyeeee': 6507, 'dilemma': 4571, 'lag': 8442, 'spahkly': 13507, 'simon': 13092, 'personality': 10940, 'boyfrann': 2530, 'mines': 9504, 'sprung': 13624, 'pencils': 10889, 'sharp': 12865, 'sharpened': 12866, 'pencil': 10888, 'philosophy': 10980, 'funding': 6162, 'stalking': 13672, 'thumbs': 14577, 'wheat': 15885, 'potatoes': 11335, 'protein': 11544, 'nooo': 10198, 'crush': 4030, 'heehee': 7084, 'shadduppp': 12833, 'cinemas': 3371, 'butts': 2807, 'tortellini': 14781, 'notoriously': 10249, 'unreliable': 15303, 'grid': 6670, 'iqbal': 7836, 'dept': 4437, 'barry': 1954, 'gibb': 6372, 'williamssssss': 15986, 'lexus': 8647, 'hopw': 7331, 'calyx': 2891, 'cf2yuj': 3118, 'brainbone': 2550, 'c9ryqc': 2838, 'stereo': 13751, 'skyline': 13183, 'razr': 11811, 'sonny': 13417, 'mystery': 9899, 'thoughh': 14541, 'crusty': 4033, 'aawww': 885, 'insults': 7750, 'cries': 3987, 'clarify': 3405, 'poo': 11257, 'maï': 9282, 'asos': 1618, 'coupons': 3885, 'horseback': 7346, 'jodies': 8052, 'dudes': 4938, 'clowns': 3478, 'nesbitt': 10047, 'cutest': 4116, 'dslr': 4921, 'parent': 10763, 'trap': 14878, 'created': 3961, 'believed': 2120, 'laff': 8440, 'mass': 9232, 'papers': 10745, 'shitt': 12941, 'funkey': 6169, 'xmlrpc': 16254, 'codeignite': 3523, 'output': 10591, 'amf': 1300, 'zend': 16493, 'closely': 3461, 'interfere': 7771, 'insulting': 7749, 'fare': 5605, 'blockparty': 2340, '_d_': 610, 'increasingly': 7652, 'elevator': 5144, 'danas': 4196, 'roseanne': 12346, 'jerk': 8006, 'pets': 10951, 'attraction': 1684, 'assembly': 1624, 'required': 12109, 'traumatized': 14883, 'edge': 5063, 'table': 14193, 'bruise': 2666, 'solid': 13380, 'twisted': 15078, 'jared': 7959, 'yee': 16341, 'asylm': 1648, 'disorganized': 4663, 'everyond': 5388, 'hollaback': 7258, 'geocaching': 6335, '_song': 803, 'wondrous': 16096, 'beltaine': 2135, 'greek': 6651, 'resturant': 12148, 'johnny': 8061, 'oz': 10660, 'adding': 1001, 'property': 11536, 'ruin': 12404, 'bakers': 1879, 'spider': 13563, 'calming': 2886, 'fears': 5661, 'greeeeen': 6650, 'thorny': 14534, 'bushes': 2784, 'sayin': 12584, 'requirements': 12110, 'grtsat': 6713, 'bggeting': 2192, 'mcmcbuddy': 9297, 'rainbow': 11757, 'burritos': 2776, 'cancellation': 2922, 'beaumont': 2057, 'chillaxing': 3279, 'botcon': 2495, 'darl': 4225, 'mua': 9805, 'tracie': 14832, 'weaver': 15771, 'scrabbled': 12647, 'creativity': 3964, 'pinned': 11078, 'sarcy': 12552, 'sickness': 13053, 'toes': 14702, 'xuxu': 16263, 'aaauuuggghhh': 874, 'mcdonald': 9288, 'cheetos': 3236, 'hehehe': 7092, 'condom': 3693, 'boxes': 2526, 'geez': 6312, 'tow': 14815, 'ahhhhhhhh': 1133, 'pridelines': 11438, 'dresss': 4872, 'haul': 6978, 'wagner': 15623, 'bend': 2137, 'f1': 5526, 'dads': 4167, 'discussions': 4642, 'anurag': 1410, 'placement': 11115, 'charge': 3175, 'argument': 1539, 'aptitude': 1502, 'heeey': 7083, 'kitchen': 8317, 'brooklyn': 2648, 'horror': 7343, 'bigi': 2216, 'fashion': 5620, 'spree': 13616, 'sih': 13077, '____': 554, 'rated': 11796, 'youuuuuu': 16442, 'labs': 8422, 'stead': 13730, 'dissapoint': 4668, 'methinks': 9418, 'depresse': 4430, 'hoedown': 7240, 'throwdown': 14566, '_refugee_': 781, 'exiting': 5461, 'linux': 8725, 'amusing': 1320, 'rubiks': 12395, 'recyle': 11924, 'salvation': 12508, 'army': 1552, 'recycle': 11921, 'pervert': 10945, 'mario': 9191, 'kart': 8185, 'structure': 13874, 'suffication': 13967, 'overall': 10609, 'forecast': 5964, 'peckish': 10872, 'wah': 15625, 'pleaseeeeeeeeeeeeee': 11168, 'reuters': 12181, 'anyarticle': 1416, 'rdt': 11821, 'url': 15355, 'bigmoney': 2217, 'idus1284981420090508': 7534, 'renouncing': 12064, 'worms': 16150, 'un': 15206, 'lean': 8571, 'bandung': 1912, 'doggy': 4743, 'vegan': 15444, 'four': 6017, '5z3ij': 410, 'aila': 1144, 'cyclone': 4138, 'sunshade': 14036, 'balcony': 1885, 'labeled': 8418, 'electric': 5131, '28ï': 184, 'vancity': 15429, 'data': 4240, 'customer': 4109, 'entered': 5249, 'elses': 5160, 'wherever': 15899, 'reminding': 12047, 'chipotle': 3294, 'bunuelos': 2759, 'pen': 10886, '6urrxta8dom': 459, 'winning': 16016, 'crutch': 4034, 'tattoos': 14290, 'peircings': 10885, 'hahahaha': 6819, 'cheaper': 3206, 'carries': 3002, 'foreverrr': 5973, 'wal': 15643, 'mart': 9212, 'dubai': 4926, 'ughhh': 15174, 'doggone': 4742, 'ponderosa': 11253, 'progress': 11511, 'individually': 7665, 'loafing': 8800, '_fam': 637, 'julyish': 8129, 'mothersday': 9741, 'impose': 7610, 'formidable': 5986, 'waters': 15728, 'casual': 3032, 'offense': 10363, 'orange': 10525, 'beetle': 2097, 'whoo': 15938, '200lbs': 139, 'brody': 2638, 'ventura': 15462, '_gal': 649, 'debugging': 4311, 'decemberists': 4317, 'wreckers': 16182, 'tooooo': 14757, 'regrettin': 11986, 'duster': 4967, 'shoppping': 12970, 'greenhills': 6654, 'balikbayans': 1891, 'slum': 13250, 'landlords': 8472, 'backed': 1833, 'sewage': 12818, 'inspiration': 7728, '20gf': 143, 'insomniac': 7724, 'daphne': 4217, 'wif': 15968, 'meeee': 9340, 'toniight': 14741, 'bruh': 2665, 'erection': 5298, 'stroll': 13867, 'beaches': 2034, 'jmjb': 8042, 'pincode': 11063, 'expired': 5479, 'sea': 12686, 'duff': 4942, 'muthafuckin': 9868, 'moovie': 9697, 'sited': 13136, 'sara': 12548, 'cart': 3007, 'si': 13037, 'fog': 5909, '_aston': 573, 'applications': 1479, 'starring': 13694, 'wwe': 16215, 'wrestler': 16185, 'kane': 8174, 'york': 16413, 'distraction': 4680, 'smoothie': 13296, 'predicted': 11388, 'knowledgable': 8352, 'tp': 14825, 'tetep': 14413, 'kurang': 8396, '3bt': 267, 'buat': 2692, 'shooting': 12965, 'kamis': 8172, 'nih': 10128, 'huaaaa': 7409, 'gila': 6385, 'syapa': 14166, 'lg': 8650, 'craptastic': 3934, 'girrrrl': 6412, 'alyssa': 1261, 'weeps': 15808, 'sixty': 13149, 'seek': 12719, 'planned': 11127, 'anne': 1368, 'judy': 8118, 'mysweetebony': 9901, 'lmk': 8790, 'paysite': 10853, 'everyon': 5387, 'arent': 1529, 'abc': 894, 'tragedy': 14849, 'france': 6031, 'overcome': 10611, 'negative': 10016, 'reaction': 11829, 'netherlands': 10052, 'waaa': 15605, '_21thanks': 546, '_chapman': 597, 'whuffaoke': 15952, 'gillette': 6388, 'stadium': 13658, 'undergarments': 15227, 'recap': 11876, 'wasting': 15712, 'pinkberry': 11072, 'embarrass': 5171, 'yeaterday': 16338, 'hahahha': 6828, 'swear': 14122, 'mar': 9168, 'suss': 14105, 'chap': 3166, 'arabs': 1509, 'insist': 7722, 'ochh': 10343, 'blister': 2332, 'supose': 14056, 'yas': 16302, 'thaught': 14458, 'thee': 14467, 'lovley': 8960, 'nuttin': 10296, 'topman': 14769, 'lolipop': 8834, 'esploder': 5323, 'paperwork': 10747, 'pleaseeeee': 11165, 'birthdayyyy': 2251, 'bookmarked': 2441, 'vinegar': 15517, 'witdrawal': 16040, 'tie': 14605, 'demos': 4406, 'cher': 3248, 'warmly': 15686, 'disgusted': 4648, 'effort': 5106, 'shwasty': 13034, 'shloshed': 12949, 'siberia': 13039, '8mm': 507, 'maximum': 9268, 'listenin': 8745, 'girl5': 6400, 'aaron': 883, 'sunnys': 14030, '_127': 543, 'thorw': 14536, 'varsity': 15437, 'fanclub': 5587, 'sway': 14121, 'meow': 9393, 'trippin': 14929, '32': 241, 'tabby': 14192, 'shore': 12972, 'league': 8567, 'diabetes': 4517, 'mountain': 9754, 'dew': 4501, '_slamma_': 801, 'tixs': 14670, 'lolzor': 8845, 'naturally': 9967, 'farm': 5608, 'cents': 3106, 'comps': 3664, 'horay': 7334, 'preston': 11420, 'smaller': 13262, 'companies': 3631, 'itï': 7899, 'slight': 13225, 'hint': 7202, 'mamas': 9132, 'lunchtime': 9005, '37': 261, '1am': 116, '11am': 46, 'amanda': 1268, 'eastern': 5026, 'carolina': 2995, 'pancras': 10725, 'ahmazing': 1134, 'harm': 6946, 'afaik': 1061, 'awlll': 1782, 'summa': 13992, 'longest': 8856, 'trackk': 14836, 'everrr': 5379, 'strek': 13843, 'fitting': 5826, '5jg16': 397, 'grader': 6590, 'thowing': 14547, 'jokes': 8070, 'sicker': 13047, 'messes': 9407, '2000': 130, 'sanfran': 12536, '58': 387, 'cple': 3909, '86': 493, 'degr': 4371, 'dmv': 4713, 'makeup': 9116, 'sizes': 13153, 'bullying': 2744, 'hype': 7485, 'rainin': 11760, 'languish': 8479, 'habbo': 6794, 'cinematography': 3372, 'bai': 1871, 'kt': 8388, 'parade': 10751, 'float': 5871, 'popup': 11289, 'blocker': 2338, 'spidey': 13566, 'boxers': 2525, 'quasi': 11685, 'sequel': 12773, 'plot': 11184, 'terms': 14395, 'quinn': 11710, 'cstm': 4055, 'buaa': 2691, 'moneyy': 9655, 'fraser': 6040, '_jayr': 696, 'budgeting': 2706, 'tango': 14249, 'ug': 15165, 'loveeeee': 8939, 'profession': 11496, 'sterling': 13753, 'chad': 3127, 'dylan': 4984, 'cooper': 3820, 'pathetic': 10817, 'rearranging': 11861, 'shortening': 12980, 'poet': 11214, 'oprah': 10513, 'xbl': 16243, 'plzzzzz': 11200, 'belong': 2130, 'institute': 7743, 'compute': 3665, 'technology': 14328, 'twibes': 15066, 'pict': 11026, 'cantazaro': 2936, 'italy': 7876, 'calabria': 2863, 'lose': 8906, 'bak': 1875, 'neewwww': 10014, 'wardrobe': 15681, 'interventio': 7784, 'aahhh': 881, 'placements': 11116, 'upgrades': 15324, 'kohls': 8364, 'flip': 5867, 'flops': 5876, 'annnnd': 1374, 'accept': 925, 'inconvenience': 7647, 'arts': 1589, 'bahhhh': 1870, 'pwnage_org': 11648, 'pcfopc': 10856, 'sdk': 12684, 'kills': 8291, 'financially': 5773, 'ellipital': 5155, 'demand': 4400, 'operation': 10502, 'mischevious': 9539, 'pinwheel': 11080, 'queueing': 11702, 'cryy': 4044, 'deer': 4347, 'downsides': 4823, 'carnivore': 2991, 'goslings': 6553, 'trolley': 14940, 'pusher': 11636, 'sooooooo': 13430, 'yaaw': 16282, '5529634599': 379, '_ary': 571, 'aaaww': 876, 'courseeee': 3887, 'pooped': 11270, 'postin': 11328, 'wmiad': 16068, 'sweaters': 14125, 'curly': 4095, 'justify': 8150, 'curler': 4092, 'covered': 3897, 'carry': 3004, 'childhood': 3269, 'sutzkever': 14106, 'backorder': 1839, 'belated': 2115, 'ajc': 1160, 'maxwell': 9270, 'bill': 2225, 'fishing': 5818, 'teaser': 14319, 'chipolte': 3293, 'dibs': 4529, 'sulu': 13987, 'savvv': 12578, 'donot': 4772, 'fotolog': 6010, 'sokristen': 13374, 'luckiest': 8981, 'tweeterz': 15048, '_cabrera': 592, 'deprived': 4436, 'assumptions': 1637, 'phase': 10965, 'sings': 13112, 'owl': 10642, 'depress': 4429, 'summe': 13997, 'infect': 7677, 'brunswick': 2674, 'java': 7966, 'hmph': 7232, 'retweet': 12175, 'cici': 3360, 'baltimore': 1900, 'suburbs': 13933, 'yupp': 16468, 'rude': 12398, 'depending': 4424, 'witness': 16051, 'sweets': 14144, 'drank': 4845, 'cyderrrrrrrrr': 4139, 'ruining': 12406, 'instances': 7738, 'yyyyyyyyyoooooooooouuuuu': 16475, 'teevee': 14340, 'adventure': 1048, 'norf': 10214, 'yessir': 16365, 'status': 13723, 'cursing': 4101, 'juddday': 8112, 'location': 8807, 'ubertwitter': 15158, 'include': 7639, 'shriya': 13017, 'sneezy': 13318, 'buff': 2710, 'sophie': 13441, 'vips': 15524, 'acts': 985, 'believes': 2122, 'snoozing': 13329, 'stubborn': 13881, 'caring': 2980, 'temperature': 14368, 'opening': 10496, 'diamond': 4521, 'hurtful': 7463, 'glitter': 6443, 'misog': 9550, 'answers': 1395, 'curved': 4103, 'grading': 6593, 'desire': 4460, 'isle': 7863, 'mediterranean': 9333, 'presenting': 11413, 'owens': 10640, '8pm': 508, 'deserved': 4451, 'towed': 14817, 'redgie': 11928, 'splashtown': 13582, 'ripstick': 12254, 'retract': 12165, 'statement': 13713, 'thennnnnnnn': 14482, 'login': 8826, 'participate': 10779, 'carrie': 3000, 'cambodian': 2894, 'waitress': 15634, 'ti89': 14591, 'immune': 7599, 'favs': 5653, 'cameo': 2900, 'creams': 3957, 'thoe': 14529, 'faile': 5548, 'deceitful': 4314, 'unreal': 15301, 'steve': 13754, 'jones': 8077, 'channing': 3161, 'tatum': 14291, 'frozen': 6126, 'bones': 2422, 'capital': 2947, 'allah': 1200, 'hafiz': 6807, 'universe': 15279, '42': 291, 'hitchikers': 7215, 'baffles': 1860, 'shortened': 12979, 'urls': 15356, 'context': 3768, 'auto': 1717, 'expand': 5464, 'ihop': 7551, 'maths': 9253, 'singer': 13107, 'thanking': 14439, 'omgsh': 10435, 'camping': 2907, 'coupla': 3883, 'fist': 5822, 'biker': 2221, 'boby': 2397, 'contributing': 3781, 'retirement': 12163, 'employer': 5198, '_milano': 742, 'bleeds': 2314, 'mayer': 9276, 'modeling': 9616, 'phobia': 10985, 'phobias': 10986, '21life': 150, 'sentences': 12763, 'chester': 3252, 'stiles': 13769, 'doughnut': 4806, 'mourn': 9756, 'windy': 16006, 'eddy': 5060, 'dumbo': 4947, 'manhattan': 9152, 'nintendo': 10140, 'gameboy': 6245, '_marie': 735, 'hudgens': 7417, 'shuffle': 13024, 'wooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo': 16115, 'doubled': 4799, 'punk': 11615, 'muses': 9844, 'vivid': 15555, 'spiderweb': 13565, 'spiders': 13564, 'tynisha': 15140, 'keli': 8220, 'tynishakeli': 15141, 'zensify': 16495, 'landscape': 8474, 'typing': 15147, 'pleaseeee': 11164, 'concerned': 3678, 'batt': 1995, 'abby': 893, 'l8er': 8410, 'humira': 7438, 'enbrel': 5204, 'rheumy': 12204, 'cutie': 4117, 'exausted': 5420, 'xanax': 16241, 'logins': 8827, 'everytime': 5396, 'grueling': 6716, 'gordo': 6547, 'communicating': 3621, 'minty': 9525, 'oww': 10651, 'operate': 10500, 'schedules': 12620, 'soy': 13498, 'teets': 14339, 'cork': 3839, 'sevens': 12812, 'lam': 8450, 'heheheheh': 7094, 'yaaay': 16280, '1992': 112, 'coolio': 3812, 'poverty': 11350, 'moood': 9689, 'lameness': 8457, 'traitor': 14861, 'aphrodisiac': 1451, 'intake': 7753, 'bowlful': 2521, 'knackered': 8335, 'shouldnt': 12995, 'demi': 4401, 'tisdale': 14662, 'mak': 9110, 'bbm': 2017, 'phn': 10982, 'tmail': 14676, 'dated': 4244, 'peroni': 10933, 'awesomeeeeee': 1766, 'poopy': 11271, 'patrol': 10824, 'doggies': 4741, 'pounding': 11342, 'develop': 4491, 'remedies': 12039, 'twinge': 15075, '3333': 246, 'leaked': 8569, 'filling': 5759, 'ughhhhhhhh': 15177, 'juuuuuust': 8153, 'hai': 6831, '_call': 593, 'castle': 3030, 'chez': 3255, 'moi': 9623, 'intent': 7761, 'kathleen': 8190, '_souljaa': 804, 'fabric': 5531, 'audun': 1701, 'propped': 11539, 'fluffy': 5889, 'pillows': 11054, 'suckss': 13952, 'culture': 4075, 'increasing': 7651, 'india': 7658, 'govt': 6568, 'charges': 3178, 'rapes': 11788, 'lighter': 8681, 'flashed': 5851, 'unheard': 15262, 'pasty': 10812, 'freckle': 6052, 'bash': 1966, 'everybodys': 5385, 'qo': 11667, 'fridaaaayyyyy': 6090, 'sowwy': 13495, 'denton': 4416, 'musician': 9854, 'profit': 11500, '2215': 155, '2morrow': 207, 'cia': 3356, 'represent': 12095, 'actors': 982, 'careers': 2972, 'killers': 8287, 'jonathan': 8075, 'epitomised': 5283, 'fangirl': 5591, 'friendlyand': 6099, 'joey': 8055, 'eight': 5116, 'yourock': 16428, 'amen': 1292, 'brotha': 2653, 'pennsylvania': 10895, 'hsbc': 7397, 'savings': 12577, 'apy': 1504, '55': 378, 'underneath': 15228, 'mattress': 9263, '67kvt': 447, 'porter': 11296, 'haunting': 6979, 'forces': 5961, 'strategies': 13831, 'competitive': 3643, 'advantage': 1046, 'heavenly': 7066, 'siyal8r': 13150, 'dreambears': 4859, '33333': 247, 'hahahahaha': 6821, 'sociable': 13350, 'shaycarl': 12877, 'vlog': 15557, 'textin': 14419, 'beds': 2084, 'brandy': 2560, 'whiskers': 15913, 'conversation': 3788, 'multitask': 9828, 'mature': 9264, 'britney': 2626, 'lamb': 8451, 'shirts': 12936, 'cookers': 3804, 'koreans': 8373, 'buisnesses': 2731, 'mussoooooo': 9859, 'youï': 16443, '½re': 16545, '221': 154, 'dedicated': 4339, 'slice': 13220, 'sql': 13629, 'axed': 1797, 'ombra': 10423, 'mai': 9091, 'fï': 6208, 'largo': 8495, 'hï': 7493, '½ndel': 16541, 'scrap': 12649, 'showering': 13006, 'ecaytrade': 5043, 'vice': 15492, 'president': 11415, 'cultural': 4074, 'affairs': 1062, 'southwestern': 13490, 'bamboo': 1903, 'jade': 7931, 'mollie': 9635, 'kaelah': 8161, 'admire': 1018, 'vietnam': 15505, 'ratty': 11803, 'disturbed': 4685, 'toadtastic': 14685, 'rosie': 12350, 'goal': 6468, 'leav': 8581, 'rescheduled': 12117, 'crawled': 3945, 'givee': 6419, 'daniel': 4212, 'schuhmacher': 12629, 'studentfinance': 13884, 'pounds': 11343, 'maaaannnnnnnnn': 9043, 'tritonlink': 14933, 'maaaaannnn': 9041, 'holidayzzzzz': 7256, 'coworkers': 3905, 'followfridays': 5926, 'tantra': 14254, 'kirki': 8310, 'drowning': 4901, 'sorrows': 13454, 'ip': 7826, 'brittany': 2630, 'sinse': 13122, 'nuffin': 10275, 'charleston': 3185, 'bees': 2096, 'trapped': 14880, 'honeypot': 7295, 'bran': 2553, '2n': 210, 'kateage': 8188, 'owi': 10641, 'credo': 3970, 'brighi': 2608, 'twitt': 15087, 'heartache': 7051, 'ctrl': 4058, 'underscore': 15233, 'e71': 4990, 'ere': 5297, 'profy': 11503, 'ayt': 1801, 'uspln': 15379, 'cautious': 3056, 'youuuuu': 16441, 'savvy': 12579, 'thus': 14585, 'ss': 13644, 'ben': 2136, 'incl': 7637, 'coaches': 3504, 'terri': 14398, 'crisps': 3996, '_bbcrew': 581, 'whhaacck': 15903, '200f2': 137, '85': 492, 'tweddin': 15030, 'alllll': 1209, 'consumption': 3752, 'specials': 13533, 'bensons': 2146, '2001': 131, 'maniacs': 9153, 'lush': 9013, 'env2': 5265, 'pov': 11349, '99': 529, 'tataindicom': 14281, 'tatasky': 14283, 'airtel': 1156, 'withdraws': 16046, 'victims': 15496, 'brad': 2542, 'olympic': 10420, 'rack': 11739, 'lifeball': 8671, 'vienna': 15504, '17jiy8': 99, 'username': 15374, 'papa': 10741, 'helped': 7119, 'mill': 9486, 'regions': 11977, 'neti': 10053, 'vi': 15484, '5jfu9': 396, 'alittle': 1197, 'fear': 5660, 'stolen': 13790, 'bale': 1888, 'cotton': 3866, 'milkshake': 9484, 'uuuup': 15402, 'glorious': 6448, 'contemplate': 3759, 'awesomerer': 1770, 'happpppy': 6923, 'chocr': 3309, 'rounds': 12362, 'gourds': 6566, 'letdown': 8634, 'zombies': 16506, 'huggers': 7421, 'muhahahaaaa': 9821, 'murder': 9839, 'ruleaz': 12409, 'zic': 16500, 'ron': 12325, 'griffin': 6672, 'denver': 4417, 'nosebleeds': 10230, 'ahugs': 1137, 'deli': 4384, 'doable': 4720, '_kotenok': 712, 'pointless': 11222, 'nan': 9937, 'hte': 7402, 'jittery': 8035, 'musicboat': 9853, 'university': 15280, 'benicks': 2142, 'crashed': 3936, 'ttytomorrow': 14983, 'sngs': 13319, 'wks': 16064, 'csh': 4050, 'dayyyy': 4273, 'gu': 6730, 'keepin': 8216, 'missinmydgbigtyme': 9560, 'cheesecake': 3230, 'polar': 11232, '_kong': 710, 'prix': 11464, 'perks': 10927, 'towels': 14819, 'lovey': 8954, 'agreeable': 1108, 'yays': 16307, 'choked': 3313, 'retainers': 12156, 'discovered': 4634, 'shortcoming': 12976, 'integrated': 7754, 'subtract': 13932, 'limit': 8702, 'shampoo': 12851, 'sumthin': 14005, 'akankah': 1163, 'suatu': 13913, 'hari': 6942, 'ku': 8390, 'mendapatkannya': 9378, '201th': 141, 'remarks': 12036, 'blondes': 2352, 'nz': 10313, 'umm': 15202, 'massage': 9234, 'erin': 5304, 'baguio': 1864, 'damned': 4186, 'ungrateful': 15260, 'trades': 14841, 'comp': 3627, '3mb': 276, 'anthropomorphic': 1399, 'planter': 11132, 'm6sru3': 9035, '31st': 240, '930a': 524, '730p': 465, 'lube': 8974, 'welsh': 15837, 'ninja': 10139, 'dominicks': 4760, 'gummy': 6755, 'bre': 2569, 'relatives': 12008, 'loon': 8876, 'dario': 4221, 'tk': 14672, 'harlem': 6944, 'pollution': 11245, 'ethnicity': 5346, 'fellas': 5691, 'dueces': 4940, 'bagel': 1862, 'maxin': 9269, 'ftsk': 6140, 'rapids': 11789, 'alike': 1195, 'nervvoouus': 10045, 'pta': 11578, 'alerts': 1180, 'generally': 6321, 'stands': 13681, 'association': 1634, 'confuse': 3709, 'decluttering': 4331, 'wannnnaaaa': 15666, 'goooooo': 6534, 'blimey': 2325, 'nov': 10254, 'gether': 6349, 'jp': 8101, 'iv': 7901, 'sucker': 13946, 'lat': 8507, 'negativity': 10017, 'bump': 2750, 'seemingly': 12723, 'unable': 15209, 'muster': 9865, 'tsi': 14975, '160hp': 90, 'imtetsting': 7625, 'visual': 15552, 'chickadee': 3262, 'messenger': 9406, 'supp': 14057, 'princecharming': 11444, 'maternity': 9248, 'converter': 3793, 'converting': 3794, 'fanfic': 5590, 'worn': 16152, 'performance': 10918, 'randomly': 11778, 'hangup': 6903, 'morrisons': 9722, 'weirder': 15818, 'any1': 1415, 'tipping': 14650, 'unusual': 15312, 'activity': 980, 'bicycle': 2204, 'goood': 6525, 'seven': 12811, 'fraktastic': 6027, 'zoidberg': 16504, 'tweeted': 15044, 'bubblies': 2696, '_steve': 810, 'exeter': 5448, 'marked': 9194, 'funy': 6183, 'housework': 7378, 'gasp': 6284, 'replay': 12080, 'rankings': 11784, '_cupcake': 608, 'wrestlefest': 16184, 'roughnight': 12357, 'buttershots': 2803, 'faire': 5557, 'boreddddd': 2479, '_a4l': 562, 'arrange': 1561, 'archies': 1518, '5am': 389, 'lawnmower': 8541, 'stood': 13795, 'tall': 14241, 'carbonated': 2962, 'nofair': 10174, 'eyeshadows': 5522, 'hooked': 7306, 'koala': 8362, 'pubquizzing': 11585, '11e': 47, 'sinon': 13120, 'mardi': 9178, 'requested': 12103, 'guest': 6740, 'speaking': 13528, 'tweeples': 15036, 'acquainted': 968, 'spoke': 13592, 'sunshiines': 14037, 'nostalgia': 10231, 'leavers': 8583, '_writes': 843, 'camcorder': 2897, 'rove': 12369, 'mcmanus': 9296, 'footage': 5949, 'ohyeahhh': 10393, 'geeky': 6308, 'ellen': 5153, 'skott': 13179, 'gooodnight': 6527, 'hacker': 6799, 'carolyn': 2996, 'thereeeeeee': 14489, 'duetter': 4941, 'crepe': 3979, 'medal': 9321, 'drivin': 4889, 'glued': 6454, 'copier': 3827, 'goooooodmoring': 6536, 'greaattt': 6638, 'toddler': 14699, '_val_4_now': 828, 'ferris': 5702, 'wheel': 15888, 'ri': 12210, 'section': 12709, 'gino': 6398, 'laughs': 8526, 'medhurst': 9322, 'kashtam': 8186, 'similey': 13091, 'csk': 4052, 'irish': 7841, 'upstairs': 15341, 'biz': 2268, 'receive': 11877, 'handbags': 6882, 'heheee': 7091, 'fresno': 6085, 'wednesay': 15787, 'shepards': 12901, 'bush': 2783, 'baptist': 1933, 'missionaries': 9562, '_cath': 596, 'whos': 15947, 'monica': 9658, 'iono': 7824, 'l8r': 8411, 'soetimes': 13362, 'lives': 8769, 'deep': 4345, 'communications': 3622, 'roomie': 12331, 'survivor': 14095, 'denny': 4411, 'jessie': 8012, 'mosquito': 9733, 'mentioned': 9388, 'undrstand': 15242, 'busays': 2782, 'tagalog': 14207, 'waray': 15676, 'initials': 7691, 'grumpy': 6721, 'blamed': 2294, 'receiving': 11879, '_roe': 786, 'chunky': 3347, 'beef': 2089, 'srry': 13641, 'urselff': 15360, 'yayyyyyyy': 16312, 'hungryyyyyy': 7450, 'expecting': 5467, 'broadway': 2637, 'walc': 15644, 'ers': 5315, 'reminiscing': 12049, 'uggh': 15169, 'miller': 9488, 'cubs': 4063, 'leading': 8564, 'chica': 3258, 'goosebumps': 6545, 'tones': 14734, 'dealz': 4300, 'freaky': 6051, 'becomin': 2078, 'shopaholic': 12967, 'organ': 10538, 'ejamming': 5124, 'undeveloped': 15240, 'sprint': 13621, '8330': 491, 'nightmares': 10122, 'perky': 10928, 'chipped': 3295, 'girlies': 6404, 'bothering': 2500, 'desperation': 4468, 'whoah': 15930, 'effective': 5100, 'rendered': 12057, 'magic': 9080, 'funfunfun': 6167, 'childrens': 3272, 'yuuum': 16472, 'android': 1332, 'pushed': 11635, 'chemist': 3245, 'runway': 12427, 'parked': 10768, 'fs': 6137, 'yoooooooooooooooooooou': 16409, 'thas': 14451, 'bouts': 2518, 'ericson': 5302, 'touched': 14799, 'asia': 1608, 'topics': 14767, 'telephone': 14352, 'barn': 1949, 'hay': 7000, 'prices': 11434, 'brett': 2592, 'teach': 14306, 'blindly': 2327, 'label': 8417, 'tops': 14772, 'latin': 8516, 'nothings': 10241, 'aswel': 1644, 'lmfao': 8786, 'commands': 3603, 'educate': 5076, 'velvet': 15452, 'cupcakes': 4084, 'master': 9239, 'frosting': 6122, 'coolest': 3811, 'confusing': 3712, 'scratches': 12653, 'henpecking': 7129, 'cheating': 3210, 'blond': 2350, 'karma': 8184, 'dawg': 4259, 'hampa': 6876, 'hatiku': 6975, 'whatz': 15883, 'waaaaaaaaaah': 15608, 'unpopular': 15297, 'chantal': 3163, 'tittle': 14668, 'breathe': 2582, 'lurveeeeee': 9012, 'outfits': 10587, 'spoilers': 13589, 'speller': 13550, 'craziness': 3951, 'w00t': 15602, 'funds': 6164, 'drunken': 4913, 'longing': 8861, 'hoodie': 7304, 'brrr': 2661, 'adelaide': 1009, 'marie': 9187, 'aiza': 1159, '_live': 724, 'halls': 6857, 'throte': 14562, 'demon': 4404, 'pinkpawsforlife': 11075, 'org': 10537, 'wooo': 16109, 'nuggets': 10277, 'gob': 6471, 'phoned': 10991, 'craziier': 3950, 'sigma': 13067, 'splendid': 13583, 'ev': 5359, 'boom': 2445, 'sta': 13651, 'focused': 5907, 'skinned': 13169, 'tweetsuite': 15057, 'acs': 971, 'ther': 14485, 'ecstatic': 5053, 'mutual': 9870, 'admiration': 1017, 'somewhat': 13406, 'whaaat': 15859, 'herd': 7134, 'somalions': 13389, 'stuffted': 13896, 'forbidden': 5958, 'cholocate': 3315, '_lokelani': 726, 'hitched': 7214, 'cuidalo': 4071, 'beeen': 2088, 'onee': 10447, 'additional': 1003, 'stepmom': 13748, 'sweating': 14126, 'driven': 4885, 'auditioning': 1698, 'southpark': 13488, '2hear': 200, 'kerry': 8237, 'gmtv': 6459, 'hyper': 7487, 'idiots': 7528, 'juggler': 8120, 'bandwagon': 1913, 'wr': 16174, 'tape': 14258, 'fungus': 6168, 'munch': 9834, 'watson': 15734, 'gigs': 6382, 'rly': 12267, 'disaster': 4618, 'breathing': 2583, 'believing': 2123, 'waaah': 15612, 'masseuse': 9236, 'yourname': 16427, 'tricky': 14920, 'moncton': 9651, 'jill': 8027, 'idkk': 7530, '_g': 647, 'rugby': 12400, '_mar_mey': 733, 'disbanding': 4620, 'americana': 1297, 'options': 10520, 'wacky': 15618, 'amigo': 1303, 'ofcourse': 10359, 'experiment': 5476, 'supervise': 14055, 'midterms': 9459, 'wipe': 16020, 'slate': 13196, 'jelly': 7993, 'runnin': 12423, 'pdx': 10861, 'fired': 5798, 'plays': 11155, 'iplayer': 7830, 'supporters': 14065, 'athletic': 1656, 'dinna': 4582, 'hink': 7201, 'shown': 13010, 'elgin': 5146, 'nae1': 9912, 'oll': 10418, 'applebees': 1475, 'shadow': 12836, 'realistic': 11846, 'rolling': 12316, 'burden': 2761, 'russ': 12432, '_dj': 622, 'xmas': 16250, 'keswick': 8238, 'cheesecakes': 3231, 'pilot': 11056, 'burnin': 2768, '04': 6, 'source': 13482, 'lad': 8430, 'mite': 9578, 'sumthn': 14006, 'doin': 4746, 'ay': 1799, 'suuuuck': 14108, 'peas': 10870, 'nakuh': 9926, 'grabeh': 6584, 'twitchy': 15083, 'joeman': 8054, 'mor': 9699, 'g1freaks': 6211, 'togetha': 14703, 'navy': 9974, 'nailpolish': 9919, 'stained': 13663, 'shiiite': 12918, 'rhyming': 12208, 'creeping': 3976, 'charlie': 3187, 'neglect': 10020, 'wnt': 16071, 'zsg': 16513, 'orillia': 10550, 'arsenal': 1577, 'mitch': 9575, 'awwwwwww': 1793, 'sats': 12564, 'teddy': 14329, 'humid': 7436, 'organizing': 10543, 'herrrrr': 7146, 'fletcher': 5859, 'disturbance': 4684, 'sapinsidetrack': 12547, 'palo': 10718, 'alto': 1257, 'lines': 8714, 'unmuted': 15288, 'opinion': 10507, 'saved': 12575, 'sexxxy': 12822, 'tiiiiiimmmmmmeee': 14618, 'blushing': 2380, 'headrush': 7033, 'favour': 5649, 'tweetilicious': 15052, 'schoool': 12628, 'hittt': 7220, 'beck': 2073, 'overdue': 10613, 'overstuffed': 10628, 'grill': 6673, 'congratulation': 3723, 'almighty': 1221, 'yey': 16378, 'starbuck': 13686, 'deeply': 4346, 'cheeks': 3220, 'rebel': 11867, 'meets': 9347, 'dac': 4159, 'pantera': 10737, 'cowboys': 3902, 'paracetamol': 10750, 'swallow': 14118, 'sanzz': 12546, 'ex': 5414, 'baristas': 1944, 'commonalities': 3619, 'socialising': 13353, 'tomor': 14726, 'fate': 5630, 'ysa': 16448, 'drums': 4910, '_uh_knee': 825, 'charm': 3189, 't4': 14188, 'brownie': 2658, 'rsvp': 12389, 'qiuqiu': 11662, 'degree': 4372, '90mph': 514, 'basis': 1972, 'cld': 3419, 'furious': 6185, 'mph': 9785, 'forte': 5998, 'mystic': 9900, 'bientï': 2209, '½t': 16550, 'lire': 8737, 'quincy': 11709, 'fashionisthenextcity': 5622, 'tee': 14331, 'limited': 8704, 'edition': 5070, 'twic': 15067, '_daarling': 612, 'distance': 4674, 'tiki': 14620, 'oo': 10462, 'opened': 10494, 'lauren': 8533, 'conrad': 3735, 'andswere': 1334, 'quesadiaas': 11694, 'bombbb': 2418, 'animals': 1353, '_kryptik': 713, 'statuses': 13724, 'wel': 15826, 'arbit': 1512, 'ireland': 7840, 'x2bp2': 16232, 'gang': 6257, 'mwahs': 9876, 'stretches': 13856, 'selection': 12731, 'flavors': 5857, 'roomate': 12330, 'naina': 9921, '3wordsaftersex': 281, 'effy': 5108, '90mm': 513, 'mamiya': 9134, 'mf': 9427, 'lens': 8620, 'shocking': 12953, 'suxs': 14112, 'dosen': 4791, 'sedaris': 12714, 'insulted': 7748, '35th': 256, '2getha': 198, 'hahaa': 6813, 'td': 14302, 'battled': 1998, 'cs3': 4047, 'processor': 11481, 'nambu': 9930, 'unintuitive': 15267, 'interminable': 7773, 'sandman': 12528, 'wot': 16165, 'partyin': 10790, '2nyt': 216, 'taps': 14261, 'capes': 2946, 'build': 2726, 'drinker': 4877, 'shake': 12839, 'hid': 7169, 'confidence': 3699, 'ies': 7538, 'martwo': 9216, 'accadentally': 922, 'xvd1wankt': 16264, 'struggling': 13877, 'jelous': 7995, 'position': 11310, 'jummy': 8130, 'ewwwww': 5413, 'marra': 9206, 'ecxcited': 5056, 'annabel': 1363, 'hearts': 7058, 'arrest': 1564, 'wavves': 15742, 'meltdown': 9361, 'barcelona': 1941, 'drummer': 4908, 'logically': 8825, 'motorways': 9753, 'walnut': 15659, 'becca': 2072, 'yaay': 16283, 'flown': 5886, 'clamped': 3402, 'laffy': 8441, 'taffy': 14205, 'momies': 9639, 'sk': 13156, 'dha': 4511, 'cece': 3081, 'chanqes': 3162, 'los': 8905, 'tab': 14191, 'randoms': 11779, 'licking': 8661, 'crumbs': 4027, 'twirl': 15077, 'wrapper': 16178, 'zammo': 16482, '_garner': 652, 'suggested': 13973, 'fannie': 5593, 'darwin': 4235, 'fetch': 5712, 'ouchie': 10574, 'excedrine': 5422, 'compare': 3634, 'notes': 10237, 'haih': 6832, 'unc': 15217, 'cuts': 4119, 'rejected': 11999, 'upgrade': 15322, 'panicky': 10734, 'sec': 12705, 'ojcf5l': 10396, 'cassandra': 3026, 'wilcox': 15978, 'bloss': 2357, 'bucket': 2697, 'doors': 4784, 'byee': 2827, 'rainstorm': 11763, 'cheat': 3208, 'gooooonight': 6533, 'arnold': 1555, 'rus': 12429, '_cullen8': 607, 'uglier': 15179, 'realllllllllly': 11853, 'cycling': 4137, 'campjitterbug': 2908, 'thnks': 14526, 'clara': 3404, 'sotomayor': 13468, 'branches': 2555, 'damit': 4180, 'hahhh': 6830, 'nerve': 10041, 'askin': 1613, '5hours': 392, 'helll': 7110, 'selling': 12739, 'exhibition': 5454, 'gallery': 6240, 'otara': 10563, 'birmingham': 2246, 'whoooooo': 15942, 'spcn': 13524, 'opff': 10504, 'bbl8r': 2016, 'tells': 14362, 'pretending': 11423, 'assassinate': 1621, 'eaat': 4992, 'subtly': 13931, 'temme': 14364, 'firangs': 5796, 'steal': 13735, 'vocab': 15559, 'relieve': 12024, 'speeches': 13539, 'foad': 5904, 'taught': 14292, 'asians': 1610, 'ge': 6300, 'strep': 13845, 'shifting': 12913, 'ghey': 6364, 'ughhhh': 15175, 'sheli': 12893, 'flood': 5872, 'murdere': 9840, 'accidently': 936, 'cured': 4090, 'hunger': 7446, 'h8': 6786, 'withdrew': 16047, 'permission': 10931, 'idp': 7532, 'camps': 2909, 'idprelief': 7533, 'vinyl': 15521, 'clad': 3395, 'playset': 11156, 'mfr': 9430, 'warranty': 15693, 'swing': 14156, 'slots': 13240, 'climbing': 3446, 'amish': 1305, 'arrggh': 1566, 'kava': 8197, 'liam': 8654, '_pase': 767, 'kaleidoscope': 8168, 'norma': 10216, 'terabyte': 14387, 'contacted': 3755, '2ce': 191, 'response': 12135, 'systems': 14185, 'smoothly': 13297, 'upkeep': 15325, 'crisis': 3994, 'eavy': 5037, 'doe': 4733, 'firts': 5811, 'impressions': 7617, 'ageing': 1094, 'tytn': 15153, 'aaawww': 877, 'soooooooo': 13431, 'unfortunetly': 15257, 'tengo': 14379, 'dinero': 4578, 'luvv': 9020, 'excedrin': 5421, 'vent': 15460, 'domestic': 4758, 'artery': 1579, 'hagen': 6808, 'daz': 4274, 'websites': 15783, 'ethics': 5344, 'kilkenny': 8282, 'clown': 3476, 'interrupt': 7781, 'blankets': 2298, 'hostage': 7356, '_joy': 702, 'entirely': 5261, 'pizzas': 11109, 'price': 11431, 'wireless': 16023, 'drops': 4898, 'thesis': 14495, 'joyfull': 8097, 'shoulda': 12991, 'pierced': 11033, 'piercings': 11035, 'aaah': 869, 'incarnated': 7633, 'mutant': 9867, 'ghalib': 6361, 'unanswered': 15210, 'nigguhs': 10115, 'ilh': 7562, 'florence': 5879, 'flasher': 5852, 'zack': 16480, 'alpine': 1234, 'chattin': 3200, 'foreve': 5971, 'zavaroni': 16485, 'sp': 13500, 'shields': 12911, 'barrymore': 1955, 'garage': 6264, 'storage': 13803, 'upbeat': 15315, 'backk': 1836, 'hadn': 6805, 'hallmark': 6853, 'fails': 5553, 'nyeh': 10310, 'signing': 13072, 'fannish': 5594, 'inquisition': 7711, 'dot': 4795, 'shelf': 12892, 'cameras': 2903, 'mash': 9227, 'warhammer': 15683, 'uwl7m': 15407, 'spoiled': 13588, 'donï': 4776, 'canï': 2941, 'severe': 12816, 'ryt': 12447, 'dougie': 4808, 'poynter': 11359, 'replyed': 12085, 'ticketless': 14596, 'payat': 10843, 'melbourne': 9357, 'ugggh': 15168, 'dancers': 4200, 'blogging': 2344, 'passion': 10802, 'chit': 3299, 'mummyyyyyyyy': 9832, 'wkp': 16063, 'feature': 5666, 'heeeey': 7080, 'boc': 2398, 'raj': 11769, 'squad': 13630, 'gyms': 6784, 'machines': 9059, 'indulge': 7670, 'allow': 1217, 'thicke': 14500, 'serenade': 12778, 'naked': 9925, 'woooh': 16110, 'beers': 2095, 'awesomness': 1773, 'sourness': 13484, 'stalkersaturday': 13671, 'gloom': 6445, 'whoohoo': 15940, 'dyeing': 4982, 'tilaaa': 14623, '_troy': 823, '_mcloven': 739, 'garbage': 6266, 'popping': 11282, 'pwnd': 11649, 'sides': 13057, 'goodniqht': 6516, 'hilly': 7195, 'lmbo': 8784, 'litterally': 8757, 'lalala': 8449, 'whilst': 15907, 'chf': 3256, '670': 437, 'hardware': 6941, 'wud': 16207, 'overheating': 10616, 'avatarcamp': 1730, 'hotttie': 7368, 'poooh': 11266, 'kerri': 8236, 'locker': 8811, 'restoration': 12145, 'banks': 1927, 'proposals': 11537, 'avocets': 1738, 'gut': 6768, 'exception': 5428, 'chillen': 3281, 'csla': 4053, 'pacquiao': 10679, 'rerun': 12113, 'tackle': 14198, 'mare': 9179, 'paradice': 10753, 'heyyy': 7158, 'rr': 12383, '_world': 841, 'brewers': 2594, 'leafs': 8566, 'oliver': 10416, 'anooyed': 1387, 'pla': 11112, '4wh0o': 353, 'posterrr': 11326, 'faking': 5566, 'newark': 10065, 'crochet': 4003, 'stitches': 13782, 'afgan': 1068, 'lamentablemente': 8458, 'paso': 10797, 'jrztwitterlunch': 8107, 'developement': 4493, '13tolife': 70, 'mentioning': 9389, 'links': 8722, 'contests': 3767, 'reminder': 12045, 'ummm': 15204, 'pisssing': 11091, 'whens': 15895, 'lova': 8927, 'bbff': 2014, 'twenty': 15062, 'superstar': 14051, '2b': 190, 'lyk': 9030, 'othaa': 10564, 'gurrrrrl': 6765, 'rieger': 12236, 'begonia': 2104, 'basket': 1974, 'seemed': 12722, 'casualties': 3033, 'discography': 4626, 'teardrops': 14314, 'christa': 3335, '07k6e': 11, 'pril': 11439, 'yeps': 16358, 'ihate': 7550, '_nesmith': 752, 'lisette': 8739, 'alejandra': 1178, 'entrepreneurs': 5263, '4w425': 338, 'parrrtty': 10776, 'twister': 15079, 'tipsy': 14652, 'desperate': 4466, 'adjustment': 1015, 'perez': 10912, 'littttttle': 8759, '160': 88, 'hurrah': 7458, 'va': 15411, 'muggy': 9819, 'cursed': 4100, 'reinforced': 11994, 'sorts': 13464, 'newly': 10071, '20min': 144, 'rundown': 12419, 'paste': 10809, 'tosser': 14790, 'flap': 5848, '10000000000': 25, '_the_moon': 817, 'jim': 8029, 'fruitbat': 6128, 'cldnt': 3420, 'recipe': 11887, 'cig': 3362, 'sticky': 13766, 'attitude': 1682, 'iznt': 7911, 'perils': 10925, 'tragedies': 14848, 'funn': 6171, 'dancer': 4199, 'lammmeeee': 8461, 'tournament': 14809, 'destroy': 4474, 'quil': 11707, 'grinning': 6680, 'tragidy': 14851, 'wirting': 16025, 'almos': 1224, 'listenint': 8747, 'afh': 1069, 'sacto': 12463, 'proflowers': 11502, 'fiasco': 5725, 'itell': 7882, 'sosososo': 13467, 'cory': 3854, 'weiss': 15824, '67jzp': 445, 'concerts': 3682, 'experienced': 5474, 'apathy': 1448, 'empathy': 5192, '_fc': 639, 'alias': 1190, 'approximately': 1496, '117th': 45, 'extended': 5504, 'recoverable': 11917, 'sweep': 14136, 'lovato': 8931, 'trader': 14840, '71st': 463, 'firemen': 5804, 'wth': 16206, 'pause': 10834, 'creating': 3962, 'frequent': 6080, '½h': 16534, 'borrow': 2490, 'anythin': 1426, 'arg': 1531, 'eirtaku': 5121, 'bots': 2501, 'hii': 7186, 'thatsï': 14456, '½mee': 16538, '_lipz': 722, 'ravenclaw': 11806, 'verse': 15471, 'blazing': 2305, '25mbps': 174, 'faa': 5529, 'thaa': 14428, 'tru': 14951, 'orc': 10527, 'hilton': 7196, 'beeman': 2092, 'commentaries': 3606, 'mayyyybe': 9278, 'charicee': 3180, 'doomed': 4780, 'assholes': 1627, 'acing': 964, 'crooning': 4007, 'peewee': 10883, 'napping': 9945, 'sfo': 12826, 'missin': 9558, 'camden': 2898, 'prego': 11393, 'smuts': 13300, 'sicckkkk': 13042, 'metaverse': 9414, 'virtual': 15530, 'worlds': 16146, 'gamier': 6251, 'terra': 14396, 'usage': 15363, 'tn': 14681, 'metaverseu': 9415, 'wabble': 15615, 'sickkk': 13049, 'ichat': 7512, 'critique': 4001, 'activities': 979, 'minutos': 9530, 'biting': 2263, 'varnish': 15435, 'gain': 6234, 'tag': 14206, 'thousand': 14545, 'corpes': 3846, '95': 526, 'overrated': 10621, 'fcukkk': 5659, 'marvelous': 9219, 'routines': 12368, 'leeds': 8593, 'stamford': 13673, 'sweety': 14145, 'waht': 15627, 'hugz': 7427, 'xxxxxxxxxx': 16272, 'goooooddd': 6531, 'chale': 3133, 'ver': 15468, 'define': 4359, 'evernote': 5375, 'sd': 12682, 'fllwng': 5869, 'thm': 14521, 'evernote_eyefi': 5376, 'mpxn': 9786, 'julio': 8127, 'bac': 1826, '15minutes': 84, 'shallowness': 12846, 'annoy': 1381, 'straighten': 13817, 'pj': 11110, 'fyi': 6207, '_03': 539, 'tupac': 15006, 'lyrics': 9033, 'jacked': 7923, 'sr': 13640, '234': 159, 'sunblock': 14013, 'venezuela': 15456, 'classmates': 3413, 'marriage': 9207, 'graceful': 6586, 'accepted': 928, 'freckles': 6053, 'oooohhh': 10476, 'chino': 3290, 'apathetic': 1447, 'elizabeth': 5150, 'squeeze': 13635, 'investigated': 7806, 'ermintrude': 5306, 'nabbed': 9909, 'ble': 2306, 'stanky': 13682, 'sunset': 14035, 'cliffs': 3444, 'headlights': 7028, 'spotlight': 13609, '5jib6': 399, 'godamn': 6474, 'planted': 11131, 'backyard': 1846, 'armhole': 1550, 'involving': 7820, 'omgaah': 10430, 'creepering': 3975, 'announce': 1376, 'marathon': 9170, 'downstairs': 4824, 'hollie': 7261, 'steel': 13740, 'wharra': 15867, 'ccna': 3072, 'ccnp': 3073, 'ccie': 3071, 'nishiki': 10145, 'alhamdulilah': 1187, 'nafa': 9913, 'sanjaya': 12540, 'hahahahahaha': 6822, 'cure': 4089, 'spontaneously': 13596, 'spontaneity': 13595, 'rediculous': 11931, 'hrdest': 7394, 'celtics': 3098, 'redsox': 11939, 'platypuses': 11142, 'mammals': 9136, 'breezy': 2589, 'tasty': 14279, 'avail': 1724, 'tkd': 14673, 'instructor': 7744, 'dodger': 4730, '2maro': 204, 'ewl': 5410, 'hives': 7221, 'allergic': 1202, 'itches': 7878, 'yeeee': 16344, 'nets': 10054, 'paiseh': 10703, 'xbox720': 16246, 'lousy': 8925, 'embarrased': 5170, '15gb': 83, 'srsly': 13642, 'lettuce': 8641, 'spinach': 13572, 'happpy': 6924, 'hooping': 7311, 'distractions': 4681, 'believers': 2121, 'warped': 15692, 'surprising': 14086, 'bazillions': 2009, 'americans': 1299, 'douchebag': 4803, 'invented': 7800, 'sensibility': 12758, 'thier': 14501, 'guns': 6763, '5z5kz': 413, 'immensely': 7597, 'annual': 1385, '_a_hart': 563, 'wrox': 16201, 'aparantly': 1442, 'soccer': 13349, 'defited': 4363, 'peed': 10876, 'sheets': 12888, 'underpants': 15230, 'trees': 14902, 'craz': 3947, 'dï': 4988, '½a': 16529, 'sol': 13375, 'grrrrrrr': 6711, '½why': 16552, '½whyyy': 16553, 'cramming': 3925, 'exciteeeddd': 5435, '395': 265, 'hungary': 7445, 'nurburgring': 10287, 'mtfye3': 9799, 'simoneserhan': 13093, 'hollyoaks': 7265, 'quashed': 11684, 'twibble': 15065, 'based': 1963, 'phones': 10992, 'requires': 12111, 'washer': 15701, 'kitchenfire': 8318, 'kay': 8199, 'destroyalllines': 4475, '_news': 755, 'teignmouth': 14345, 'dawlish': 4260, 'hmmmm': 7228, 'alaska': 1170, 'debt': 4310, 'hidden': 7170, 'file': 5751, 'nott': 10250, 'throwin': 14568, 'rah': 11751, 'illi': 7566, 'shalonda': 12847, 'bangin': 1919, 'repetative': 12075, 'passyunk': 10806, '145': 74, '165': 91, 'yawn': 16304, 'starfleet': 13691, 'gate': 6286, 'presen': 11408, '3mdce': 277, 'destination': 4472, 'quarters': 11683, 'bookstore': 2444, 'helpful': 7120, 'josiah': 8087, 'roller': 12314, 'blading': 2286, 'contrary': 3778, 'tributary': 14915, 'endlessly': 5213, 'malicious': 9128, '_jolene': 700, 'yuk': 16454, 'fa': 5528, 'thoug': 14539, 'dent': 4412, 'theyre': 14497, 'celebritytweet': 3092, 'cuckoo': 4064, 'chirping': 3298, 'scrambled': 12648, 'fartingloud': 5616, 'delish': 4390, 'eeeeeeek': 5087, 'ahhhaaaaa': 1128, 'nola': 10185, 'screenshots': 12668, 'released': 12020, 'funnn': 6178, 'celtic': 3097, 'borde': 2471, 'frasier': 6041, 'twelve': 15061, 'aaaaaawwwesome': 859, 'cheeky': 3221, 'handheld': 6887, 'detour': 4486, 'frighten': 6106, '_sprigge': 805, 'skydivers': 13182, 'thousands': 14546, 'planes': 11124, 'fifties': 5737, 'silence': 13081, '_laertesgirl': 717, 'woodvine': 16103, 'zoe': 16503, 'thorne': 14533, 'programme': 11506, 'moreton': 9705, 'stroppy': 13872, 'teenager': 14334, 'bladder': 2285, 'sorehead': 13449, 'creators': 3966, 'developers': 4494, 'hving': 7476, 'sandwichesss': 12534, 'grillz': 6675, 'plaque': 11137, 'unity': 15276, 'mcr': 9299, 'yaa': 16274, 'fond': 5930, 'kittens': 8321, 'balloon': 1898, 'duh': 4943, 'necks': 9997, 'spoilt': 13590, '4days': 310, 'competitions': 3642, 'hk': 7224, 'twitterers': 15096, 'leather': 8580, 'size': 13151, 'boogah': 2432, 'contain': 3758, 'happenning': 6917, 'stackeoverflow': 13657, 'youuu': 16439, 'wattup': 15735, 'jv': 8155, 'twurl': 15131, 'nl': 10158, 'ogzbd': 10382, 'sweat': 14124, 'contracts': 3777, 'uti': 15391, '_duerden': 625, 'jealousmuch': 7982, 'outttt': 10600, 'dragon': 4836, 'attracted': 1683, 'tabloid': 14195, 'headlines': 7030, 'labor': 8421, 'investigation': 7807, 'ec': 5042, 'gamers': 6248, 'webdesign': 15778, 'punch': 11610, 'nikki': 10131, 'bracelet': 2540, 'grea': 6635, 'dix': 4699, 'gush': 6766, 'unbelievable': 15215, 'tscc': 14972, '_envy': 631, 'sunburnnn': 14016, 'achurley': 962, 'nauseatingly': 9972, 'saddd': 12466, 'resolve': 12130, 'questioning': 11699, 'scoreless': 12639, 'sized': 13152, 'snickers': 13320, 'screamin': 12658, 'nefuew': 10015, 'forehead': 5966, 'klutz': 8332, '_lynn': 730, '_raquel': 778, 'yal': 16290, 'freedom': 6059, 'whever': 15901, 'notices': 10244, 'framework': 6029, 'sl': 13187, 'product': 11491, 'registration': 11981, 'comms': 3620, 'bruises': 2668, 'allmothers': 1215, '_ladie': 716, 'swamp': 14119, 'observatory': 10326, 'reports': 12092, 'connecticut': 3729, 'warbler': 15677, 'metzger': 9424, 'magee': 9077, 'sleeep': 13204, 'disappearing': 4613, 'kaila': 8164, 'ocampo': 10334, 'rainbowholic': 11758, 'eerie': 5095, 'demad': 4399, 'jailbreaking': 7936, 'stacie': 13655, 'pple': 11363, 'dayquil': 4266, 'equals': 5287, 'dieing': 4542, 'dozen': 4827, 'amazzzing': 1286, 'didgeridoo': 4534, 'missy': 9567, 'higgins': 7174, 'voegele': 15567, 'nocturnals': 10169, 'kellie': 8223, 'bffls': 2187, 'dates': 4245, 'brokennn': 2642, 'vry': 15593, 'capacity': 2944, 'sneezed': 13316, 'prime': 11442, 'minister': 9513, '21st': 151, 'appology': 1485, 'recall': 11875, 'flame': 5845, 'stream': 13837, 'riot': 12249, 'smitten': 13286, 'tryin': 14966, '669l2': 431, '511q': 371, 'jijr': 8026, 'hulz': 7431, '511r': 372, '2ve': 220, 'xpg0': 16260, '_davis': 614, 'conference': 3696, 'ban': 1907, '_beachmex': 582, 'bare': 1942, 'coach': 3502, 'definently': 4360, 'sexify': 12821, '67i82': 444, 'bribe': 2598, 'geje': 6314, 'rqaoe': 12381, 'underpaid': 15229, 'theraflu': 14486, 'drowsy': 4902, 'watering': 15727, 'cancelling': 2925, 'javaon': 7967, 'guinness': 6751, 'coogars': 3800, 'blokey': 2349, 'towel': 14818, 'dash': 4237, 'budurl': 2707, 'f9p5': 5527, 'collect': 3559, 'acquired': 969, 'freecycle': 6057, 'turtles': 15017, '4wehl': 350, 'yosemite': 16415, 'throwback': 14565, 'gott': 6563, 'shuts': 13029, 'basil': 1971, 'greatness': 6643, 'telegraph': 14349, '_elmo_': 628, 'awhh': 1779, 'fof': 5908, 'recovered': 11918, 'x116r': 16229, 'potential': 11336, 'ilove': 7572, 'physically': 11011, 'delicio': 4385, 'toilet': 14707, 'yayyyyyyyyyyy': 16313, 'urself': 15359, '4sx96': 332, 'smokers': 13290, 'vandalize': 15431, '14th': 77, 'deaths': 4304, '_foster': 643, 'themes': 14478, 'mariners': 9189, 'trustyfotografie': 14961, 'irked': 7842, 'kristine': 8384, 'be4': 2030, 'waraw': 15675, 'taipei': 14215, 'eluded': 5162, 'earlyer': 5006, 'thot': 14538, 'spill': 13568, 'corn': 3840, 'flakes': 5843, 'harmon': 6950, '_pierce': 774, 'nx': 10304, '01': 4, 'ncc': 9981, '1701': 97, 'popcorn': 11276, 'peeling': 10878, 'champion': 3143, 'defunct': 4367, 'cebu': 3080, 'gems': 6318, 'haileys': 6835, 'prayers': 11376, 'backseat': 1844, 'crud': 4020, 'celebs': 3093, 'frikken': 6109, 'baaaaaaaaaaackkkkkkkk': 1812, 'chops': 3321, 'lolly': 8840, 'gna': 6460, 'broom': 2649, 'standards': 13677, 'seatbelt': 12701, 'whispers': 15918, 'swiffer': 14147, 'snoring': 13331, 'eva': 5360, 'whip': 15911, 'hellyeah': 7116, 'err': 5309, 'timthumb': 14639, 'listning': 8749, 'kurt': 8397, 'spin': 13571, 'differents': 4552, 'goingg': 6489, 'vermouth': 15470, 'argue': 1537, 'complain': 3647, 'baadly': 1814, 'crop': 4008, 'fountain': 6016, 'youth': 16436, 'supastition': 14042, 'imeem': 7591, 'kpeqpg7vuy': 8376, 'laminate': 8460, 'footy': 5954, 'pour': 11344, 'dt': 4922, 'bklyn': 2274, 'umbrellaless': 15199, 'dum': 4945, 'diggin': 4562, 'gotdamn': 6560, 'angle': 1345, '_jenkins': 699, 'explosion': 5488, 'gandang': 6254, 'mas': 9224, 'mahal': 9089, 'sya': 14164, 'eeet': 5092, 'charming': 3191, 'shutters': 13030, 'relocate': 12031, 'challanges': 3135, 'photography': 10999, 'neko': 10031, 'offering': 10367, 'stevens': 13755, 'waterhouse': 15726, 'jhy': 8023, 'catbook': 3036, '6402509': 425, 'presence': 11409, 'subconscience': 13916, 'parcs': 10759, 'internets': 7778, 'alwaysss': 1260, 'cuuuba': 4123, 'upshot': 15339, 'uhh': 15183, 'nuh': 10278, 'scout': 12646, 'athletes': 1655, 'joanna': 8045, 'hayes': 7001, 'heather': 7063, 'bown': 2523, 'digging': 4563, '_greenwizard': 657, 'shorty': 12986, 'anywayss': 1431, '_oliver': 766, 'jamies': 7944, 'reflecting': 11954, 'sunlight': 14025, 'improperly': 7621, 'hammy': 6875, 'turtle': 15016, 'contract': 3775, 'spazz': 13523, 'relaxant': 12010, 'narcotic': 9947, 'moist': 9624, 'looong': 8879, 'ofc': 10357, 'genuinely': 6334, 'nuthin': 10292, 'waaaa': 15606, 'lemsip': 8616, 'apaently': 1441, 'ittt': 7894, 'sunfay': 14023, 'afterall': 1076, 'reali': 11840, 'deadpool': 4294, 'reynolds': 12198, 'lucas': 8976, 'czech': 4147, 'republic': 12099, 'collector': 3562, 'logan': 8819, 'bc': 2022, 'reds': 11937, 'hollywood': 7266, 'nai': 9917, 'trapeze': 14879, 'teaches': 14309, 'whooo': 15941, 'hoooo': 7308, 'journalism': 8092, 'olina': 10414, 'shoreee': 12973, 'foo': 5939, 'kontakt': 8369, 'exploit': 5485, 'kindred': 8301, 'racers': 11733, 'hammered': 6873, 'dramathon': 4843, 'marc': 9172, 'travels': 14889, 'mills': 9491, 'martini': 9215, 'fcking': 5657, 'overseas': 10625, 'reiki': 11992, 'reiki88': 11993, 'aitn': 1158, 'starburst': 13688, 'stu': 13878, 'lantz': 8482, 'hearn': 7049, '1030': 30, 'norton': 10225, 'mmmmm': 9594, 'loveeeeeeeeeeeeeeeeeeeeee': 8941, 'raimi': 11755, 'arrgghhhggguuuiiissshhhh': 1567, 'adoring': 1034, 'sash': 12553, 'sil': 13080, 'jacqueline': 7930, 'wilson': 15992, 'cbbc': 3068, 'ram': 11772, 'designated': 4454, 'angie': 1344, '_ahoy': 566, 'career': 2971, 'scrubbing': 12677, 'amigui': 1304, 'moan': 9604, 'whinge': 15909, 'trains': 14860, 'seann': 12691, 'scott': 12643, 'proof': 11530, 'ofcoooursee': 10358, 'pluss': 11197, 'incentive': 7634, 'directv': 4602, 'ooops': 10481, 'ridiculously': 12233, 'parking': 10770, 'bedalii': 2081, 'doll': 4750, 'streetcar': 13841, 'plis': 11181, '253ce': 173, 'doberman': 4721, 'skies': 13164, 'yards': 16300, 'noone': 10196, 'embossers': 5176, 'papersource': 10746, 'dijon': 4569, 'vcr': 15440, 'jessica': 8011, 'alba': 1171, '138': 65, '468': 299, 'movement': 9764, '9followers': 533, 'pooch': 11258, '_tickle': 820, 'pan': 10722, 'doubles': 4800, 'dish': 4650, '_connors': 603, '1999': 115, 'complex': 3655, 'condolences': 3692, 'vega': 15443, 'fireeeeee': 5799, 'robs': 12287, 'sammy': 12516, 'microsize': 9450, 'debby': 4307, 'burns': 2770, 'reaaaallly': 11823, 'copies': 3828, '1doe': 117, 'princes': 11446, 'crabs': 3914, '4d8': 309, 'horten': 7349, 'moss': 9734, 'kï': 8408, '182': 102, 'general': 6320, 'admission': 1019, 'written': 16195, 'references': 11952, 'strikes': 13861, 'burton': 2779, 'sod': 13359, 'nugget': 10276, 'sorrryy': 13457, 'broth': 2652, 'sorrrry': 13455, 'kl': 8329, 'bbyshower': 2021, 'jays': 7973, 'sharon': 12864, '_kinda_guy': 709, 'politics': 11242, 'makerfaire': 9113, 'forgets': 5976, 'lovelovelove': 8947, 'sickk': 13048, 'sistah': 13131, 'lax': 8544, 'sources': 13483, 'tails': 14213, 'youself': 16433, 'lynne': 9032, 'mmmmmmmm': 9596, 'charmer': 3190, 'haox': 6909, 'weirdos': 15823, 'individual': 7664, 'vehicle': 15451, 'qfc': 11658, 'broad': 2635, 'taxes': 14295, 'images': 7585, 'weï': 15853, 'grumbles': 6719, 'eink': 5119, 'shortcut': 12977, 'burst': 2777, 'moldy': 9633, 'squishy': 13639, 'reasons': 11864, 'cracker': 3918, 'kaitlyn': 8165, 'bobbi': 2395, 'lewis': 8644, 'tantrum': 14255, 'apologetic': 1455, 'cor': 3833, 'baccck': 1828, 'poolside': 11264, 'toniht': 14740, 'proves': 11553, '3pjnc': 278, 'pointed': 11220, 'involved': 7818, 'iphon': 7827, 'maxim': 9267, 'treats': 14899, 'loaders': 8796, '_control': 604, 'babyy': 1825, '333': 245, 'tuuneee': 15023, 'sooooooooooo': 13435, 'consuming': 3750, 'engrossing': 5228, 'salute': 12507, 'af': 1060, 'hairs': 6844, 'inboxes': 7631, 'headsup': 7035, 'brazillians': 2567, 'expression': 5496, 'eqbwe': 5286, 'cussing': 4106, 'slutted': 13254, 'favorites': 5648, 'diary': 4527, 'clashes': 3407, 'uswitch': 15386, 'euphoria': 5352, 'allll': 1208, '9pm': 535, 'enna': 5243, 'kodumai': 8363, 'idhu': 7524, 'arrr': 1575, 'bwahahaha': 2820, 'inflicted': 7681, 'iloveyoutwoooo': 7577, 'painted': 10696, 'looming': 8875, 'panda': 10727, 'redi': 11930, 'frens': 6078, '2gather': 197, 'manchester': 9144, 'wayyyyy': 15751, 'alien': 1193, 'congratses': 3720, 'tew': 14414, 'stunning': 13902, 'pineapple': 11065, 'pies': 11038, 'pos': 11305, 'mayday': 9275, '_cantus_': 594, 'brewing': 2595, '½20': 16525, 'dippin': 4593, 'iz': 7909, 'levels': 8643, 'topped': 14770, 'jenson': 8004, 'leigh': 8610, 'evo': 5404, 'typos': 15148, 'dealer': 4296, 'sentence': 12762, 'uuuups': 15403, 'doesnï': 4738, 'booored': 2456, 'expert': 5477, 'fightstar': 5742, 'mercury': 9397, 'dbm4n6': 4278, 'cnkhev': 3497, 'musicmonday': 9856, 'freemusic': 6065, 'rash': 11792, 'whyyyy': 15957, 'livechat': 8762, 'cus': 4104, 'weekdays': 15798, 'reupload': 12180, 'aaaaaahhhhhhhh': 858, 'sweeeeeeet': 14132, 'etha': 5342, 'sens': 12753, 'matic': 9254, 'bradie': 2543, 'nitentdo': 10149, 'ds': 4916, 'showcase': 13003, 'excercise': 5429, 'shemar': 12899, 'moore': 9695, 'embassy': 5174, 'madame': 9064, 'tussaud': 15018, 'nothinbg': 10239, 'streets': 13842, 'dooo': 4781, 'tweetshrinking': 15055, 'tweed': 15031, 'tenenen': 14376, 'tenen': 14375, 'whooaaa': 15939, 'overwheolming': 10634, 'itus': 7896, 'worrying': 16157, 'visits': 15549, 'saaaaaaaaaaaaaad': 12454, 'charley': 3186, 'horses': 7347, 'directors': 4600, 'uhhhg': 15185, 'lovingly': 8959, 'vengaboys': 15457, 'zane': 16483, 'lowe': 8964, 'wolf': 16079, 'gladiators': 6430, '_boduch': 588, '67': 436, 'voucher': 15587, 'onna': 10453, 'sleeeeeeeepy': 13200, 'wholey': 15936, 'evolution': 5405, 'wholely': 15935, 'composed': 3661, 'hateeee': 6971, 'eek': 5093, 'beatweetup': 2055, 'badge': 1852, 'counter': 3875, 'nerves': 10042, 'westin': 15850, 'discounts': 4630, 'anime': 1356, 'expoï': 5494, 'provides': 11557, 'hotels': 7362, 'expo': 5490, '2396': 163, 'nowadays': 10261, 'oregon': 10534, 'sandiego': 12527, 'cda': 3075, 'convert': 3792, 'mp3': 9782, 'dinners': 4584, 'fintster': 5794, '_thrifty': 819, 'pairs': 10701, 'aberdeen': 897, 'forsaken': 5994, 'su0yr': 13912, 'footyball': 5955, 'llama': 8778, 'riah': 12211, 'shakas': 12838, 'againn': 1089, 'salads': 12496, 'cydia': 4140, 'reload': 12030, 'springboard': 13618, 'whattaburger': 15880, 'initial': 7690, 'rant': 11785, 'twune': 15130, 'felicia': 5687, 'carrrr': 3003, '08kaifj': 15, 'shark': 12862, 'micah': 9442, 'provided': 11554, 'blacks': 2284, 'overhere': 10617, 'dub': 4925, 'loveees': 8942, 'temp': 14365, '15c': 82, '30c': 230, 'sickee': 13044, 'breakingg': 2577, 'glitch': 6442, 'mix': 9581, 'delivered': 4392, 'ducks': 4935, 'magical': 9081, 'northbound': 10222, 'tuning': 15004, 'raising': 11768, 'handz': 6897, 'limp': 8706, 'coquitlam': 3832, 'logo': 8828, 'shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit': 12917, 'meeeaaannn': 9339, 'tai': 14211, 'rblpn': 11813, '5z10': 408, 'churchill': 3349, 'downs': 4821, 'goods': 6517, 'announcement': 1378, 'trekked': 14904, 'talkers': 14235, 'unabashedly': 15208, 'byw': 2831, 'spoon': 13598, 'screening': 12664, 'el': 5126, 'capitan': 2948, 'guniea': 6760, 'eachother': 4995, 'eeeeevvveeerrrr': 5088, 'hash': 6962, 'tags': 14209, 'novemeber': 10259, '_haze': 670, 'ray': 11809, 'appeared': 1467, 'aots': 1438, 'nekkid': 10030, 'religion': 12026, 'wen': 15839, 'venus': 15466, 'thngs': 14523, 'stuf': 13892, '_sis': 799, 'ging': 6394, 'summaa': 13993, 'declined': 4330, 'thehodge': 14470, 'inclusion': 7643, 'mono': 9663, 'dh': 4510, 'crumbling': 4026, 'graduates': 6596, 'cudve': 4069, 'kdg': 8208, 'cheek': 3219, 'americ': 1294, 'lingering': 8717, 'sipping': 13125, 'partyyyy': 10793, 'bang': 1915, 'pleaseeeeee': 11166, 'contentment': 3762, '7pm': 481, 'dat': 4239, 'enthusiastic': 5259, '888': 498, 'cubes': 4061, 'vacationing': 15415, 'clogged': 3454, 'mooorrreeee': 9693, 'movin': 9770, 'arghhh': 1535, 'korean': 8372, 'canazarro': 2918, 'bricked': 2600, 'secured': 12710, 'element': 5137, 'dom': 4756, 'cob': 3508, 'alexi': 1183, 'mh': 9434, 'lok': 8831, 'headeache': 7024, 'toda': 14691, 'grice': 6669, 'kc': 8206, 'pque': 11365, 'las': 8498, 'pinas': 11060, 'kenan': 8227, 'distorted': 4676, 'disapointed': 4610, 'mmmmmm': 9595, 'jazzy': 7975, 'catchy': 3040, 'krogers': 8386, 'journaling': 8091, 'shish': 12937, 'pennies': 10894, 'balance': 1883, 'millionaire': 9490, 'hellooooooo': 7114, '_augustine': 574, 'coles': 3547, 'gashes': 6282, 'steakhouse': 13734, 'woowoo': 16122, 'danger': 4205, 'adsense': 1038, '47': 300, '060': 9, 'inr': 7712, 'tigerheat': 14612, '_liljess_x': 721, 'nonlong': 10190, 'gardens': 6272, 'impulse': 7624, 'buys': 2815, 'envelope': 5266, '2d': 192, 'vids': 15503, 'disabled': 4607, 'wishhh': 16034, 'prehistoric': 11395, 'patricia': 10822, 'cope': 3826, 'filter': 5765, 'gracious': 6587, 'wrecking': 16183, 'soaking': 13343, 'joys': 8100, 'ulcers': 15191, '4jaw9': 313, 'chelsey': 3241, 'affected': 1064, 'behaved': 2108, 'appreci': 1486, '_sb': 792, 'procrastinating': 11484, 'cornwall': 3844, 'phenomenal': 10968, 'heheï': 7096, '½i': 16535, 'confit': 3705, 'duck': 4931, 'canad': 2912, 'burning': 2769, 'piglets': 11044, 'chucks': 3346, 'ashton': 1607, 'slacking': 13190, 'pilates': 11050, 'fastest': 5626, 'gch': 6297, 'chiodos': 3291, 'dissapointed': 4669, 'heyyhoo': 7157, 'heyhey': 7153, 'dull': 4944, 'uncool': 15222, 'discouraged': 4631, 'todayyyyyy': 14697, 'starsailor': 13696, 'duong': 4962, '19': 107, 'cnn': 3498, 'foto': 6009, 'finden': 5775, 'hea': 7016, 'bracket': 2541, 'dallas': 4175, 'odessey': 10351, 'ideal': 7519, 'terrific': 14402, 'aid': 1140, '2x2924': 221, '1x2610': 126, '3x2500': 283, '1841': 103, 'desktops': 4464, 'slpy': 13248, 'zzzz': 16521, 'luncheon': 9001, 'planters': 11133, 'ou': 10571, 'bom': 2415, 'apetite': 1450, 'nobodys': 10168, 'dips': 4594, 'francisco': 6033, 'djspy': 4707, 'playdate': 11146, 'transmission': 14875, 'aah': 880, 'woofers': 16105, 'diagram': 4519, 'mainstream': 9103, 'adoption': 1030, 'curve': 4102, 'grr': 6703, 'discussion': 4641, 'loll': 8835, 'kansai': 8175, 'current': 4097, 'groups': 6696, 'juniors': 8140, 'korea': 8371, 'refs': 11962, 'yao': 16296, 'playoffs': 11154, 'cruising': 4024, 'wcf': 15757, 'subscribe': 13925, 'everr': 5378, '30stm': 236, 'weeooow': 15804, 'pfffffffffffffffffffffftttttttt': 10955, 'tourney': 14810, 'coool': 3815, 'wasnï': 15706, 'eeeh': 5089, 'whattttever': 15882, 'choc': 3304, 'chip': 3292, 'voicemail': 15570, 'ffs': 5722, 'suse': 14097, 'shweeeeet': 13035, 'cï': 4149, 'qua': 11674, 'jmccartney': 8041, 'twittpic': 15119, 'diddy': 4533, 'spooning': 13599, 'opportunity': 10509, 'wakes': 15639, '67xv3': 450, 'former': 5985, 'pawnshop': 10840, 'jimi': 8030, 'cashing': 3020, 'sickly': 13052, 'yippeee': 16391, 'enlargement': 5238, 'steph': 13744, 'improve': 7622, '2837': 183, 'heavennn': 7067, 'stare': 13690, 'chandler': 3150, 'meal': 9305, 'corey': 3838, '30pm': 232, 'noones': 10197, 'dcgeyv': 4282, 'elet': 5142, 'br': 2538, 'dampen': 4191, 'sliverlight': 13234, 'mariahs': 9186, 'pci': 10857, 'sets': 12803, 'cytheria': 4145, 'hulu': 7430, 'thaanks': 14430, 'wokking': 16078, 'amateur': 1273, 'ark': 1544, 'builder': 2727, 'political': 11240, 'affiliation': 1066, 'wana': 15660, 'manning': 9158, 'norwood': 10227, 'haaaaaa': 6789, 'yaaaaaaay': 16275, '_in': 686, 'raul': 11804, 'julia': 8124, 'wimpers': 15994, 'hyped': 7486, 'mmemarko': 9591, 'schilderweb': 12622, 'homepage': 7279, 'pritchard': 11460, 'olds': 10412, 'hairloss': 6842, 'voyed': 15590, 'slumdog': 13251, 'wbt6': 15755, 'cooks': 3808, 'happeh': 6912, 'blankie': 2299, 'icebox': 7508, 'stomatch': 13793, 'wacom': 15619, 'gorgeou': 6548, 'donny': 4771, 'nottt': 10252, 'commercials': 3613, 'dared': 4220, 'wealthy': 15762, 'minted': 9523, 'plural': 11194, 'mope': 9698, 'nos': 10228, 'livewriter': 8770, 'accessible': 932, 'sniffles': 13322, 'jaunty': 7965, 'jackalope': 7921, 'intrepid': 7790, 'ibex': 7501, 'among': 1310, 'giveaways': 6418, 'tati': 14285, 'emal': 5168, 'nastiness': 9954, 'bestfriends': 2159, 'wheniwerealad': 15894, 'explanations': 5483, 'pleeezzze': 11178, 'madeleines': 9069, 'baking': 1881, 'answerer': 1392, '67gzx': 441, 'presents': 11414, 'sisa': 13128, 'afterwork': 1083, 'blogs': 2346, 'tuned': 15001, 'kwijt': 8403, 'pinging': 11069, 'opera': 10498, 'kellynn': 8225, '789': 472, 'pacman': 10678, 'stronger': 13869, 'upwards': 15345, '600k': 416, 'restart': 12138, 'offered': 10366, 'shivashankar': 12943, 'spellin': 13551, 'inertial': 7674, 'oneself': 10449, 'unofficially': 15291, 'fi': 5724, 'chota': 3330, 'reflexie': 11956, 'arcade': 1515, 'areply': 1530, 'buggin': 2718, 'sole': 13378, 'supporter': 14064, 'credi': 3968, 'basics': 1970, 'analysis': 1324, 'theatres': 14465, 'twittttty': 15122, 'bores': 2482, 'bullied': 2739, 'needing': 10003, 'math11': 9252, 'acct1b': 947, 'bio19': 2236, 'dreary': 4866, 'rubbing': 12393, 'jessi': 8010, 'quiero': 11705, 'rosa': 12342, 'guadalupe': 6731, 'damm': 4182, 'virgins': 15529, 'reactions': 11830, 'stroking': 13866, 'welts': 15838, 'ie': 7535, '30s': 234, 'earbud': 4999, 'ceased': 3079, 'shure': 13026, 'workyy': 16144, '3lin': 274, 'uninvited': 15268, 'deid': 4374, 'bd': 2027, 'sundayyyy': 14021, 'inshalla': 7719, 'devo': 4499, 'astor': 1640, 'dope': 4785, 'smashing': 13270, 'reznor': 12200, 'mariqueen': 9192, 'creme': 3978, 'brulee': 2669, 'tiramisu': 14653, 'models': 9617, 'banjo': 1921, 'tooie': 14750, 'springfield': 13619, 'noisy': 10183, 'piggls': 11043, 'pickles': 11021, 'needless': 10006, 'drool': 4893, '33hus': 251, 'yeyah': 16379, 'fink': 5789, 'popeye': 11278, 'hopin': 7328, 'helsinki': 7124, 'presentations': 11412, 'ankle': 1360, 'stupido': 13906, 'catwalk': 3050, 'janes': 7950, 'sm': 13257, 'sweatshirt': 14128, 'doughnuts': 4807, 'gardening': 6271, 'paintings': 10698, 'mi': 9438, 'ideia': 7521, '_ming': 743, 'gg': 6358, 'sch': 12613, 'attachment': 1667, 'yan': 16292, 'kinny': 8307, 'jayem': 7971, 'b2b': 1806, 'krisisdnb': 8380, 'asx': 1647, 'worm': 16149, 'volt': 15575, 'dxd': 4978, 'didnï': 4538, 'ood': 10465, 'diana': 4522, 'tutor': 15020, 'c1': 2833, 'c2': 2834, 'lulz': 8995, 'notting': 10251, 'prick': 11436, 'touching': 14801, 'handshakes': 6893, 'pokes': 11230, 'ys': 16447, 'hairrr': 6843, 'treasures': 14894, 'seos': 12767, 'submitting': 13921, 'information': 7684, 'pains': 10693, 'handful': 6886, 'wellllllllllllll': 15835, 'pllleeeaaassseeeeee': 11183, 'munching': 9836, 'rent': 12065, 'unexpected': 15245, 'complications': 3657, 'grapes': 6620, '4wauk': 346, 'ic': 7503, 'iyaa': 7908, 'lasts': 8506, 'buddi': 2702, 'blasting': 2302, 'celine': 3094, 'dion': 4587, 'untill': 15311, 'rides': 12231, 'procrastination': 11485, 'assumption': 1636, 'rum': 12412, 'gin': 6392, 'preferred': 11391, 'previously': 11430, 'whit': 15919, 'fidel': 5732, 'shemms': 12900, 'wxsgy': 16223, 'sanctity': 12522, 'tainted': 14214, '360': 258, 'mercenaries': 9396, 'explosions': 5489, 'database': 4241, 'superstition': 14052, 'frost': 6120, 'nofakery': 10175, 'blacklisted': 2282, 'warner': 15688, 'bros': 2651, 'straw': 13833, 'manly': 9155, 'babygirl': 1822, 'crazier': 3949, 'appearance': 1466, 'itouch': 7887, 'noggin': 10176, 'rey': 12196, 'mysterio': 9897, 'sleepin': 13208, 'aga': 1085, 'waterfront': 15724, 'princess_i': 11448, 'butineedhelp': 2796, 'csi': 4051, 'aus': 1709, 'silicone': 13083, 's05e04': 12448, 'evryone': 5407, 'pleaseee': 11163, 'toss': 14787, 'pd': 10859, 'kittie': 8322, 'berlin': 2150, 'holllaaa': 7262, 'analytics': 1325, 'tostitos': 14792, 'piknik': 11049, 'dip': 4590, 'unfortunate': 15253, '_la_mania': 715, 'mask': 9230, 'whoooops': 15943, 'confusion': 3713, 'competetion': 3639, 'counting': 3877, 'jibber': 8024, 'rodney': 12304, 'chasing': 3197, 'fireflies': 5800, 'stfu': 13757, 'moooooorning': 9691, 'depot': 4428, 'plywood': 11198, 'apprentice': 1491, 'upppp': 15332, 'tourture': 14812, 'lovies': 8956, 'thinker': 14510, 'loool': 8878, 'killen': 8285, 'recharge': 11884, 'aaahhh': 872, 'showers': 13007, 'meanwhile': 9315, 'bigweekend': 2219, 'awaits': 1745, 'misse': 9553, 'ties': 14608, 'rebound': 11873, 'meetin': 9344, 'jewelery': 8019, 'pooling': 11262, 'gremlin': 6662, 'challenging': 3139, 'slipped': 13230, 'unplugging': 15296, 'ouchhhhhh': 10573, 'frkn': 6112, 'watir': 15732, 'wiki': 15975, 'hydro': 7484, 'abandoned': 888, 'niley': 10133, 'doppppe': 4786, 'moshie': 9731, 'moshhhh': 9730, 'blistered': 2333, 'yaaaaaay': 16276, 'jaydiohead': 7970, 'twt': 15128, '91610': 521, 'vegetables': 15448, 'hellboy': 7109, 'acsm': 972, 'unfathomable': 15247, 'bedrooms': 2083, 'revive': 12192, 'spirits': 13576, 'bun': 2754, 'vicodin': 15494, 'bjaday': 2270, 'drivers': 4887, '_mean': 740, 'niece': 10111, 'lite': 8752, 'p250': 10662, 'vcenter': 15439, 'x64': 16239, 'compability': 3628, 'matrix': 9256, 'grabbing': 6582, 'carafe': 2956, 'caffiene': 2851, 'amcmain': 1290, 'teh': 14342, 'vague': 15421, 'southland': 13487, 'raw': 11807, 'misunderstood': 9573, 'nephews': 10036, 'cowering': 3903, 'pigeon': 11040, 'feathers': 5665, 'sittting': 13142, 'tyra': 15149, 'scream': 12656, 'rrrrr': 12386, 'richmond': 12220, 'conditions': 3689, 'goodtimes': 6518, 'pitch': 11096, 'legends': 8600, '6nkpuz': 455, 'manned': 9157, 'overspend': 10626, '11th': 49, 'saget': 12487, 'kit': 8316, 'avaialble': 1723, 'dhc4hg': 4512, 'mod': 9613, 'giftcert': 6376, 'hipfabric': 7204, 'accent': 924, 'lucy': 8988, 'gweg': 6778, 'thoughtful': 14543, 'conti': 3769, 'yjpq': 16393, 'dadgum': 4166, 'nations': 9963, 'freight': 6071, 'carriers': 3001, 'fella': 5690, 'edmonton': 5073, 'nokia': 10184, 'mozilla': 9780, 'os': 10556, 'operating': 10501, 'journalists': 8093, 'prs': 11563, 'ironing': 7847, 'oceans': 10342, 'madly': 9071, 'monte': 9674, 'cristo': 3999, 'sandwich': 12531, 'bi': 2199, 'focals': 5905, 'irresponsible': 7854, 'transatlantic': 14865, 'flights': 5866, 'lolol': 8842, 'ohshit': 10390, 'pleaseeeeeeeeeee': 11167, '_rosie': 788, 'everrrr': 5380, 'bandwidth': 1914, 'scholl': 12624, 'sandal': 12525, 'inserts': 7718, 'abit': 901, 'loocie': 8867, 'soonish': 13425, 'ane': 1336, 'dining': 4581, 'plangi': 11126, 'lung': 9006, 'coat': 3506, 'cfs': 3119, 'midday': 9454, 'millenia': 9487, 'bulb': 2732, 'syncing': 14179, 'blairr': 2290, '80th': 488, 'fallower': 5573, 'xxxxxxxx': 16271, 'asbestos': 1598, 'backroom': 1841, 'processing': 11480, 'technique': 14325, 'pp': 11360, 'buffalo': 2711, 'worshipper': 16159, 'schack': 12614, '4w2ls': 337, 'cauzinhoooo': 3057, '_bandoni': 580, 'environmental': 5269, 'kbs': 8205, 'rescuers': 12120, 'dru': 4904, 'mickey': 9447, 'sequels': 12774, 'brutal': 2678, 'nate': 9959, 'judo': 8117, 'retail': 12155, 'therapy': 14487, 'awesomest': 1771, 'knooow': 8348, 'uve': 15405, 'avenue': 1732, 'fade': 5545, 'regrets': 11985, 'cigarettes': 3364, 'bands': 1911, 'headaaaaaaaaaaaache': 7018, 'ecpm': 5052, 'sly': 13256, 'keane': 8210, 'sunnn': 14027, 'finnalllyyy': 5792, 'dubbed': 4927, 'forwarding': 6006, 'germï': 6346, 'rodrï': 12305, '½guez': 16533, 'xxxrebelrebelxxx': 16266, 'pups': 11622, 'raven': 11805, 'thors': 14535, 'wonderfur': 16093, 'kittykisses': 8325, 'shitload': 12939, 'bananas': 1909, 'abandoning': 889, 'parvo': 10795, 'pup': 11618, 'moons': 9688, '94': 525, 'dndn': 4714, 'sanctuarysunday': 12524, 'sanctuary': 12523, 'requiem': 12106, 'hahahahahahahaa': 6824, 'vw': 15599, 'alriiightt': 1243, 'featured': 5667, 'ykyat': 16394, '37nnd': 263, 'pile': 11051, 'delay': 4376, 'cnyhp': 3500, 'touchin': 14800, 'starss': 13698, 'everyoneeeeeeeeee': 5392, 'items': 7884, 'etsy': 5347, 'beloved': 2132, 'barack': 1937, '626': 421, 'derbyshire': 4440, 'percy': 10911, 'thrower': 14567, 'gardner': 6273, 'residence': 12126, 'allo': 1216, 'winds': 16004, 'outfield': 10585, 'scanlon': 12593, 'hoarse': 7235, 'pastor': 10810, 'wiped': 16021, 'xxxxxxx': 16270, 'plebs': 11173, 'thennn': 14481, '72': 464, '65': 426, 'knack': 8334, 'melted': 9362, 'speaker': 13526, 'rings': 12245, 'oatmeal': 10319, 'gmail': 6456, 'altanta': 1249, 'flying': 5898, '_crow': 606, 'granted': 6617, 'poltergeist': 11248, 'neeeeeeeed': 10012, 'tedtalks': 14330, 'foul': 6014, 'dialed': 4520, 'payed': 10847, 'gameplay': 6247, 'eleminis': 5139, 'dhlq5t': 4514, 'screamm': 12660, '613': 420, 'installments': 7737, 'lottery': 8915, 'combo': 3580, 'wayyy': 15750, 'lvoe': 9025, 'triumph': 14934, 'luc': 8975, 'bourdon': 2515, 'ck': 3390, 'eztv': 5524, 'torrent': 14779, 'movieee': 9767, 'donkey': 4769, 'smokey': 13291, 'jemi': 7996, 'cramples': 3928, 'bursting': 2778, 'nab': 9908, 'larenz': 8492, 'fineass': 5779, 'tate': 14284, 'popularity': 11286, 'clubhouse': 3481, 'wl': 16065, 'spaces': 13505, 'aniya666': 1358, 'suspicion': 14103, 'standard': 13676, 'ourselves': 10579, 'remeber': 12038, 'dolphin': 4755, 'peers': 10882, 'creeper': 3974, 'swensens': 14146, 'woody': 16104, 'bullseye': 2741, 'crime': 3989, 'ooc': 10464, 'yearbook': 16334, 'narnia': 9949, 'blurb': 2377, 'aslan': 1616, 'skandar': 13157, 'hosp': 7352, 'pedicure': 10873, 'embarrassed': 5172, 'irate': 7839, 'callers': 2879, 'arre': 1563, 'followin': 5927, 'booooored': 2455, 'lindsay': 8709, 'fansite': 5596, 'ultimatelohan': 15193, 'rolled': 12313, '4we51': 349, 'bugzy': 2723, 'tinkered': 14643, 'virtualbox': 15531, 'numan': 10280, 'remixes': 12051, 'pipers': 11082, 'molars': 9631, '33333333333': 249, 'cooool': 3817, 'lovve': 8962, 'ro': 12269, 'shoooow': 12963, 'tmwr': 14680, 'uncomfortable': 15219, 'rss': 12388, 'borred': 2488, '400mb': 288, 'onpeak': 10458, 'downloads': 4819, 'appointments': 1484, 'farrah': 5611, 'cashflow': 3018, 'forecasts': 5965, 'myka': 9885, 'yeahs': 16332, 'softshock': 13369, 'jager': 7933, 'licked': 8660, 'trashed': 14882, 'jalapeno': 7939, 'chaparros': 3167, 'del': 4375, 'ee': 5081, 'minits': 9514, 'leff': 8594, 'librefm': 8658, 'audacious': 1690, 'combination': 3578, 'wink': 16012, 'matte': 9259, 'lcd': 8558, 'hongkong': 7297, 'international': 7776, 'ignor': 7544, 'lookd': 8869, 'familar': 5578, 'appropriate': 1494, 'seduced': 12715, 'homee': 7274, 'okiee': 10405, 'angels': 1342, 'makingfun': 9119, 'inlove': 7698, 'oooooooooo': 10479, 'dancin': 4201, 'steals': 13737, 'jumpstart': 8134, 'wotd': 16166, 'jape': 7958, 'qq': 11670, 'comppetitive': 3663, 'overcompetitive': 10612, 'responses': 12136, 'painfully': 10692, 'pange': 10732, 'ahahahahaha': 1121, '_truong': 824, 'hb': 7010, 'encore': 5205, 'snsd': 13334, '_85': 551, 'aaaargh': 866, 'conspiracy': 3742, 'lizzi': 8775, 'environment': 5268, 'tweeet': 15032, 'shareeee': 12859, 'monthly': 9677, '5days': 390, 'gangsterrr': 6259, 'unit': 15273, 'flordia': 5878, 'shrits': 13016, 'cinnamon': 3375, 'rolls': 12317, '4wp8l': 358, 'chilly': 3284, 'oracle': 10522, 'sock': 13356, 'summit': 14000, 'spew': 13560, 'withb': 16042, 'alrer': 1239, 'addictive': 998, 'slowed': 13243, 'quickly': 11704, 'creatur': 3967, 'dne': 4715, 'unbelievably': 15216, 'firing': 5806, 'districtlines': 4682, 'revertfashion': 12184, 'didntb': 4537, 'cy_k': 4132, 'cymk': 4141, 'legend': 8599, 'inotia': 7707, '31': 239, 'immediately': 7596, 'funnest': 6173, 'righ': 12238, 'moab': 9603, 'phil': 10971, 'fullest': 6153, 'rih': 12241, '4jhp8': 323, '67kb6': 446, 'youl': 16419, 'cert': 3112, 'excursion': 5444, 'mixer': 9583, 'reminders': 12046, 'hummmmm': 7439, 'thar': 14450, 'jayk': 7972, 'handmade': 6891, 'menudo': 9392, 'permanently': 10930, 'weathers': 15769, 'dazzle': 4276, 'widddd': 15962, 'als': 1245, 'peoplebrowsr': 10901, 'urge': 15349, 'sofas': 13364, 'tiling': 14624, 'shock': 12951, 'bonding': 2420, 'autumn': 1722, 'sheeeeit': 12882, 'claude': 3415, 'summahkayy': 13994, 'effin': 5104, 'overwhelming': 10633, 'birfdayy': 2245, 'soupy': 13480, 'heinz': 7099, 'bbl': 2015, 'win1': 15996, 'mito': 9579, 'stealing': 13736, 'tiesto': 14609, 'evicted': 5400, 'suspend': 14101, 'fewer': 5717, 'mem': 9364, 'leaks': 8570, 'replied': 12082, 'custom': 4108, 'designed': 4455, 'superfresh': 14046, 'somebodies': 13394, 'rails': 11753, 'sup': 14041, 'sits': 13138, 'summat': 13996, 'ju': 8111, 'witty': 16054, 'situational': 13144, 'equations': 5289, 'graphs': 6624, 'motivating': 9746, 'emotionally': 5189, '_rose': 787, 'follows': 5929, 'nites': 10150, 'haribo': 6943, 'relatively': 12007, 'vespa': 15477, 'deena': 4344, 'paranoif': 10758, 'lolz': 8844, 'retweets': 12177, 'bid': 2205, 'leaders': 8563, 'theatlantic': 14463, 'facebookhumor': 5538, 'mhtml': 9437, 'w12th': 15603, 'kaiboshed': 8163, 'wtg': 16205, 'dvsca': 4976, 'bh': 2195, 'technologically': 14327, 'challenged': 3137, 'tweetb4ueat': 15041, 'positivity': 11313, 'received': 11878, 'standin': 13679, 'maddest': 9066, 'cofffeeeeeee': 3532, 'gossiping': 6555, 'bia': 2200, 'lion': 8728, 'problematic': 11473, 'maps': 9167, 'snapped': 13308, 'katy': 8195, 'perry': 10934, 'soundtrack': 13478, 'arr': 1559, 'sowbur': 13491, 'promiss': 11526, 'styoopid': 13910, 'hazzunt': 7009, 'kum': 8395, 'owt': 10650, 'agane': 1091, 'itt': 7892, 'hyding': 7482, 'sumware': 14008, 'larffing': 8493, 'att': 1666, 'rambly': 11773, 'cathylo': 3045, 'fran': 6030, 'bestfriend': 2158, 'aubrey': 1687, 'pearl': 10869, 'uninteresting': 15266, 'snuggled': 13338, 'beneath': 2138, 'duvet': 4973, 'vibe': 15487, 'downed': 4810, 'barrio': 1953, 'records': 11911, 'cmyk': 3495, 'madrid': 9073, 'choke': 3312, 'py3': 11652, '_boo': 589, 'unfortunatley': 15255, 'amazed': 1276, 'perform': 10917, 'halu': 6865, 'yï': 16476, 'tï': 15154, 'tinh': 14641, 'thï': 14589, '½ng': 16542, 'sanh': 12538, 'mï': 9903, 'chï': 3354, 'pineforest': 11067, 'ojugsb': 10397, 'conjunctivitis': 3726, 'vedanta': 15442, 'vibrations': 15490, 'remembering': 12042, 'mazembe': 9280, 'lamps': 8463, 'offended': 10362, 'lovelies': 8945, 'okami': 10400, 'jose': 8082, 'noooooooooo': 10206, 'pointy': 11225, 'enw': 5271, 'kaput': 8178, 'swords': 14162, 'reallly': 11855, 'horrid': 7340, 'dealin': 4298, 'impatiently': 7603, 'hor': 7332, 'heartbreaks': 7053, 'rescue': 12118, 'bootcamp': 2461, 'knoxville': 8358, 'tabs': 14197, 'attics': 1681, 'eden': 5061, '½9': 16528, '½13': 16523, 'ct': 4056, 'steadily': 13731, 'pns': 11203, 'availble': 1727, 'tent': 14385, 'mosh': 9729, 'edgefesssssst': 5064, 'boyzone': 2536, 'shoesshoesshoes': 12957, 'yayyayyay': 16310, 'advance': 1044, 'sponsor': 13594, 'bea': 2031, 'eithe': 5122, 'appears': 1469, 'signs': 13074, '4jax3': 314, 'ableton': 903, 'chars': 3194, 'tarte': 14270, 'dde2v6': 4285, 'newsletter': 10077, '2for1': 195, 'awesomeupdater': 1772, 'loney': 8853, 'pimm': 11058, 'tattered': 14287, 'hoovering': 7315, 'warcraft': 15679, 'lazzzy': 8554, 'indonesia': 7667, 'stating': 13717, 'thatd': 14454, 'campus': 2910, 'cov': 3894, 'producers': 11490, 'mustard': 9864, 'veges': 15446, 'constructivist': 3749, 'isnï': 7867, 'hahahahahahahahahahahaha': 6826, 'cravings': 3942, 'crispy': 3997, 'skyping': 13186, 'allison': 1206, 'mooned': 9687, 'liverpool': 8768, '_brads': 590, 'curling': 4093, 'ribbon': 12213, '_de_baillon': 615, 'antibiotics': 1401, 'gnight': 6464, 'squirted': 13637, 'irrelevant': 7853, 'innocently': 7705, 'rda2009cla': 11819, 'labels': 8419, 'needy': 10008, '29': 185, 'guiness': 6750, 'bidding': 2206, 'dropping': 4897, 'urgent': 15350, 'persist': 10936, 'blossom': 2358, 'almonds': 1223, 'incorporate': 7649, '_girl': 653, 'notebook': 10235, 'wrinkles': 16188, 'canadians': 2915, 'tur': 15007, 'drumset': 4911, 'whaaaaaaaahhhh': 15857, 'sighs': 13063, '4wn9q': 357, 'nerding': 10039, 'andrew': 1330, 'yolanda': 16402, 'equivalent': 5291, 'overheardinlondon': 10614, 'runaway': 12417, 'weatherrrrr': 15768, 'knotts': 8349, 'mannnn': 9159, 'wagon': 15624, 'jogged': 8057, 'turbines': 15008, 'atlanits': 1658, 'bryan': 2679, 'wingnuts': 16009, 'ironic': 7846, 'auditions': 1699, 'virtues': 15534, 'roc': 12289, 'chu': 3343, 'creeped': 3973, 'dodgy': 4732, 'neighbourhood': 10025, 'lovesu': 8953, 'ababa': 887, 'rutledge': 12441, 'jbcg': 7977, 'magners': 9083, 'themselves': 14479, 'shell': 12894, 'slave': 13197, 'command': 3602, 'lifts': 8678, 'boo0o0o0o00oring': 2429, 'dollars': 4752, 'crazi': 3948, 'mustv': 9866, 'thunderstorming': 14582, 'listened': 8743, 'domination': 4759, 'grooveshark': 6686, 'tinysong': 14646, '36pz': 260, 'cahnge': 2855, 'pw': 11647, 'ufff': 15163, 'dc101': 4280, 'ant': 1396, 'djing': 4705, 'boooooo': 2452, 'ditto': 4688, 'wrapped': 16177, 'outtamyleague': 10599, 'pardon': 10760, 'relapse': 12002, 'explained': 5482, 'iloveyouuu': 7578, 'bamboozle': 1904, '_2008': 545, 'sadie': 12474, 'included': 7640, 'mental': 9383, 'ov': 10605, '13f5m0': 67, 'disbelief': 4621, 'alternatives': 1254, 'posited': 11309, 'tweepsland': 15038, 'shaved': 12873, 'beards': 2041, 'minority': 9521, 'phillies': 10974, 'gamee': 6246, 'oyy': 10659, 'geas': 6303, 'ts': 14971, 'crikey': 3988, 'vineri': 15518, 'nimic': 10134, 'sau': 12569, 'probabil': 11468, 'alt': 1247, 'pierdut': 11036, 'ease': 5019, 'syncs': 14180, 'appealing': 1464, 'clunky': 3486, 'foll0w': 5919, 'friidays': 6108, 'f0llowers': 5525, 'fjgkfld': 5834, 'sdh': 12683, 'hometown': 7284, 'turnon': 15013, 'dillematic': 4572, 'damp': 4190, 'adopted': 1028, 'ub40': 15156, '7af72': 477, 'russia': 12434, 'cracking': 3921, 'economics': 5049, 'tommorow': 14721, 'retaking': 12157, 'hva': 7474, 'shw': 13032, 'scifi': 12632, 'flake': 5842, 'jacks': 7928, 'metros': 9422, 'nextday': 10082, 'uprooted': 15334, '5jg6f': 398, 'bri': 2596, 'alreadi': 1235, 'craigslist': 3923, 'blouse': 2359, 'unconditonally': 15220, 'cupboards': 4082, 'terminal': 14392, 'ignorance': 7545, 'controls': 3784, 'marrying': 9210, 'lovable': 8928, 'guuud': 6774, 'wilshire': 15991, 'stalkerishly': 13669, 'middlesbrough': 9456, 'dreaded': 4853, 'katieheidie': 8193, 'sperm': 13559, 'clone': 3457, 'heater': 7062, 'suddenly': 13958, 'chelsea': 3239, 'hurtig': 7464, 'vaccines': 15418, 'todayyy': 14696, 'loveeya': 8943, '4wk9i': 354, 'pose': 11306, 'ericsson': 5303, 'disturb': 4683, 'maaate': 9045, 'grooovin': 6684, 'chain': 3129, 'sluzzaa': 13255, 'gumbo': 6754, 'tommorrow': 14722, 'description': 4447, 'picturisation': 11029, 'whom': 15937, 'picks': 11022, 'blury': 2379, 'tweep': 15034, 'businesses': 2787, 'sinking': 13117, 'youregreat': 16426, 'mirrors': 9535, 'thermal': 14492, '75c': 470, 'gpu': 6575, 'vin': 15515, 'greeting': 6659, 'subtl': 13929, 'prima': 11440, 'catchyy': 3041, 'clogging': 3456, 'clocked': 3453, 'comforting': 3593, 'savechuck': 12574, 'takin': 14223, 'pwned': 11650, 'shottie': 12989, 'exahausted': 5417, 'tallebudgera': 14242, '5pm': 403, 'bumped': 2751, 'shelaaaaaaaa': 12890, 'numero': 10284, 'sarcastic': 12551, 'toughest': 14803, 'firth': 5810, 'groupie': 6694, 'slovakian': 13241, 'siccck': 13041, 'redbull': 11926, 'toilets': 14708, 'toasting': 14688, 'waffles': 15621, 'crampsss': 3930, 'workmen': 16138, 'extending': 5505, '45am': 297, 'racket': 11740, 'birth': 2247, 'misshu': 9557, 'cal': 2862, 'irvine': 7855, 'tolerate': 14714, 'iemoticons': 7537, 'appstore': 1498, '79ï': 475, 'basketball': 1975, 'iron': 7844, 'curls': 4094, 'mason': 9231, '_b_10': 577, 'sed': 12713, 'zguqp': 16499, 'memorizing': 9372, 'prologue': 11518, 'canterbury': 2937, 'tales': 14230, 'juhs': 8121, 'florists': 5882, 'cassadee': 3025, 'fricken': 6088, 'florist': 5881, 'noob': 10193, 'wooooo': 16113, 'yankees': 16294, 'enlightening': 5241, 'masseusse': 9237, 'buttocks': 2805, 'quarter': 11682, 'inch': 7635, 'joyologist': 8098, 'freak': 6046, 'chipping': 3296, 'barks': 1947, 'strict': 13859, 'lonley': 8863, 'boyzzzz': 2537, 'gahd': 6229, 'wearin': 15764, 'brudder': 2664, 'prosper': 11541, 'worr': 16153, 'detention': 4483, '_gun': 660, '89': 501, '_b': 576, 'comming': 3614, 'outdated': 10583, 'bigoted': 2218, 'patronising': 10825, 'imperialist': 7604, 'monoculturalist': 9664, 'righteous': 12240, 'helloooo': 7113, 'hairsss': 6846, 'actinggg': 975, 'noooooooooooo': 10208, 'awesomely': 1768, 'slices': 13221, 'cheddar': 3217, '3000': 225, 'wafting': 15622, 'papaw': 10743, 'smoker': 13289, 'trop': 14943, 'awa': 1744, 'lofnotc': 8816, 'jordan': 8078, 'jeffs': 7991, 'seventeen': 12813, 'fascination': 5619, 'pretend': 11421, 'shakedown': 12841, 'tomm': 14720, 'seafood': 12687, 'chow': 3331, 'hall': 6851, 'noooowww': 10211, 'tomrrow': 14732, 'amisha': 1306, 'patel': 10814, '1984': 111, 'yesssssir': 16368, 'pampering': 10721, 'fem': 5695, 'kool': 8370, 'tink': 14642, 'senior': 12751, 'recognition': 11894, 'carpet': 2998, 'booze': 2466, 'vouchers': 15588, 'meaningful': 9310, 'carlton': 2988, 'benz': 2147, 'serviced': 12794, 'jennah': 7999, 'amaaaazing': 1264, 'kis': 8312, '2am': 189, 'loveeeeeeee': 8940, 'weho': 15810, 'brit': 2621, 'beyonce': 2181, 'uhmygawddd': 15186, 'trivia': 14935, 'braggin': 2546, 'einstein': 5120, 'yesss': 16366, 'aaaaaaaaaaa': 854, 'rogers': 12307, 'travesty': 14890, 'stressing': 13853, 'associated': 1633, 'global': 6444, 'agencies': 1096, 'viva': 15554, 'juicy': 8123, 'testers': 14409, 'asap': 1596, 'sacred': 12461, 'yoursel': 16430, 'bastard': 1979, 'inerne': 7673, 'gtg': 6727, 'santi': 12545, 'interlock': 7772, 'calculus': 2866, 'derivative': 4443, 'identity': 7522, 'asaran': 1597, '_shaw': 793, 'tekenen': 14346, 'awfully': 1777, 'coogan': 3799, 'moran': 9701, 'uncontrolable': 15221, 'covering': 3898, 'scarf': 12600, 'washington': 15703, 'calfornia': 2873, 'greet': 6658, 'against': 1090, 'bsb': 2683, 'loooooong': 8883, 'shutting': 13031, 'reprezent': 12098, 'zen': 16492, 'pag': 10682, 'ugghhh': 15170, 'views': 15509, 'estk': 5336, 'nemonem': 10034, 'hateee': 6970, 'glamorous': 6433, 'insanely': 7715, 'unbearable': 15213, 'probly': 11476, 'rlich': 12265, 'doogie': 4779, 'howser': 7389, 'captain': 2950, 'pius': 11102, 'pep': 10903, 'rally': 11770, 'concerns': 3679, 'cupie': 4085, 'mariage': 9184, 'ants': 1409, 'lollipop': 8836, 'posse': 11314, 'vacant': 15413, 'dynamic': 4985, '_it_good': 692, 'elite': 5148, 'epl': 5284, 'derby': 4439, 'upp': 15330, 'handsom': 6894, 'motorcycle': 9751, 'nudged': 10273, 'diane': 4523, 'relized': 12029, 'blech': 2310, 'idiom': 7525, 'chopped': 3320, 'liver': 8767, 'unravel': 15299, 'bambi': 1902, 'kansas': 8176, 'carthage': 3008, 'compliments': 3659, 'shontelle': 12959, 'layne': 8549, 'kingston': 8306, 'illuminated': 7569, 'dreaaaming': 4851, 'membership': 9367, 'grrrreat': 6707, 'railsbridge': 11754, 'picat': 11016, 'jeeeez': 7987, 'inbound': 7630, 'lincoln': 8707, 'tunnel': 15005, 'selfishness': 12736, 'pwns': 11651, 'workdone': 16131, 'redtape': 11940, '44': 294, '8k': 506, 'btween': 2689, 'comedyqueen': 3587, 'fundraiser': 6163, 'percent': 10910, 'platium': 11141, 'package': 10671, 'umma': 15203, 'wannabe': 15665, 'njoy': 10152, 'shuting': 13028, 'creative': 3963, 'tweetage': 15040, 'amongst': 1311, 'bags': 1863, 'obbsessed': 10321, 'thirsty': 14515, 'zulu': 16517, 'meth': 9417, '1million': 121, 'typical': 15144, 'thatwouldmake1of': 14457, 'myfriends': 9883, 'xxxxx': 16268, '5z4p7': 411, 'geoff': 6336, 'tenerife': 14377, 'b25651': 1805, 'digg': 4561, 'cyberstalking': 4135, 'privacy': 11461, 'prettie': 11424, 'esspensive': 5331, 'nao': 9942, 'haff': 6806, 'bearfoot': 2042, 'jolly': 8071, 'pirated': 11084, 'usefu': 15367, 'benn': 2144, 'beb': 2064, 'innn': 7703, 'chemical': 3243, 'sodahead': 13361, 'mychemicalromance': 9882, '_writer': 842, 'dissapeared': 4667, 'iknow': 7558, 'tutle': 15019, 'leiiin': 8611, 'introoo': 7794, 'varnishing': 15436, 'engine': 5223, 'bikes': 2222, '167': 93, 'weep': 15805, 'sos': 13465, 'accompanied': 938, 'gliss': 6441, 'brandi': 2558, 'carlile': 2984, 'gigwise': 6383, 'fillings': 5760, 'gates': 6287, 'charles': 3184, '6jwjmy': 454, 'mn': 9600, 'flicker': 5862, 'stephane': 13745, 'sympathy': 14173, 'suwweeeeet': 14110, 'fondont': 5931, 'howeva': 7384, 'mk': 9586, 'pam': 10720, 'exs': 5498, 'growl': 6701, 'tshirt': 14973, 'yoooouuuu': 16412, 'hooooommmeeee': 7309, 'tattooed': 14289, 'sizzling': 13154, '250e': 170, 'crazzyyyy': 3953, 'ci': 3355, 'rd': 11818, 'achan': 955, 'designia': 4458, 'tweaking': 15029, 'pleb': 11172, 'freezer': 6067, 'gprof': 6573, 'trailhead': 14855, 'sched': 12616, 'bazillionz': 2010, 'supprtin': 14069, 'jovani': 8095, 'asks': 1615, 'tiredd': 14656, 'mentaly': 9386, 'debit': 4309, 'dey': 4503, 'ina': 7627, 'gown': 6569, 'morrow': 9724, 'cramp': 3926, 'layout': 8550, 'funnel': 6172, 'mag': 9075, 'aches': 957, 'broompark': 2650, 'destroys': 4477, 'looove': 8891, 'cristal': 3998, 'bleeding': 2313, 'owwiee': 10652, 'deflated': 4364, '2345': 160, 'tissue': 14665, 'melrose': 9359, 'cashier': 3019, 'registrar': 11980, 'unfun': 15258, 'angles': 1346, 'hotness': 7363, 'personified': 10942, 'yayyy': 16311, 'haaaa': 6788, 'hoe': 7239, 'aero': 1057, 'louder': 8917, '_i_am_jes': 678, 'looooved': 8890, 'midnite': 9458, 'jumpy': 8135, 'brightly': 2612, 'darkest': 4223, 'stakc': 13665, 'pulaski': 11598, 'barre': 1952, 'chord': 3322, 'mixed': 9582, 'chocolatey': 3308, 'yoooo': 16407, 'rocstar': 12302, 'exhusband': 5456, 'submitted': 13920, 'repaired': 12072, 'xpvt7': 16261, 'mistakes': 9570, 'lettin': 8639, 'kettle': 8241, 'equation': 5288, 'nicest': 10102, 'basten': 1981, 'harrump': 6956, 'gagging': 6226, 'calexico': 2872, 'hanna': 6906, 'keeper': 8215, 'ile': 7561, 'unimpressed': 15264, 'ame': 1291, 'refused': 11968, 'shorten': 12978, 'luvd': 9017, 'cupcake': 4083, 'comfy': 3595, 'judgement': 8114, 'wgn': 15854, 'airing': 1152, 'programing': 11505, 'shouting': 12999, 'kacie': 8159, '_galore': 650, 'aaaw': 875, 'instincts': 7742, 'frustration': 6135, 'furniture': 6187, 'yeaah': 16324, 'fooood': 5947, 'confirm': 3702, 'signups': 13076, '67zgz': 451, '14m': 75, 'exctied': 5443, 'whispering': 15917, 'redmango': 11934, 'folding': 5915, 'shaking': 12843, 'prerecorded': 11402, 'jj': 8036, 'admk': 1023, '914044621160': 520, 'hyderabad': 7481, 'connections': 3732, 'slash': 13195, 'tyres': 15150, 'cuold': 4078, 'archetype': 1516, 'studied': 13886, 'archetypes': 1517, 'guts': 6769, 'hounslow': 7371, 'nw': 10303, 'kimbeommie': 8294, 'mainly': 9101, 'twi': 15064, 'duckraces': 4934, 'angus': 1350, '10jsep': 37, 'nicc': 10096, 'bon': 2419, 'iver': 7904, 'gents': 6333, 'retiring': 12164, 'beefin': 2090, 'gerbil': 6342, 'procrastinate': 11483, 'luking': 8992, '6500km': 427, 'gis': 6414, 'queries': 11692, 'goddam': 6476, 'gauge': 6290, 'dixon': 4700, 'gps': 6574, 'bttr': 2687, 'bulk': 2734, 'lipstick': 8732, 'glory': 6449, 'eternally': 5341, 'talkshow': 14239, 'larry': 8497, 'cage': 2854, 'watered': 15722, 'agents': 1099, 'wham': 15866, 'disproves': 4665, 'holland': 7259, '930': 523, 'intriguing': 7792, '__jazz__': 560, '8weeks': 510, 'greece': 6648, 'differently': 4551, 'shopped': 12968, 'uff': 15162, 'fusterated': 6194, 'vintage': 15520, 'topgear': 14764, 'freeeeeeeeeeeeeeeezing': 6062, 'soooooooon': 13432, 'yehhaaaaaaa': 16352, 'noe': 10172, 'hmmmmmmm': 7229, 'dvds': 4975, 'snuggles': 13339, 'dongggg': 4768, 'breakie': 2574, 'cisco': 3380, 'solve': 13386, 'whose': 15948, 'eewwwww': 5097, 'sergi': 12781, 'riverside': 12262, 'sosad': 13466, 'delightful': 4388, 'sgb': 12828, 'scrape': 12650, 'ville': 15513, 'shardup': 12856, 'admeeet': 1016, 'sadder': 12469, 'sweetdreams': 14138, 'landlord': 8471, 'peeing': 10877, 'sensation': 12754, 'suuuks': 14107, 'louisa': 8922, 'warden': 15680, 'dahh': 4170, 'thirty': 14516, 'daddddd': 4163, 'cousins': 3892, 'disembarking': 4645, 'tswassen': 14976, 'whales': 15865, 'pointing': 11221, 'memes': 9369, 'twitterring': 15109, 'myspac': 9893, 'bullshitting': 2742, '144': 73, 'geeta': 6311, 'plague': 11118, 'blending': 2318, 'newbie': 10067, 'bitched': 2258, 'particular': 10781, 'markets': 9199, 'decline': 4329, 'macarena': 9050, 'gota': 6558, 'alter': 1250, 'criminal': 3990, 'artificial': 1584, 'oofm': 10467, 'rihanna': 12242, 'arizzard': 1543, 'twhirl': 15063, 'expresso': 5497, 'weights': 15815, 'atlanta': 1659, 'commented': 3608, 'pixar': 11104, 'laavly': 8415, 'villains': 15512, 'chew': 3254, 'aerobics': 1058, 'tickled': 14599, 'dada': 4162, 'beachwood': 2035, '_missrachel': 745, 'sofa': 13363, 'pirating': 11085, 'clair': 3399, '_defcon1': 616, 'gather': 6288, 'continued': 3772, 'bunchh': 2756, '_dinasadik': 620, 'forensic': 5969, 'mein': 9353, 'kya': 8407, 'bas': 1959, 'jaao': 7918, 'micro': 9448, 'diagnosis': 4518, 'highland': 7178, 'screws': 12672, 'assfuck': 1626, 'dosriosrestaurant': 4794, 'telemarketers': 14350, 'perfection': 10914, 'renegades': 12059, 'closest': 3464, 'greenock': 6655, 'kilmacolm': 8292, 'awesomeily': 1767, 'obession': 10322, 'ugliest': 15180, 'uv': 15404, 'prisnor': 11458, 'trans': 14864, 'engage': 5221, '_griffin': 658, 'vodka': 15565, 'yessssss': 16369, 'planting': 11135, 'grandaddy': 6605, 'whisked': 15912, 'googled': 6523, 'blowout': 2364, 'permanent': 10929, 'earthquake': 5017, 'submarine': 13919, 'fiber': 5726, 'optics': 10515, 'damaged': 4178, 'bonnie': 2426, 'oster': 10559, 'alltel': 1219, 'welcom': 15828, 'zemote': 16491, 'litter': 8756, 'azalea': 1803, 'bunnies': 2757, 'heyyyyy': 7159, 'nancy': 9939, 'moves': 9765, 'laters': 8511, 'ux': 15408, 'criticisms': 4000, 'bccg': 2023, 'geeez': 6305, 'immobilizer': 7598, 'recliner': 11892, 'dhq': 4515, 'longgggg': 8859, 'mimis': 9496, 'tiredddd': 14658, 'odeeee': 10350, '_ryan': 790, 'carol': 2994, 'wildomar': 15981, 'towing': 14820, 'ape': 1449, 'javascript': 7968, 'mootools': 9696, 'odqwgh': 10353, 'responding': 12134, 'ganda': 6253, 'mga': 9431, 'scenes': 12609, 'svkch': 14114, 'downloadfestival': 4817, 'lineup': 8715, 'festivals': 5707, 'dl': 4709, 'uzbekistan': 15409, 'convenient': 3786, 'darlin': 4226, 'easties': 5027, 'silent': 13082, 'treatment': 14897, 'torture': 14784, 'method': 9419, 'psprint': 11571, 'karaoke': 8180, 'doze': 4826, 'snappy': 13312, 'gooodmorning': 6526, 'badminton': 1857, 'cooperate': 3821, 'itd': 7881, 'miranda': 9533, 'apply': 1482, 'sonics': 13415, 'bred': 2585, 'blu': 2367, 'decently': 4319, 'priced': 11432, 'jkin': 8039, '_nobel': 758, 'tapes': 14259, 'separate': 12769, 'desks': 4462, 'dividers': 4694, 'ceiling': 3083, 'scratching': 12654, 'kelli': 8222, 'edits': 5072, 'sombody': 13391, 'inpu': 7708, 'occasion': 10335, 'socal': 13348, 'concer': 3676, 'chilee': 3274, 'sleeptime': 13215, 'lannen': 8481, 'sass': 12555, 'scenie': 12610, 'denied': 4409, 'desperately': 4467, 'wahts': 15628, 'coffeclub': 3528, 'passes': 10800, 'wookiee': 16107, 'increased': 7650, 'suiva': 13982, 'transform': 14869, '74': 466, 'cinnamin': 3374, 'crunch': 4028, 'debussy': 4312, 'homeboy': 7273, 'clownin': 3477, 'flooded': 5873, 'htown': 7405, 'april': 1500, '_angel': 568, 'mms': 9599, 'waer': 15620, 'nexxt': 10083, 'feckin': 5670, 'bumps': 2752, 'festivus': 5710, 'mud': 9815, 'wrestling': 16186, 'spilling': 13570, 'wonderfully': 16092, 'harrassment': 6955, 'panera': 10731, '4j8yk': 312, 'thelma': 14473, 'rebeca': 11866, 'fernanda': 5701, 'symonds': 14169, 'cordova': 3836, 'lightning': 8684, 'scarededededed': 12598, 'gingg': 6396, 'cleanse': 3427, 'regarding': 11973, 'sista': 13130, 'chutzpah': 3351, 'kudai': 8392, 'chile': 3273, 'npr': 10265, 'choreographing': 3324, 'discover': 4633, 'ditch': 4686, 'myspaces': 9895, 'wwww': 16218, 'fiftyfivethreads': 5738, 'clothing': 3472, 'yoko': 16401, 'ono': 10455, 'gmorning': 6457, 'terrib': 14399, 'amazes': 1278, 'salt': 12506, 'grrrrr': 6708, 'twittered': 15094, 'hooome': 7307, 'expense': 5469, 'corrected': 3848, 'mobiles': 9606, 'slaves': 13198, 'advantages': 1047, 'imissu': 7592, 'pug': 11591, 'ight': 7543, 'bugg': 2715, 'shawna': 12875, 'damon': 4189, 'referring': 11953, 'castlebar': 3031, 'galway': 6242, 'adoptive': 1031, 'mommies': 9643, 'isint': 7859, 'ahhahahaha': 1126, 'ariyan': 1542, 'hamish': 6872, 'podcas': 11209, 'calorie': 2888, 'phishing': 10981, 'succeed': 13935, 'janessa': 7951, 'cinncinatti': 3376, 'adobe': 1026, 'registered': 11979, 'salaried': 12497, 'territory': 14404, 'freeeeeeee': 6061, 'britneys': 2627, 'spears': 13530, 'disgust': 4647, 'ovie': 10637, 'highlights': 7180, 'toke': 14710, '1thing': 125, 'led2': 8591, 'wants2': 15673, 'madison': 9070, 'deliveries': 4393, 'instructors': 7745, 'suprised': 14070, 'ion': 7823, 'itx': 7898, 'earliest': 5004, 'bikeshed': 2223, 'provolone': 11560, 'beefsteak': 2091, 'tomatoes': 14719, 'divine': 4695, 'leo': 8623, 'carillo': 2979, 'lhq8': 8651, 'microsoft': 9451, 'candidate': 2930, 'elora': 5157, 'danan': 4195, 'nsw': 10269, 'melbs': 9358, 'sorors': 13450, 'toni': 14736, 'gng': 6462, 'portable': 11295, 'sais': 12493, 'excist': 5431, 'photographer': 10997, '57': 385, 'trackpad': 14837, 'pak': 10706, '64': 424, 'boise': 2411, 'debate': 4305, 'waxed': 15744, 'okok': 10406, '_indian': 690, 'tlc': 14674, '_in_tx': 689, '30am': 229, 'yow': 16444, 'attal': 1671, 'bulls': 2740, 'alrite': 1244, 'omfgggg': 10428, 'suckkkkkk': 13950, 'reque': 12100, 'similar': 13089, 'soichi': 13372, 'negishi': 10019, 'amai': 1267, 'koibito': 8366, 'wonderfu': 16089, '20mins': 145, 'hibernate': 7164, 'unsettled': 15305, 'transfer': 14867, 'suspect': 14099, 'offend': 10361, 'nay': 9976, '41': 290, 'migraines': 9466, '_yours13': 848, 'cont': 3753, 'stale': 13666, 'musicistheheartofoursoul': 9855, 'niptuck': 10142, 's5': 12452, '_hope': 676, 'coordinate': 3823, 'fetti': 5713, 'applies': 1481, 'willblok': 15983, 'ne6twc': 9987, 'muay': 9810, 'romantic': 12322, 'suzy': 14113, 'uptown': 15344, 'coo': 3798, 'ghd': 6362, 'straightener': 13818, 'subponea': 13923, '80s': 487, 'dismal': 4658, 'disapointing': 4611, 'audioo': 1694, 'newborn': 10068, 'kubbur': 8391, 'applaud': 1473, 'accepting': 929, 'rels': 12032, 'chloes': 3303, 'trike': 14922, 'thingo': 14506, 'girraffe': 6410, 'probobly': 11477, 'yelled': 16354, 'mushy': 9849, 'ammmmazing': 1308, 'crib': 3985, 'fedex': 5672, 'recommending': 11904, '123': 53, 'kamei': 8171, 'sensei': 12756, 'kuso': 8399, 'baaaaaaaad': 1813, 'payment': 10849, 'publishers': 11584, 'janeiro': 7949, 'babelfish': 1816, 'physical': 11010, 'milage': 9473, 'apologised': 1457, 'tweefight': 15033, 'opp': 10508, 'hangy': 6904, 'hapy': 6928, 'sowwwy': 13494, 'unsurprised': 15308, 'healed': 7037, 'injuries': 7693, 'hp': 7391, 'sucking': 13948, 'lunching': 9004, 'visitng': 15547, 'boyet': 2529, 'famm': 5582, 'mtn': 9802, 'praising': 11372, '8gb8r': 505, 'material': 9247, 'bsame': 2682, 'encourage': 5206, 'twitwoo': 15123, 'blowin': 2361, 'immboredddd': 7595, 'cba': 3067, 'rec': 11874, 'anto': 1405, 'ngh': 10087, 'condolence': 3691, 'crutches': 4035, 'iflowers': 7541, 'trough': 14948, 'kwnx': 8404, 'cla': 3394, 'amost': 1314, 'ohwell': 10392, 'innabit': 7699, 'misery': 9545, 'omr': 10442, 'wy': 16224, 'lhr': 8652, '911': 518, 'carbonara': 2961, 'tutoring': 15022, 'overr': 10620, 'terrorizing': 14405, 'snipurl': 13324, 'hbp3g': 7011, 'canalway': 2917, 'cavalcade': 3058, 'venice': 15458, 'warwick': 15697, 'invert': 7804, '_toni': 822, 'gallore': 6241, 'kayleigh': 8201, 'coatandkay': 3507, 'soompiradio': 13421, 'itch': 7877, 'fools': 5945, 'butteflies': 2799, 'tippers': 14649, 'mt4opc': 9796, 'mule': 9824, 'dumped': 4952, 'holmbury': 7267, 'absolute': 910, 'bueno': 2708, 'salon': 12505, 'trim': 14923, 'eyebrows': 5519, 'una': 15207, 'aussies': 1711, 'todo': 14700, '_starla': 807, 'cringe': 3992, 'worthy': 16163, 'delcious': 4379, 'commercially': 3612, 'viable': 15486, 'officejet': 10372, 'j4550': 7916, 'ink': 7696, 'windows7': 16003, 'fax': 5654, 'racism': 11738, 'closeness': 3462, 'resume': 12153, 'scrub': 12676, 'ustre': 15382, '2uhs': 219, 'ebtg': 5040, 'bett': 2167, 'weeping': 15807, 'grumble': 6718, 'yaayy': 16284, 'saynow': 12586, 'jbs': 7978, 'landline': 8470, 'whoshere': 15949, 'artesia': 1580, 'cerritos': 3111, 'quest': 11697, 'guna': 6759, 'dsaa09': 4918, 'horrendous': 7336, 'emmm': 5185, 'geee': 6304, 'uup': 15398, 'lub': 8973, 'aaaaaaaahhhhhhhh': 856, 'widgets': 15965, 'practically': 11367, 'lolll': 8837, 'buzy': 2816, 'eatin': 5035, 'outage': 10582, 'surveys': 14091, 'genre': 6330, 'paving': 10838, 'intelligent': 7758, 'adopting': 1029, 'exotic': 5462, '9100': 517, 'tres': 14910, 'victor': 15498, 'weatler': 15770, 'lo0l': 8793, 'brummie': 2671, 'jennnnnn': 8001, 'richhhh': 12218, 'wast': 15709, 'winding': 15999, 'rode': 12303, 'duc': 4930, 'pops': 11284, 'adrenaline': 1035, 'wears': 15766, 'godtalk': 6484, 'segment': 12728, 'intern': 7774, 'dangit': 4210, 'sheriff': 12903, 'goooooooooooooooooooooooood': 6542, 'prank': 11373, 'aobut': 1436, 'han': 6880, 'awh': 1778, 'greattttt': 6645, 'reeeejuvinated': 11945, 'outright': 10592, 'deny': 4418, 'accusations': 948, 'depeche': 4423, 'stoooopit': 13796, 'stooopit': 13798, 'rendition': 12058, 'selenagomezlast': 12733, 'pleeeeeassseee': 11175, 'athens': 1654, 'shoulve': 12997, 'rm': 12268, 'yacht': 16285, 'joshstore': 8086, 'joshmobile': 8085, 'pft': 10958, 'economic': 5048, '5ghz': 391, 'freezes': 6068, 'sweats': 14127, 'bebe': 2065, 'vegetable': 15447, 'honesty': 7292, 'straightening': 13819, 'overrr': 10623, 'rught': 12401, 'carlos': 2986, 'interactive': 7765, 'orals': 10524, 'calicut': 2875, '4wfeo': 351, 'shudnt': 13022, 'mowgli': 9773, 'kealie': 8209, 'kia': 8264, '_ci': 601, 'reasonable': 11863, '35am': 254, 'unfortuantley': 15252, 'picker': 11019, 'grabbed': 6581, 'cherry': 3250, 'soprano': 13443, 'yardhouse': 16299, 'waikiki': 15629, 'dany': 4216, 'sorcha': 13447, 'yer': 16359, 'ladybug': 8436, 'borders': 2473, 'visa': 15539, 'gehts': 6313, 'abi': 898, 'delusional': 4396, 'errg': 5311, '___': 553, '90th': 516, 'downer': 4811, 'leslie': 8628, 'siiiick': 13078, 'paintballin': 10695, 'battle': 1997, 'mnet': 9601, 'vocals': 15561, 'ad': 989, 'freehugs': 6063, 'derek': 4441, 'iwish': 7906, 'criado': 3984, 'snapping': 13310, 'cna': 3496, '½17': 16524, '½27': 16526, 'lappie': 8486, 'whatta': 15879, 'doo': 4777, 'duncan': 4955, 'orders': 10532, 'leaves': 8584, 'iraq': 7838, 'waching': 15616, 'antomy': 1406, 'kq47ah': 8378, 'dachshund': 4160, 'breed': 2586, 'crucial': 4019, 'gums': 6757, 'earpiece': 5013, 'erica': 5300, 'bean': 2037, 'rinse': 12247, 'conditoner': 3690, 'harding': 6939, 'pitty': 11100, 'arnd': 1553, 'wld': 16067, 'rlly': 12266, 'inspire': 7731, 'thankyooooou': 14444, 'feasts': 5664, 'tomoo': 14725, 'dedicating': 4340, '300th': 226, 'eventually': 5372, 'lobby': 8802, 'exited': 5460, '22nd': 157, 'helen': 7104, 'kayla': 8200, 'nightly': 10120, 'routine': 12367, 'gahh': 6230, 'bahama': 1868, 'fij': 5747, 'cambs': 2896, 'recharger': 11885, 'pratchett': 11374, 'thingie': 14505, 'laterz': 8512, '4wsst': 362, 'stacey': 13654, 'whys': 15955, 'esquire': 5326, 'fantasize': 5599, '4jgro': 322, 'reader': 11832, 'newb': 10066, 'lolzz': 8846, 'bannished': 1930, 'readyyy': 11837, 'fright': 6105, 'regal': 11972, 'slab': 13188, 'hahaaaa': 6814, 'grief': 6671, '_of_the_dead': 762, '_staack': 806, 'aaggh': 879, 'gash': 6281, 'advertisement': 1051, 'munched': 9835, 'fanatic': 5585, 'favourites': 5651, 'caramel': 2957, 'muchachomalo': 9812, 'bffs': 2188, 'q100': 11654, 'cooker': 3803, 'combined': 3579, 'disastrously': 4619, 'carville': 3011, 'bald': 1886, 'tactical': 14201, 'belinda': 2124, 'jensen': 8003, '3647': 259, 'subbed': 13915, 'pika': 11047, 'nichi': 10103, 'heeeee': 7078, 'coined': 3537, 'hottie': 7367, 'switchfoot': 14160, 'hsg': 7399, 'zappa': 16484, 'inflating': 7680, 'kellan': 8221, 'lutz': 9015, 'safer': 12483, '_double': 623, 'fkl': 5838, 'flirt': 5868, 'contributors': 3782, 'batty': 2004, 'homesick': 7282, 'mornig': 9710, 'everone': 5377, 'diplo': 4591, 'kathryn': 8191, 'smartbar': 13265, 'pleaase': 11158, 'dating': 4247, 'gar': 6263, 'relight': 12025, 'budapest': 2701, 'samee': 12512, 'obv': 10330, 'herman': 7138, 'underpriveledged': 15231, 'li': 8653, 'ridin': 12234, 'cb': 3066, 'gaga': 6225, 'timee': 14629, 'guitars': 6753, 'hut': 7471, 'crunchy': 4029, 'veggie': 15450, 'technically': 14323, 'chingo': 3289, 'busting': 2792, 'lucia': 8978, 'waaaaay': 15610, 'halloween': 6856, 'carnt': 2992, 'biggs': 2215, '53am': 377, 'changedd': 3154, 'picnik': 11024, 'bastos': 1982, 'sixteenth': 13147, 'acceptable': 926, 'golfing': 6495, 'frolic': 6117, 'consummate': 3751, 'hogging': 7243, 'hice': 7166, 'maruchan': 9218, 'willy': 15990, 'valdez': 15422, 'muchly': 9813, 'blackird72': 2281, 'masekela': 9226, 'pix': 11103, 'contern': 3764, 't71': 14189, 'clada': 3396, 'lu': 8972, 'naturally7': 9968, 'curb': 4088, 'maid': 9092, 'cplt7p': 3910, 'aaaah': 864, 'portuguese': 11304, 'national': 9962, '1995': 113, 'tx': 15133, 'anyones': 1423, '_von_abe': 832, 'flyer': 5895, 'notary': 10233, 'retrograde': 12169, 'rachellovespeace': 11736, 'noiiiiice': 10180, 'bmth': 2383, 'youknowimsofreshtilldeath': 16418, 'smiled': 13280, 'degenerate': 4369, 'occupants': 10338, 'reprehensibles': 12094, 'minethatbird': 9505, 'servings': 12796, '88db': 499, 'mixing': 9584, '808': 486, 'providing': 11558, 'geelong': 6309, 'toyota': 14823, 'landcruiser': 8467, '1996': 114, 'hcc': 7013, '_thomas': 818, 'fangs': 5592, '10p': 39, 'dig': 4558, 'selfridges': 12737, 'jfk': 8021, 'tasted': 14277, 'hurdle': 7456, 'gooooodnight': 6532, 'cuss': 4105, '_louise': 729, 'ingredients': 7687, 'seperate': 12770, 'depressi': 4433, 'grimestopper': 6677, 'drowned': 4900, 'pi': 11013, 'stricken': 13858, 'omaha': 10422, 'calvin': 2890, 'kleins': 8330, 'yearsssssss': 16337, 'expires': 5480, 'parker': 10769, 'batcave': 1984, 'startup': 13704, 'motorbikes': 9749, 'biafra': 2201, 'momotlv': 9647, 'iva09': 7902, 'trumps': 14957, 'minor': 9520, 'heal': 7036, 'furbabies': 6184, 'hog': 7242, 'retweeted': 12176, 'gryffindor': 6722, 'ching': 3288, 'chong': 3317, 'wing': 16008, 'wong': 16097, 'pong': 11254, 'dong': 4767, 'tabloids': 14196, 'aaa': 852, 'hiii': 7187, 'bara': 1936, 'closets': 3466, 'aggghhhh': 1101, 'muak': 9809, 'mcm': 9295, 'sudah': 13956, 'cripple': 3993, 'twizzler': 15124, 'rolynn719': 12318, 'awayyy': 1754, 'moooorning': 9692, 'hellloooo': 7111, 'g4p': 6213, 'rockstars': 12300, 'havnt': 6992, 'answerd': 1390, 'replys': 12087, '_gant': 651, 'trendy': 14908, 'hysterical': 7492, 'deffinately': 4355, 'bookbag': 2436, 'linoleum': 8723, 'carving': 3012, 'aweesome': 1758, 'errors': 5313, 'thankss': 14441, 'farming': 5610, 'catty': 3047, 'omigoodness': 10436, 'orthodontist': 10555, 'and_jay': 1327, 'har': 6929, '4jb4o': 315, 'santana': 12544, 'wraith': 16175, '7a10a': 476, '1386am': 66, 'muttering': 9869, 'arseholes': 1576, 'rickroll': 12224, 'collabs': 3551, 'aroun': 1557, 'tweettttt': 15058, 'leads': 8565, 'uce': 15160, 'pgce': 10960, 'leicester': 8609, 'hairdo': 6839, '250gb': 171, 'nia': 10092, 'dayton': 4268, 'cincinnati': 3365, 'wiz': 16057, 'ebtter': 5041, 'stopping': 13801, 'adn': 1024, 'shortstack': 12985, 'friggin': 6104, 'bbye': 2020, 'biochem': 2237, 'waaaaahhhhhh': 15609, 'wxizo': 16220, 'booking': 2439, 'brave': 2563, 'belive': 2125, 'yous': 16432, '333333333': 248, 'chats': 3199, 'cyrus': 4143, 'runday': 12418, 'conan': 3670, 'forsure': 5996, 'yvr': 16473, 'conflicts': 3707, 'acid': 963, 'reflux': 11957, 'appending': 1470, 'verticalchinese': 15474, 'damjust': 4181, 'jaoo': 7955, 'damper': 4192, 'georgetown': 6340, 'invoices': 7816, 'upside': 15340, 'wt': 16203, 'yaer': 16286, 'moshing': 9732, 'confetti': 3698, 'whatev': 15873, 'carlsbad': 2987, 'spilled': 13569, 'involve': 7817, 'ning': 10138, 'reg': 11971, 'fff': 5721, 'naturalismo': 9966, 'elliott10': 5154, 'uterus': 15390, 'saddo': 12472, 'jonaswebcast': 8074, 'misadventures': 9537, 'sociology': 13355, 'worldsbestpornmovies': 16147, 'faithmov3': 5562, 'htm': 7403, 'whores': 15946, 'umbrella': 15198, 'carpenters': 2997, 'topical': 14766, 'okayy': 10402, 'burped': 2774, 'lmfa0': 8785, 'recordings': 11910, '_stiller': 811, 'adios': 1013, 'vanilla': 15433, 'oxoxoteambreezy': 10657, 'rez': 12199, 'reboot': 11870, 'osx': 10561, 'supported': 14063, 'tampa': 14245, 'gtgs': 6728, 'decongestant': 4333, '_laiter': 718, 'discussed': 4640, 'yeaaaaaah': 16322, 'chichis': 3260, 'grande': 6607, '66of2': 432, 'polly': 11246, 'arrested': 1565, '4uy8l': 335, 'drumming': 4909, 'twin': 15074, 'gazing': 6295, 'webkit': 15781, '_flora': 642, 'optimized': 10517, 'caching': 2847, 'deploy': 4426, 'siad': 13038, 'fav5': 5642, 'rex': 12195, 'wonderfull': 16091, 'dins': 4585, 'epicentre': 5279, 'wheelock': 15890, '250': 168, 'units': 15275, 'tourists': 14808, '_bizzle': 585, 'speakers': 13527, 'polaroid': 11233, 'cleavage': 3433, 'yorkshire': 16414, 'survived': 14093, 'kuya': 8402, 'owns': 10649, 'wating': 15731, 'wereld': 15844, '_steph': 809, 'alo': 1226, 'wowza': 16172, '0f': 18, 'fried': 6094, 'offensive': 10364, 'earlierrr': 5003, 'tooz': 14762, 'misbehaved': 9538, 'thelovelybones': 14474, 'spy': 13627, 'premire': 11397, 'xbb1qyx0e': 16242, 'guttah': 6770, 'happydance': 6926, 'piercing': 11034, 'tryst': 14970, 'fishys': 5820, 'happymothersday': 6927, '50p': 370, 'o_o': 10318, 'ivs': 7905, 'pikachu': 11048, '104': 32, 'buckonellen': 2698, 'livingroom': 8774, 'bfe': 2185, 'donating': 4764, 'cj': 3389, 'wc': 15756, 't20': 14187, 'benefiting': 2141, 'caps': 2949, 'barbs': 1939, 'dsl': 4920, 'realising': 11845, 'sheltered': 12896, 'upbringing': 15316, 'commitments': 3615, 'pfftt': 10957, 'pinched': 11062, 'unpleasant': 15295, 'mindblowing': 9500, 'nahh': 9916, 'sowwieee': 13493, 'git': 6415, 'dez': 4504, 'sneakerz': 13314, 'aghhh': 1104, '2dives': 194, 'dive': 4691, '10m': 38, 'colours': 3575, 'dynamite': 4986, 'reefs': 11948, 'edwin': 5080, 'batch': 1985, 'eveyone': 5398, 'cloggin': 3455, 'tweetup': 15059, 'pushit': 11638, 'sundress': 14022, 'bogus': 2404, 'txts': 15137, 'weirdness': 15822, 'lmgtfy': 8789, '49am': 304, 'su': 13911, 'goodb': 6504, 'heap': 7043, 'bitchy': 2259, 'knoww': 8356, '_renee': 783, 'shay': 12876, 'shlda': 12948, 'knwn': 8360, 'whn': 15926, 'crd': 3954, 'scorpian': 12641, 'bwl': 2822, 'aud': 1689, 'timestamp': 14635, 'chantellie': 3164, 'colorful': 3570, 'atmosphere': 1664, 'mozert': 9779, 'covina': 3899, 'goooo': 6528, 'veryy': 15476, 'el_rumi': 5127, 'fii': 5746, 'mushkila': 9846, 'router': 12366, 'apnea': 1454, 'causing': 3055, '4w9zb': 344, 'dimple': 4575, 'cave': 3059, 'adt': 1039, 'princelple': 11445, 'plate': 11139, 'adtï': 1040, '½000': 16522, 'nyhmz': 10311, 'dionusia': 4588, 'congratuations': 3722, 'tyson': 15152, 'kitteh': 8319, '1980s': 110, 'combed': 3577, 'poofy': 11260, '30yo': 237, 'anywayz': 1432, 'rye': 12446, 'speedway': 13546, 'utd': 15389, 'horrific': 7341, 'wack': 15617, 'swimsuit': 14152, 'dd7': 4284, 'treating': 14896, 'ds9': 4917, 'casino': 3023, 'unpacking': 15293, 'restful': 12143, 'alternatively': 1253, 'sprinkle': 13620, 'doubtful': 4802, 'biscuit': 2252, '4wtom': 363, 'appetite': 1471, 'disorders': 4662, 'vomit': 15579, 'amazeeeeeee': 1277, '527': 375, 'quesadillas': 11695, 'ai': 1139, 'stephen': 13746, 'sho': 12950, 'oreo': 10535, 'milkskake': 9485, '_cook': 605, 'gran': 6601, 'drift': 4875, 'seaworld': 12704, 'wid': 15961, 'phaket': 10962, 'anneliese': 1369, '_wi_no_name': 840, 'someones': 13399, 'runner': 12421, 'greatt': 6644, 'roaming': 12272, '24hrs': 166, 'tfa': 14423, 'x2rgl': 16236, 'sliders': 13223, 'barjohnnys': 1945, 'omelette': 10425, 'dim': 4574, 'feast': 5662, 'mariah': 9185, 'shhhweeeet': 12908, 'storming': 13808, 'starship': 13697, 'garnier': 6276, 'roasted': 12275, 'drugstore': 4906, 'rember': 12037, 'riped': 12251, 'mags': 9087, 'beated': 2047, 'heartburn': 7055, 'bitch': 2257, 'showss': 13013, 'trials': 14913, 'puppies': 11619, 'solved': 13387, 'oreos': 10536, 'owls': 10643, 'slurred': 13252, 'keyla': 8254, 'janice': 7952, 'getttin': 6355, '_meirizka': 741, 'ladyhawke': 8438, 'rutger': 12439, 'hauer': 6977, '4jbzv': 316, 'richie': 12219, 'fuzzball': 6202, '0a7v3j': 17, 'waaay': 15613, 'cruisey': 4023, 'relaxed': 12012, 'sporting': 13602, 'tm': 14675, '_lightmare': 720, 'yeeet': 16347, 'longggg': 8858, '007': 3, 'cooler': 3810, 'hawk': 6994, 'virgin': 15527, 'openhacklondon': 10495, 'tty': 14981, 'barnes': 1950, 'noble': 10166, 'comforts': 3594, 'custody': 4107, 'settled': 12807, 'skool': 13178, 'excite': 5433, 'wohoo': 16075, 'wis': 16026, '4w52z': 339, 'withdrawal': 16043, 'cingular': 3373, 'jr': 8106, 'cockatiels': 3513, 'mishaneedschapstick': 9549, 'beads': 2036, 'ipswitch': 7833, 'shannon': 12854, 'moral': 9700, 'likewise': 8692, 'copyed': 3831, 'chicks': 3265, 'yesssss': 16367, 'sue': 13961, 'banned': 1928, 'blt': 2366, 'nï': 10316, '½mme': 16539, 'equipment': 5290, 'telescopic': 14354, 'trekking': 14905, 'poles': 11235, 'boots': 2465, 'coolmax': 3813, 'fontanas': 5936, 'summmmmerrrrr': 14001, 'yeahhhhhyaaaaaa': 16331, 'organic': 10539, 'proto': 11545, 'furries': 6188, 'acen': 951, 'bubba': 2693, 'rubs': 12396, 'hairdresser': 6840, 'cutted': 4120, 'laaame': 8414, 'gays': 6293, 'bais': 1874, '_are_fire': 570, 'wowzers': 16173, 'addicting': 996, 'kalahari': 8166, 'biggie': 2214, 'puked': 11597, 'conditioning': 3688, 'bret': 2591, 'scarce': 12595, 'bestest': 2157, 'xams': 16240, 'roman': 12319, 'lq': 8968, 'agreeing': 1110, 'yogurt': 16399, '8am': 502, 'hughesy': 7425, 'rafferty': 11746, 'investigate': 7805, 'ye': 16317, 'operational': 10503, 'types': 15143, 'lmaoz': 8783, 'con': 3669, 'streamline': 13839, 'halfway': 6850, 'hoooray': 7310, 'bahamas': 1869, 'sealers': 12688, '_kimbalicious': 708, 'slippery': 13232, 'peep': 10879, 'singles': 13111, 'ipsohot': 7832, '_hong': 675, 'cosplay': 3857, 'marathons': 9171, 'affect': 1063, 'dreadweave': 4857, 'holds': 7250, 'chloe': 3302, 'pitiful': 11099, 'gre': 6634, 'floyd': 5887, 'busssssss': 2789, 'stickler': 13764, 'dpi': 4828, 'export': 5491, 'value': 15426, 'morga': 9706, 'mander': 9145, 'goooooooooooood': 6541, 'morrrrrrrrning': 9726, 'josie': 8088, 'pandora': 10728, 'birthdaaaay': 2248, 'unsalvageable': 15304, 'vibrating': 15489, 'et': 5338, 'genitals': 6327, 'glowing': 6452, 'babydoll': 1821, 'spaghetti': 13506, 'strap': 13828, 'smokinggg': 13293, 'nto': 10271, 'gaskarth': 6283, 'wraps': 16179, 'cofo': 3534, 'buyer': 2812, 'wifes': 15970, 'poke': 11226, 'quirky': 11711, 'concerto': 3681, 'reallllyyy': 11854, 'defend': 4351, '_ashley': 572, 'graduated': 6595, 'rentaphone': 12067, 'yesy': 16374, 'endometriosis': 5214, 'sez': 12824, 'quadriceps': 11676, '4we4j': 348, 'screens': 12666, 'somthing': 13408, 'dreamland': 4862, 'fruity': 6130, 'pebbles': 10871, 'charms': 3193, 'tiiiiiiired': 14617, 'fucckinggg': 6143, 'flaky': 5844, 'vrbo': 15592, 'tuff': 14991, '88th': 500, 'buffet': 2712, 'fabulously': 5533, 'satz': 12568, 'blend': 2317, 'ks': 8387, 'preheatin': 11394, 'indeedy': 7656, 'dios': 4589, 'mio': 9531, 'mailbox': 9095, 'loic': 8830, 'scheduler': 12619, 'halo3': 6861, 'freeze': 6066, 'halt': 6864, '79l0': 474, 'packages': 10672, 'wathing': 15730, 'dollhouse': 4753, 'taquito': 14263, 'employed': 5196, 'clint': 3448, 'haiku': 6834, 'yayschoolisout': 16308, 'sarcasm': 12550, 'eassy': 5023, 'wo': 16072, 'possessed': 11315, 'honking': 7298, 'minibar': 9509, 'starups': 13705, 'bridget': 2605, 'farted': 5615, 'broody': 2645, 'predicting': 11389, 'kindergarten': 8298, 'lof': 8815, 'malamang': 9121, 'chef': 3237, 'neeeeed': 10011, 'melting': 9363, 'trackflashback': 14834, 'darkness': 4224, 'mah': 9088, 'rotation': 12354, 'prowse': 11561, 'factory': 5544, 'yesh': 16363, 'howdyyy': 7383, 'answering': 1393, 'islip': 7864, 'benefit': 2140, 'customers': 4110, 'adulthood': 1042, 'surprisingly': 14087, 'mallorca': 9130, 'mypict': 9889, 'fuz': 6200, 'meadowbank': 9303, 'schuhz': 12630, 'arabyrd': 1510, 'seeyuhhh': 12727, '3hrs': 271, 'lightly': 8683, 'misted': 9571, 'ignoring': 7549, 'frowns': 6124, 'sympathise': 14171, '4ws8w': 361, '56': 383, 'sulumits': 13988, 'retsambew': 12170, 'billion': 2228, 'adds': 1007, 'technician': 14324, '_0': 537, 'whatthefuck': 15881, 'suns': 14033, 'tossa': 14788, 'hubz': 7416, 'organising': 10541, 'robins': 12285, 'collared': 3556, 'psst': 11573, 'latvian': 8521, 'beaurocracy': 2058, 'resident': 12127, 'publish': 11582, 'braille': 2548, 'highway': 7184, 'farsi': 5614, 'hopeflly': 7319, 'socialily': 13352, 'naiv': 9924, 'lsat': 8970, 'wallpapers': 15657, '_okeefe': 765, 'celeb': 3084, 'mattcutts': 9258, 'umzug': 15205, 'und': 15223, 'neues': 10058, 'redir': 11932, 'if2b': 7540, 'sorted': 13462, 'empathise': 5191, 'luvvie': 9021, 'tartan': 14269, 'dodgers': 4731, 'giants': 6370, 'quizfarm': 11717, 'quizrunner': 11718, 'dies': 4543, 'nvm': 10301, 'retreat': 12166, 'nike': 10129, 'clingy': 3447, 'purty': 11633, 'qa': 11656, 'frehley': 6070, 'coherent': 3535, 'fortunate': 6001, 'meditating': 9331, 'partial': 10778, 'sobre': 13347, 'maxed': 9266, 'concentration': 3673, 'smoothness': 13298, 'ilycecily': 7580, 'bampa': 1906, 'disdrict': 4643, 'advised': 1054, 'ridden': 12228, 'eppy': 5285, 'bats': 1994, 'tht': 14572, 'puffy': 11590, 'actresses': 984, '_jayytee': 698, 'community': 3623, 'moderators': 9619, 'niandra': 10094, 'cracked': 3917, 'daaaang': 4156, 'regards': 11974, 'archive': 1521, 'postings': 11330, '_stokoe': 812, 'sor': 13445, 'garcia': 6268, 'bleah': 2309, 'maddies': 9067, 'cuteset': 4115, 'looooooove': 8886, '_kill_boy': 707, 'yummmm': 16462, 'selected': 12730, 'fortunately': 6002, 'nkorea': 10155, 'abbey': 890, 'whatï': 15884, 'latter': 8520, 'op': 10488, 'paraded': 10752, 'rejection': 12000, 'coulnd': 3873, 'marry': 9209, 'dana': 4194, 'x29ss': 16231, 'boohoohoo': 2434, 'fotoreportage': 6011, 'mashup': 9229, 'hysteria': 7491, 'blows': 2365, 'faillllllllll': 5552, 'amma': 1307, 'demistylesource': 4402, 'fought': 6013, 'jered': 8005, 'chop': 3319, 'tomarrow': 14717, 'foreword': 5974, 'adidas': 1012, 'denyer': 4419, 'xoxoxoxoxoxo': 16259, 'rogue': 12308, 'x2dvj': 16233, 'boverd': 2519, 'keynote': 8255, 'screencastsonline': 12662, 'jobros': 8049, 'gentoo': 6332, 'reinstallation': 11996, 'conor': 3734, 'quarantined': 11680, 'pinkeye': 11073, 'bej': 2113, 'semanggi': 12741, 'siggghh': 13060, 'futile': 6195, 'conditioner': 3687, '_peer': 772, 'wishful': 16033, 'wuv': 16210, 'reserve': 12125, 'lowest': 8966, '664': 430, '_whacker': 838, 'skypeeeeee': 13185, 'oy': 10658, 'writin': 16193, 'lik': 8686, 'venti': 15461, 'prettier': 11425, 'spendin': 13556, 'haley': 6848, 'leyton': 8648, 'dgtcj2': 4509, 'neeed': 10009, 'seed': 12717, 'regionals': 11976, 'plugs': 11192, 'patent': 10815, 'earplugs': 5014, 'benedryl': 2139, 'addressed': 1005, 'lols': 8843, 'junction': 8136, 'ollies': 10419, '_hayes': 668, 'sharkeez': 12863, 'graphic': 6622, 'designers': 4457, 'flushed': 5893, 'collapses': 3554, 'nup': 10286, 'zeros': 16497, 'crest': 3981, 'plaid': 11119, 'mashed': 9228, 'limb': 8700, 'searched': 12693, 'yahh': 16288, 'wxmt': 16221, 'grim': 6676, 'fandango': 5589, 'xbla': 16244, 'psn': 11569, 'plana': 11122, 'strippers': 13865, 'stripey': 13863, '_hayward': 669, 'marche': 9174, 'rosti': 12352, 'crepes': 3980, '2nit': 214, 'reporter': 12090, 'forreal': 5991, 'rowing': 12372, 'sounders': 13475, 'cartoon': 3009, 'cgi': 3121, 'volunteer': 15577, 'fkkk': 5837, 'nurses': 10289, 'mores': 9704, 'cait': 2857, 'realis': 11841, '_sunshine': 813, 'yiiiit': 16388, 'jarn': 7960, '8c36ej': 503, 'artomatic': 1588, 'gawd': 6292, 'ughhhhhhhhh': 15178, 'triste': 14932, 'brlliant': 2632, 'amanzimtoti': 1270, 'quaver': 11686, 'sandwiches': 12533, 'muji': 9822, 'yipee': 16390, 'bsg': 2684, 'disks': 4654, 'gabe': 6221, 'afterparty': 1080, 'iq': 7835, 'wheelchair': 15889, 'buzzing': 2819, 'homebound': 7272, 'yeahhhhh': 16329, 'kn3mp': 8333, 'dann': 4213, 'misconnected': 9541, 'midway': 9460, 'undies': 15241, 'bestttt': 2162, 'gem': 6317, 'anddd': 1328, 'angrily': 1347, 'extemely': 5502, 'abusive': 917, 'mentally': 9385, 'nightss': 10125, 'bastards': 1980, 'congradts': 3717, 'phoning': 10993, 'lovebank': 8933, 'audit': 1696, 'portrait': 11300, 'unbecominglily': 15214, 'announcing': 1380, 'ecomonday': 5047, '_fischer': 641, 'recommendations': 11902, 'programmer': 11508, 'mcflurry': 9290, 'aaaaahhhh': 860, 'foking': 5913, 'eilish': 5117, 'competiton': 3644, 'pineapples': 11066, '4bckp': 307, 'moy': 9775, 'vs2003': 15595, 'generates': 6322, 'uninstall': 15265, 'guesss': 6739, 'duo': 4961, 'matilda': 9255, 'tabletop': 14194, 'programming': 11509, 'heidi': 7097, 'stiff': 13767, 'aundy': 1703, 'lamee': 8456, 'compatible': 3637, 'satisfaction': 12560, 'afraidiowe': 1072, 'bordatella': 2470, 'harbor': 6931, 'fallout': 5571, 'ammo': 1309, 'pvajlm': 11646, 'rome': 12323, 'elegance': 5136, 'tulane': 14992, 'unique': 15271, 'keren': 8234, 'hairdressers': 6841, 'fuschia': 6190, 'sheath': 12879, '_nicole': 756, 'lambastes': 8452, 'bankers': 1923, 'insurers': 7752, '½greed': 16532, '½stupidityï': 16549, 'wyab': 16225, 'amercia': 1293, 'jetsetter': 8016, 'dinghy': 4580, 'tossed': 14789, 'strategic': 13830, 'waterfall': 15723, 'redirects': 11933, 'item': 7883, 'pulish': 11599, 'proving': 11559, 'sympathies': 14170, 'loadsa': 8799, 'shizze': 12945, 'farrells': 5612, 'parlor': 10772, 'staging': 13661, '5ygpg': 406, 'gazillion': 6294, '_benson': 584, 'bawkmarked': 2005, 'condition': 3686, 'ironpython': 7849, 'compact': 3629, 'reflection': 11955, 'emit': 5182, 'amason': 1272, 'gos': 6551, 'rounding': 12361, 'bases': 1965, 'relays': 12016, 'purate': 11623, 'odyssey': 10354, 'rrod': 12385, 'odour': 10352, 'unemployment': 15244, 'spasy': 13521, 'lovage': 8929, 'essex': 5330, '4jkes': 326, '36': 257, 'clea': 3421, 'lold': 8833, 'releasd': 12018, '_molecule': 746, 'sackiroth': 12459, 'mcflys': 9292, 'halla': 6852, 'haloom': 6862, 'eeem': 5091, 'rationale': 11800, 'eyaseer': 5517, 'inshallah': 7720, 'dolidh': 4749, 'mister': 9572, 'rhiannon': 12205, 'evidence': 5401, 'nooobody': 10199, 'tada': 14203, 'prada': 11370, 'fragrance': 6026, '_nye': 759, 'arabic': 1508, 'suprisingly': 14072, 'nighter': 10119, 'suggesting': 13974, 'restarting': 12139, 'tribute': 14916, 'chrissy': 3333, 'laaaaaaaaave': 8413, 'horrors': 7344, 'sthlm': 13758, 'pizzahut': 11108, 'irratated': 7851, 'dw': 4977, 'twitterr': 15108, 'impostor': 7614, 'booster': 2458, 'woork': 16119, 'opposed': 10510, 'distract': 4677, 'fristy': 6111, 'polo': 11247, 'wl9yl': 16066, 'diggnation': 4564, 'prays': 11379, 'canon': 2934, 'i900d': 7495, 'fisheye': 5816, 'exp': 5463, 'nims': 10135, 'annie': 1370, 'downnnn': 4820, 'depresses': 4432, 'goodbey': 6505, 'pressents': 11418, 'mobypicture': 9607, 'uqi0h2': 15346, 'mtml': 9801, '2500': 169, 'hmmpph': 7230, 'linz': 8727, 'lsats': 8971, 'cissbury': 3381, 'hired': 7207, 'monkeys': 9662, 'izzy': 7914, 'eastbay': 5025, 'unlike': 15284, 'suspended': 14102, 'aaaa': 853, 'pagee': 10684, 'twittername': 15105, 'hosted': 7357, 'twitterfridge': 15099, 'sinned': 13118, 'univ': 15277, 'strauss': 13832, 'terence': 14390, 'cao': 2942, 'dose': 4790, 'fainting': 5555, '66shw': 434, 'davey': 4254, 'flygroups': 5897, 'bells': 2128, 'refunds': 11965, 'foreboding': 5963, 'shave': 12872, 'byebye': 2826, 'fudge': 6149, 'grahmcracker': 6598, 'nc': 9980, 'hobby': 7236, 'fuckyoumonday': 6148, 'cocktails': 3515, 'reduculous': 11941, 'uduhn': 15161, 'lun': 8997, 'mustangs': 9863, 'homesssssskooooler': 7283, 'everton': 5381, 'chanclas': 3149, 'diy': 4701, '4jccd': 317, 'reeboks': 11943, 'cornerstone': 3842, 'eur': 5353, 'jpy': 8105, '132': 63, '130': 60, '131': 62, 'executed': 5446, 'yesssssssssss': 16371, 'rocket': 12295, 'oral': 10523, 'welllll': 15834, 'pending': 10890, 'proofing': 11531, 'sessions': 12801, 'ankile': 1359, 'healthified': 7041, 'streusel': 13857, 'sleeper': 13207, 'bootleg': 2464, 'pharos': 10964, 'weighed': 15813, 'highest': 7177, 'proven': 11551, 'powerblog': 11353, 'followe': 5921, 'speeding': 13542, 'sufficient': 13968, 'satisfying': 12563, 'hayfever': 7002, 'piriton': 11086, 'croatian': 4002, 'puncture': 11612, 'sheena': 12884, 'othman': 10569, 'productivity': 11494, 'shelves': 12898, 'diva': 4690, 'elk': 5151, 'grove': 6697, 'cass': 3024, 'nes': 10046, 'pussycat': 11639, 'degeneres': 4370, 'hxlfm': 7480, 'satan': 12558, 'hamptons': 6878, 'relaxation': 12011, 'westt': 15851, 'stint': 13779, 'live360': 8761, 'baylee': 2008, 'alreadt': 1236, 'ilovemymommy': 7573, 'yeeeeah': 16345, 'empireonline': 5194, 'harold': 6952, 'distracting': 4679, 'stil': 13768, '4w70j': 341, '_iain': 680, '_mcfly': 737, '_saurus': 791, 'surprises': 14085, 'damnnn': 4188, 'timeee': 14630, 'unrelated': 15302, 'himym': 7199, 'fukin': 6151, 'constructions': 3748, 'poping': 11279, 'interface': 7770, 'teething': 14338, 'ringing': 12244, 'asbos': 1599, 'supper': 14058, 'kurumi': 8398, 'direction': 4597, 'mailed': 9096, 'rhinitis': 12206, 'sukked': 13983, 'dats': 4248, 'groundbreaking': 6691, 'production': 11492, 'zul': 16516, 'jin': 8033, 'crumbles': 4025, 'bonfires': 2423, '_idiots': 683, 'alrighty': 1242, 'nicley': 10107, 'palmdale': 10717, 'mspacers': 9795, 'kewl': 8245, 'missmickey': 9564, 'vast': 15438, 'stereos': 13752, 'colleague': 3557, '539': 376, 'dms': 4712, 'acw': 988, 'dazzleglasses': 4277, 'poker': 11229, 'padestrian': 10681, 'skanking': 13158, 'salmon': 12504, 'sashimi': 12554, 'aaaaaw': 862, '14mph': 76, 'intelligence': 7757, 'hike': 7188, 'everyonee': 5390, 'itll': 7886, '_gyrl': 663, 'penalty': 10887, '_zwitschert': 850, 'sso': 13646, 'luckyyyyyyy': 8987, 'enduring': 5218, 'boba': 2394, 'xoxox': 16258, 'sims2': 13100, 'minutee': 9528, 'destroyed': 4476, 'pf': 10953, 'changs': 3158, 'yogurtland': 16400, 'livenation': 8766, 'boooooooored': 2454, 'iat': 7499, 'rivercenter': 12261, 'pokemon': 11228, 'handed': 6884, 'personalit': 10939, 'blunt': 2376, 'tos': 14786, '11pm': 48, 'workk': 16136, 'dandy': 4203, 'darnit': 4229, 'interwebs': 7788, 'wor': 16123, '_com': 602, '8830': 497, 'pistons': 11094, 'yooooooo': 16408, 'robe': 12281, 'trnds3trs': 14938, 'pleaseyour': 11170, 'muffins': 9816, 'vocie': 15562, 'adele': 1010, 'collage': 3552, 'feminism': 5698, 'arnie': 1554, 'cities': 3384, 'rescued': 12119, 'luxurious': 9024, 'ba115': 1810, 'enlisted': 5242, 'crs': 4018, 'conflict': 3706, 'labo': 8420, 'wrigley': 16187, 'dims': 4576, '_matta': 736, 'yday': 16316, 'feds': 5673, 'topping': 14771, 'adem': 1011, 'uit': 15188, 'contemplating': 3760, 'somethings': 13403, 'tellin': 14360, 'bishop': 2254, 'pugged': 11592, 'bombard': 2417, 'pleeeeeeeassseeeeeee': 11176, 'mel': 9355, 'siento': 13059, 'dint': 4586, 'bapang': 1932, 'dito': 4687, 'addiction': 997, 'sameee': 12513, 'v0': 15410, '99pb5': 530, 'commodores': 3617, 'achieving': 960, 'xml': 16253, 'cumbersome': 4076, 'ciao': 3357, 'packin': 10675, 'poems': 11213, 'known': 8354, 'haahaha': 6793, 'minging': 9506, 'blogher09': 2345, 'squee': 13633, '_kay': 704, 'named': 9932, 'neaby': 9988, 'grans': 6616, 'grandkids': 6609, 'amt': 1318, '_n': 750, 'ooze': 10487, 'wiping': 16022, 'janis': 7953, 'ibood': 7502, 'ladiez': 8434, 'boffert': 2402, 'pumkpin': 11605, '_20': 544, 'fakes': 5564, '_370': 549, 'ohmygod': 10389, 'flattened': 5855, 'cast': 3029, 'raid': 11752, 'excepting': 5427, 'liquor': 8735, 'lexi': 8646, '_dave': 613, 'meany': 9316, 'haz': 7006, 'obnoxiously': 10325, 'cl': 3393, '66': 429, 'blaaaqhhh': 2276, 'givers': 6421, 'takers': 14221, 'talaga': 14226, 'chux': 3352, 'witnessing': 16053, 'historical': 7211, 'cote': 3865, 'fuuuuuuck': 6199, 'youuuu': 16440, 'hamster': 6879, 'strangely': 13824, 'allllllllright': 1214, 'exposed': 5492, 'milkkk': 9483, 'brits': 2628, 'esther': 5333, 'rhymes': 12207, 'investor': 7809, 'tester': 14408, 'jester': 8013, 'pester': 10947, 'polyester': 11249, 'sylvester': 14168, 'requester': 12104, 'occured': 10339, 'typin': 15146, 'sidekick': 13056, 'similarity': 13090, 'alexander': 1182, 'mylan': 9886, '60hrs': 418, 'compensate': 3638, 'vzerohost': 15600, 'hosting': 7358, 'drawings': 4849, '674p1': 438, 'itto': 7893, 'entertain': 5252, 'oldest': 10410, 'celebration': 3088, 'passive': 10803, 'agressive': 1113, 'owners': 10648, 'owne': 10646, 'limitation': 8703, 'shakespeare': 12842, 'garrulous': 6278, 'scribe': 12673, 'tps': 14827, 'schoolbooks': 12626, 'confusions': 3714, 'understanding': 15236, 'require': 12108, 'paragraph': 10754, 'para5': 10749, 'opting': 10518, '9ftuv3xmrn0': 534, 'purrrrs': 11631, 'ooooh': 10475, 'orchid': 10528, 'zak': 16481, 'wuld': 16208, 'x2qkb': 16235, 'cluedo': 3484, 'queue': 11701, 'deranged': 4438, 'anticipation': 1402, 'bullwinkle': 2743, 'fractured': 6025, 'fairy': 5560, 'stations': 13719, 'struck': 13873, 'lllooovvveee': 8779, 'boldly': 2413, 'teng': 14378, 'blake': 2292, 'sandbox': 12526, 'objects': 10324, 'autoreturn': 1721, '5z05g': 407, 'runny': 12425, 'unfort': 15251, 'abbreviation': 892, 'crowntown': 4017, 'hyperventilating': 7490, 'changin': 3156, 'johny': 8063, 'khayyam': 8260, 'wakil': 15641, 'scotts': 12644, 'tanya': 14256, 'microwave': 9452, 'larin': 8496, 'itv': 7897, '18mos': 105, 'nickname': 10105, 'mtaby': 9797, 'uniqname': 15270, 'punkin': 11616, 'appartment': 1462, 'cds': 3076, 'wooohooo': 16111, 'evacuating': 5361, 'coldddd': 3544, 'barked': 1946, 'cigarette': 3363, 'g00d': 6209, 'calms': 2887, 'embrace': 5177, 'deficiency': 4357, 'pitching': 11098, 'lackluster': 8427, 'naa': 9906, 'nm': 10159, '4get': 311, 'gina': 6393, 'corrections': 3850, 'officer': 10373, 'misplaced': 9551, 'waterguns': 15725, 'neighbourss': 10027, 'heya': 7152, 'famine': 5581, 'moleskineï': 9634, 'notebooks': 10236, 'worldwide': 16148, 'heeels': 7082, 'cutee': 4114, 'lullaby': 8993, 'failfriday': 5550, 'okaaaay': 10399, '4jj43': 324, 'ari': 1540, 'funniset': 6177, 'staples': 13684, 'fucktards': 6147, 'dread': 4852, '_kookie': 711, 'forrealll': 5992, '48hours': 302, 'cryed': 4039, 'mentality': 9384, 'evangelizing': 5363, 'cubicle': 4062, 'p4': 10666, 'brantley': 2561, 'tnite': 14682, 'sawn': 12581, 'misss': 9565, 'youhhhhhhh': 16417, 'touche': 14798, 'paparazzi': 10742, 'avian': 1735, 'risking': 12258, 'fisnihsed': 5821, 'yesert': 16362, 'admitted': 1021, 'discharge': 4623, 'henry': 7130, 'gunner': 6762, 'wwdc': 16214, 'exit': 5459, 'wayne': 15747, '½jï': 16536, 'vu': 15597, 'gq': 6576, 'stooopid': 13797, 'reconnect': 11905, 'interent': 7766, 'sads': 12479, 'screenings': 12665, 'troubleshooting': 14947, 'clusters': 3487, 'availability': 1725, 'horatio': 7333, 'caine': 2856, 'effed': 5102, 'graandma': 6579, 'houseee': 7375, 'havee': 6983, 'spreading': 13614, 'hayley': 7003, 'battlegrounds': 2000, 'battleground': 1999, 'tpc': 14826, 'boxing': 2527, 'phenomenon': 10969, 'superbad': 14045, 'invalid': 7798, 'euggh': 5349, 'hawt': 6997, 'motherland': 9739, 'caro': 2993, 'boak': 2387, 'madness': 9072, 'torts': 14783, 'anoher': 1386, 'grates': 6627, 'collide': 3564, 'imposed': 7611, 'outlets': 10589, 'neemah': 10013, 'philos': 10978, 'ophy': 10506, 'mat': 9241, 'intersubjectively': 7782, 'liar': 8656, 'recovery': 11920, 'firms': 5807, 'chabibi': 3125, 'recollecting': 11897, 'unkown': 15281, 'toasties': 14687, 'behin': 2109, 'thumping': 14578, 'podcasters': 11211, 'emporium': 5199, 'needa': 10001, 'stastics': 13710, 'waved': 15738, 'eddie': 5058, 'izzard': 7912, 'theese': 14469, 'noticing': 10245, '81': 489, 'jury': 8145, 'removed': 12055, 'recite': 11888, '_diesel': 619, '_web_desig': 836, 'sammie': 12515, 'omggg': 10432, 'cobras': 3512, 'wantewd': 15670, 'noting': 10247, 'meteor': 9416, 'yooo': 16406, 'kb': 8204, 'df': 4505, 'layenn': 8547, 'uughh': 15397, 'dunt': 4960, 'caffine': 2852, 'nicola': 10109, 'naisee': 9923, 'flares': 5849, 'arond': 1556, 'scurred': 12681, 'stillll': 13771, 'civics': 3387, 'wiff': 15971, 'whitby': 15920, 'lill': 8697, 'betta': 2168, 'fantasy': 5602, 'zenjar': 16494, 'mit': 9574, 'courseware': 3888, 'ygt5': 16382, 'disclaimer': 4624, 'external': 5507, 'coffees': 3531, 'ripping': 12253, 'path': 10816, 'alenka': 1179, 'chicky': 3266, 'energized': 5219, 'ceasar': 3078, 'sacrilege': 12462, 'traumatizing': 14884, 'vacay': 15417, 'spares': 13517, 'liesgirlstell': 8669, 'threads': 14550, 'relationships': 12005, 'twitaddicted': 15082, 'ooommmmggggg': 10473, 'technical': 14322, 'agenda': 1097, 'evan': 5362, 'longoria': 8862, 'cheesey': 3233, 'orignal': 10549, 'pissssssing': 11092, 'kitties': 8323, 'banana': 1908, 'yaaayyy': 16281, 'kd': 8207, '880': 496, 'derham': 4442, '3yeem': 284, 'crocodile': 4005, 'ballarat': 1895, '03': 5, 'welcomed': 15830, 'endorsement': 5215, 'rover': 12370, 'spencer': 13554, 'elevated': 5143, 'streamkeys': 13838, 'migration': 9469, '_uppercut': 827, 'kazim': 8203, 'becky': 2075, 'chorleywood': 3326, 'cambridge': 2895, 'caled': 2869, '_gov': 656, '_sims': 798, 'scoop': 12636, 'kwod': 8405, 'zoomed': 16511, 'trace': 14830, 'tireddd': 14657, 'rosalie': 12343, 'motorcades': 9750, 'clinton': 3449, 'overwhelmi': 10632, 'louise': 8923, 'rennison': 12062, 'tradition': 14843, 'crawfish': 3943, 'khichadi': 8261, 'andrews': 1331, 'sands': 12529, '4jerc': 320, 'morocco': 9718, 'straits': 13821, 'gibraltar': 6373, 'europa': 5355, 'gib': 6371, 'dirteeh': 4604, 'coffeeclub': 3530, 'lee': 8592, '_nj': 757, 'underbelly': 15226, 'bootay': 2460, 'caca': 2845, 'metreon': 9420, 'matchmaker': 9245, 'noice': 10178, 'amazake': 1274, 'powder': 11351, 'agave': 1092, 'jeep': 7988, 'strength': 13844, 'momsen': 9649, 'iowa': 7825, 'creamy': 3958, 'mushrooms': 9848, 'ktfbr': 8389, 'desert': 4448, 'universal': 15278, 'celebrities': 3090, 'carr': 2999, 'threee': 14552, '__hell': 559, 'beeeeaaaaatooooo': 2087, 'purposely': 11629, 'schade': 12615, 'tv_addict': 15025, 'aweee': 1757, 'showwwww': 13014, 'sadifying': 12475, 'oj': 10395, 'soldiers': 13377, 'concepts': 3675, 'mozconcept': 9778, 'thash': 14452, 'manics': 9154, 'speedy': 13547, 'vinny': 15519, 'opposite': 10511, 'hahahahahahahahahah': 6825, 'guility': 6746, 'nds': 9985, 'mettallica': 9423, 'christine': 3337, 'fones': 5934, 'treadmill': 14893, 'penetration': 10892, 'rabbits': 11731, 'recommendation': 11901, 'legalisation': 8598, 'sunshineeeeeee': 14039, 'microplaza': 9449, 'abrjp': 907, 'actions': 977, 'volumes': 15576, 'inspiring': 7733, 'girlie': 6403, 'lester': 8632, 'sox': 13497, 'sturday': 13907, 'discretion': 4636, 'lovah': 8930, 'charter': 3195, 'rohan': 12309, 'sowy': 13496, 'tripping': 14930, 'p2l88x': 10664, 'impatient': 7602, 'gratitude': 6629, 'bymyself': 2830, 'thnk': 14524, 'bruno': 2673, 'trafffffffic': 14846, 'suckssss': 13954, 'headahce': 7022, 'okiebud': 10404, 'quad': 11675, 'ssd': 13645, 'arrives': 1571, 'enable': 5203, 'plugin': 11191, 'duped': 4963, 'notifications': 10246, 'werent': 15846, 'yeaa': 16320, 'performances': 10919, 'silverstone': 13087, 'iracing': 7837, 'scanned': 12594, 'vado': 15420, 'bookmarks': 2442, 'ahold': 1135, 'fuzz': 6201, 'magnet': 9084, 'supplies': 14059, 'gla': 6427, 'valley': 15425, 'forseeable': 5995, 'spaceports': 13504, 'quarantine': 11679, 'clubs': 3482, 'summers': 13999, 'cheescake': 3228, '_jackson': 695, 'cabbage': 2842, 'freeballing': 6056, 'peer': 10881, 'trained': 14857, 'partners': 10786, 'kp': 8375, 'contributed': 3780, 'stimulus': 13772, 'replacements': 12078, 'promiscuous': 11521, 'surfing': 14080, 'chaos': 3165, 'ewww': 5412, 'upppppp': 15333, 'calmin': 2885, 'thristy': 14557, 'gwenyth': 6779, 'scarlett': 12604, 'grumpiness': 6720, 'uniservity': 15272, 'tutorials': 15021, 'motorbike': 9748, 'roundtrip': 12364, 'hapee': 6910, 'hada': 6803, 'retro': 12168, 'southridge': 13489, 'cardiff': 2967, 'girllll': 6405, 'horsing': 7348, 'hubb': 7412, 'naming': 9936, 'importantly': 7609, 'lure': 9008, 'fluids': 5891, 'buwieser': 2810, 'philadelphia': 10972, 'shucks': 13019, 'signings': 13073, 'meka': 9354, 'raspberry': 11793, 'sherbert': 12902, 'frankly': 6036, 'fixing': 5832, 'laptops': 8490, 'diseases': 4644, 'bubblewrap': 2695, 'busyy': 2794, 'astronomy': 1641, '_addict': 564, 'sweeney': 14135, '67f8o': 440, 'treck': 14900, 'lafayette': 8439, 'batonrouge': 1993, 'ashminov': 1606, 'deplurk': 4427, 'buhbyeee': 2725, 'rp3ir': 12375, 'spok': 13591, 'como': 3626, 'surfers': 14078, 'rats': 11801, 'aggressive': 1102, 'sketches': 13161, 'yeeeehaaa': 16346, 'stupidly': 13905, 'clearing': 3431, 'antonio': 1407, 'waitressing': 15635, 'haveto': 6987, 'claire': 3400, 'dayuuum': 4270, '5o': 402, 'edinburghac': 5066, 'pllleeeaaasse': 11182, '_eclectic': 627, 'illogical': 7568, '701': 462, 'inspirative': 7730, 'porto': 11299, 'alegre': 1177, 'fixd': 5830, 'suspicious': 14104, 'dabbling': 4158, 'intros': 7795, 'probability': 11469, 'otherdad': 10566, 'mischief': 9540, 'disgraced': 4646, 'perfectionist': 10915, 'luckkkk': 8983, 'yeehah': 16348, 'loudly': 8918, 'coworker': 3904, 'klemm': 8331, '_0_tronic': 541, 'dyededed': 4981, 'pocketwit': 11207, 'twikini': 15071, 'annapolis': 1365, 'froyo': 6125, 'handsome': 6895, 'colbert': 3542, 'boringgg': 2486, 'lampions': 8462, 'clay': 3418, 'aiken': 1143, 'autograph': 1718, 'blackpool': 2283, 'amandas': 1269, 'crystal': 4042, 'calendars': 2871, 'frat': 6042, 'slooowww': 13237, 'chays': 3204, 'dfizzy': 4507, 'stuffs': 13895, 'arvo': 1593, 'skl': 13177, 'ships': 12932, 'dfs7fy': 4508, 'cavitie': 3062, 'gdit': 6299, 'ranger': 11782, 'yj': 16392, 'hulk': 7429, '88': 495, 'whyyyyyy': 15958, 'loveyoufletch': 8955, 'organize': 10542, 'guarantee': 6733, 'oldies': 10411, 'engulfed': 5229, 'shooooes': 12962, 'maany': 9047, 'annoyingly': 1384, 'annas': 1366, 'sytycd': 14186, 'izzayyy': 7913, 'mwan': 9877, 'carwash': 3013, '48hoursnz': 303, 'goy': 6570, 'naughty': 9970, 'gab': 6219, 'footer': 5952, 'silvera': 13085, 'jenny': 8002, 'greeat': 6647, '8d': 504, 'rolland': 12312, 'garros': 6277, 'amazning': 1284, 'bgn': 2193, '_j9': 694, 'weirdly': 15821, 'greenbelt': 6653, 'dfb': 4506, 'werder': 15842, 'suckiest': 13947, 'tiasha': 14593, 'yiha': 16387, 'feta': 5711, 'cups': 4087, 'armani': 1548, 'pantone': 10739, '109': 35, 'mug': 9818, 'scarfed': 12601, 'feastfriday': 5663, 'requests': 12105, 'nkkairplay': 10154, 'smo': 13287, 'mpg': 9784, 'hasa': 6961, 'casserole': 3027, 'sundaes': 14018, 'dumpster': 4953, '371': 262, 'specilist': 13536, 'ectopic': 5054, 'uritors': 15353, 'ultrasound': 15195, 'surgury': 14082, 'humane': 7433, 'gel': 6315, 'snacks': 13303, 'momol': 9646, 'rplpr': 12377, 'womp': 16086, 'woooomp': 16112, '_yavanna': 847, 'sunrays': 14031, 'acoustic': 967, 'caleb': 2868, 'gump': 6756, 'marcus': 9177, 'macrina': 9061, 'lawyer': 8543, 'trial': 14912, '3to7': 280, 'chief': 3267, 'exhaaaausted': 5450, '_i_girl': 679, 'twitterbff': 15092, 'luch': 8977, 'poetry': 11215, 'catullus': 3048, 'ovid': 10636, 'extract': 5510, 'aeneid': 1056, 'addicts': 999, 'poll': 11244, 'yolonda': 16403, 'pedicures': 10874, 'recouperating': 11915, 'natalies': 9957, 'faceeee': 5539, 'positioned': 11311, 'poof': 11259, 'ringtones': 12246, 'iphones': 7829, 'alicia': 1192, 'yaaawn': 16279, 'pho': 10984, 'harpers': 6953, 'whatsup': 15878, 'peanuts': 10867, 'pounces': 11341, 'thanksss': 14442, 'tania': 14250, 'halved': 6866, 'workload': 16137, 'mtb': 9798, '4wry2': 360, 'rosemary': 12348, 'camerabag': 2902, 'lolo': 8841, 'grandad': 6604, 'estimation': 5335, 'wonders': 16095, 'marshmellows': 9211, 'cramcrackers': 3924, 'mckenna': 9294, 'diets': 4545, 'lathargic': 8515, 'sqeaky': 13628, '_0robertpatt': 542, 'fruit': 6127, 'sparring': 13520, 'rerecordings': 12112, 'cs': 4046, 'fmlllll': 5901, 'beatiful': 2051, 'kiwi': 8326, 'artis': 1585, 'overheat': 10615, 'eabeauty': 4993, 'restricted': 12146, 'priviledges': 11463, 'landon': 8473, 'announces': 1379, 'yell': 16353, 'monroe': 9665, 'trx': 14963, 'ropes': 12341, '30sec': 235, 'swings': 14157, 'windmills': 16000, 'wve': 16212, 'aquats': 1505, 'walker': 15650, 'dismissed': 4659, 'tonyt': 14747, 'frend': 6077, 'l8': 8409, 'wernt': 15847, 'letin': 8635, 'aplyin': 1452, 'agen': 1095, 'trips': 14931, 'enjoyyitverymu': 5237, 'giggles': 6381, 'interupted': 7783, 'ao': 1435, 'gadget': 6222, 'yhere': 16384, 'hickups': 7168, 'exposure': 5493, 'lonesome': 8852, 'nervou': 10043, 'havelunch': 6984, 'lily': 8699, 'allen': 1201, 'oof': 10466, 'moronmonday': 9719, 'aam': 882, 'sunbeam': 14012, 'cafï': 2853, 'grinder': 6679, '198': 109, 'primavera': 11441, 'lorenzo': 8901, 'jarvis': 7962, 'novacaine': 10256, 'pancake': 10723, 'damnation': 4185, 'yon': 16404, 'nearest': 9991, 'walgreens': 15647, 'proverbs': 11552, 'coins': 3538, 'tossin': 14791, 'turnin': 15011, 'awwwwwwwwwwe': 1796, 'wizard': 16058, 'waverly': 15739, 'meantime': 9314, 'packs': 10677, 'liter': 8753, 'kiddo': 8277, 'transcribing': 14866, 'tenth': 14386, 'snore': 13330, 'recos': 11913, 'bles': 2320, 'radishes': 11745, 'blooms': 2356, 'enthused': 5257, 'erock': 5308, 'enjoys': 5236, 'ooh_bell': 10469, 'whatchu': 15872, 'monsterpalooza': 9667, 'contestant': 3766, 'x0': 16228, 'meanie': 9308, '67rt8': 448, '_mounce': 749, '_ofoz': 763, 'cheered': 3223, 'hospice': 7353, 'gpa': 6571, 'compose': 3660, 'architect': 1519, 'tar': 14264, 'promenade': 11520, 'injustice': 7695, 'pox': 11358, 'syphilis': 14183, 'neeeddd': 10010, 'foooddd': 5946, 'lewishhh': 8645, 'mlb': 9588, 'mlbn': 9589, 'televising': 14355, 'det': 4479, 'bal': 1882, 'wieters': 15967, 'chainsaw': 3131, 'sssnoring': 13648, 'wrecked': 16181, 'yessssssir': 16370, 'cancerfree': 2927, 'revisingg': 12189, 'friending': 6097, 'zoozoo': 16512, 'voda': 15564, 'ilike': 7563, 'qljyb': 11664, '_newnew': 754, '_in_nh': 688, '0zywwj': 21, 'ladybug602': 8437, 'nun': 10285, '__cullen_': 556, 'emmett': 5184, 'jorge': 8080, 'tgxzu': 14425, 'pcvs': 10858, 'cider': 3361, 'forgiving': 5978, '_go': 655, 'hwy': 7479, 'cwack': 4130, 'od': 10347, 'pe': 10862, 'fil': 5749, 'breakfasted': 2573, 'playoff': 11153, 'helio': 7106, 'greentea': 6656, 'gettt': 6354, 'clickin': 3440, 'poets': 11216, 'medici': 9327, 'talkedabout': 14234, 'sting': 13773, 'aracheologist': 1511, 'thooo': 14532, 'comeeeeee': 3588, 'whaat': 15860, 'scale': 12590, 'misshimalready': 9556, 'blurryness': 2378, 'monumental': 9681, 'rural': 12428, '2630': 177, 'interviews': 7787, 'rented': 12068, 'roomy': 12335, 'welp': 15836, 'oopsie': 10485, 'origami': 10545, 'partyyyyy': 10794, 'luketic': 8991, 'heathfox': 7064, 'rhythms': 12209, 'sinc': 13102, 'inutero': 7796, 'trainer': 14858, 'babycham': 1820, 'ouuuuuuuuuchhhhhhhh': 10604, 'barca': 1940, 'spanking': 13515, 'impersonal': 7605, 'aaarrrgggghhh': 873, 'decribe': 4337, 'funnily': 6176, 'scarey': 12599, '_kid': 706, 'jobless': 8048, 'cordoning': 3835, 'fantabulous': 5598, '2hrs': 201, 'tanner': 14252, 'avocado': 1737, 'goodgirl': 6509, 'malay': 9123, 'warrior': 15695, 'workaholic': 16130, 'lifestyle': 8675, '55hq2o': 382, 'kristen': 8381, 'venom': 15459, 'styles': 13909, 'scooby': 12635, 'clot': 3469, 'clutches': 3488, 'depaul': 4422, 'origins': 10548, 'spit': 13579, 'ciber': 3358, 'sabip': 12456, 'fp': 6021, 'char': 3172, 'outlook': 10590, 'selective': 12732, 'opacity': 10489, 'wingstop': 16011, '4jken': 325, 'noida': 10179, 'venues': 15465, 'crashes': 3937, 'cmd': 3493, 'qood': 11668, 'morninq': 9716, 'aunty': 1708, 'buenos': 2709, 'dias': 4528, 'mundo': 9837, 'orphanage': 10553, 'combonations': 3581, 'worcester': 16124, 'clive': 3451, 'precise': 11385, 'inconsiderate': 7646, 'yout': 16435, 'swearing': 14123, 'harmed': 6947, 'procuts': 11487, 'weeekend': 15796, 'outsidee': 10594, 'revisionn': 12191, 'kiddnation': 8276, 'puppyy': 11621, 'squirells': 13636, '_sweethearts': 815, 'hydra': 7483, 'wisely': 16029, 'connector': 3733, 'yes2': 16361, 'carefully': 2975, 'deserv': 4449, 'singz': 13114, 'girrrlfriend': 6411, 'cm': 3492, 'aced': 950, 'defense': 4353, 'plumber': 11193, 'chelseavantol': 3240, '_bennett': 583, 'njoying': 10153, 'riveting': 12263, 'britta': 2629, 'absolutley': 912, 'balling': 1897, 'witnessed': 16052, 'fickleness': 5729, 'urm': 15357, 'soonest': 13424, 'fields': 5735, '13pdrmj': 68, 'overstressed': 10627, 'absolves': 914, 'suckd': 13944, 'challenger': 3138, '_exp': 636, 'virtualkiss': 15532, 'alwas': 1258, 'brilliantly': 2615, 'ox': 10655, 'beckett': 2074, 'hiyaaaaaa': 7223, 'defying': 4368, 'alll': 1207, '52': 374, 'tooshers': 14759, 'invitw': 7815, 'omw': 10443, 'kaggra': 8162, 'mistaken': 9569, '1k': 120, 'gastos': 6285, 'namen': 9933, 'warblers': 15678, 'gnatcatcher': 6461, 'awesoome': 1774, 'allllllllll': 1211, 'thrilling': 14556, 'passport': 10804, 'serena': 12777, 'darrian': 4232, 'confess': 3697, 'centro': 3105, 'transfered': 14868, 'sadpanda': 12478, 'ek': 5125, 'makati': 9111, 'stnt0': 13783, 'ahd': 1123, 'parental': 10764, 'halp': 6863, 'longgggggggg': 8860, 'coooolest': 3818, 'offiacial': 10370, 'express': 5495, 'lane': 8475, 'liquid': 8734, 'mmmmmmmmmmmm': 9597, 'euhm': 5351, 'handles': 6890, 'ufr1u': 15164, '121908inlove': 52, 'biddy': 2207, 'bops': 2468, 'kaotic': 8177, 'gossipy': 6556, 'likable': 8687, 'hahahahahahaha': 6823, 'abuzz': 918, 'advertising': 1052, 'qi': 11660, 'davis': 4256, 'tortoiseshell': 14782, 'indoor': 7668, 'colette': 3548, 'shoesless': 12956, 'ardillaaaaaaaaaaaa': 1524, 'whuahahhaha': 15951, 'forrea': 5990, 'jamlegend': 7945, 'petey': 10950, 'rollerskate': 12315, 'graveyard': 6631, 'atltweet': 1662, 'utrecht': 15392, 'marg': 9180, 'cloth': 3470, 'pradas': 11371, 'dunks': 4957, 'getonu2': 6350, 'bethhh': 2165, 'cuong': 4079, 'brokey': 2643, 'borat': 2469, 'izkab': 7910, 'stickam': 13760, 'awwwwww': 1792, 'photowalkingutah': 11003, 'calendar': 2870, 'photowalks': 11004, 'lunches': 9002, 'rp': 12374, '55am': 380, 'iloveyopu': 7574, 'greshamblake': 6664, 'sicken': 13045, 'pav': 10836, 'bhaaji': 2196, 'finely': 5781, 'desappointed': 4445, '_0407': 540, 'havn': 6990, 'sumptuous': 14004, '_2229': 547, 'chased': 3196, '_fan': 638, 'trumping': 14956, 'omlette': 10438, 'omnomlette': 10439, 'ecw': 5055, 'urk': 15354, 'glammyyy': 6432, 'crocker': 4004, 'kaushik': 8196, 'sucka': 13943, 'booker': 2438, 'specified': 13535, 'creator': 3965, '2005': 132, 'buggered': 2717, 'succesful': 13936, 'enroll': 5246, 'successfully': 13939, '_xo': 845, 'colds': 3546, 'assedly': 1622, 'ginniejean': 6397, 'mojo': 9628, 'expedited': 5468, 'geeksonaplane': 6307, 'web1': 15773, 'web2': 15774, 'automated': 1719, 'downloader': 4816, 'youporn': 16423, 'strongest': 13870, 'cvs': 4128, 'wandering': 15662, 'motorola': 9752, 'naini': 9922, '2my': 209, 'hv': 7473, 'snt': 13335, 'multimedia': 9826, 'subscription': 13926, 'repost': 12093, 'deceiving': 4315, 'proclamation': 11482, 'rebellioustwitwhoknowsacoolcatcook': 11869, 'hypertrophy': 7488, 'shoulders': 12993, 'defrag': 4366, 'reassembled': 11865, 'lodging': 8814, 'sportv': 13605, 'connect': 3727, 'rang': 11780, 'glimpse': 6440, 'opps': 10512, 'robbed': 12279, 'approaching': 1493, 'ckas': 3391, 'foster': 6008, 'chubba': 3344, 'winston': 16018, 'inherent': 7689, 'humility': 7437, 'eater': 5034, 'ttc': 14977, 'schmoo': 12623, 'moisturize': 9626, 'inclined': 7638, 'rephael': 12076, '_herdman': 673, '_peek': 771, 'mkay': 9587, 'bfgurelgbsr': 2189, 'orthodontisssttt': 10554, 'uwian': 15406, 'phplurk': 11007, 'sun82': 14010, 'drs': 4903, 'howre': 7387, 'princeton': 11449, 'goodwill': 6519, 'awessomee': 1775, '3lqux': 275, 'harassing': 6930, 'lance': 8465, 'thks': 14520, 'bettiye': 2174, 'riqht': 12255, 'prioritizing': 11455, 'kew': 8244, '_of_chas': 761, 'puter': 11641, 'whee': 15887, 'rebootiness': 11871, 'lorry': 8904, 'signaling': 13070, 'friench': 6095, '4wg12': 352, 'muh': 9820, 'samantha': 12510, 'rustling': 12438, 'pane': 10729, 'shipwrecked': 12933, 'risk': 12257, 'overreaction': 10622, 'yeahhhhhhhhhhhhh': 16330, 'phne': 10983, 'unsociable': 15306, 'earful': 5000, 'psychopath': 11577, 'creep': 3972, 'inconvenient': 7648, '½sseldorf': 16547, 'deffo': 4356, 'waz': 15752, '518': 373, 'joss': 8089, 'whedon': 15886, 'animation': 1355, 'leonardo': 8624, 'dicaprio': 4530, 'jennifer': 8000, 'davisson': 4257, 'killoran': 8290, 'cynthia': 4142, 'coffin': 3533, 'drinkers': 4878, 'mort': 9727, 'subite': 13917, 'luvin': 9018, 'fishfingers': 5817, 'tiday': 14601, 'celli': 3096, 'murphys': 9842, 'tatami': 14282, 'practicing': 11369, 'haaaate': 6790, '67tp9': 449, 'fjeam': 5833, 'reli': 12022, 'buh': 2724, 'balistic': 1892, 'dayuumm': 4269, 'alexis': 1184, 'bledel': 2311, 'ick': 7513, 'winks': 16013, 'heckitty': 7072, '____________': 555, '15yyid': 86, 'twitterin': 15102, 'frustrade': 6131, 'tropical': 14944, 'depression': 4435, 'mrsal65': 9790, 'hurricane': 7460, 'subs': 13924, 'sleeeeeppyyyyyy': 13202, '128': 54, '_reid': 782, 'dissing': 4672, 'octopus': 10346, 'redeems': 11927, 'rescuing': 12121, 'anymoree': 1421, 'zune': 16519, 'bible': 2203, 'consist': 3741, 'realitychec': 11848, 'sprite': 13623, '20somethin': 146, 'bkk': 2273, 'doug': 4805, '_roy': 789, 'fretful': 6086, 'mumbling': 9830, 'muffled': 9817, 'breesaholic': 2588, 'bullhorn': 2738, 'onnnnnnnnnnnn': 10454, 'strangle': 13827, 'salesman': 12501, 'shamwow': 12852, 'relise': 12028, 'respite': 12132, 'yayay': 16306, 'slopes': 13238, 'aarrgghh': 884, 'hose': 7351, 'balearic': 1889, 'sins': 13121, 'hhaha': 7161, 'mhmhmh': 9436, 'biking': 2224, 'sprawled': 13611, 'owww': 10653, 'tempat': 14366, 'apa': 1440, 'yang': 16293, 'paling': 10713, 'cocok': 3518, 'jupiter': 8143, 'seru': 12789, 'juga': 8119, 'vanessa': 15432, 'vfactory': 15481, 'namerebecca': 9934, 'chroma': 3340, 'cumulus': 4077, 'seminars': 12745, 'dagnamit': 4169, 'clickable': 3438, 'maitreya': 9106, 'bobby': 2396, 'produced': 11489, 'machineeeee': 9058, 'extremly': 5515, 'pointlesss': 11223, 'negghead': 10018, 'cantt': 2938, 'sleepp': 13212, 'omq': 10441, 'yuppie': 16469, 'opens': 10497, 'vampire': 15427, 'p274b': 10663, 'wango': 15663, 'elementary': 5138, 'zion': 16502, 'reporting': 12091, 'hayward': 7005, 'summary': 13995, 'harmful': 6948, 'crab': 3913, '5jeu7': 395, 'homemade': 7278, 'lasagna': 8499, 'supwp': 14073, 'tisk': 14664, 'upto': 15343, 'waxing': 15745, 'mustache': 9861, 'cecilia': 3082, 'eddplant': 5059, 'uugghh': 15396, 'storyyyyy': 13813, 'erasure': 5296, 'aaaaaah': 857, 'isabelle': 7857, 'erase': 5294, 'architecture': 1520, 'idgaf': 7523, 'arbiter': 1513, 'judith': 8116, 'volcom': 15573, 'choregoraphy': 3323, 'shaked': 12840, 'fists': 5823, 'abuse': 916, 'dpressed': 4829, 'ust': 15381, 'oriole': 10551, 'suet': 13962, 'feeder': 5677, 'thistle': 14519, '_44': 550, 'reeesee': 11946, 'pulleaase': 11601, 'saimee': 12491, 'underage': 15225, 'roasting': 12277, 'postcard': 11323, 'century': 3107, 'farewells': 5607, 'divorce': 4696, 'acess': 953, 'acceptance': 927, 'tommy': 14723, 'maintanance': 9104, 'womens': 16085, 'alternate': 1252, 'seeley': 12720, 'myoh': 9888, 'tarsier': 14268, 'pins': 11079, 'agrees': 1112, 'dontlike': 4774, 'stalin': 13667, '16ï': 95, '½c': 16530, '25ï': 175, '30ï': 238, 'reaped': 11859, 'nutty': 10298, 'newsire': 10076, 'twitterfeed': 15097, 'bodyshop': 2401, 'takeaway': 14218, 'kardashian': 8181, 'droids': 4892, '_monstr': 748, 'prism': 11457, 'hs': 7396, 'hou': 7369, 'retired': 12162, 'compan': 3630, 'reference': 11951, 'eldorado': 5128, 'aliante': 1189, 'torchwood': 14773, 'lillestrï': 8698, 'struggles': 13876, 'lassetti': 8502, 'fame': 5577, 'unfuzzy': 15259, 'lux': 9023, 'dentists': 4415, 'brainfreeze': 2551, 'mediation': 9324, 'pornstar': 11293, 'dessert': 4470, 'bordom': 2474, 'fatigue': 5635, 'settling': 12808, 'laundering': 8530, 'constructed': 3746, 'possession': 11316, 'rs': 12387, 'counteract': 3876, 'discouraging': 4632, 'tempe': 14367, 'marketplace': 9198, 'filipno': 5755, 'eff': 5098, 'shannen': 12853, 'improves': 7623, 'lashes': 8501, 'tined': 14640, 'moisturiser': 9625, 'glossary': 6450, 'brunt': 2675, 'ratt': 11802, 'pearcy': 10868, 'freed': 6058, 'carefree': 2973, 'feeeeeet': 5679, 'newsbites': 10075, 'wyb4h': 16226, 'fau': 5638, 'steet': 13742, 'slowmotion': 13247, 'hollykins': 7264, 'soproudofyo': 13444, 'multi': 9825, 'taskin': 14273, 'n95': 9904, 'opda': 10491, 'palmade': 10716, 'yeaaaah': 16323, 'livejournal': 8764, 'hedge': 7075, 'dee': 4341, 'btr': 2686, 'lautner': 8535, 'cameron': 2904, 'darts': 4234, 'brass': 2562, 'bmap': 2382, '2134': 148, 'sameway': 12514, '2006': 133, 'joyride': 8099, 'roadtrip': 12271, 'pai': 10687, 'paulo': 10833, '_fr': 644, 'digestif': 4560, 'swimmer': 14150, '291km': 186, '3k': 272, '170th': 98, 'stressiest': 13851, '_chick': 600, 'phantom': 10963, 'stitch': 13781, 'racing': 11737, 'creased': 3959, 'goljwp': 6496, 'roundabouts': 12360, 'proximately': 11562, 'tmw': 14679, 'smartest': 13268, 'newss': 10079, 'honeymoon': 7294, 'bask': 1973, '_dreamer': 624, 'greaaaaat': 6636, 'allies': 1205, 'holga': 7253, 'tomoz': 14731, 'bludge': 2368, '_team': 816, 'etb8h': 5339, 'thu': 14574, 'surface': 14077, 'afro': 1074, 'certificate': 3115, 'sn': 13301, 'cincy': 3366, 'collabro': 3550, 'jphlip': 8104, 'tweethearts': 15049, 'traveling': 14886, '302': 227, 'faires': 5558, 'hub': 7411, 'plugged': 11189, '_hearts': 671, 'cannae': 2932, 'hen': 7125, 'nae': 9911, 'livi': 8771, 'xxxx': 16267, 'twitskies': 15086, 'r20': 11727, '2l': 203, 'webcam': 15775, 'begining': 2100, 'tighten': 14615, 'bolts': 2414, 'lazying': 8553, 'annabelle': 1364, 'thxx': 14588, 'rhcp': 12203, 'electronic': 5134, 'keyhole': 8253, 'belgrade': 2118, 'nyayhahahah': 10308, 'assignemtn': 1628, 'remmebered': 12052, 'umbrellaï': 15200, '2aa0m': 188, 'grrrrrrrr': 6712, '67hac': 443, '67ha': 442, 'plantin': 11134, 'soil': 13373, 'populated': 11287, 'tegan': 14341, 'hazy': 7008, 'gh3': 6360, 'myweakness': 9902, 'besos': 2155, 'grandes': 6608, 'guapo': 6732, 'mileey': 9477, 'voe': 15566, 'mv': 9873, 'eeecontrol': 5083, '1500rpm': 80, '55c': 381, '0rpm': 19, 'twittersucks': 15113, 'noghty': 10177, 'twitterific': 15101, 'painf': 10690, 'applescript': 1477, 'sears': 12695, 'minh': 9507, 'sauna': 12571, 'mgcotd': 9432, 'maxxie': 9271, 'anwar': 1411, 'stressfree': 13849, 'appetizing': 1472, 'rugrats': 12402, 'anndd': 1367, 'lawl': 8539, 'greetings': 6660, 'lighten': 8680, 'spiced': 13561, 'yummmmmyyyy': 16463, 'herbs': 7132, 'shared': 12858, 'wealth': 15761, 'awesomeeeee': 1765, 'yeg': 16350, 'desserts': 4471, 'rethinking': 12160, 'toggling': 14706, 'ganesh': 6256, 'jaju': 7937, 'clicking': 3441, '_ife': 684, 'omnomnom': 10440, '69': 452, 'backlog': 1838, 'lxjd': 9026, 'shades': 12835, 'afireinside687': 1070, 'publicist': 11581, 'learnin': 8575, 'speechless': 13540, 'avalina': 1728, 'dud': 4936, 'humous': 7441, 'dorito': 4787, 'num': 10279, 'sharsies': 12867, 'renew': 12060, 'highways': 7185, '_j': 693, '_logos': 725, 'supply': 14060, 'useing': 15369, 'timothy': 14638, 'ceramic': 3108, 'shatter': 12870, 'singers': 13108, 'pooorr': 11268, '_what_': 839, 'skipped': 13173, 'pinkish': 11074, 'sweeter': 14139, 'seuss': 12810, '½you': 16554, 'accounting': 944, 'hedache': 7074, 'policy': 11237, 'puzzle': 11644, 'aber': 896, 'keanu': 8211, 'eagles': 4997, 'pooof': 11265, 'nirvana': 10144, 'tdl': 14303, 'sleepiness': 13209, 'zelda': 16490, 'pand_i': 10726, 'minnish': 9519, 'd7jvop': 4152, '1800': 101, 'lucien': 8979, 'kerk': 8235, 'vocalists': 15560, 'survey': 14090}\n\n\n\n# let us check the first tweet in the train set\n[invert_voc[k] for k in sorted([e for e in X[0,:].nonzero()[1]])]\n\n['after',\n 'at',\n 'but',\n 'chillin',\n 'continually',\n 'door',\n 'in',\n 'is',\n 'just',\n 'knocking',\n 'long',\n 'my',\n 'pjs',\n 'short',\n 'someone',\n 'week',\n 'why']\n\n\n\n# compare with original tweet\ndf_train['selected_text'].iloc[0]\n\n'Just chillin` in pjs after a short, but long week - why is someone continually knocking at my door?'\n\n\n\nConvert occurrencies to frequencies. Make another version with tf-idf.\n\n\n# check form the sklearn tutorial\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=False).fit(X)\nX_tf = tf_transformer.fit_transform(X)\nX_tf.shape\n\n(24732, 16556)\n\n\n\n# Let's check on the first document:\nfor e in X_tf[0,:]: print(e)\n\n  (0, 1075) 0.24253562503633297\n  (0, 1650) 0.24253562503633297\n  (0, 2795) 0.24253562503633297\n  (0, 3282) 0.24253562503633297\n  (0, 3770) 0.24253562503633297\n  (0, 4783) 0.24253562503633297\n  (0, 7626) 0.24253562503633297\n  (0, 7856) 0.24253562503633297\n  (0, 8148) 0.24253562503633297\n  (0, 8347) 0.24253562503633297\n  (0, 8854) 0.24253562503633297\n  (0, 9880) 0.24253562503633297\n  (0, 11111)    0.24253562503633297\n  (0, 12974)    0.24253562503633297\n  (0, 13398)    0.24253562503633297\n  (0, 15797)    0.24253562503633297\n  (0, 15953)    0.24253562503633297\n\n\n\n# do the same with inverse fequencies\nfrom sklearn.feature_extraction.text import TfidfTransformer\nitf_transformer = TfidfTransformer(use_idf=True).fit(X)\nX_itf = itf_transformer.fit_transform(X)\nX_itf.shape\n\n(24732, 16556)\n\n\n\n# Let's check on the first document:\nfor e in X_itf[0,:]: print(e)\n\n  (0, 15953)    0.20354703308587963\n  (0, 15797)    0.20898321420927882\n  (0, 13398)    0.2373398838636749\n  (0, 12974)    0.27287194297865613\n  (0, 11111)    0.3628915357919444\n  (0, 9880) 0.12289245253150947\n  (0, 8854) 0.22165871192301748\n  (0, 8347) 0.3628915357919444\n  (0, 8148) 0.15137551849249775\n  (0, 7856) 0.13049471067513022\n  (0, 7626) 0.13373857958694677\n  (0, 4783) 0.3000529254510452\n  (0, 3770) 0.3628915357919444\n  (0, 3282) 0.28329367170798353\n  (0, 2795) 0.14697601887809775\n  (0, 1650) 0.15791012471766455\n  (0, 1075) 0.2239315055018362\n\n\n\nChoose a classifier to predict the sentiment on the validation set. Compute the confusion matrix.\n\nWe are now ready to use the features to build a classifier. A common choice of a classifier (used for spam-detection among oters) is a Naive Bayes.\n\n# we can define and train the classifier in one single line\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_itf, df_train['sentiment'])\n\nTo use the new classifier on the test set, we need first to compute the features on the test set. We need to use exactly the same procedure as for the train set.\n\nimport numpy as np\n\n\n# note that we are using `transform`, not `fit_transform` as we are not recomputing the f\nX_test = count_vect.transform(df_test['selected_text'])\nX_itf_test = itf_transformer.transform(X_test)\n\n\npredictions = clf.predict(X_itf_test)\n\n\n# we can now use ready-made functions to compute statistics\nfrom sklearn import metrics\nprint(metrics.classification_report(df_test['sentiment'], predictions))\n\n\n# or start from the confusion matrix\n\n\nmetrics.confusion_matrix(df_test['sentiment'], predictions)\n\narray([[ 458,  317,   24],\n       [  14, 1055,   33],\n       [  12,  206,  629]])\n\n\n\n# comments...\n\n(2748,)"
  },
  {
    "objectID": "tutorials/session_8/old_homework copy.html",
    "href": "tutorials/session_8/old_homework copy.html",
    "title": "Predicting booking cancellations",
    "section": "",
    "text": "The scientific review Data-in-Brief, publishes raw data after a rigorous referee process.\nThe following entry contains booking data for two hotels in Portugal, with many informations about the clients.\nYour goal is to propose a machine learning model to predict whether a given booking will be cancelled.\n(note that the dataset is rather large and that some operations may take some time to complete)\n\n\nImport the dataset. Describe it.\n\nimport pandas\ndf = pandas.read_csv(\"hotel_booking.csv\")\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\nSplit the dataset between a train set and a validation set.\nThe validation set should not be touched until the very end.\n\nimport sklearn\nimport sklearn.model_selection\n\n\ndf_ml, df_validation = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=56)\n# until the very last question, you should use *only* the training set\n\nSplit the df_ml dataframe between a training set and a test set.\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df_ml)\n# the various algorithms can be trained and tested using df_train and df_test\n\n\n\n\nJustify why a machine learning model seems appropriate to predict cancellation. Which one(s) could you use?\nImplement two or more classification models, to predict cancellation.\nCompare their performance on the test set. Which one would you choose?\n\n\n\nUsing your preferred model, use the validation set to compute the confusion matrix. Comment."
  },
  {
    "objectID": "tutorials/session_8/old_homework copy.html#predicting-booking-cancellations",
    "href": "tutorials/session_8/old_homework copy.html#predicting-booking-cancellations",
    "title": "Predicting booking cancellations",
    "section": "",
    "text": "The scientific review Data-in-Brief, publishes raw data after a rigorous referee process.\nThe following entry contains booking data for two hotels in Portugal, with many informations about the clients.\nYour goal is to propose a machine learning model to predict whether a given booking will be cancelled.\n(note that the dataset is rather large and that some operations may take some time to complete)\n\n\nImport the dataset. Describe it.\n\nimport pandas\ndf = pandas.read_csv(\"hotel_booking.csv\")\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\nSplit the dataset between a train set and a validation set.\nThe validation set should not be touched until the very end.\n\nimport sklearn\nimport sklearn.model_selection\n\n\ndf_ml, df_validation = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=56)\n# until the very last question, you should use *only* the training set\n\nSplit the df_ml dataframe between a training set and a test set.\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df_ml)\n# the various algorithms can be trained and tested using df_train and df_test\n\n\n\n\nJustify why a machine learning model seems appropriate to predict cancellation. Which one(s) could you use?\nImplement two or more classification models, to predict cancellation.\nCompare their performance on the test set. Which one would you choose?\n\n\n\nUsing your preferred model, use the validation set to compute the confusion matrix. Comment."
  },
  {
    "objectID": "tutorials/session_4/Regressions_correction.html",
    "href": "tutorials/session_4/Regressions_correction.html",
    "title": "(Multiple) Regressions",
    "section": "",
    "text": "In this tutorial you will learn to run regressions with statsmodels.",
    "crumbs": [
      "tutorials",
      "(Multiple) Regressions"
    ]
  },
  {
    "objectID": "tutorials/session_4/Regressions_correction.html#linear-regressions",
    "href": "tutorials/session_4/Regressions_correction.html#linear-regressions",
    "title": "(Multiple) Regressions",
    "section": "Linear regressions",
    "text": "Linear regressions\nImport the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\", cache=True)\ndf = dataset.data\ndf.head()\n\nTimeoutError: The read operation timed out\n\n\n\n# quickly explore the database:\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\nHere we use the formula from the course. This is not needed in general as the libraries compute all of that for us.\n\n# Compute covariance matrix:\nΣ = df[ ['income', 'education'] ].cov()\nΣ\n\n\n\n\n\n\n\n\nincome\neducation\n\n\n\n\nincome\n597.072727\n526.871212\n\n\neducation\n526.871212\n885.707071\n\n\n\n\n\n\n\n\n# compute averages\nμ = df[ ['income', 'education'] ].mean()\nμ\n\nincome       41.866667\neducation    52.555556\ndtype: float64\n\n\n\nβ = Σ.loc['income','education'] / Σ.loc['education','education']\nβ\n\n0.5948594400410561\n\n\n\nα = μ['income'] - β*μ['education']\n\n\nα\n\n10.603498317842273\n\n\n\nprediction = α + β*df['education']\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'], df['income'], '.')\nplt.plot(df['education'], prediction)\n\n\n\n\n\n\n\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\nSame as before. We do it only to check that the formulas yield the same results as the libraries used below.\n\ndf['prediction'] = α + β*df['education']\ndf['error_term'] =  df['income'] - prediction\n\n\nSigma = df[['income', 'education', 'prediction', 'error_term']].cov()\n\n\ntotal_variance = Sigma.loc['income','income'] \nprediction_variance = Sigma.loc['prediction','prediction']\nerror_variance = Sigma.loc['error_term', 'error_term']\n\n\ntotal_variance\n\n597.0727272727273\n\n\n\nprediction_variance\n\n313.4143142161768\n\n\n\nerror_variance\n\n283.6584130565506\n\n\n\n# we observe that total variance is equal to prediction + error variance\nprediction_variance + error_variance\n\n597.0727272727274\n\n\n\n# we can now copute R^2:\nmyRsquared = 1 - error_variance/total_variance\n\n\nmyRsquared\n\n0.5249181546907554\n\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nWed, 21 Feb 2024\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n14:04:58\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\n# we just need to change the formula\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nWed, 21 Feb 2024\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n14:04:58\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\n# again, we just change the formula, as seen during the lectures\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nWed, 21 Feb 2024\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n14:04:58\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf[['prestige','education']].corr()\n\n\n\n\n\n\n\n\nprestige\neducation\n\n\n\n\nprestige\n1.000000\n0.851916\n\n\neducation\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\nx = df['prestige']\n\n\n# compute predicted values:\na = res_2.params.Intercept\nb = res_2.params.prestige\ny = a + b*x\n\n\n# shorter version\ny = res_2.predict(x)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.hist(resid)\n\n(array([ 1.,  0.,  0.,  2., 19., 11.,  6.,  4.,  1.,  1.]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nThe plot is not very supportive of a normal distribution. The distribution of errors seems skewed to the right.",
    "crumbs": [
      "tutorials",
      "(Multiple) Regressions"
    ]
  },
  {
    "objectID": "tutorials/session_4/Regressions_correction.html#taylor-rule",
    "href": "tutorials/session_4/Regressions_correction.html#taylor-rule",
    "title": "(Multiple) Regressions",
    "section": "Taylor Rule",
    "text": "Taylor Rule\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\n\nimport statsmodels\n\n## google: stats models macrodata\n## google: statsmodels datasets  -&gt; example in the tutorial\n\n# https://www.statsmodels.org/0.6.1/datasets/index.html\n# example about how to use lengley database\n\n\nimport statsmodels.api as sm\n\n\nsm.datasets.macrodata\n\n&lt;module 'statsmodels.datasets.macrodata' from '/opt/conda/lib/python3.10/site-packages/statsmodels/datasets/macrodata/__init__.py'&gt;\n\n\n\nds = sm.datasets.macrodata.load_pandas()\n\n\ndf = ds.raw_data\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nCreate a database with all variables of interest including detrended gdp\n\ngdp = df['realgdp']\ninflation = df['infl']\nrealint = df['realint']\n\n\nddf = df # \n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nWe use the fisher relation: \\(r_t = i_t - \\pi_t\\)\n\nddf['ir'] = ddf['realint'] + ddf['infl']\n\nto detrend the gdp, we use hp-filter function from scipy google: hpfilter scipy. This is a classical tool in macroeconomics.\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\n\n\ncycle, trend = hpfilter(ddf['realgdp'])\n\n\nplt.subplot(211)\nplt.plot(trend, label='Detrended Data')\nplt.plot(trend+cycle, label='Actual data')\nplt.title(\"GDP\")\nplt.legend(loc='upper left')\nplt.subplot(212)\nplt.plot(cycle)\nplt.title(\"Cyclical Component\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nddf['gdp'] = cycle/trend*100 # nominal interest rate and inflation are in percent\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\nRun the basic regression\n\nfrom statsmodels.formula import api as sm\n\n\nmodel = sm.ols(\"ir ~ infl + gdp\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared:\n0.389\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.65\n\n\nDate:\nWed, 21 Feb 2024\nProb (F-statistic):\n4.06e-22\n\n\nTime:\n14:05:00\nLog-Likelihood:\n-448.17\n\n\nNo. Observations:\n203\nAIC:\n902.3\n\n\nDf Residuals:\n200\nBIC:\n912.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2035\n0.252\n12.696\n0.000\n2.706\n3.701\n\n\ninfl\n0.5288\n0.050\n10.557\n0.000\n0.430\n0.628\n\n\ngdp\n0.0795\n0.105\n0.759\n0.449\n-0.127\n0.286\n\n\n\n\n\n\nOmnibus:\n30.222\nDurbin-Watson:\n0.417\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n50.662\n\n\nSkew:\n0.796\nProb(JB):\n9.98e-12\n\n\nKurtosis:\n4.858\nCond. No.\n8.56\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\n\nmodel = sm.ols(\"ir ~ infl + gdp + pop + unemp -1\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared (uncentered):\n0.884\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n380.2\n\n\nDate:\nWed, 21 Feb 2024\nProb (F-statistic):\n5.64e-92\n\n\nTime:\n14:05:00\nLog-Likelihood:\n-432.84\n\n\nNo. Observations:\n203\nAIC:\n873.7\n\n\nDf Residuals:\n199\nBIC:\n886.9\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ninfl\n0.4380\n0.049\n8.895\n0.000\n0.341\n0.535\n\n\ngdp\n0.5710\n0.120\n4.739\n0.000\n0.333\n0.809\n\n\npop\n-0.0050\n0.002\n-2.068\n0.040\n-0.010\n-0.000\n\n\nunemp\n0.8064\n0.108\n7.458\n0.000\n0.593\n1.020\n\n\n\n\n\n\nOmnibus:\n5.307\nDurbin-Watson:\n0.391\n\n\nProb(Omnibus):\n0.070\nJarque-Bera (JB):\n7.501\n\n\nSkew:\n0.070\nProb(JB):\n0.0235\n\n\nKurtosis:\n3.931\nCond. No.\n247.\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAt confidence level 2.5% gdp is between 0.333 and 0.809.\nAt confidence level 2.5% infl is between 0.341 and 0.535.\nThe coefficients would be significantly different from 0.5 if 0.5 was not in the condifence interval.",
    "crumbs": [
      "tutorials",
      "(Multiple) Regressions"
    ]
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables.html",
    "href": "tutorials/session_5/instrumental_variables.html",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\nA problem arises when the confounding factor \\(z\\) is not observed. In that case, we can estimate the direct effect of \\(x\\) on \\(y\\) by using \\(q\\) as an instrument.\nRun the follwing code to create a mock dataset.\n\nimport numpy as np\nimport pandas as pd\n\n\nN = 100000\nϵ_z = np.random.randn(N)*0.1\nϵ_x = np.random.randn(N)*0.1\nϵ_q = np.random.randn(N)*0.01\nϵ_y = np.random.randn(N)*0.01\n\n\nz = 0.1 + ϵ_z\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\n# here we must change the definition so that q affects x:\n# x = 0.1 + z + ϵ_x\nx = 0.1 + z + q + ϵ_x\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the dataframe. Compute the correlations between the variables. Are they compatible with the hypotheses for IV?\n\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\n0\n0.458958\n1.428012\n-0.006158\n0.489963\n\n\n1\n0.536544\n1.535235\n0.119739\n0.484211\n\n\n2\n0.550795\n1.561178\n0.149119\n0.470899\n\n\n3\n0.774870\n1.773310\n0.184079\n0.501838\n\n\n4\n0.761371\n1.769818\n0.197662\n0.501622\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.699704\n1.669716\n0.099930\n0.499973\n\n\nstd\n0.150261\n0.164733\n0.099805\n0.015818\n\n\nmin\n-0.037950\n0.874772\n-0.301234\n0.429432\n\n\n25%\n0.598491\n1.558824\n0.032560\n0.489285\n\n\n50%\n0.699121\n1.669068\n0.099462\n0.499978\n\n\n75%\n0.800179\n1.780555\n0.167347\n0.510695\n\n\nmax\n1.329398\n2.349659\n0.522942\n0.567387\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.981526\n0.662762\n0.618551\n\n\ny\n0.981526\n1.000000\n0.786408\n0.507140\n\n\nz\n0.662762\n0.786408\n1.000000\n-0.002547\n\n\nq\n0.618551\n0.507140\n-0.002547\n1.000000\n\n\n\n\n\n\n\n\n\n\nUse linearmodels to run a regression estimating the effect of \\(x\\) on \\(y\\) (note the slight API change w.r.t. statsmodels). Comment.\n\nfrom linearmodels import OLS, IV2SLS\n\n\nmodel = OLS.from_formula(\"y ~ x\", df)\nres = model.fit()\nres      # in statsmodels would be res.summary()\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9634\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9634\n\n\nNo. Observations:\n100000\nF-statistic:\n2.637e+06\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:08:54\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9168\n0.0005\n1932.5\n0.0000\n0.9159\n0.9177\n\n\nx\n1.0761\n0.0007\n1624.0\n0.0000\n1.0748\n1.0774\n\n\n\nid: 0x7f79702b7c50\n\n\nThe regression is globally very significant (p-value &lt; 1.e-5). The predictive power is very high (R^2=0.96).\nConstants and coefficients are both statistically very significant (p-values&lt;1e-5 for both) and the confidence intervals are very small.\nAssume briefly that z is known and control the regression by z. What happens?\n\nmodel = OLS.from_formula(\"y ~ x + z\", df)\nres = model.fit()\nres  \n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9963\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9963\n\n\nNo. Observations:\n100000\nF-statistic:\n2.713e+07\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:08:59\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0000\n0.0002\n5752.0\n0.0000\n0.9997\n1.0004\n\n\nx\n0.9000\n0.0003\n3213.3\n0.0000\n0.8994\n0.9005\n\n\nz\n0.4000\n0.0004\n944.90\n0.0000\n0.3992\n0.4008\n\n\n\nid: 0x7f79702eb020\n\n\n\n\n\nMake a causality graph, summarizing what you know from the equations.\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"y ~ 1 + [x~q]\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9389\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.9389\n\n\nNo. Observations:\n100000\nF-statistic:\n4.316e+05\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n11:30:43\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0383\n0.0010\n1071.1\n0.0000\n1.0364\n1.0402\n\n\nx\n0.9022\n0.0014\n656.96\n0.0000\n0.8995\n0.9048\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7fcf7dd34d10\n\n\n\ncomment\n\n\n\n\n\nWe follow the excellent R tutorial from the (excellent) Econometrics with R book.\nThe goal is to measure the effect of schooling on earnings, while correcting the endogeneity bias by using distance to college as an instrument.\nDownload the college distance dataset with statsmodels. Describe the dataset and extract the dataframe.\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html\n\nimport statsmodels.api as sm\nds = sm.datasets.get_rdataset(\"CollegeDistance\", \"AER\")\n\n\nds?\n\n\nType:            Dataset\nString form:     &lt;class 'statsmodels.datasets.utils.Dataset'&gt;\nLength:          5\nFile:            ~/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/statsmodels/datasets/utils.py\nDocstring:      \n.. container::\n=============== ===============\nCollegeDistance R Documentation\n=============== ===============\n.. rubric:: College Distance Data\n   :name: CollegeDistance\n.. rubric:: Description\n   :name: description\nCross-section data from the High School and Beyond survey conducted\nby the Department of Education in 1980, with a follow-up in 1986. The\nsurvey included students from approximately 1,100 high schools.\n.. rubric:: Usage\n   :name: usage\n.. code:: R\n   data(\"CollegeDistance\")\n.. rubric:: Format\n   :name: format\nA data frame containing 4,739 observations on 14 variables.\ngender\n   factor indicating gender.\nethnicity\n   factor indicating ethnicity (African-American, Hispanic or other).\nscore\n   base year composite test score. These are achievement tests given\n   to high school seniors in the sample.\nfcollege\n   factor. Is the father a college graduate?\nmcollege\n   factor. Is the mother a college graduate?\nhome\n   factor. Does the family own their home?\nurban\n   factor. Is the school in an urban area?\nunemp\n   county unemployment rate in 1980.\nwage\n   state hourly wage in manufacturing in 1980.\ndistance\n   distance from 4-year college (in 10 miles).\ntuition\n   average state 4-year college tuition (in 1000 USD).\neducation\n   number of years of education.\nincome\n   factor. Is the family income above USD 25,000 per year?\nregion\n   factor indicating region (West or other).\n.. rubric:: Details\n   :name: details\nRouse (1995) computed years of education by assigning 12 years to all\nmembers of the senior class. Each additional year of secondary\neducation counted as a one year. Students with vocational degrees\nwere assigned 13 years, AA degrees were assigned 14 years, BA degrees\nwere assigned 16 years, those with some graduate education were\nassigned 17 years, and those with a graduate degree were assigned 18\nyears.\nStock and Watson (2007) provide separate data files for the students\nfrom Western states and the remaining students. ``CollegeDistance``\nincludes both data sets, subsets are easily obtained (see also\nexamples).\n.. rubric:: Source\n   :name: source\nOnline complements to Stock and Watson (2007).\n.. rubric:: References\n   :name: references\nRouse, C.E. (1995). Democratization or Diversion? The Effect of\nCommunity Colleges on Educational Attainment. *Journal of Business &\nEconomic Statistics*, **12**, 217–224.\nStock, J.H. and Watson, M.W. (2007). *Introduction to Econometrics*,\n2nd ed. Boston: Addison Wesley.\n.. rubric:: See Also\n   :name: see-also\n``StockWatson2007``\n.. rubric:: Examples\n   :name: examples\n.. code:: R\n   ## exclude students from Western states\n   data(\"CollegeDistance\")\n   cd &lt;- subset(CollegeDistance, region != \"west\")\n   summary(cd)\nClass docstring:\ndict() -&gt; new empty dictionary\ndict(mapping) -&gt; new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -&gt; new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -&gt; new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)\n\n\n\n\n# describe dataset\n# print(ds.__doc__)\n\n\ndf = ds.data # dataframe attached to dataset\n\n\n# describe dataframe\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\n\n\nrownames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n\n\n\n\n\n\n\nHow is income encoded? Create a binary variable income_binary to replace it.\n\ndf['income'] # takes two values `high` an `low`\n\nrownames\n1       high\n2        low\n3        low\n4        low\n5        low\n        ... \n9391    high\n9401    high\n9411    high\n9421    high\n9431    high\nName: income, Length: 4739, dtype: object\n\n\n\ndf['income'].unique()\n\narray(['high', 'low'], dtype=object)\n\n\n\n# define a binary variable `income_binary` : 0 if income is low, 1 otherwise\n\n\ndf['income_binary'] = (df['income'] == 'high')*1.0\n# multiply by 1.0 to convert booleans into binary\n\n\ndf['income_binary']\n\nrownames\n1       1.0\n2       0.0\n3       0.0\n4       0.0\n5       0.0\n       ... \n9391    1.0\n9401    1.0\n9411    1.0\n9421    1.0\n9431    1.0\nName: income_binary, Length: 4739, dtype: float64\n\n\nPlot an histogram of distance to college.\n\ndf['distance'].describe()\n\ncount    4739.000000\nmean        1.802870\nstd         2.297128\nmin         0.000000\n25%         0.400000\n50%         1.000000\n75%         2.500000\nmax        20.000000\nName: distance, dtype: float64\n\n\n\ndf['distance'].hist()\n\n\n\n\n\n\n\n\n\n# other ways to do a histogram\n\n\nfrom matplotlib import pyplot as plt\n# plt.hist(df['distance']) # same graph\n\n\n# use seaborn\nimport seaborn as sns\nsns.histplot(df['distance'])\n\n\n\n\n\n\n\n\nRun the naive regression \\(income_{binary}=\\beta_0 + \\beta_1 \\text{education} + u\\).\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0480\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0478\n\n\nNo. Observations:\n4739\nF-statistic:\n227.43\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n11:57:29\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4780\n0.0499\n-9.5702\n0.0000\n-0.5759\n-0.3801\n\n\neducation\n0.0555\n0.0037\n15.081\n0.0000\n0.0483\n0.0627\n\n\n\nid: 0x7fcf62b1dc40\n\n\nAugment the regression with unemp, hispanic, af-am, female and urban. Notice that categorical variables are encoded automatically. What are the treatment values? Change it using the syntax (C(var,Treatment='ref'))\n\ndf.columns\n\nIndex(['gender', 'ethnicity', 'score', 'fcollege', 'mcollege', 'home', 'urban',\n       'unemp', 'wage', 'distance', 'tuition', 'education', 'income', 'region',\n       'income_binary'],\n      dtype='object')\n\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education + gender + ethnicity + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0830\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0818\n\n\nNo. Observations:\n4739\nF-statistic:\n456.93\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:02:32\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4159\n0.0538\n-7.7362\n0.0000\n-0.5212\n-0.3105\n\n\neducation\n0.0511\n0.0036\n14.030\n0.0000\n0.0440\n0.0583\n\n\ngender[T.male]\n0.0490\n0.0128\n3.8341\n0.0001\n0.0239\n0.0740\n\n\nethnicity[T.hispanic]\n-0.0296\n0.0185\n-1.6006\n0.1095\n-0.0660\n0.0067\n\n\nethnicity[T.other]\n0.1235\n0.0167\n7.3992\n0.0000\n0.0908\n0.1563\n\n\nurban[T.yes]\n-0.0470\n0.0149\n-3.1556\n0.0016\n-0.0763\n-0.0178\n\n\nunemp\n-0.0115\n0.0023\n-5.0997\n0.0000\n-0.0159\n-0.0071\n\n\n\nid: 0x7fcf62a4b4d0\n\n\n\ndf['ethnicity'].unique()\n\narray(['other', 'afam', 'hispanic'], dtype=object)\n\n\nThe library has created automatically dummy variable for categories. For the ethnicity variable, it has used afam as reference value. Let’s use other as reference value.\n\n#\n\n# C(ethnicity) -&gt; flags ethnicity as categorical variable\n$ C(ethnicity, Treatment(reference='other'))\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education + gender + C(ethnicity, Treatment(reference='other')) + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0830\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0818\n\n\nNo. Observations:\n4739\nF-statistic:\n456.93\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:10:11\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.2923\n0.0541\n-5.4047\n0.0000\n-0.3983\n-0.1863\n\n\neducation\n0.0511\n0.0036\n14.030\n0.0000\n0.0440\n0.0583\n\n\ngender[T.male]\n0.0490\n0.0128\n3.8341\n0.0001\n0.0239\n0.0740\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.1235\n0.0167\n-7.3992\n0.0000\n-0.1563\n-0.0908\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1532\n0.0151\n-10.127\n0.0000\n-0.1828\n-0.1235\n\n\nurban[T.yes]\n-0.0470\n0.0149\n-3.1556\n0.0016\n-0.0763\n-0.0178\n\n\nunemp\n-0.0115\n0.0023\n-5.0997\n0.0000\n-0.0159\n-0.0071\n\n\n\nid: 0x7fcf62b1c5c0\n\n\nThe regression is highly significant (R^2&gt;0 with p-value &lt; 1e.-5). The predictive power is low, with only 8% of total variance explained by education.\nAll coefficients are significant at a 2% confidence level.\nAccording to the results:\nComment the results and explain the selection problem\nExplain why distance to college might be used to instrument the effect of schooling.\nRun an IV regression, where distance is used to instrument schooling.\nlook at: https://bashtage.github.io/linearmodels/ (two-stage least squares)\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + [education~distance] + gender + C(ethnicity, Treatment(reference='other')) + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n-0.2734\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n-0.2750\n\n\nNo. Observations:\n4739\nF-statistic:\n213.68\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:16:16\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-2.4218\n0.5313\n-4.5582\n0.0000\n-3.4631\n-1.3805\n\n\ngender[T.male]\n0.0456\n0.0150\n3.0301\n0.0024\n0.0161\n0.0750\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.0456\n0.0283\n-1.6123\n0.1069\n-0.1011\n0.0098\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1075\n0.0223\n-4.8322\n0.0000\n-0.1511\n-0.0639\n\n\nurban[T.yes]\n-0.0527\n0.0182\n-2.8947\n0.0038\n-0.0884\n-0.0170\n\n\nunemp\n-0.0101\n0.0027\n-3.7771\n0.0002\n-0.0153\n-0.0048\n\n\neducation\n0.2032\n0.0378\n5.3800\n0.0000\n0.1292\n0.2773\n\n\n\nEndogenous: educationInstruments: distanceRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7fcf624aa3f0\n\n\nComment the results. Compare with the R tutorials."
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables.html#iv-example-on-mock-dataset",
    "href": "tutorials/session_5/instrumental_variables.html#iv-example-on-mock-dataset",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\nA problem arises when the confounding factor \\(z\\) is not observed. In that case, we can estimate the direct effect of \\(x\\) on \\(y\\) by using \\(q\\) as an instrument.\nRun the follwing code to create a mock dataset.\n\nimport numpy as np\nimport pandas as pd\n\n\nN = 100000\nϵ_z = np.random.randn(N)*0.1\nϵ_x = np.random.randn(N)*0.1\nϵ_q = np.random.randn(N)*0.01\nϵ_y = np.random.randn(N)*0.01\n\n\nz = 0.1 + ϵ_z\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\n# here we must change the definition so that q affects x:\n# x = 0.1 + z + ϵ_x\nx = 0.1 + z + q + ϵ_x\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the dataframe. Compute the correlations between the variables. Are they compatible with the hypotheses for IV?\n\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\n0\n0.458958\n1.428012\n-0.006158\n0.489963\n\n\n1\n0.536544\n1.535235\n0.119739\n0.484211\n\n\n2\n0.550795\n1.561178\n0.149119\n0.470899\n\n\n3\n0.774870\n1.773310\n0.184079\n0.501838\n\n\n4\n0.761371\n1.769818\n0.197662\n0.501622\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.699704\n1.669716\n0.099930\n0.499973\n\n\nstd\n0.150261\n0.164733\n0.099805\n0.015818\n\n\nmin\n-0.037950\n0.874772\n-0.301234\n0.429432\n\n\n25%\n0.598491\n1.558824\n0.032560\n0.489285\n\n\n50%\n0.699121\n1.669068\n0.099462\n0.499978\n\n\n75%\n0.800179\n1.780555\n0.167347\n0.510695\n\n\nmax\n1.329398\n2.349659\n0.522942\n0.567387\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.981526\n0.662762\n0.618551\n\n\ny\n0.981526\n1.000000\n0.786408\n0.507140\n\n\nz\n0.662762\n0.786408\n1.000000\n-0.002547\n\n\nq\n0.618551\n0.507140\n-0.002547\n1.000000\n\n\n\n\n\n\n\n\n\n\nUse linearmodels to run a regression estimating the effect of \\(x\\) on \\(y\\) (note the slight API change w.r.t. statsmodels). Comment.\n\nfrom linearmodels import OLS, IV2SLS\n\n\nmodel = OLS.from_formula(\"y ~ x\", df)\nres = model.fit()\nres      # in statsmodels would be res.summary()\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9634\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9634\n\n\nNo. Observations:\n100000\nF-statistic:\n2.637e+06\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:08:54\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9168\n0.0005\n1932.5\n0.0000\n0.9159\n0.9177\n\n\nx\n1.0761\n0.0007\n1624.0\n0.0000\n1.0748\n1.0774\n\n\n\nid: 0x7f79702b7c50\n\n\nThe regression is globally very significant (p-value &lt; 1.e-5). The predictive power is very high (R^2=0.96).\nConstants and coefficients are both statistically very significant (p-values&lt;1e-5 for both) and the confidence intervals are very small.\nAssume briefly that z is known and control the regression by z. What happens?\n\nmodel = OLS.from_formula(\"y ~ x + z\", df)\nres = model.fit()\nres  \n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9963\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9963\n\n\nNo. Observations:\n100000\nF-statistic:\n2.713e+07\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:08:59\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0000\n0.0002\n5752.0\n0.0000\n0.9997\n1.0004\n\n\nx\n0.9000\n0.0003\n3213.3\n0.0000\n0.8994\n0.9005\n\n\nz\n0.4000\n0.0004\n944.90\n0.0000\n0.3992\n0.4008\n\n\n\nid: 0x7f79702eb020\n\n\n\n\n\nMake a causality graph, summarizing what you know from the equations.\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"y ~ 1 + [x~q]\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9389\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.9389\n\n\nNo. Observations:\n100000\nF-statistic:\n4.316e+05\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n11:30:43\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0383\n0.0010\n1071.1\n0.0000\n1.0364\n1.0402\n\n\nx\n0.9022\n0.0014\n656.96\n0.0000\n0.8995\n0.9048\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7fcf7dd34d10\n\n\n\ncomment"
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables.html#return-on-education",
    "href": "tutorials/session_5/instrumental_variables.html#return-on-education",
    "title": "Instrumental variables",
    "section": "",
    "text": "We follow the excellent R tutorial from the (excellent) Econometrics with R book.\nThe goal is to measure the effect of schooling on earnings, while correcting the endogeneity bias by using distance to college as an instrument.\nDownload the college distance dataset with statsmodels. Describe the dataset and extract the dataframe.\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html\n\nimport statsmodels.api as sm\nds = sm.datasets.get_rdataset(\"CollegeDistance\", \"AER\")\n\n\nds?\n\n\nType:            Dataset\nString form:     &lt;class 'statsmodels.datasets.utils.Dataset'&gt;\nLength:          5\nFile:            ~/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/statsmodels/datasets/utils.py\nDocstring:      \n.. container::\n=============== ===============\nCollegeDistance R Documentation\n=============== ===============\n.. rubric:: College Distance Data\n   :name: CollegeDistance\n.. rubric:: Description\n   :name: description\nCross-section data from the High School and Beyond survey conducted\nby the Department of Education in 1980, with a follow-up in 1986. The\nsurvey included students from approximately 1,100 high schools.\n.. rubric:: Usage\n   :name: usage\n.. code:: R\n   data(\"CollegeDistance\")\n.. rubric:: Format\n   :name: format\nA data frame containing 4,739 observations on 14 variables.\ngender\n   factor indicating gender.\nethnicity\n   factor indicating ethnicity (African-American, Hispanic or other).\nscore\n   base year composite test score. These are achievement tests given\n   to high school seniors in the sample.\nfcollege\n   factor. Is the father a college graduate?\nmcollege\n   factor. Is the mother a college graduate?\nhome\n   factor. Does the family own their home?\nurban\n   factor. Is the school in an urban area?\nunemp\n   county unemployment rate in 1980.\nwage\n   state hourly wage in manufacturing in 1980.\ndistance\n   distance from 4-year college (in 10 miles).\ntuition\n   average state 4-year college tuition (in 1000 USD).\neducation\n   number of years of education.\nincome\n   factor. Is the family income above USD 25,000 per year?\nregion\n   factor indicating region (West or other).\n.. rubric:: Details\n   :name: details\nRouse (1995) computed years of education by assigning 12 years to all\nmembers of the senior class. Each additional year of secondary\neducation counted as a one year. Students with vocational degrees\nwere assigned 13 years, AA degrees were assigned 14 years, BA degrees\nwere assigned 16 years, those with some graduate education were\nassigned 17 years, and those with a graduate degree were assigned 18\nyears.\nStock and Watson (2007) provide separate data files for the students\nfrom Western states and the remaining students. ``CollegeDistance``\nincludes both data sets, subsets are easily obtained (see also\nexamples).\n.. rubric:: Source\n   :name: source\nOnline complements to Stock and Watson (2007).\n.. rubric:: References\n   :name: references\nRouse, C.E. (1995). Democratization or Diversion? The Effect of\nCommunity Colleges on Educational Attainment. *Journal of Business &\nEconomic Statistics*, **12**, 217–224.\nStock, J.H. and Watson, M.W. (2007). *Introduction to Econometrics*,\n2nd ed. Boston: Addison Wesley.\n.. rubric:: See Also\n   :name: see-also\n``StockWatson2007``\n.. rubric:: Examples\n   :name: examples\n.. code:: R\n   ## exclude students from Western states\n   data(\"CollegeDistance\")\n   cd &lt;- subset(CollegeDistance, region != \"west\")\n   summary(cd)\nClass docstring:\ndict() -&gt; new empty dictionary\ndict(mapping) -&gt; new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -&gt; new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -&gt; new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)\n\n\n\n\n# describe dataset\n# print(ds.__doc__)\n\n\ndf = ds.data # dataframe attached to dataset\n\n\n# describe dataframe\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\n\n\nrownames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n\n\n\n\n\n\n\nHow is income encoded? Create a binary variable income_binary to replace it.\n\ndf['income'] # takes two values `high` an `low`\n\nrownames\n1       high\n2        low\n3        low\n4        low\n5        low\n        ... \n9391    high\n9401    high\n9411    high\n9421    high\n9431    high\nName: income, Length: 4739, dtype: object\n\n\n\ndf['income'].unique()\n\narray(['high', 'low'], dtype=object)\n\n\n\n# define a binary variable `income_binary` : 0 if income is low, 1 otherwise\n\n\ndf['income_binary'] = (df['income'] == 'high')*1.0\n# multiply by 1.0 to convert booleans into binary\n\n\ndf['income_binary']\n\nrownames\n1       1.0\n2       0.0\n3       0.0\n4       0.0\n5       0.0\n       ... \n9391    1.0\n9401    1.0\n9411    1.0\n9421    1.0\n9431    1.0\nName: income_binary, Length: 4739, dtype: float64\n\n\nPlot an histogram of distance to college.\n\ndf['distance'].describe()\n\ncount    4739.000000\nmean        1.802870\nstd         2.297128\nmin         0.000000\n25%         0.400000\n50%         1.000000\n75%         2.500000\nmax        20.000000\nName: distance, dtype: float64\n\n\n\ndf['distance'].hist()\n\n\n\n\n\n\n\n\n\n# other ways to do a histogram\n\n\nfrom matplotlib import pyplot as plt\n# plt.hist(df['distance']) # same graph\n\n\n# use seaborn\nimport seaborn as sns\nsns.histplot(df['distance'])\n\n\n\n\n\n\n\n\nRun the naive regression \\(income_{binary}=\\beta_0 + \\beta_1 \\text{education} + u\\).\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0480\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0478\n\n\nNo. Observations:\n4739\nF-statistic:\n227.43\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n11:57:29\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4780\n0.0499\n-9.5702\n0.0000\n-0.5759\n-0.3801\n\n\neducation\n0.0555\n0.0037\n15.081\n0.0000\n0.0483\n0.0627\n\n\n\nid: 0x7fcf62b1dc40\n\n\nAugment the regression with unemp, hispanic, af-am, female and urban. Notice that categorical variables are encoded automatically. What are the treatment values? Change it using the syntax (C(var,Treatment='ref'))\n\ndf.columns\n\nIndex(['gender', 'ethnicity', 'score', 'fcollege', 'mcollege', 'home', 'urban',\n       'unemp', 'wage', 'distance', 'tuition', 'education', 'income', 'region',\n       'income_binary'],\n      dtype='object')\n\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education + gender + ethnicity + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0830\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0818\n\n\nNo. Observations:\n4739\nF-statistic:\n456.93\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:02:32\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4159\n0.0538\n-7.7362\n0.0000\n-0.5212\n-0.3105\n\n\neducation\n0.0511\n0.0036\n14.030\n0.0000\n0.0440\n0.0583\n\n\ngender[T.male]\n0.0490\n0.0128\n3.8341\n0.0001\n0.0239\n0.0740\n\n\nethnicity[T.hispanic]\n-0.0296\n0.0185\n-1.6006\n0.1095\n-0.0660\n0.0067\n\n\nethnicity[T.other]\n0.1235\n0.0167\n7.3992\n0.0000\n0.0908\n0.1563\n\n\nurban[T.yes]\n-0.0470\n0.0149\n-3.1556\n0.0016\n-0.0763\n-0.0178\n\n\nunemp\n-0.0115\n0.0023\n-5.0997\n0.0000\n-0.0159\n-0.0071\n\n\n\nid: 0x7fcf62a4b4d0\n\n\n\ndf['ethnicity'].unique()\n\narray(['other', 'afam', 'hispanic'], dtype=object)\n\n\nThe library has created automatically dummy variable for categories. For the ethnicity variable, it has used afam as reference value. Let’s use other as reference value.\n\n#\n\n# C(ethnicity) -&gt; flags ethnicity as categorical variable\n$ C(ethnicity, Treatment(reference='other'))\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + education + gender + C(ethnicity, Treatment(reference='other')) + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n0.0830\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0818\n\n\nNo. Observations:\n4739\nF-statistic:\n456.93\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:10:11\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.2923\n0.0541\n-5.4047\n0.0000\n-0.3983\n-0.1863\n\n\neducation\n0.0511\n0.0036\n14.030\n0.0000\n0.0440\n0.0583\n\n\ngender[T.male]\n0.0490\n0.0128\n3.8341\n0.0001\n0.0239\n0.0740\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.1235\n0.0167\n-7.3992\n0.0000\n-0.1563\n-0.0908\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1532\n0.0151\n-10.127\n0.0000\n-0.1828\n-0.1235\n\n\nurban[T.yes]\n-0.0470\n0.0149\n-3.1556\n0.0016\n-0.0763\n-0.0178\n\n\nunemp\n-0.0115\n0.0023\n-5.0997\n0.0000\n-0.0159\n-0.0071\n\n\n\nid: 0x7fcf62b1c5c0\n\n\nThe regression is highly significant (R^2&gt;0 with p-value &lt; 1e.-5). The predictive power is low, with only 8% of total variance explained by education.\nAll coefficients are significant at a 2% confidence level.\nAccording to the results:\nComment the results and explain the selection problem\nExplain why distance to college might be used to instrument the effect of schooling.\nRun an IV regression, where distance is used to instrument schooling.\nlook at: https://bashtage.github.io/linearmodels/ (two-stage least squares)\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"income_binary ~ 1 + [education~distance] + gender + C(ethnicity, Treatment(reference='other')) + urban + unemp\"   # IV2SLS has no intercept by default\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\nincome_binary\nR-squared:\n-0.2734\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n-0.2750\n\n\nNo. Observations:\n4739\nF-statistic:\n213.68\n\n\nDate:\nWed, Feb 14 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n12:16:16\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-2.4218\n0.5313\n-4.5582\n0.0000\n-3.4631\n-1.3805\n\n\ngender[T.male]\n0.0456\n0.0150\n3.0301\n0.0024\n0.0161\n0.0750\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.0456\n0.0283\n-1.6123\n0.1069\n-0.1011\n0.0098\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1075\n0.0223\n-4.8322\n0.0000\n-0.1511\n-0.0639\n\n\nurban[T.yes]\n-0.0527\n0.0182\n-2.8947\n0.0038\n-0.0884\n-0.0170\n\n\nunemp\n-0.0101\n0.0027\n-3.7771\n0.0002\n-0.0153\n-0.0048\n\n\neducation\n0.2032\n0.0378\n5.3800\n0.0000\n0.1292\n0.2773\n\n\n\nEndogenous: educationInstruments: distanceRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7fcf624aa3f0\n\n\nComment the results. Compare with the R tutorials."
  },
  {
    "objectID": "tutorials/session_5/happiness_regression_correction.html",
    "href": "tutorials/session_5/happiness_regression_correction.html",
    "title": "Regressions 2",
    "section": "",
    "text": "We will analyse those data to find relationships between the happiness score and economy, family, health, freedom, trust, perception of corruption, generosity…\nThe dataset contains the following variables:\n\nCountry : Country name\nOverall rank : Country ranking based on happiness score\nScore : Individual personal happiness rating from 0 to 10.\nGDP per capita : GDP per capita of each country in terms of purchasing power parity (PPP) (in USD)\nSocial support : Individual rating that determines whether, when you have problems, your family or friends would help you. Binary responses (0 or 1).\nHealthy life expectancy : Healthy life expectancy at birth is based on data from the World Health Organization (WHO)\nFreedom to make life choices : Individual rating that determines whether you are atisfied or dissatisfied with your freedom to choose hat you do with your life. Binary responses (0 or 1).\nGenerosity : Generosity is the residual from the regression of the national mean of responses to the question “Have you donated money to a charity in the last month?” on GDP per capita.\nPerceptions of corruption : Average of binary responses to two GWP questions: corruption in government and corruption in business.\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.formula import api as smf\n\nOpen the csv “happiness_index_2019”\n\ndf=pd.read_csv('happiness_index_2019.csv')\ndf.columns = df.columns.str.replace(' ', '_')\n\nExplore the dataset by using head function.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nOverall_rank\nCountry_or_region\nScore\nGDP_per_capita\nSocial_support\nHealthy_life_expectancy\nFreedom_to_make_life_choices\nGenerosity\nPerceptions_of_corruption\n\n\n\n\n0\n1\nFinland\n7.769\n1.340\n1.587\n0.986\n0.596\n0.153\n0.393\n\n\n1\n2\nDenmark\n7.600\n1.383\n1.573\n0.996\n0.592\n0.252\n0.410\n\n\n2\n3\nNorway\n7.554\n1.488\n1.582\n1.028\n0.603\n0.271\n0.341\n\n\n3\n4\nIceland\n7.494\n1.380\n1.624\n1.026\n0.591\n0.354\n0.118\n\n\n4\n5\nNetherlands\n7.488\n1.396\n1.522\n0.999\n0.557\n0.322\n0.298\n\n\n5\n6\nSwitzerland\n7.480\n1.452\n1.526\n1.052\n0.572\n0.263\n0.343\n\n\n6\n7\nSweden\n7.343\n1.387\n1.487\n1.009\n0.574\n0.267\n0.373\n\n\n7\n8\nNew Zealand\n7.307\n1.303\n1.557\n1.026\n0.585\n0.330\n0.380\n\n\n8\n9\nCanada\n7.278\n1.365\n1.505\n1.039\n0.584\n0.285\n0.308\n\n\n9\n10\nAustria\n7.246\n1.376\n1.475\n1.016\n0.532\n0.244\n0.226\n\n\n\n\n\n\n\nCompute descriptive statistics using a Pandas function\n\ndf.describe()\n\n\n\n\n\n\n\n\nOverall_rank\nScore\nGDP_per_capita\nSocial_support\nHealthy_life_expectancy\nFreedom_to_make_life_choices\nGenerosity\nPerceptions_of_corruption\n\n\n\n\ncount\n156.000000\n156.000000\n156.000000\n156.000000\n156.000000\n156.000000\n156.000000\n156.000000\n\n\nmean\n78.500000\n5.407096\n0.905147\n1.208814\n0.725244\n0.392571\n0.184846\n0.110603\n\n\nstd\n45.177428\n1.113120\n0.398389\n0.299191\n0.242124\n0.143289\n0.095254\n0.094538\n\n\nmin\n1.000000\n2.853000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n39.750000\n4.544500\n0.602750\n1.055750\n0.547750\n0.308000\n0.108750\n0.047000\n\n\n50%\n78.500000\n5.379500\n0.960000\n1.271500\n0.789000\n0.417000\n0.177500\n0.085500\n\n\n75%\n117.250000\n6.184500\n1.232500\n1.452500\n0.881750\n0.507250\n0.248250\n0.141250\n\n\nmax\n156.000000\n7.769000\n1.684000\n1.624000\n1.141000\n0.631000\n0.566000\n0.453000\n\n\n\n\n\n\n\nPlot variables that may have a positive correlation using matplotlib\n\nplt.plot(df['Score'],df['GDP_per_capita'],'o')\n\n\n\n\n\n\n\n\n\nplt.plot(df['Score'],df['Healthy_life_expectancy'],'o')\n\n\n\n\n\n\n\n\nPlot the correlation matrix of the main variables using heatmap function of Seaborn package. It should already be installed on your Nuvolos instance (use escpython kernel).\n\n#Correlation Map\n\nlist1=[\"Score\",\"GDP_per_capita\",\"Social_support\",\"Healthy_life_expectancy\",\"Freedom_to_make_life_choices\",\"Generosity\",\"Perceptions_of_corruption\"]\nfig, ax = plt.subplots(figsize=(12,10)) \nsns.heatmap(df[list1].corr(), annot=True, cmap=\"YlGnBu\", linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above charts, we can obtain the following conclusions:\nThe Happiness Score is highly related with the GDP per Capita, Social Support and Healthy Life Expectancy.\nThe Happines Score is not related at all with the Generosity Variable.\nSo, as a first conclusion, we could say that the Happiest Countries will be the ones with higher GDP per capita, Social Support and Life Expectancy.\n\n\n\n\n\n\nPerform various linear regressions to predict the Happiness score using one of the variables available in the dataset.\n\ndf.columns\n\nIndex(['Overall_rank', 'Country_or_region', 'Score', 'GDP_per_capita',\n       'Social_support', 'Healthy_life_expectancy',\n       'Freedom_to_make_life_choices', 'Generosity',\n       'Perceptions_of_corruption'],\n      dtype='object')\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita\", df)\nres_1 = model_1.fit()\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.630\n\n\nModel:\nOLS\nAdj. R-squared:\n0.628\n\n\nMethod:\nLeast Squares\nF-statistic:\n262.5\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n4.32e-35\n\n\nTime:\n11:00:56\nLog-Likelihood:\n-159.97\n\n\nNo. Observations:\n156\nAIC:\n323.9\n\n\nDf Residuals:\n154\nBIC:\n330.0\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3993\n0.135\n25.120\n0.000\n3.132\n3.667\n\n\nGDP_per_capita\n2.2181\n0.137\n16.202\n0.000\n1.948\n2.489\n\n\n\n\n\n\nOmnibus:\n1.139\nDurbin-Watson:\n1.378\n\n\nProb(Omnibus):\n0.566\nJarque-Bera (JB):\n1.244\n\n\nSkew:\n-0.177\nProb(JB):\n0.537\n\n\nKurtosis:\n2.742\nCond. No.\n4.77\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita + Healthy_life_expectancy + Freedom_to_make_life_choices\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.742\n\n\nModel:\nOLS\nAdj. R-squared:\n0.737\n\n\nMethod:\nLeast Squares\nF-statistic:\n146.1\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n1.42e-44\n\n\nTime:\n11:00:13\nLog-Likelihood:\n-131.75\n\n\nNo. Observations:\n156\nAIC:\n271.5\n\n\nDf Residuals:\n152\nBIC:\n283.7\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4201\n0.167\n14.519\n0.000\n2.091\n2.749\n\n\nGDP_per_capita\n1.1781\n0.210\n5.599\n0.000\n0.762\n1.594\n\n\nHealthy_life_expectancy\n1.4578\n0.348\n4.189\n0.000\n0.770\n2.145\n\n\nFreedom_to_make_life_choices\n2.1993\n0.349\n6.298\n0.000\n1.509\n2.889\n\n\n\n\n\n\nOmnibus:\n11.927\nDurbin-Watson:\n1.497\n\n\nProb(Omnibus):\n0.003\nJarque-Bera (JB):\n12.394\n\n\nSkew:\n-0.665\nProb(JB):\n0.00204\n\n\nKurtosis:\n3.369\nCond. No.\n14.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='GDP_per_capita', y='Score', data=df)\n\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"Score ~ Healthy_life_expectancy\", df)\nres_2 = model_2.fit()\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.608\n\n\nModel:\nOLS\nAdj. R-squared:\n0.606\n\n\nMethod:\nLeast Squares\nF-statistic:\n239.1\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n3.79e-33\n\n\nTime:\n20:44:17\nLog-Likelihood:\n-164.48\n\n\nNo. Observations:\n156\nAIC:\n333.0\n\n\nDf Residuals:\n154\nBIC:\n339.1\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.8068\n0.177\n15.837\n0.000\n2.457\n3.157\n\n\nHealthy_life_expectancy\n3.5854\n0.232\n15.462\n0.000\n3.127\n4.043\n\n\n\n\n\n\nOmnibus:\n6.324\nDurbin-Watson:\n1.140\n\n\nProb(Omnibus):\n0.042\nJarque-Bera (JB):\n3.543\n\n\nSkew:\n-0.148\nProb(JB):\n0.170\n\n\nKurtosis:\n2.324\nCond. No.\n6.41\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='Healthy_life_expectancy', y='Score', data=df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_3 = smf.ols(\"Score ~ Healthy_life_expectancy + Freedom_to_make_life_choices + GDP_per_capita +Social_support\", df)\nres_3= model_3.fit()\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.771\n\n\nModel:\nOLS\nAdj. R-squared:\n0.765\n\n\nMethod:\nLeast Squares\nF-statistic:\n127.0\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n2.82e-47\n\n\nTime:\n20:44:20\nLog-Likelihood:\n-122.62\n\n\nNo. Observations:\n156\nAIC:\n255.2\n\n\nDf Residuals:\n151\nBIC:\n270.5\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.8921\n0.199\n9.491\n0.000\n1.498\n2.286\n\n\nHealthy_life_expectancy\n1.1414\n0.337\n3.384\n0.001\n0.475\n1.808\n\n\nFreedom_to_make_life_choices\n1.8458\n0.340\n5.423\n0.000\n1.173\n2.518\n\n\nGDP_per_capita\n0.8105\n0.216\n3.745\n0.000\n0.383\n1.238\n\n\nSocial_support\n1.0166\n0.235\n4.331\n0.000\n0.553\n1.480\n\n\n\n\n\n\nOmnibus:\n5.077\nDurbin-Watson:\n1.641\n\n\nProb(Omnibus):\n0.079\nJarque-Bera (JB):\n4.685\n\n\nSkew:\n-0.413\nProb(JB):\n0.0961\n\n\nKurtosis:\n3.198\nCond. No.\n17.8\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n### By comparing adjusted $R^2$, find the regression which explains best happiness.\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n\nx=df.Score.values.reshape(-1,1)\ny=df[\"Healthy_life_expectancy\"].values.reshape(-1,1)\nplt.scatter(df[\"Score\"],df[\"Healthy_life_expectancy\"],color=\"green\")\n\npolynomial_regression=PolynomialFeatures(degree=4)\nx_polynomial=polynomial_regression.fit_transform(x)\n\n#%% fit\nlinear_regression2=LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n#%%\ny_head2=linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head2,color=\"red\",label=\"poly\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "tutorials/session_5/happiness_regression_correction.html#a-simple-linear-regression",
    "href": "tutorials/session_5/happiness_regression_correction.html#a-simple-linear-regression",
    "title": "Regressions 2",
    "section": "",
    "text": "Perform various linear regressions to predict the Happiness score using one of the variables available in the dataset.\n\ndf.columns\n\nIndex(['Overall_rank', 'Country_or_region', 'Score', 'GDP_per_capita',\n       'Social_support', 'Healthy_life_expectancy',\n       'Freedom_to_make_life_choices', 'Generosity',\n       'Perceptions_of_corruption'],\n      dtype='object')\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita\", df)\nres_1 = model_1.fit()\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.630\n\n\nModel:\nOLS\nAdj. R-squared:\n0.628\n\n\nMethod:\nLeast Squares\nF-statistic:\n262.5\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n4.32e-35\n\n\nTime:\n11:00:56\nLog-Likelihood:\n-159.97\n\n\nNo. Observations:\n156\nAIC:\n323.9\n\n\nDf Residuals:\n154\nBIC:\n330.0\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.3993\n0.135\n25.120\n0.000\n3.132\n3.667\n\n\nGDP_per_capita\n2.2181\n0.137\n16.202\n0.000\n1.948\n2.489\n\n\n\n\n\n\nOmnibus:\n1.139\nDurbin-Watson:\n1.378\n\n\nProb(Omnibus):\n0.566\nJarque-Bera (JB):\n1.244\n\n\nSkew:\n-0.177\nProb(JB):\n0.537\n\n\nKurtosis:\n2.742\nCond. No.\n4.77\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_1 = smf.ols(\"Score ~ GDP_per_capita + Healthy_life_expectancy + Freedom_to_make_life_choices\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.742\n\n\nModel:\nOLS\nAdj. R-squared:\n0.737\n\n\nMethod:\nLeast Squares\nF-statistic:\n146.1\n\n\nDate:\nWed, 15 Feb 2023\nProb (F-statistic):\n1.42e-44\n\n\nTime:\n11:00:13\nLog-Likelihood:\n-131.75\n\n\nNo. Observations:\n156\nAIC:\n271.5\n\n\nDf Residuals:\n152\nBIC:\n283.7\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4201\n0.167\n14.519\n0.000\n2.091\n2.749\n\n\nGDP_per_capita\n1.1781\n0.210\n5.599\n0.000\n0.762\n1.594\n\n\nHealthy_life_expectancy\n1.4578\n0.348\n4.189\n0.000\n0.770\n2.145\n\n\nFreedom_to_make_life_choices\n2.1993\n0.349\n6.298\n0.000\n1.509\n2.889\n\n\n\n\n\n\nOmnibus:\n11.927\nDurbin-Watson:\n1.497\n\n\nProb(Omnibus):\n0.003\nJarque-Bera (JB):\n12.394\n\n\nSkew:\n-0.665\nProb(JB):\n0.00204\n\n\nKurtosis:\n3.369\nCond. No.\n14.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='GDP_per_capita', y='Score', data=df)\n\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"Score ~ Healthy_life_expectancy\", df)\nres_2 = model_2.fit()\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.608\n\n\nModel:\nOLS\nAdj. R-squared:\n0.606\n\n\nMethod:\nLeast Squares\nF-statistic:\n239.1\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n3.79e-33\n\n\nTime:\n20:44:17\nLog-Likelihood:\n-164.48\n\n\nNo. Observations:\n156\nAIC:\n333.0\n\n\nDf Residuals:\n154\nBIC:\n339.1\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.8068\n0.177\n15.837\n0.000\n2.457\n3.157\n\n\nHealthy_life_expectancy\n3.5854\n0.232\n15.462\n0.000\n3.127\n4.043\n\n\n\n\n\n\nOmnibus:\n6.324\nDurbin-Watson:\n1.140\n\n\nProb(Omnibus):\n0.042\nJarque-Bera (JB):\n3.543\n\n\nSkew:\n-0.148\nProb(JB):\n0.170\n\n\nKurtosis:\n2.324\nCond. No.\n6.41\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.regplot(x='Healthy_life_expectancy', y='Score', data=df)"
  },
  {
    "objectID": "tutorials/session_5/happiness_regression_correction.html#b-multiple-linear-regression",
    "href": "tutorials/session_5/happiness_regression_correction.html#b-multiple-linear-regression",
    "title": "Regressions 2",
    "section": "",
    "text": "model_3 = smf.ols(\"Score ~ Healthy_life_expectancy + Freedom_to_make_life_choices + GDP_per_capita +Social_support\", df)\nres_3= model_3.fit()\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nScore\nR-squared:\n0.771\n\n\nModel:\nOLS\nAdj. R-squared:\n0.765\n\n\nMethod:\nLeast Squares\nF-statistic:\n127.0\n\n\nDate:\nTue, 14 Feb 2023\nProb (F-statistic):\n2.82e-47\n\n\nTime:\n20:44:20\nLog-Likelihood:\n-122.62\n\n\nNo. Observations:\n156\nAIC:\n255.2\n\n\nDf Residuals:\n151\nBIC:\n270.5\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.8921\n0.199\n9.491\n0.000\n1.498\n2.286\n\n\nHealthy_life_expectancy\n1.1414\n0.337\n3.384\n0.001\n0.475\n1.808\n\n\nFreedom_to_make_life_choices\n1.8458\n0.340\n5.423\n0.000\n1.173\n2.518\n\n\nGDP_per_capita\n0.8105\n0.216\n3.745\n0.000\n0.383\n1.238\n\n\nSocial_support\n1.0166\n0.235\n4.331\n0.000\n0.553\n1.480\n\n\n\n\n\n\nOmnibus:\n5.077\nDurbin-Watson:\n1.641\n\n\nProb(Omnibus):\n0.079\nJarque-Bera (JB):\n4.685\n\n\nSkew:\n-0.413\nProb(JB):\n0.0961\n\n\nKurtosis:\n3.198\nCond. No.\n17.8\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n### By comparing adjusted $R^2$, find the regression which explains best happiness.\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n\nx=df.Score.values.reshape(-1,1)\ny=df[\"Healthy_life_expectancy\"].values.reshape(-1,1)\nplt.scatter(df[\"Score\"],df[\"Healthy_life_expectancy\"],color=\"green\")\n\npolynomial_regression=PolynomialFeatures(degree=4)\nx_polynomial=polynomial_regression.fit_transform(x)\n\n#%% fit\nlinear_regression2=LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n#%%\ny_head2=linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head2,color=\"red\",label=\"poly\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "slides/session_6/graphs/inference.html",
    "href": "slides/session_6/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_10/index.html#bias-vs-variance",
    "href": "slides/session_10/index.html#bias-vs-variance",
    "title": "Some Important Points",
    "section": "Bias vs Variance",
    "text": "Bias vs Variance\n\n\nA model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (bad fit) vs Variance (bad prediction) tradeoff\nThe no free lunch theorem"
  },
  {
    "objectID": "slides/session_10/index.html#overfitting",
    "href": "slides/session_10/index.html#overfitting",
    "title": "Some Important Points",
    "section": "Overfitting",
    "text": "Overfitting"
  },
  {
    "objectID": "slides/session_10/index.html#explanation-vs-prediction",
    "href": "slides/session_10/index.html#explanation-vs-prediction",
    "title": "Some Important Points",
    "section": "Explanation vs Prediction",
    "text": "Explanation vs Prediction\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power"
  },
  {
    "objectID": "slides/session_10/index.html#read-regression-results",
    "href": "slides/session_10/index.html#read-regression-results",
    "title": "Some Important Points",
    "section": "Read Regression Results",
    "text": "Read Regression Results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\n\n\n\nCheck:\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\nadj. R2: predictive power corrected for excessive degrees of freedom\nglobal significance (p-value of Fisher test): chances we would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\n\n\n\n\nCoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals"
  },
  {
    "objectID": "slides/session_10/index.html#read-a-regression-table",
    "href": "slides/session_10/index.html#read-a-regression-table",
    "title": "Some Important Points",
    "section": "Read A Regression Table",
    "text": "Read A Regression Table\n\nExample of a Regression Table"
  },
  {
    "objectID": "slides/session_10/index.html#can-there-be-too-many-variables",
    "href": "slides/session_10/index.html#can-there-be-too-many-variables",
    "title": "Some Important Points",
    "section": "Can there be too many variables?",
    "text": "Can there be too many variables?\n\nOverfitting\n\nbad predictions\n\nColinearity\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium"
  },
  {
    "objectID": "slides/session_10/index.html#colinearity",
    "href": "slides/session_10/index.html#colinearity",
    "title": "Some Important Points",
    "section": "Colinearity",
    "text": "Colinearity\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\n\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)"
  },
  {
    "objectID": "slides/session_10/index.html#ommitted-variable",
    "href": "slides/session_10/index.html#ommitted-variable",
    "title": "Some Important Points",
    "section": "Ommitted Variable",
    "text": "Ommitted Variable\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests"
  },
  {
    "objectID": "slides/session_10/index.html#omitted-variable",
    "href": "slides/session_10/index.html#omitted-variable",
    "title": "Some Important Points",
    "section": "Omitted Variable",
    "text": "Omitted Variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\nNote that adding the indicator\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\ncorrected for the omitted variable bias (true value of b is actually 0.2)\nprovided an estimate for the effect of variable gender"
  },
  {
    "objectID": "slides/session_10/index.html#endogeneity",
    "href": "slides/session_10/index.html#endogeneity",
    "title": "Some Important Points",
    "section": "Endogeneity",
    "text": "Endogeneity\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: omitted variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/session_10/index.html#instrument",
    "href": "slides/session_10/index.html#instrument",
    "title": "Some Important Points",
    "section": "Instrument",
    "text": "Instrument\n\\[y = a + b x + \\epsilon\\]\n\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with any component of \\(\\epsilon\\)\nit needs to predict \\(y\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "slides/session_10/index_handout.html",
    "href": "slides/session_10/index_handout.html",
    "title": "Some Important Points",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (bad fit) vs Variance (bad prediction) tradeoff\nThe no free lunch theorem"
  },
  {
    "objectID": "slides/session_10/index_handout.html#bias-vs-variance",
    "href": "slides/session_10/index_handout.html#bias-vs-variance",
    "title": "Some Important Points",
    "section": "",
    "text": "A model is fitted (trained / regressed) on a given amount of data\nA model can be more or less flexible\n\nhave more or less independent parameters (aka degrees of freedom)\nex: \\(y = a + b x\\) (2) vs \\(y = a + b x_1 + c x_1^2 + e x_2 + f x_3\\) (5)\n\nMore flexible models fit the training data better…\n…but tend to perform worse for predictions\nThis is known as:\n\nThe Bias (bad fit) vs Variance (bad prediction) tradeoff\nThe no free lunch theorem"
  },
  {
    "objectID": "slides/session_10/index_handout.html#overfitting",
    "href": "slides/session_10/index_handout.html#overfitting",
    "title": "Some Important Points",
    "section": "Overfitting",
    "text": "Overfitting"
  },
  {
    "objectID": "slides/session_10/index_handout.html#explanation-vs-prediction",
    "href": "slides/session_10/index_handout.html#explanation-vs-prediction",
    "title": "Some Important Points",
    "section": "Explanation vs Prediction",
    "text": "Explanation vs Prediction\n\n\nThe goal of machine learning consists in making the best predictions:\n\nuse enough data to maximize the fit…\n… but control the number of independent parameters to prevent overfitting\n\nex: LASSO regression has lots of parameters, but tries to keep most of them zero\n\nultimately quality of prediction is evaluated on a test set, independent from the training set\n\nIn econometrics we can perform\n\npredictions: sames issues as ML\nexplanatory analysis: focus on the effect of one (or a few) explanatory variables\n\nthis does not necessary require strong predictive power"
  },
  {
    "objectID": "slides/session_10/index_handout.html#read-regression-results",
    "href": "slides/session_10/index_handout.html#read-regression-results",
    "title": "Some Important Points",
    "section": "Read Regression Results",
    "text": "Read Regression Results\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.252\nModel:                            OLS   Adj. R-squared:                  0.245\nMethod:                 Least Squares   F-statistic:                     33.08\nDate:                Tue, 30 Mar 2021   Prob (F-statistic):           1.01e-07\nTime:                        02:34:12   Log-Likelihood:                -111.39\nNo. Observations:                 100   AIC:                             226.8\nDf Residuals:                      98   BIC:                             232.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     -0.1750      0.162     -1.082      0.282      -0.496       0.146\nx              0.1377      0.024      5.751      0.000       0.090       0.185\n==============================================================================\nOmnibus:                        2.673   Durbin-Watson:                   1.118\nProb(Omnibus):                  0.263   Jarque-Bera (JB):                2.654\nSkew:                           0.352   Prob(JB):                        0.265\nKurtosis:                       2.626   Cond. No.                         14.9\n==============================================================================\n\nUnderstand p-value: chances that a given statistics might have been obtained, under the H0 hypothesis\n\n\n\n\nCheck:\n\nR2: provides an indication of predictive power. Does not prevent overfitting.\nadj. R2: predictive power corrected for excessive degrees of freedom\nglobal significance (p-value of Fisher test): chances we would have obtained this R2 if all real coefficients were actually 0 (H0 hypothesis)\n\n\n\n\n\nCoefficient:\n\np-value probability that coefficient might have been greater than observed, if it was actually 0.\nif p-value is smaller than 5%: the coefficient is significant at a 5% level\nconfidence intervals (5%): if the true coefficient was out of this interval, observed value would be very implausible\n\nhigher confidence levels -&gt; bigger intervals"
  },
  {
    "objectID": "slides/session_10/index_handout.html#read-a-regression-table",
    "href": "slides/session_10/index_handout.html#read-a-regression-table",
    "title": "Some Important Points",
    "section": "Read A Regression Table",
    "text": "Read A Regression Table\n\n\n\nExample of a Regression Table"
  },
  {
    "objectID": "slides/session_10/index_handout.html#can-there-be-too-many-variables",
    "href": "slides/session_10/index_handout.html#can-there-be-too-many-variables",
    "title": "Some Important Points",
    "section": "Can there be too many variables?",
    "text": "Can there be too many variables?\n\nOverfitting\n\nbad predictions\n\nColinearity\n\ncan bias a coefficient of interest\nnot a problem for prediction\nexact colinearity makes traditional OLS fail\n\nTo choose the right amount of variables find a combination which maximizes adjusted R2 or an information criterium"
  },
  {
    "objectID": "slides/session_10/index_handout.html#colinearity",
    "href": "slides/session_10/index_handout.html#colinearity",
    "title": "Some Important Points",
    "section": "Colinearity",
    "text": "Colinearity\n\n\n\\(x\\) is colinear with \\(y\\) if \\(cor(x,y)\\) very close to 1\nmore generally \\(x\\) is colinear with \\(y_1, ... y_n\\) if \\(x\\) can be deduced linearly from \\(y_1...y_n\\)\n\nthere exists \\(\\lambda_1, ... \\lambda_n\\) such that \\(x = \\lambda_1 x_1 + ... + \\lambda_n x_n\\)\nexample: hours of sleep / hours awake (sleep=24-awake)\n\nperfect colinearity is a problem: coefficients are not well defined\n\n\\(\\text{productivity} = 0.1 + 0.5 \\text{sleep} + 0.5 \\text{awake}\\) or \\(\\text{productivity} = -11.9 + 1 \\text{sleep} + 1 \\text{awake}\\) ?\n\nbest regressions have regressors that:\nexplain independent variable\nare independent from each other (as much as possible)"
  },
  {
    "objectID": "slides/session_10/index_handout.html#ommitted-variable",
    "href": "slides/session_10/index_handout.html#ommitted-variable",
    "title": "Some Important Points",
    "section": "Ommitted Variable",
    "text": "Ommitted Variable\nWhat if you don’t have enough variables?\n\\(y = a + bx\\)\n\nR2 can be low. It’s ok for explanatory analysis.\nas long as residuals are normally distributed\n\ncheck graphically to be sure\n(more advanced): there are statistical tests"
  },
  {
    "objectID": "slides/session_10/index_handout.html#omitted-variable",
    "href": "slides/session_10/index_handout.html#omitted-variable",
    "title": "Some Important Points",
    "section": "Omitted Variable",
    "text": "Omitted Variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we want to know the effect of \\(x\\) on \\(y\\).\nWe run the regression \\(y = a + b x\\)\n\nwe find \\(y = 0.21 + \\color{red}{0.15} x\\)\n\nWe then realize we have access to a categorical variable \\(gender \\in {male, female}\\)\nWe then add the \\(\\delta\\) dummy variable to the regression: \\(y = a + bx + c \\delta\\)\n\nwe find $ y = -0.04 + x - 0.98 $\n\nNote that adding the indicator\n\nimproved the fit (\\(R^2\\) is 0.623 instead of 0.306)\ncorrected for the omitted variable bias (true value of b is actually 0.2)\nprovided an estimate for the effect of variable gender"
  },
  {
    "objectID": "slides/session_10/index_handout.html#endogeneity",
    "href": "slides/session_10/index_handout.html#endogeneity",
    "title": "Some Important Points",
    "section": "Endogeneity",
    "text": "Endogeneity\n\n\nConsider the regression model \\(y = a + b x + \\epsilon\\)\nWhen \\(\\epsilon\\) is correlated with \\(x\\) we have an endogeneity problem.\n\nwe can check in the regression results whether the residuals ares correlated with \\(y\\) or \\(x\\)\n\nEndogeneity can have several sources: omitted variable, measurement error, simultaneity\n\nit creates a bias in the estimate of \\(a\\) and \\(b\\)\n\nWe say we control for endogeneity by adding some variables\nA special case of endogeneity is a confounding factor a variable \\(z\\) which causes at the same time \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/session_10/index_handout.html#instrument",
    "href": "slides/session_10/index_handout.html#instrument",
    "title": "Some Important Points",
    "section": "Instrument",
    "text": "Instrument\n\\[y = a + b x + \\epsilon\\]\n\n\nRecall: endogeneity issue when \\(\\epsilon\\) is correlated with \\(x\\)\nInstrument: a way to keep only the variability of \\(x\\) that is independent from \\(\\epsilon\\)\n\nit needs to be correlated with \\(x\\)\nnot with any component of \\(\\epsilon\\)\nit needs to predict \\(y\\)\n\nAn instrument can be used to solve endogeneity issues\nIt can also establish the causality from \\(x\\) to \\(y\\):\n\nsince it is independent from \\(\\epsilon\\), all its effect on \\(y\\) goes through \\(x\\)"
  },
  {
    "objectID": "slides/session_10/graphs/Untitled1.html",
    "href": "slides/session_10/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_7/graphs/inference.html",
    "href": "slides/session_7/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_3/index.html#a-simple-dataset",
    "href": "slides/session_3/index.html#a-simple-dataset",
    "title": "Linear Regression",
    "section": "A Simple Dataset",
    "text": "A Simple Dataset\nDuncan’s Occupational Prestige Data - Many occupations in 1950. - Education and prestige associated to each occupation\n\n\\(x\\): education\n\nPercentage of occupational incumbents in 1950 who were high school graduates\n\n\\(y\\): income\n\nPercentage of occupational incumbents in the 1950 US Census who earned $3,500 or more per year\n\n\\(z\\): Percentage of respondents in a social survey who rated the occupation as “good” or better in prestige"
  },
  {
    "objectID": "slides/session_3/index.html#quick-look",
    "href": "slides/session_3/index.html#quick-look",
    "title": "Linear Regression",
    "section": "Quick look",
    "text": "Quick look\nImport the data from statsmodels’ dataset:\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\nrownames\n\n\n\n\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90"
  },
  {
    "objectID": "slides/session_3/index.html#descriptive-statistics-1",
    "href": "slides/session_3/index.html#descriptive-statistics-1",
    "title": "Linear Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nFor any variable \\(v\\) with \\(N\\) observations:\n\nmean: \\(\\overline{v} = \\frac{1}{N} \\sum_{i=1}^N v_i\\)\nvariance \\(V({v}) = \\frac{1}{N} \\sum_{i=1}^N \\left(v_i - \\overline{v} \\right)^2\\)\nstandard deviation : \\(\\sigma(v)=\\sqrt{V(v)}\\)\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000"
  },
  {
    "objectID": "slides/session_3/index.html#relation-between-variables",
    "href": "slides/session_3/index.html#relation-between-variables",
    "title": "Linear Regression",
    "section": "Relation between variables",
    "text": "Relation between variables\n\n\nHow do we measure relations between two variables (with \\(N\\) observations)\n\nCovariance: \\(Cov(x,y) = \\frac{1}{N}\\sum_i (x_i-\\overline{x})(y_i-\\overline{y})\\)\nCorrelation: \\(Cor(x,y) = \\frac{Cov(x,y)}{\\sigma(x)\\sigma(y)}\\)\n\nBy construction, \\(Cor(x,y)\\in[-1,1]\\)\n\nif \\(Cor(x,y)&gt;0\\), x and y are positively correlated\nif \\(Cor(x,y)&lt;0\\), x and y are negatively correlated\n\nInterpretation:\n\nno interpretation!\ncorrelation is not causality\nalso: data can be correlated by pure chance (spurious correlation)"
  },
  {
    "objectID": "slides/session_3/index.html#examples",
    "href": "slides/session_3/index.html#examples",
    "title": "Linear Regression",
    "section": "Examples",
    "text": "Examples\n\n\n\ndf[['income','education','prestige']].cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\n\n\ndf[['income','education','prestige']].corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\n\n\n\n\nCan we visualize correlations?"
  },
  {
    "objectID": "slides/session_3/index.html#quick",
    "href": "slides/session_3/index.html#quick",
    "title": "Linear Regression",
    "section": "Quick",
    "text": "Quick\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Income\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'],df['prestige'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Prestige\")\nplt.savefig(\"data_description.png\")"
  },
  {
    "objectID": "slides/session_3/index.html#quick-look-1",
    "href": "slides/session_3/index.html#quick-look-1",
    "title": "Linear Regression",
    "section": "Quick look",
    "text": "Quick look\nUsing matplotlib (3d)"
  },
  {
    "objectID": "slides/session_3/index.html#quick-look-2",
    "href": "slides/session_3/index.html#quick-look-2",
    "title": "Linear Regression",
    "section": "Quick look",
    "text": "Quick look\n\nimport seaborn as sns\nsns.pairplot(df[['education', 'prestige', 'income']])\n\n\n\n\n\n\n\n\nThe pairplot made with seaborn gives a simple sense of correlations as well as information about the distribution of each variable."
  },
  {
    "objectID": "slides/session_3/index.html#a-linear-model",
    "href": "slides/session_3/index.html#a-linear-model",
    "title": "Linear Regression",
    "section": "A Linear Model",
    "text": "A Linear Model\n\n\nNow we want to build a model to represent the data:\nConsider the line: \\[y = α + β x\\]\n\nSeveral possibilities. Which one do we choose to represent the model?\n\n\nWe need some criterium."
  },
  {
    "objectID": "slides/session_3/index.html#least-square-criterium",
    "href": "slides/session_3/index.html#least-square-criterium",
    "title": "Linear Regression",
    "section": "Least Square Criterium",
    "text": "Least Square Criterium\n\n\n\nCompare the model to the data: \\[y_i = \\alpha + \\beta x_i + \\underbrace{e_i}_{\\text{prediction error}}\\]\nSquare Errors \\[{e_i}^2 = (y_i-\\alpha-\\beta x_i)^2\\]\nLoss Function: sum of squares \\[L(\\alpha,\\beta) = \\sum_{i=1}^N (e_i)^2\\]"
  },
  {
    "objectID": "slides/session_3/index.html#minimizing-least-squares",
    "href": "slides/session_3/index.html#minimizing-least-squares",
    "title": "Linear Regression",
    "section": "Minimizing Least Squares",
    "text": "Minimizing Least Squares\n\n\n\n\nTry to chose \\(\\alpha, \\beta\\) so as to minimize the sum of the squares \\(L(α, β)\\)\n\nIt is a convex minimization problem: unique solution\n\nThis direct iterative procedure is used in machine learning"
  },
  {
    "objectID": "slides/session_3/index.html#ordinary-least-squares-1",
    "href": "slides/session_3/index.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares (1)",
    "text": "Ordinary Least Squares (1)\n\nThe mathematical problem \\(\\min_{\\alpha,\\beta} L(\\alpha,\\beta)\\) has one unique solution1\nSolution is given by the explicit formula: \\[\\hat{\\alpha} = \\overline{y} - \\hat{\\beta} \\overline{x}\\] \\[\\hat{\\beta} = \\frac{Cov({x,y})}{Var(x)} = Cor(x,y) \\frac{\\sigma(y)}{\\sigma({x})}\\]\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimators.\n\nHence the hats.\nMore on that later.\n\n\nProof not important here."
  },
  {
    "objectID": "slides/session_3/index.html#concrete-example",
    "href": "slides/session_3/index.html#concrete-example",
    "title": "Linear Regression",
    "section": "Concrete Example",
    "text": "Concrete Example\nIn our example we get the result: \\[\\underbrace{y}_{\\text{income}} = 10 + 0.59 \\underbrace{x}_{education}\\]\nWe can say:\n\nincome and education are positively correlated\na unit increase in education is associated with a 0.59 increase in income\na unit increase in education explains a 0.59 increase in income\n\n\nBut:\n\nhere explains does not mean cause"
  },
  {
    "objectID": "slides/session_3/index.html#predictions",
    "href": "slides/session_3/index.html#predictions",
    "title": "Linear Regression",
    "section": "Predictions",
    "text": "Predictions\nIt is possible to make predictions with the model:\n\nHow much would an occupation which hires 60% high schoolers fare salary-wise?\n\n\n\n\nPrediction: salary measure is \\(45.4\\)\n\n\n\nOK, but that seems noisy, how much do I really predict ? Can I get a sense of the precision of my prediction ?"
  },
  {
    "objectID": "slides/session_3/index.html#look-at-the-residuals",
    "href": "slides/session_3/index.html#look-at-the-residuals",
    "title": "Linear Regression",
    "section": "Look at the residuals",
    "text": "Look at the residuals\n\n\n\nPlot the residuals: \n\n\n\nAny abnormal observation?\nTheory requires residuals to be:\n\nzero-mean\nnon-correlated\nnormally distributed\n\nThat looks like a normal distribution\n\nstandard deviation is \\(\\sigma(e_i) = 16.84\\)\n\nA more honnest prediction would be \\(45.6 ± 16.84\\)"
  },
  {
    "objectID": "slides/session_3/index.html#what-could-go-wrong",
    "href": "slides/session_3/index.html#what-could-go-wrong",
    "title": "Linear Regression",
    "section": "What could go wrong?",
    "text": "What could go wrong?\n\n\na well specified model, residuals must look like white noise (i.i.d.: independent and identically distributed)\nwhen residuals are clearly abnormal, the model must be changed"
  },
  {
    "objectID": "slides/session_3/index.html#explained-variance-1",
    "href": "slides/session_3/index.html#explained-variance-1",
    "title": "Linear Regression",
    "section": "Explained Variance",
    "text": "Explained Variance\n\n\n\nWhat is the share of the total variance explained by the variance of my prediction? \\[R^2 = \\frac{\\overbrace{Var(\\hat{\\alpha} + \\hat{\\beta} x_i)}^{ \\text{MSS} } } {\\underbrace{Var(y_i)}_{ \\text{TSS} } } = \\frac{MSS}{TSS} = (Cor(x,y))^2\\] \\[R^2 = 1-\\frac{\\overbrace{Var(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i)}^{\\text{RSS}} } { \\underbrace{Var(y_i)}_{ {\\text{TSS}  }}} = 1 - \\frac{RSS}{TSS} \\]\n\n\n\n\n\nMSS: model sum of squares, explained variance\nRSS: residual sum of square, unexplained variance\nTSS: total sum of squares, total variance\n\n\n\n\nCoefficient of determination is a measure of the explanatory power of a regression\n\nbut not of the significance of a coefficient\nwe’ll get back to it when we see multivariate regressions\n\nIn one-dimensional case, it is possible to have small R2, yet a very precise regression coefficient."
  },
  {
    "objectID": "slides/session_3/index.html#graphical-representation",
    "href": "slides/session_3/index.html#graphical-representation",
    "title": "Linear Regression",
    "section": "Graphical Representation",
    "text": "Graphical Representation"
  },
  {
    "objectID": "slides/session_3/index.html#statistical-model",
    "href": "slides/session_3/index.html#statistical-model",
    "title": "Linear Regression",
    "section": "Statistical model",
    "text": "Statistical model\n\n\n\n\nImagine the true model is: \\[y = α + β x + \\epsilon\\] \\[\\epsilon_i  \\sim \\mathcal{N}\\left({0,\\sigma^{2}}\\right)\\]\n\nerrors are independent …\nand normallly distributed …\nwith constant variance (homoscedastic)\n\n\n\nUsing this data-generation process, I have drawn randomly \\(N\\) data points (a.k.a. gathered the data)\n\nmaybe an acual sample (for instance \\(N\\) patients)\nan abstract sample otherwise\n\n\n\nThen computed my estimate \\(\\hat{α}\\), \\(\\hat{β}\\)\nHow confident am I in these estimates ?\n\nI could have gotten a completely different one…\nclearly, the bigger \\(N\\), the more confident I am…"
  },
  {
    "objectID": "slides/session_3/index.html#statistical-inference-2",
    "href": "slides/session_3/index.html#statistical-inference-2",
    "title": "Linear Regression",
    "section": "Statistical inference (2)",
    "text": "Statistical inference (2)\n\n\n\n\n\nAssume we have computed \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\) from the data. Let’s make a thought experiment instead.\nImagine the actual data generating process was given by \\(\\hat{α} + \\hat{\\beta} x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0,Var({e_i}))\\)\n\n\n\n\nIf I draw randomly \\(N\\) points using this D.G.P. I get new estimates.\nAnd if I make randomly many draws, I get a distribution for my estimate.\n\nI get an estimated \\(\\hat{\\sigma}(\\hat{\\beta})\\)\nwere my initial estimates very likely ?\nor could they have taken any value with another draw from the data ?\nin the example, we see that estimates around of 0.7 or 0.9, would be compatible with the data\n\nHow do we formalize these ideas?\n\nStatistical tests."
  },
  {
    "objectID": "slides/session_3/index.html#first-estimates",
    "href": "slides/session_3/index.html#first-estimates",
    "title": "Linear Regression",
    "section": "First estimates",
    "text": "First estimates\n\n\nGiven the true model, all estimators are random variables of the data generating process\nGiven the values \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\) of the true model, we can model the distribution of the estimates.\nSome closed forms:\n\n\\(\\hat{\\sigma}^2 = Var(y_i - \\alpha -\\beta x_i)\\) estimated variance of the residuals\n\\(mean(\\hat{\\beta}) = \\beta\\) (unbiased)\n\\(\\sigma(\\hat{\\beta}) =  \\frac{\\sigma^2}{Var(x_i)}\\)\n\nThese statististics or any function of them can be computed exactly, given the data.\nTheir distribution depends, on the data-generating process\nCan we produce statistics whose distribution is known given mild assumptions on the data-generating process?\n\nif so, we can assess how likely are our observations"
  },
  {
    "objectID": "slides/session_3/index.html#fisher-statistic",
    "href": "slides/session_3/index.html#fisher-statistic",
    "title": "Linear Regression",
    "section": "Fisher-Statistic",
    "text": "Fisher-Statistic\n\n\n\n\nTest\n\nHypothesis H0:\n\n\\(α=β=0\\)\nmodel explains nothing, i.e. \\(R^2=0\\)\n\nHypothesis H1: (model explains something)\n\nmodel explains something, i.e. \\(R^2&gt;0\\)\n\n\nFisher Statistics: \\[\\boxed{F=\\frac{Explained Variance}{Unexplained Variance}}\\]\n\n\nDistribution of \\(F\\) is known theoretically.\n\nAssuming the model is actually linear and the shocks normal.\nIt depends on the number of degrees of Freedom. (Here \\(N-2=18\\))\nNot on the actual parameters of the model.\n\n\n\nIn our case, \\(Fstat=40.48\\).\nWhat was the probability it was that big, under the \\(H0\\) hypothesis?\n\nextremely small: \\(Prob(F&gt;Fstat|H0)=5.41e-6\\)\nwe can reject \\(H0\\) with \\(p-value=5e-6\\)\n\nIn social science, typical required p-value is 5%.\nIn practice, we abstract from the precise calculation of the Fisher statistics, and look only at the p-value."
  },
  {
    "objectID": "slides/session_3/index.html#student-test",
    "href": "slides/session_3/index.html#student-test",
    "title": "Linear Regression",
    "section": "Student test",
    "text": "Student test\n\nSo our estimate is \\(y = \\underbrace{0.121}_{\\tilde{\\alpha}} + \\underbrace{0.794}_{\\tilde{\\beta}} x\\).\n\nwe know \\(\\tilde{\\beta}\\) is a bit random (it’s an estimator)\nare we even sure \\(\\tilde{\\beta}\\) could not have been zero?\n\nStudent Test:\n\nH0: \\(\\beta=0\\)\nH1: \\(\\beta \\neq 0\\)\nStatistics: \\(t=\\frac{\\hat{\\beta}}{\\sigma(\\hat{\\beta})}\\)\n\nintuitively: compare mean of estimator to its standard deviation\nalso a function of degrees of freedom\n\n\nSignificance levels (read in a table or use software):\n\nfor 18 degrees of freedom, \\(P(|t|&gt;t^{\\star})=0.05\\) with \\(t^{\\star}=1.734\\)\nif \\(t&gt;t^{\\star}\\) we are \\(95%\\) confident the coefficient is significant"
  },
  {
    "objectID": "slides/session_3/index.html#student-tables",
    "href": "slides/session_3/index.html#student-tables",
    "title": "Linear Regression",
    "section": "Student tables",
    "text": "Student tables"
  },
  {
    "objectID": "slides/session_3/index.html#confidence-intervals",
    "href": "slides/session_3/index.html#confidence-intervals",
    "title": "Linear Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe student test can also be used to construct confidence intervals.\n\nGiven estimate, \\(\\hat{\\beta}\\) with standard deviation \\(\\sigma(\\hat{\\beta})\\)\nGiven a probability threshold \\(\\alpha\\) (for instance \\(\\alpha=0.05\\)) we can compute \\(t^{\\star}\\) such that \\(P(|t|&gt;t*)=\\alpha\\)\nWe construct the confidence interval:\n\n\\[I^{\\alpha} = [\\hat{\\beta}-t\\sigma(\\hat{\\beta}), \\hat{\\beta}+t\\sigma(\\hat{\\beta})]\\]\n\nInterpretation:\n\nif the true value was outside of the confidence interval, the probability of obtaining the value that we got would be less than 5%.\nwe can say the true value is within the interval with 95% confidence level"
  },
  {
    "objectID": "slides/session_3/index_handout.html",
    "href": "slides/session_3/index_handout.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Duncan’s Occupational Prestige Data - Many occupations in 1950. - Education and prestige associated to each occupation\n\n\\(x\\): education\n\nPercentage of occupational incumbents in 1950 who were high school graduates\n\n\\(y\\): income\n\nPercentage of occupational incumbents in the 1950 US Census who earned $3,500 or more per year\n\n\\(z\\): Percentage of respondents in a social survey who rated the occupation as “good” or better in prestige\n\n\n\n\nImport the data from statsmodels’ dataset:\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\nrownames\n\n\n\n\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\n\n\n\n\nFor any variable \\(v\\) with \\(N\\) observations:\n\nmean: \\(\\overline{v} = \\frac{1}{N} \\sum_{i=1}^N v_i\\)\nvariance \\(V({v}) = \\frac{1}{N} \\sum_{i=1}^N \\left(v_i - \\overline{v} \\right)^2\\)\nstandard deviation : \\(\\sigma(v)=\\sqrt{V(v)}\\)\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we measure relations between two variables (with \\(N\\) observations)\n\nCovariance: \\(Cov(x,y) = \\frac{1}{N}\\sum_i (x_i-\\overline{x})(y_i-\\overline{y})\\)\nCorrelation: \\(Cor(x,y) = \\frac{Cov(x,y)}{\\sigma(x)\\sigma(y)}\\)\n\nBy construction, \\(Cor(x,y)\\in[-1,1]\\)\n\nif \\(Cor(x,y)&gt;0\\), x and y are positively correlated\nif \\(Cor(x,y)&lt;0\\), x and y are negatively correlated\n\nInterpretation:\n\nno interpretation!\ncorrelation is not causality\nalso: data can be correlated by pure chance (spurious correlation)\n\n\n\n\n\n\n\n\n\ndf[['income','education','prestige']].cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\n\n\ndf[['income','education','prestige']].corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\n\n\n\n. . .\nCan we visualize correlations?\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Income\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'],df['prestige'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Prestige\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing matplotlib (3d)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.pairplot(df[['education', 'prestige', 'income']])\n\n\n\n\n\n\n\n\nThe pairplot made with seaborn gives a simple sense of correlations as well as information about the distribution of each variable.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#a-simple-dataset",
    "href": "slides/session_3/index_handout.html#a-simple-dataset",
    "title": "Linear Regression",
    "section": "",
    "text": "Duncan’s Occupational Prestige Data - Many occupations in 1950. - Education and prestige associated to each occupation\n\n\\(x\\): education\n\nPercentage of occupational incumbents in 1950 who were high school graduates\n\n\\(y\\): income\n\nPercentage of occupational incumbents in the 1950 US Census who earned $3,500 or more per year\n\n\\(z\\): Percentage of respondents in a social survey who rated the occupation as “good” or better in prestige",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#quick-look",
    "href": "slides/session_3/index_handout.html#quick-look",
    "title": "Linear Regression",
    "section": "",
    "text": "Import the data from statsmodels’ dataset:\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\nrownames\n\n\n\n\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#descriptive-statistics-1",
    "href": "slides/session_3/index_handout.html#descriptive-statistics-1",
    "title": "Linear Regression",
    "section": "",
    "text": "For any variable \\(v\\) with \\(N\\) observations:\n\nmean: \\(\\overline{v} = \\frac{1}{N} \\sum_{i=1}^N v_i\\)\nvariance \\(V({v}) = \\frac{1}{N} \\sum_{i=1}^N \\left(v_i - \\overline{v} \\right)^2\\)\nstandard deviation : \\(\\sigma(v)=\\sqrt{V(v)}\\)\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#relation-between-variables",
    "href": "slides/session_3/index_handout.html#relation-between-variables",
    "title": "Linear Regression",
    "section": "",
    "text": "How do we measure relations between two variables (with \\(N\\) observations)\n\nCovariance: \\(Cov(x,y) = \\frac{1}{N}\\sum_i (x_i-\\overline{x})(y_i-\\overline{y})\\)\nCorrelation: \\(Cor(x,y) = \\frac{Cov(x,y)}{\\sigma(x)\\sigma(y)}\\)\n\nBy construction, \\(Cor(x,y)\\in[-1,1]\\)\n\nif \\(Cor(x,y)&gt;0\\), x and y are positively correlated\nif \\(Cor(x,y)&lt;0\\), x and y are negatively correlated\n\nInterpretation:\n\nno interpretation!\ncorrelation is not causality\nalso: data can be correlated by pure chance (spurious correlation)",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#examples",
    "href": "slides/session_3/index_handout.html#examples",
    "title": "Linear Regression",
    "section": "",
    "text": "df[['income','education','prestige']].cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\n\n\ndf[['income','education','prestige']].corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\n\n\n\n. . .\nCan we visualize correlations?",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#quick",
    "href": "slides/session_3/index_handout.html#quick",
    "title": "Linear Regression",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Income\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'],df['prestige'],'o')\nplt.grid()\nplt.xlabel(\"Education\")\nplt.ylabel(\"Prestige\")\nplt.savefig(\"data_description.png\")",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#quick-look-1",
    "href": "slides/session_3/index_handout.html#quick-look-1",
    "title": "Linear Regression",
    "section": "",
    "text": "Using matplotlib (3d)",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#quick-look-2",
    "href": "slides/session_3/index_handout.html#quick-look-2",
    "title": "Linear Regression",
    "section": "",
    "text": "import seaborn as sns\nsns.pairplot(df[['education', 'prestige', 'income']])\n\n\n\n\n\n\n\n\nThe pairplot made with seaborn gives a simple sense of correlations as well as information about the distribution of each variable.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#a-linear-model",
    "href": "slides/session_3/index_handout.html#a-linear-model",
    "title": "Linear Regression",
    "section": "A Linear Model",
    "text": "A Linear Model\n\n\nNow we want to build a model to represent the data:\nConsider the line: \\[y = α + β x\\]\n\nSeveral possibilities. Which one do we choose to represent the model?\n\n\nWe need some criterium.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#least-square-criterium",
    "href": "slides/session_3/index_handout.html#least-square-criterium",
    "title": "Linear Regression",
    "section": "Least Square Criterium",
    "text": "Least Square Criterium\n\n\n\nCompare the model to the data: \\[y_i = \\alpha + \\beta x_i + \\underbrace{e_i}_{\\text{prediction error}}\\]\nSquare Errors \\[{e_i}^2 = (y_i-\\alpha-\\beta x_i)^2\\]\nLoss Function: sum of squares \\[L(\\alpha,\\beta) = \\sum_{i=1}^N (e_i)^2\\]",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#minimizing-least-squares",
    "href": "slides/session_3/index_handout.html#minimizing-least-squares",
    "title": "Linear Regression",
    "section": "Minimizing Least Squares",
    "text": "Minimizing Least Squares\n\n\n\n\nTry to chose \\(\\alpha, \\beta\\) so as to minimize the sum of the squares \\(L(α, β)\\)\n\nIt is a convex minimization problem: unique solution\n\nThis direct iterative procedure is used in machine learning",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#ordinary-least-squares-1",
    "href": "slides/session_3/index_handout.html#ordinary-least-squares-1",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares (1)",
    "text": "Ordinary Least Squares (1)\n\nThe mathematical problem \\(\\min_{\\alpha,\\beta} L(\\alpha,\\beta)\\) has one unique solution1\nSolution is given by the explicit formula: \\[\\hat{\\alpha} = \\overline{y} - \\hat{\\beta} \\overline{x}\\] \\[\\hat{\\beta} = \\frac{Cov({x,y})}{Var(x)} = Cor(x,y) \\frac{\\sigma(y)}{\\sigma({x})}\\]\n\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimators.\n\nHence the hats.\nMore on that later.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#concrete-example",
    "href": "slides/session_3/index_handout.html#concrete-example",
    "title": "Linear Regression",
    "section": "Concrete Example",
    "text": "Concrete Example\nIn our example we get the result: \\[\\underbrace{y}_{\\text{income}} = 10 + 0.59 \\underbrace{x}_{education}\\]\nWe can say:\n\nincome and education are positively correlated\na unit increase in education is associated with a 0.59 increase in income\na unit increase in education explains a 0.59 increase in income\n\n. . .\nBut:\n\nhere explains does not mean cause",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#predictions",
    "href": "slides/session_3/index_handout.html#predictions",
    "title": "Linear Regression",
    "section": "Predictions",
    "text": "Predictions\nIt is possible to make predictions with the model:\n\nHow much would an occupation which hires 60% high schoolers fare salary-wise?\n\n\n. . .\n\nPrediction: salary measure is \\(45.4\\)\n\n. . .\nOK, but that seems noisy, how much do I really predict ? Can I get a sense of the precision of my prediction ?",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#look-at-the-residuals",
    "href": "slides/session_3/index_handout.html#look-at-the-residuals",
    "title": "Linear Regression",
    "section": "Look at the residuals",
    "text": "Look at the residuals\n\n\n\nPlot the residuals: \n\n\n\nAny abnormal observation?\nTheory requires residuals to be:\n\nzero-mean\nnon-correlated\nnormally distributed\n\nThat looks like a normal distribution\n\nstandard deviation is \\(\\sigma(e_i) = 16.84\\)\n\nA more honnest prediction would be \\(45.6 ± 16.84\\)",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#what-could-go-wrong",
    "href": "slides/session_3/index_handout.html#what-could-go-wrong",
    "title": "Linear Regression",
    "section": "What could go wrong?",
    "text": "What could go wrong?\n\n\na well specified model, residuals must look like white noise (i.i.d.: independent and identically distributed)\nwhen residuals are clearly abnormal, the model must be changed",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#explained-variance-1",
    "href": "slides/session_3/index_handout.html#explained-variance-1",
    "title": "Linear Regression",
    "section": "Explained Variance",
    "text": "Explained Variance\n\n\n\nWhat is the share of the total variance explained by the variance of my prediction? \\[R^2 = \\frac{\\overbrace{Var(\\hat{\\alpha} + \\hat{\\beta} x_i)}^{ \\text{MSS} } } {\\underbrace{Var(y_i)}_{ \\text{TSS} } } = \\frac{MSS}{TSS} = (Cor(x,y))^2\\] \\[R^2 = 1-\\frac{\\overbrace{Var(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i)}^{\\text{RSS}} } { \\underbrace{Var(y_i)}_{ {\\text{TSS}  }}} = 1 - \\frac{RSS}{TSS} \\]\n\n\n\n\n\nMSS: model sum of squares, explained variance\nRSS: residual sum of square, unexplained variance\nTSS: total sum of squares, total variance\n\n\n\n\nCoefficient of determination is a measure of the explanatory power of a regression\n\nbut not of the significance of a coefficient\nwe’ll get back to it when we see multivariate regressions\n\nIn one-dimensional case, it is possible to have small R2, yet a very precise regression coefficient.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#graphical-representation",
    "href": "slides/session_3/index_handout.html#graphical-representation",
    "title": "Linear Regression",
    "section": "Graphical Representation",
    "text": "Graphical Representation",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#statistical-model",
    "href": "slides/session_3/index_handout.html#statistical-model",
    "title": "Linear Regression",
    "section": "Statistical model",
    "text": "Statistical model\n\n\n\n\nImagine the true model is: \\[y = α + β x + \\epsilon\\] \\[\\epsilon_i  \\sim \\mathcal{N}\\left({0,\\sigma^{2}}\\right)\\]\n\nerrors are independent …\nand normallly distributed …\nwith constant variance (homoscedastic)\n\n\n\nUsing this data-generation process, I have drawn randomly \\(N\\) data points (a.k.a. gathered the data)\n\nmaybe an acual sample (for instance \\(N\\) patients)\nan abstract sample otherwise\n\n\n\nThen computed my estimate \\(\\hat{α}\\), \\(\\hat{β}\\)\nHow confident am I in these estimates ?\n\nI could have gotten a completely different one…\nclearly, the bigger \\(N\\), the more confident I am…",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#statistical-inference-2",
    "href": "slides/session_3/index_handout.html#statistical-inference-2",
    "title": "Linear Regression",
    "section": "Statistical inference (2)",
    "text": "Statistical inference (2)\n\n\n\n\n\nAssume we have computed \\(\\hat{\\alpha}\\), \\(\\hat{\\beta}\\) from the data. Let’s make a thought experiment instead.\nImagine the actual data generating process was given by \\(\\hat{α} + \\hat{\\beta} x + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0,Var({e_i}))\\)\n\n\n\n\nIf I draw randomly \\(N\\) points using this D.G.P. I get new estimates.\nAnd if I make randomly many draws, I get a distribution for my estimate.\n\nI get an estimated \\(\\hat{\\sigma}(\\hat{\\beta})\\)\nwere my initial estimates very likely ?\nor could they have taken any value with another draw from the data ?\nin the example, we see that estimates around of 0.7 or 0.9, would be compatible with the data\n\nHow do we formalize these ideas?\n\nStatistical tests.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#first-estimates",
    "href": "slides/session_3/index_handout.html#first-estimates",
    "title": "Linear Regression",
    "section": "First estimates",
    "text": "First estimates\n\n\nGiven the true model, all estimators are random variables of the data generating process\nGiven the values \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\) of the true model, we can model the distribution of the estimates.\nSome closed forms:\n\n\\(\\hat{\\sigma}^2 = Var(y_i - \\alpha -\\beta x_i)\\) estimated variance of the residuals\n\\(mean(\\hat{\\beta}) = \\beta\\) (unbiased)\n\\(\\sigma(\\hat{\\beta}) =  \\frac{\\sigma^2}{Var(x_i)}\\)\n\nThese statististics or any function of them can be computed exactly, given the data.\nTheir distribution depends, on the data-generating process\nCan we produce statistics whose distribution is known given mild assumptions on the data-generating process?\n\nif so, we can assess how likely are our observations",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#fisher-statistic",
    "href": "slides/session_3/index_handout.html#fisher-statistic",
    "title": "Linear Regression",
    "section": "Fisher-Statistic",
    "text": "Fisher-Statistic\n\n\n\n\nTest\n\nHypothesis H0:\n\n\\(α=β=0\\)\nmodel explains nothing, i.e. \\(R^2=0\\)\n\nHypothesis H1: (model explains something)\n\nmodel explains something, i.e. \\(R^2&gt;0\\)\n\n\nFisher Statistics: \\[\\boxed{F=\\frac{Explained Variance}{Unexplained Variance}}\\]\n\n\nDistribution of \\(F\\) is known theoretically.\n\nAssuming the model is actually linear and the shocks normal.\nIt depends on the number of degrees of Freedom. (Here \\(N-2=18\\))\nNot on the actual parameters of the model.\n\n\n\nIn our case, \\(Fstat=40.48\\).\nWhat was the probability it was that big, under the \\(H0\\) hypothesis?\n\nextremely small: \\(Prob(F&gt;Fstat|H0)=5.41e-6\\)\nwe can reject \\(H0\\) with \\(p-value=5e-6\\)\n\nIn social science, typical required p-value is 5%.\nIn practice, we abstract from the precise calculation of the Fisher statistics, and look only at the p-value.",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#student-test",
    "href": "slides/session_3/index_handout.html#student-test",
    "title": "Linear Regression",
    "section": "Student test",
    "text": "Student test\n\nSo our estimate is \\(y = \\underbrace{0.121}_{\\tilde{\\alpha}} + \\underbrace{0.794}_{\\tilde{\\beta}} x\\).\n\nwe know \\(\\tilde{\\beta}\\) is a bit random (it’s an estimator)\nare we even sure \\(\\tilde{\\beta}\\) could not have been zero?\n\nStudent Test:\n\nH0: \\(\\beta=0\\)\nH1: \\(\\beta \\neq 0\\)\nStatistics: \\(t=\\frac{\\hat{\\beta}}{\\sigma(\\hat{\\beta})}\\)\n\nintuitively: compare mean of estimator to its standard deviation\nalso a function of degrees of freedom\n\n\nSignificance levels (read in a table or use software):\n\nfor 18 degrees of freedom, \\(P(|t|&gt;t^{\\star})=0.05\\) with \\(t^{\\star}=1.734\\)\nif \\(t&gt;t^{\\star}\\) we are \\(95%\\) confident the coefficient is significant",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#student-tables",
    "href": "slides/session_3/index_handout.html#student-tables",
    "title": "Linear Regression",
    "section": "Student tables",
    "text": "Student tables",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#confidence-intervals",
    "href": "slides/session_3/index_handout.html#confidence-intervals",
    "title": "Linear Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe student test can also be used to construct confidence intervals.\n\nGiven estimate, \\(\\hat{\\beta}\\) with standard deviation \\(\\sigma(\\hat{\\beta})\\)\nGiven a probability threshold \\(\\alpha\\) (for instance \\(\\alpha=0.05\\)) we can compute \\(t^{\\star}\\) such that \\(P(|t|&gt;t*)=\\alpha\\)\nWe construct the confidence interval:\n\n\\[I^{\\alpha} = [\\hat{\\beta}-t\\sigma(\\hat{\\beta}), \\hat{\\beta}+t\\sigma(\\hat{\\beta})]\\]\n. . .\nInterpretation:\n\nif the true value was outside of the confidence interval, the probability of obtaining the value that we got would be less than 5%.\nwe can say the true value is within the interval with 95% confidence level",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/index_handout.html#footnotes",
    "href": "slides/session_3/index_handout.html#footnotes",
    "title": "Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof not important here.↩︎",
    "crumbs": [
      "lectures",
      "Linear Regression"
    ]
  },
  {
    "objectID": "slides/session_3/graphs/inference.html",
    "href": "slides/session_3/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_8/index.html#how-to-deal-with-text",
    "href": "slides/session_8/index.html#how-to-deal-with-text",
    "title": "Text Analysis",
    "section": "How to deal with text?",
    "text": "How to deal with text?\n\nRecall: big data contains heterogenous data\n\ntext / images / sound"
  },
  {
    "objectID": "slides/session_8/index.html#example-1-fomc-meetings",
    "href": "slides/session_8/index.html#example-1-fomc-meetings",
    "title": "Text Analysis",
    "section": "Example 1: FOMC meetings",
    "text": "Example 1: FOMC meetings\nTaking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow\n\n\nWe are determined to ensure that inflation returns to our two per cent medium-term target in a timely manner. Based on our current assessment, we consider that the key ECB interest rates are at levels that, maintained for a sufficiently long duration, will make a substantial contribution to this goal. Our future decisions will ensure that our policy rates will be set at sufficiently restrictive levels for as long as necessary.\nMar 2024 Press conference from Christine Lagarde"
  },
  {
    "objectID": "slides/session_8/index.html#example-2",
    "href": "slides/session_8/index.html#example-2",
    "title": "Text Analysis",
    "section": "Example 2",
    "text": "Example 2\n\n\n\nSuppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis"
  },
  {
    "objectID": "slides/session_8/index.html#text-mining-what-can-we-extract-from-texts",
    "href": "slides/session_8/index.html#text-mining-what-can-we-extract-from-texts",
    "title": "Text Analysis",
    "section": "Text-mining: what can we extract from texts",
    "text": "Text-mining: what can we extract from texts\n\n\nThe main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-4…"
  },
  {
    "objectID": "slides/session_8/index.html#clarification",
    "href": "slides/session_8/index.html#clarification",
    "title": "Text Analysis",
    "section": "Clarification",
    "text": "Clarification\n\n\n\nText analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-4) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)"
  },
  {
    "objectID": "slides/session_8/index.html#the-even-less-glamorous-part",
    "href": "slides/session_8/index.html#the-even-less-glamorous-part",
    "title": "Text Analysis",
    "section": "The even-less glamorous part",
    "text": "The even-less glamorous part\nBefore getting started with text analysis, one needs to get hold of the text in the first place\n\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\nhow to store it\n\nwhat kind of database? (sql, mongodb, …)\nimportant problem when database is big\n\n\n\n\nshow github copilot"
  },
  {
    "objectID": "slides/session_8/index.html#processing-steps",
    "href": "slides/session_8/index.html#processing-steps",
    "title": "Text Analysis",
    "section": "Processing steps",
    "text": "Processing steps\n\nLet’s briefly see how text gets processed.\nGoal is to transform the text into a numerical vector of features\n\nStupid approach: “abc”-&gt;[1,2,3]\nwe need to capture some form of language structure\n\nAll the steps can be done fairly easily with nltk …\n\nnltk is comparable to sklearn in terms of widespread adoption\n\n… or with sklearn"
  },
  {
    "objectID": "slides/session_8/index.html#processing-steps-2",
    "href": "slides/session_8/index.html#processing-steps-2",
    "title": "Text Analysis",
    "section": "Processing steps (2)",
    "text": "Processing steps (2)\n\nSteps:\n\ntokenization\nstopwords\nlexicon normalization\n\nstemming\nlemmatization\n\nPOS tagging"
  },
  {
    "objectID": "slides/session_8/index.html#tokenization",
    "href": "slides/session_8/index.html#tokenization",
    "title": "Text Analysis",
    "section": "Tokenization",
    "text": "Tokenization\n\n\n\nTokenization: split input into atomic elements.\n\nWe can recognize sentences.\n\nOr words.\n\nIt is enough for some basic analysis:\n\n\nfrom nltk.probability import FreqDist\nfdist = FreqDist(words)\nprint(fdist.most_common(2))\n[('It', 1), (\"'s\", 1)]\n\n\n\n\nfrom nltk.tokenize import sent_tokenize\ntxt = \"\"\"Animal Farm is a short novel by George Orwell. It was\nwritten during World War II and published in 1945. It is about \na group of farm animals who rebel against their farmer. They \nhope to create a place where the animals can be equal, free,\n and happy.\"\"\"\nsentences  = sent_tokenize(txt)\nprint(sentences)\n\n\n['Animal Farm is a short novel by George Orwell.',\n 'It was\\nwritten during World War II and published in 1945.', \n 'It is about \\na group of farm animals who rebel against their farmer.', \n 'They \\nhope to create a place where the animals can be equal, free,\\n and happy.']\n\n\nfrom nltk.tokenize import word_tokenize\ntxt = \"It's a beautiful thing, the destruction of words.\"\nwords  = word_tokenize(txt)\nprint(words)\n['It', \"'s\", 'a', 'beautiful', 'thing', ',', 'the', 'destruction', 'of', 'words', '.']"
  },
  {
    "objectID": "slides/session_8/index.html#part-of-speech-tagging",
    "href": "slides/session_8/index.html#part-of-speech-tagging",
    "title": "Text Analysis",
    "section": "Part-of speech tagging",
    "text": "Part-of speech tagging\n\n\n\nSometimes we need information about the kind of tokens that we have\n\nWe can perform part-of-speech tagging (aka grammatical tagging)\n\nThis is useful to refine interpretation of some words\n\n“it’s not a beautiful thing”\nvs “it’s a beautiful thing”\nconnotation of beautiful changes\n\n\n\n\nfrom nltk.tokenize import word_tokenize\ntagged = nltk.pos_tag(words)\ntagged\n[('It', 'PRP'),\n (\"'s\", 'VBZ'),\n ('a', 'DT'),\n ('beautiful', 'JJ'),\n ('thing', 'NN'),\n (',', ','),\n ('the', 'DT'),\n ('destruction', 'NN'),\n ('of', 'IN'),\n ('words', 'NNS'),\n ('.', '.')]"
  },
  {
    "objectID": "slides/session_8/index.html#simplifying-the-text-1-stopwords",
    "href": "slides/session_8/index.html#simplifying-the-text-1-stopwords",
    "title": "Text Analysis",
    "section": "Simplifying the text (1): stopwords",
    "text": "Simplifying the text (1): stopwords\n\n\n\nSome words are very frequent and carry no useful meaning\n\n\nThey are called stopwords\n\n\nWe typically remove them from our word list\n\n\n\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)\n{'their', 'then', 'not', 'ma', 'here', ...}\n\n\n\nfiltered_words = [w for w in words if w not in stop_words]\nfiltered_words\n['beautiful', 'thing' 'destruction', 'words']"
  },
  {
    "objectID": "slides/session_8/index.html#simplifying-the-text-2-lexicon-normalization",
    "href": "slides/session_8/index.html#simplifying-the-text-2-lexicon-normalization",
    "title": "Text Analysis",
    "section": "Simplifying the text (2): lexicon normalization",
    "text": "Simplifying the text (2): lexicon normalization\n\n\n\nSometimes, there are several variants of a given word\n\ntight, tightening, tighten\n\n\nStemming: keeping the word root\n\nLemmatization: keeps the word base\n\nlinguistically correct contrary to stemming\n\n\n\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords =  [\"tight\", \"tightening\", \"tighten\"]\nstemmed_words=[ps.stem(w) for w in words]\n['tight', 'tighten', 'tighten']\n\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords =  [\"flying\", \"flyers\", \"fly\"]\nstemmed_words=[ps.stem(w) for w in words]\nlemmatized_words=[lem.lemmatize(w) for w in words]\n# lemmatized\n['flying', 'flyer', 'fly']\n# stemmed\n['fli', 'flyer', 'fli']"
  },
  {
    "objectID": "slides/session_8/index.html#sentiment-analysis-1",
    "href": "slides/session_8/index.html#sentiment-analysis-1",
    "title": "Text Analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nWhat do we do now that we have reduced a text to a series of word occurrences?\nTwo main approaches:\n\nlexical analysis\nmachine learning"
  },
  {
    "objectID": "slides/session_8/index.html#lexical-analysis",
    "href": "slides/session_8/index.html#lexical-analysis",
    "title": "Text Analysis",
    "section": "Lexical analysis",
    "text": "Lexical analysis\n\n\nUse a “sentiment dictionary” to provide a value (positive or negative) for each word\n\nsum the weights to get positive or negative sentiment\n\nExample: \\[\\underbrace{\\text{Sadly}}_{-}\\text{, there wasn't a glimpse of }\\underbrace{\\text{light}}_{+} \\text{ in his } \\text{world } \\text{ of intense }\\underbrace{\\text{suffering.}}_{-}\\]\nTotal:\n\n-1+1-1. Sentiment is negative.\n\n\n\n\nProblems:?\n\n\ndoesn’t capture irony\nhere, taking grammar into account would change everything\nour dictionary doesn’t have weights for what matters to us\n\n\n\\[ \\text{the central bank forecasts increased }\\underbrace{\\text{inflation}}_{?}\\]"
  },
  {
    "objectID": "slides/session_8/index.html#machine-learning",
    "href": "slides/session_8/index.html#machine-learning",
    "title": "Text Analysis",
    "section": "Machine learning",
    "text": "Machine learning\n\nIdea: we would like the weights to be endogenously determined \\[ \\underbrace{\\text{the}}_{x_1} \\underbrace{\\text{ central}}_{x_2} \\underbrace{\\text{ bank}}_{x_3} \\underbrace{\\text{ forecasts}}_{x_4} \\underbrace{\\text{ increased} }_{x_5} \\underbrace{\\text{ inflation}}_{x_6}\\]\nSuppose we had several texts: we can generate features by counting words in each of them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe\ncentral\nbank\nforecasts\nincreased\ninflation\neconomy\nexchange rate\ncrisis\nsentiment\n\n\n\n\ntext1\n1\n1\n2\n1\n1\n2\n\n\n\n-1\n\n\ntext2\n3\n\n\n\n\n1\n1\n2\n\n+1\n\n\ntext3\n4\n\n1\n\n\n1\n\n1\n1\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can the train the model: \\(y = x_1 w_1 + \\cdots x_K w_K\\) where \\(y\\) is the sentiment and \\(w_i\\) is wordcount of word \\(w_i\\)\n\nof course, we need a similar procedure as before (split the training set and evaluation set, …)\nwe can use any model (like naive bayesian updating)\n\nThis approach is called Bag of Words (BOW)"
  },
  {
    "objectID": "slides/session_8/index.html#some-issues",
    "href": "slides/session_8/index.html#some-issues",
    "title": "Text Analysis",
    "section": "Some issues",
    "text": "Some issues\nBag of words approach with raw word count has a few issues:\n\n\nit requires a big training set with labels\nit overweights long documents\nthere is noise due to the very frequent words that don’t affect sentiment\n\n\n\nImprovement: TF-IDF (Term-Frequency*Inverse-Distribution-Frequency)\n\nreplace word frequency \\(w\\) by \\[\\text{tf-idf} = wd\\frac{\\text{number of documents}}{\\text{number of documents containing $w$}}\\]\nreduces noise due to frequent words"
  },
  {
    "objectID": "slides/session_8/index.html#conclusion",
    "href": "slides/session_8/index.html#conclusion",
    "title": "Text Analysis",
    "section": "Conclusion",
    "text": "Conclusion\n    Recent trends for text analysis:\n\n\ndeep learning\n\nvery flexible model\nreplace tokenization by abstract embedding\n\ngenerative AI\n\ncan perform many text analysis text without pretraining!\n\n\n\n\nNext week:\nIntroduction to Large Language Models for Finance, by Emilie Rannou, partner at Ekimetrics\nBring your laptops!"
  },
  {
    "objectID": "slides/session_8/index_handout.html",
    "href": "slides/session_8/index_handout.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Recall: big data contains heterogenous data\n\ntext / images / sound\n\n\n\n\n\n\nTaking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow\n\n\nWe are determined to ensure that inflation returns to our two per cent medium-term target in a timely manner. Based on our current assessment, we consider that the key ECB interest rates are at levels that, maintained for a sufficiently long duration, will make a substantial contribution to this goal. Our future decisions will ensure that our policy rates will be set at sufficiently restrictive levels for as long as necessary.\nMar 2024 Press conference from Christine Lagarde\n\n\n\n\n\n\n\n\nSuppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis\n\n\n\n\n\n\n\nThe main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-4…\n\n\n\n\n\n\n\n\n\nText analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-4) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)\n\n\n\n\n\n\nBefore getting started with text analysis, one needs to get hold of the text in the first place\n\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\nhow to store it\n\nwhat kind of database? (sql, mongodb, …)\nimportant problem when database is big\n\n\n\n\nshow github copilot",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#how-to-deal-with-text",
    "href": "slides/session_8/index_handout.html#how-to-deal-with-text",
    "title": "Text Analysis",
    "section": "",
    "text": "Recall: big data contains heterogenous data\n\ntext / images / sound",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#example-1-fomc-meetings",
    "href": "slides/session_8/index_handout.html#example-1-fomc-meetings",
    "title": "Text Analysis",
    "section": "",
    "text": "Taking the Fed at its Word: A New Approach to Estimating Central Bank Objectives using Text Analysis by Adam H. Shapiro and Daniel J. Wilson link\n\nRemember the Taylor rule? We tried to estimate it from the data.\nGeneralized version: \\(i_t = \\alpha_\\pi (\\pi_t-\\pi^{\\star}) + \\alpha_y (y_t-y)\\)\nIs there a way to measure the preferences of the central bank? (coefficients and inflation target?)\nShapiro and Wilson: let’s look at the FOMC meeting transcripts\nExcerpts (there are tons of them: 704,499)\n\n\n\n\nI had several conversations at Jackson Hole with Wall Street economists and journalists, and they said, quite frankly, that they really do not believe that our effective inflation target is 1 to 2 percent. They believe we have morphed into 1+1/2 to 2+1/2 percent, and no one thought that we were really going to do anything over time to bring it down to 1 to 2.\n\nSep 2006 St. Louis Federal Reserve President William Poole\n\n\n\nLike most of you, I am not at all alarmist about inflation. I think the worst that is likely to happen would be 20 or 30 basis points over the next year. But even that amount is a little disconcerting for me. I think it is very important for us to maintain our credibility on inflation and it would be somewhat expensive to bring that additional inflation back down.\n\nMarch 2006 Chairman Ben Bernanke\n\n\n\nWith inflation remaining at such rates, we could begin to lose credibility if markets mistakenly inferred that our comfort zone had drifted higher. When we stop raising rates, we ought to be reasonably confident that policy is restrictive enough to bring inflation back toward the center of our comfort zone, which I believe is 1+1/2 percent…So for today, we should move forward with an increase of 25 basis points…\n\nJan 2006 Chicago Federal Reserve President Michael Moskow\n\n\nWe are determined to ensure that inflation returns to our two per cent medium-term target in a timely manner. Based on our current assessment, we consider that the key ECB interest rates are at levels that, maintained for a sufficiently long duration, will make a substantial contribution to this goal. Our future decisions will ensure that our policy rates will be set at sufficiently restrictive levels for as long as necessary.\nMar 2024 Press conference from Christine Lagarde",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#example-2",
    "href": "slides/session_8/index_handout.html#example-2",
    "title": "Text Analysis",
    "section": "",
    "text": "Suppose you work in the trading floor of a financial instutition\nThese kind of tweets have disturbing impact on the markets. You need to react quickly.\nYou need a machine to assess the risk in real time.\nMore generally, tweeter is a quite unique source of real-time data\nHow do you analyse the content of the tweets?\nComment: actually it’s not only the content of the tweets, but who reads, who retweets: graph analysis",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#text-mining-what-can-we-extract-from-texts",
    "href": "slides/session_8/index_handout.html#text-mining-what-can-we-extract-from-texts",
    "title": "Text Analysis",
    "section": "",
    "text": "The main branches of text analysis are:\n\nsentiment analysis\n\nassociate positivity/negativity to a text\nprecise meaning of “sentiment” is context dependent\n\n\ntopic modeling\n\nclassify texts as belonging to known categories (supervised)\nfinding likely texts (unsupervised)\n\nnamed-entity recognition\n\nfind who gets mentioned in the text\nexample: A Cross-verified Database of Notable People, 3500BC-2018AD\n\nevent-extraction\n\nrecognize mention of events\n\nplus everything that can be done with a language model like GPT-4…",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#clarification",
    "href": "slides/session_8/index_handout.html#clarification",
    "title": "Text Analysis",
    "section": "",
    "text": "Text analysis / text mining are somewhat used interchangeably\nIn general they consist in quantifying information used in a text…\n… so that it can be incorporated in machine learning analysis\nRecently, deep learning (and GPT-4) has changed this state of facts:\n\nsome models get trained direcly on text (intermediary phases are not explicited)",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#the-even-less-glamorous-part",
    "href": "slides/session_8/index_handout.html#the-even-less-glamorous-part",
    "title": "Text Analysis",
    "section": "",
    "text": "Before getting started with text analysis, one needs to get hold of the text in the first place\n\n\nhow to extract\n\nwebscraping: automate a bot to visit website and download text\ndocument extraction: for instance extract the text from pdf docs, get rid of everything irrelevant\n\nhow to store it\n\nwhat kind of database? (sql, mongodb, …)\nimportant problem when database is big\n\n\n\n\nshow github copilot",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#processing-steps",
    "href": "slides/session_8/index_handout.html#processing-steps",
    "title": "Text Analysis",
    "section": "Processing steps",
    "text": "Processing steps\n\nLet’s briefly see how text gets processed.\nGoal is to transform the text into a numerical vector of features\n\nStupid approach: “abc”-&gt;[1,2,3]\nwe need to capture some form of language structure\n\nAll the steps can be done fairly easily with nltk …\n\nnltk is comparable to sklearn in terms of widespread adoption\n\n… or with sklearn",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#processing-steps-2",
    "href": "slides/session_8/index_handout.html#processing-steps-2",
    "title": "Text Analysis",
    "section": "Processing steps (2)",
    "text": "Processing steps (2)\n\nSteps:\n\ntokenization\nstopwords\nlexicon normalization\n\nstemming\nlemmatization\n\nPOS tagging",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#tokenization",
    "href": "slides/session_8/index_handout.html#tokenization",
    "title": "Text Analysis",
    "section": "Tokenization",
    "text": "Tokenization\n\n\n\nTokenization: split input into atomic elements.\n\nWe can recognize sentences.\n\nOr words.\n\nIt is enough for some basic analysis:\n\n\nfrom nltk.probability import FreqDist\nfdist = FreqDist(words)\nprint(fdist.most_common(2))\n[('It', 1), (\"'s\", 1)]\n\n\n\n\nfrom nltk.tokenize import sent_tokenize\ntxt = \"\"\"Animal Farm is a short novel by George Orwell. It was\nwritten during World War II and published in 1945. It is about \na group of farm animals who rebel against their farmer. They \nhope to create a place where the animals can be equal, free,\n and happy.\"\"\"\nsentences  = sent_tokenize(txt)\nprint(sentences)\n\n\n['Animal Farm is a short novel by George Orwell.',\n 'It was\\nwritten during World War II and published in 1945.', \n 'It is about \\na group of farm animals who rebel against their farmer.', \n 'They \\nhope to create a place where the animals can be equal, free,\\n and happy.']\n\n\nfrom nltk.tokenize import word_tokenize\ntxt = \"It's a beautiful thing, the destruction of words.\"\nwords  = word_tokenize(txt)\nprint(words)\n['It', \"'s\", 'a', 'beautiful', 'thing', ',', 'the', 'destruction', 'of', 'words', '.']",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#part-of-speech-tagging",
    "href": "slides/session_8/index_handout.html#part-of-speech-tagging",
    "title": "Text Analysis",
    "section": "Part-of speech tagging",
    "text": "Part-of speech tagging\n\n\n\nSometimes we need information about the kind of tokens that we have\n\nWe can perform part-of-speech tagging (aka grammatical tagging)\n\nThis is useful to refine interpretation of some words\n\n“it’s not a beautiful thing”\nvs “it’s a beautiful thing”\nconnotation of beautiful changes\n\n\n\n\nfrom nltk.tokenize import word_tokenize\ntagged = nltk.pos_tag(words)\ntagged\n[('It', 'PRP'),\n (\"'s\", 'VBZ'),\n ('a', 'DT'),\n ('beautiful', 'JJ'),\n ('thing', 'NN'),\n (',', ','),\n ('the', 'DT'),\n ('destruction', 'NN'),\n ('of', 'IN'),\n ('words', 'NNS'),\n ('.', '.')]",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#simplifying-the-text-1-stopwords",
    "href": "slides/session_8/index_handout.html#simplifying-the-text-1-stopwords",
    "title": "Text Analysis",
    "section": "Simplifying the text (1): stopwords",
    "text": "Simplifying the text (1): stopwords\n\n\n\nSome words are very frequent and carry no useful meaning\n\n\nThey are called stopwords\n\n\nWe typically remove them from our word list\n\n\n\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)\n{'their', 'then', 'not', 'ma', 'here', ...}\n\n\n\nfiltered_words = [w for w in words if w not in stop_words]\nfiltered_words\n['beautiful', 'thing' 'destruction', 'words']",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#simplifying-the-text-2-lexicon-normalization",
    "href": "slides/session_8/index_handout.html#simplifying-the-text-2-lexicon-normalization",
    "title": "Text Analysis",
    "section": "Simplifying the text (2): lexicon normalization",
    "text": "Simplifying the text (2): lexicon normalization\n\n\n\nSometimes, there are several variants of a given word\n\ntight, tightening, tighten\n\n\nStemming: keeping the word root\n\nLemmatization: keeps the word base\n\nlinguistically correct contrary to stemming\n\n\n\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords =  [\"tight\", \"tightening\", \"tighten\"]\nstemmed_words=[ps.stem(w) for w in words]\n['tight', 'tighten', 'tighten']\n\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords =  [\"flying\", \"flyers\", \"fly\"]\nstemmed_words=[ps.stem(w) for w in words]\nlemmatized_words=[lem.lemmatize(w) for w in words]\n# lemmatized\n['flying', 'flyer', 'fly']\n# stemmed\n['fli', 'flyer', 'fli']",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#sentiment-analysis-1",
    "href": "slides/session_8/index_handout.html#sentiment-analysis-1",
    "title": "Text Analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nWhat do we do now that we have reduced a text to a series of word occurrences?\nTwo main approaches:\n\nlexical analysis\nmachine learning",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#lexical-analysis",
    "href": "slides/session_8/index_handout.html#lexical-analysis",
    "title": "Text Analysis",
    "section": "Lexical analysis",
    "text": "Lexical analysis\n\n\nUse a “sentiment dictionary” to provide a value (positive or negative) for each word\n\nsum the weights to get positive or negative sentiment\n\nExample: \\[\\underbrace{\\text{Sadly}}_{-}\\text{, there wasn't a glimpse of }\\underbrace{\\text{light}}_{+} \\text{ in his } \\text{world } \\text{ of intense }\\underbrace{\\text{suffering.}}_{-}\\]\nTotal:\n\n-1+1-1. Sentiment is negative.\n\n\n\n. . .\nProblems:?\n\n\ndoesn’t capture irony\nhere, taking grammar into account would change everything\nour dictionary doesn’t have weights for what matters to us\n\n\n\\[ \\text{the central bank forecasts increased }\\underbrace{\\text{inflation}}_{?}\\]",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#machine-learning",
    "href": "slides/session_8/index_handout.html#machine-learning",
    "title": "Text Analysis",
    "section": "Machine learning",
    "text": "Machine learning\n\nIdea: we would like the weights to be endogenously determined \\[ \\underbrace{\\text{the}}_{x_1} \\underbrace{\\text{ central}}_{x_2} \\underbrace{\\text{ bank}}_{x_3} \\underbrace{\\text{ forecasts}}_{x_4} \\underbrace{\\text{ increased} }_{x_5} \\underbrace{\\text{ inflation}}_{x_6}\\]\nSuppose we had several texts: we can generate features by counting words in each of them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe\ncentral\nbank\nforecasts\nincreased\ninflation\neconomy\nexchange rate\ncrisis\nsentiment\n\n\n\n\ntext1\n1\n1\n2\n1\n1\n2\n\n\n\n-1\n\n\ntext2\n3\n\n\n\n\n1\n1\n2\n\n+1\n\n\ntext3\n4\n\n1\n\n\n1\n\n1\n1\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can the train the model: \\(y = x_1 w_1 + \\cdots x_K w_K\\) where \\(y\\) is the sentiment and \\(w_i\\) is wordcount of word \\(w_i\\)\n\nof course, we need a similar procedure as before (split the training set and evaluation set, …)\nwe can use any model (like naive bayesian updating)\n\nThis approach is called Bag of Words (BOW)",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#some-issues",
    "href": "slides/session_8/index_handout.html#some-issues",
    "title": "Text Analysis",
    "section": "Some issues",
    "text": "Some issues\nBag of words approach with raw word count has a few issues:\n\n\nit requires a big training set with labels\nit overweights long documents\nthere is noise due to the very frequent words that don’t affect sentiment\n\n\n\nImprovement: TF-IDF (Term-Frequency*Inverse-Distribution-Frequency)\n\nreplace word frequency \\(w\\) by \\[\\text{tf-idf} = wd\\frac{\\text{number of documents}}{\\text{number of documents containing $w$}}\\]\nreduces noise due to frequent words",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/index_handout.html#conclusion",
    "href": "slides/session_8/index_handout.html#conclusion",
    "title": "Text Analysis",
    "section": "Conclusion",
    "text": "Conclusion\n    Recent trends for text analysis:\n\n\ndeep learning\n\nvery flexible model\nreplace tokenization by abstract embedding\n\ngenerative AI\n\ncan perform many text analysis text without pretraining!\n\n\n\n. . .\nNext week:\nIntroduction to Large Language Models for Finance, by Emilie Rannou, partner at Ekimetrics\nBring your laptops!",
    "crumbs": [
      "lectures",
      "Text Analysis"
    ]
  },
  {
    "objectID": "slides/session_8/graphs/Untitled1.html",
    "href": "slides/session_8/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_4/index.html#remember-dataset-from-last-time",
    "href": "slides/session_4/index.html#remember-dataset-from-last-time",
    "title": "Multiple Regression",
    "section": "Remember dataset from last time",
    "text": "Remember dataset from last time\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?"
  },
  {
    "objectID": "slides/session_4/index.html#prestige-or-education",
    "href": "slides/session_4/index.html#prestige-or-education",
    "title": "Multiple Regression",
    "section": "Prestige or Education",
    "text": "Prestige or Education\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education"
  },
  {
    "objectID": "slides/session_4/index.html#multiple-regression",
    "href": "slides/session_4/index.html#multiple-regression",
    "title": "Multiple Regression",
    "section": "Multiple regression",
    "text": "Multiple regression\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)"
  },
  {
    "objectID": "slides/session_4/index.html#fitting-a-model",
    "href": "slides/session_4/index.html#fitting-a-model",
    "title": "Multiple Regression",
    "section": "Fitting a model",
    "text": "Fitting a model\nNow we are trying to fit a plane to a cloud of points."
  },
  {
    "objectID": "slides/session_4/index.html#minimization-criterium",
    "href": "slides/session_4/index.html#minimization-criterium",
    "title": "Multiple Regression",
    "section": "Minimization Criterium",
    "text": "Minimization Criterium\n\nTake all observations: \\((\\text{income}_n,\\text{education}_n,\\text{prestige}_n)_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}_n + \\beta_2 \\text{prestige}_n - \\text{income}_n }_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula"
  },
  {
    "objectID": "slides/session_4/index.html#ordinary-least-square",
    "href": "slides/session_4/index.html#ordinary-least-square",
    "title": "Multiple Regression",
    "section": "Ordinary Least Square",
    "text": "Ordinary Least Square\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]"
  },
  {
    "objectID": "slides/session_4/index.html#solution",
    "href": "slides/session_4/index.html#solution",
    "title": "Multiple Regression",
    "section": "Solution",
    "text": "Solution\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "slides/session_4/index.html#explained-variance-1",
    "href": "slides/session_4/index.html#explained-variance-1",
    "title": "Multiple Regression",
    "section": "Explained Variance",
    "text": "Explained Variance\nAs in the 1d case we can compare: - the variability of the model predictions (\\(MSS\\)) - the variance of the data (\\(TSS\\), T for total)\nCoefficient of determination (same formula):\n\\[R^2 = \\frac{MSS}{TSS}\\]\nOr:\n\\[R^2 = 1-\\frac{RSS}{SST}\\]\nwhere \\(RSS\\) is the non explained variance"
  },
  {
    "objectID": "slides/session_4/index.html#adjusted-r-squared",
    "href": "slides/session_4/index.html#adjusted-r-squared",
    "title": "Multiple Regression",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]\nWhere:\n\n\\(N\\): number of observations\n\\(p\\) number of variables\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688"
  },
  {
    "objectID": "slides/session_4/index.html#making-a-regression-with-statsmodels",
    "href": "slides/session_4/index.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regression",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "slides/session_4/index.html#performing-a-regression",
    "href": "slides/session_4/index.html#performing-a-regression",
    "title": "Multiple Regression",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n=============================================================================="
  },
  {
    "objectID": "slides/session_4/index.html#formula-mini-language",
    "href": "slides/session_4/index.html#formula-mini-language",
    "title": "Multiple Regression",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)"
  },
  {
    "objectID": "slides/session_4/index.html#formula-mini-language-1",
    "href": "slides/session_4/index.html#formula-mini-language-1",
    "title": "Multiple Regression",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients"
  },
  {
    "objectID": "slides/session_4/index.html#coefficients-interpetation",
    "href": "slides/session_4/index.html#coefficients-interpetation",
    "title": "Multiple Regression",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\n\n\n\nIinterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\n\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "slides/session_4/index.html#hypotheses",
    "href": "slides/session_4/index.html#hypotheses",
    "title": "Multiple Regression",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe need some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)"
  },
  {
    "objectID": "slides/session_4/index.html#is-the-regression-significant",
    "href": "slides/session_4/index.html#is-the-regression-significant",
    "title": "Multiple Regression",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\n\n\n\n\n\n\n\n\nFisher Criterium (F-test)\n\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\n\n\n\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\] (same as 1d)\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n\n\n… the distribution of \\(F\\) is known\n\n\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected"
  },
  {
    "objectID": "slides/session_4/index.html#is-each-coefficient-significant",
    "href": "slides/session_4/index.html#is-each-coefficient-significant",
    "title": "Multiple Regression",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\n\n\n\n\n\n\n\n\nStudent Test\n\n\nGiven a coefficient \\(\\beta_k\\):\n\n\\(H0\\): true coefficient is 0\n\\(H1\\): true coefficient is not zero\n\n\n\n\n\nStatistics (student-t): \\[t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\]\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\n\n\nOr just look at the p-value:\n\nprobability that \\(t\\) would be as high as it is, assuming \\(H0\\)"
  },
  {
    "objectID": "slides/session_4/index.html#confidence-intervals",
    "href": "slides/session_4/index.html#confidence-intervals",
    "title": "Multiple Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSame as in the 1d case.\n\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\n\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)"
  },
  {
    "objectID": "slides/session_4/index.html#other-tests",
    "href": "slides/session_4/index.html#other-tests",
    "title": "Multiple Regression",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "slides/session_4/index.html#variable-selection-1",
    "href": "slides/session_4/index.html#variable-selection-1",
    "title": "Multiple Regression",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)"
  },
  {
    "objectID": "slides/session_4/index.html#not-enough-coefficients",
    "href": "slides/session_4/index.html#not-enough-coefficients",
    "title": "Multiple Regression",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\n\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\n\n\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimated \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\)"
  },
  {
    "objectID": "slides/session_4/index.html#example",
    "href": "slides/session_4/index.html#example",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise"
  },
  {
    "objectID": "slides/session_4/index.html#colinear-regressors",
    "href": "slides/session_4/index.html#colinear-regressors",
    "title": "Multiple Regression",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot"
  },
  {
    "objectID": "slides/session_4/index.html#choosing-regressors",
    "href": "slides/session_4/index.html#choosing-regressors",
    "title": "Multiple Regression",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "slides/session_4/index_handout.html",
    "href": "slides/session_4/index_handout.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}_n,\\text{education}_n,\\text{prestige}_n)_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}_n + \\beta_2 \\text{prestige}_n - \\text{income}_n }_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#remember-dataset-from-last-time",
    "href": "slides/session_4/index_handout.html#remember-dataset-from-last-time",
    "title": "Multiple Regression",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#prestige-or-education",
    "href": "slides/session_4/index_handout.html#prestige-or-education",
    "title": "Multiple Regression",
    "section": "",
    "text": "if the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#multiple-regression",
    "href": "slides/session_4/index_handout.html#multiple-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "What about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#fitting-a-model",
    "href": "slides/session_4/index_handout.html#fitting-a-model",
    "title": "Multiple Regression",
    "section": "",
    "text": "Now we are trying to fit a plane to a cloud of points.",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#minimization-criterium",
    "href": "slides/session_4/index_handout.html#minimization-criterium",
    "title": "Multiple Regression",
    "section": "",
    "text": "Take all observations: \\((\\text{income}_n,\\text{education}_n,\\text{prestige}_n)_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}_n + \\beta_2 \\text{prestige}_n - \\text{income}_n }_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#ordinary-least-square",
    "href": "slides/session_4/index_handout.html#ordinary-least-square",
    "title": "Multiple Regression",
    "section": "",
    "text": "\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#solution",
    "href": "slides/session_4/index_handout.html#solution",
    "title": "Multiple Regression",
    "section": "",
    "text": "Result: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#explained-variance-1",
    "href": "slides/session_4/index_handout.html#explained-variance-1",
    "title": "Multiple Regression",
    "section": "Explained Variance",
    "text": "Explained Variance\nAs in the 1d case we can compare: - the variability of the model predictions (\\(MSS\\)) - the variance of the data (\\(TSS\\), T for total)\nCoefficient of determination (same formula):\n\\[R^2 = \\frac{MSS}{TSS}\\]\nOr:\n\\[R^2 = 1-\\frac{RSS}{SST}\\]\nwhere \\(RSS\\) is the non explained variance",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#adjusted-r-squared",
    "href": "slides/session_4/index_handout.html#adjusted-r-squared",
    "title": "Multiple Regression",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]\nWhere:\n\n\\(N\\): number of observations\n\\(p\\) number of variables\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#making-a-regression-with-statsmodels",
    "href": "slides/session_4/index_handout.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regression",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#performing-a-regression",
    "href": "slides/session_4/index_handout.html#performing-a-regression",
    "title": "Multiple Regression",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n==============================================================================",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#formula-mini-language",
    "href": "slides/session_4/index_handout.html#formula-mini-language",
    "title": "Multiple Regression",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#formula-mini-language-1",
    "href": "slides/session_4/index_handout.html#formula-mini-language-1",
    "title": "Multiple Regression",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#coefficients-interpetation",
    "href": "slides/session_4/index_handout.html#coefficients-interpetation",
    "title": "Multiple Regression",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\n\n\n\nIinterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\n\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#hypotheses",
    "href": "slides/session_4/index_handout.html#hypotheses",
    "title": "Multiple Regression",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe need some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#is-the-regression-significant",
    "href": "slides/session_4/index_handout.html#is-the-regression-significant",
    "title": "Multiple Regression",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\n\n\n\n\n\n\n\nFisher Criterium (F-test)\n\n\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\n\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\] (same as 1d)\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n\n\n… the distribution of \\(F\\) is known\n\n\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#is-each-coefficient-significant",
    "href": "slides/session_4/index_handout.html#is-each-coefficient-significant",
    "title": "Multiple Regression",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\n\n\n\n\n\n\n\nStudent Test\n\n\n\nGiven a coefficient \\(\\beta_k\\):\n\n\\(H0\\): true coefficient is 0\n\\(H1\\): true coefficient is not zero\n\n\n\nStatistics (student-t): \\[t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\]\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\n\n\nOr just look at the p-value:\n\nprobability that \\(t\\) would be as high as it is, assuming \\(H0\\)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#confidence-intervals",
    "href": "slides/session_4/index_handout.html#confidence-intervals",
    "title": "Multiple Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSame as in the 1d case.\n\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\n\n. . .\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#other-tests",
    "href": "slides/session_4/index_handout.html#other-tests",
    "title": "Multiple Regression",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#variable-selection-1",
    "href": "slides/session_4/index_handout.html#variable-selection-1",
    "title": "Multiple Regression",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n. . .\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#not-enough-coefficients",
    "href": "slides/session_4/index_handout.html#not-enough-coefficients",
    "title": "Multiple Regression",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\n. . .\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\n. . .\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimated \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\)",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#example",
    "href": "slides/session_4/index_handout.html#example",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#colinear-regressors",
    "href": "slides/session_4/index_handout.html#colinear-regressors",
    "title": "Multiple Regression",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/index_handout.html#choosing-regressors",
    "href": "slides/session_4/index_handout.html#choosing-regressors",
    "title": "Multiple Regression",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares",
    "crumbs": [
      "lectures",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "slides/session_4/graphs/index.html",
    "href": "slides/session_4/graphs/index.html",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?\n\n\n\n\n\n\n\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance\n\n\n\n\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf\n\n\n\n\nRunning a regression with statsmodels\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\nResult:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n==============================================================================\n\n\n\n\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)\n\n\n\n\n\n\n\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients\n\n\n\n\n\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[ \\text{number_or_crimes} = 0.005\\\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[ \\log(\\text{number_or_crimes}) = 0.005\\\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\n\n\n\n\n\n\n\n\nApproach is very similar to the one-dimensional case\n\nFisher criterium (F-test):\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\n\\(H1\\): some coefficients are not 0\n\nstatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder the model assumptions, distribution of \\(F\\) is known\n\nit is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypotheses 0\nif very low, H0 is rejected\n\n\n\n\n\n\n\n\nStudent test. Given a an coefficient \\(\\beta_k\\):\n\n\n\\(H0\\): coefficient is 0\n\n\\(H1\\): coefficient is not zero\n\nstatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\nProcedure:\n\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\).\n\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\nOr compute implied acceptance rate \\(\\alpha\\) for \\(t\\).\n\nif \\(t\\) is high enough, null hypothesis is rejected\n\n\n\n\n\n\n\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\n\n\n\n\n\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course\n\n\n\n\n\n\n\n\n\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)\n\n\n\n\n\n\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\]\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\) (“control” for \\(x_2\\))\n\n\n\n\n\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise\n\n\n\n\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest: correlation plot, correlation statistics\n\n\n\n\n\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares\n\n\n\n\n\n\n\n\n\nIntro to causality"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#the-problem",
    "href": "slides/session_4/graphs/index.html#the-problem",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#explained-variance",
    "href": "slides/session_4/graphs/index.html#explained-variance",
    "title": "Multiple Regressions",
    "section": "",
    "text": "As in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance\n\n\n\n\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#interpretation-and-variable-change",
    "href": "slides/session_4/graphs/index.html#interpretation-and-variable-change",
    "title": "Multiple Regressions",
    "section": "",
    "text": "import statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf\n\n\n\n\nRunning a regression with statsmodels\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\nResult:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n==============================================================================\n\n\n\n\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)\n\n\n\n\n\n\n\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients\n\n\n\n\n\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[ \\text{number_or_crimes} = 0.005\\\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[ \\log(\\text{number_or_crimes}) = 0.005\\\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#statistical-inference",
    "href": "slides/session_4/graphs/index.html#statistical-inference",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Recall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\n\n\n\n\n\n\n\n\nApproach is very similar to the one-dimensional case\n\nFisher criterium (F-test):\n\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\n\\(H1\\): some coefficients are not 0\n\nstatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\nUnder the model assumptions, distribution of \\(F\\) is known\n\nit is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypotheses 0\nif very low, H0 is rejected\n\n\n\n\n\n\n\n\nStudent test. Given a an coefficient \\(\\beta_k\\):\n\n\n\\(H0\\): coefficient is 0\n\n\\(H1\\): coefficient is not zero\n\nstatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\n\n\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\n\nProcedure:\n\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\).\n\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\n\nOr compute implied acceptance rate \\(\\alpha\\) for \\(t\\).\n\nif \\(t\\) is high enough, null hypothesis is rejected\n\n\n\n\n\n\n\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\n\n\n\n\n\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#variable-selection",
    "href": "slides/session_4/graphs/index.html#variable-selection",
    "title": "Multiple Regressions",
    "section": "",
    "text": "I’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)\n\n\n\n\n\n\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\]\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\) (“control” for \\(x_2\\))\n\n\n\n\n\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise\n\n\n\n\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest: correlation plot, correlation statistics\n\n\n\n\n\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "slides/session_4/graphs/index.html#coming-next",
    "href": "slides/session_4/graphs/index.html#coming-next",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Intro to causality"
  },
  {
    "objectID": "slides/session_4/graphs/Untitled1.html",
    "href": "slides/session_4/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_4/Regressions_correction_2022.html",
    "href": "slides/session_4/Regressions_correction_2022.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\n\n# skipped. requires the formula from the course\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\n# skipped. requires the formula from the course\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}  + \\beta_2  \\times \\text{prestige}\\). Comment regression statistics.\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\n\nImport dataset from data.dta. Explore dataset (statistics, plots)\n\nimport pandas\n\n\ndf = pandas.read_stata('data.dta')\ndf.head()\n\n\n\n\n\n\n\n\nindex\nx\ny\nz\n\n\n\n\n0\n0\n1.504053\n0.543556\n1.917895\n\n\n1\n1\n43.619758\n0.543113\n4.058487\n\n\n2\n2\n1.226398\n0.736955\n1.785403\n\n\n3\n3\n89.103260\n0.996219\n6.321152\n\n\n4\n4\n32.117073\n0.140142\n3.445228\n\n\n\n\n\n\n\nOur goal is to explain z by x and y. Run a regression.\n\nfrom statsmodels.formula import api as smf\n\n\nmodel = smf.ols('z ~ x + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nz\nR-squared:\n0.800\n\n\nModel:\nOLS\nAdj. R-squared:\n0.791\n\n\nMethod:\nLeast Squares\nF-statistic:\n93.90\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n3.82e-17\n\n\nTime:\n10:41:10\nLog-Likelihood:\n-57.244\n\n\nNo. Observations:\n50\nAIC:\n120.5\n\n\nDf Residuals:\n47\nBIC:\n126.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.2177\n0.243\n5.019\n0.000\n0.730\n1.706\n\n\nx\n0.0356\n0.003\n12.235\n0.000\n0.030\n0.041\n\n\ny\n1.9128\n0.369\n5.177\n0.000\n1.169\n2.656\n\n\n\n\n\n\nOmnibus:\n3.205\nDurbin-Watson:\n1.859\n\n\nProb(Omnibus):\n0.201\nJarque-Bera (JB):\n2.349\n\n\nSkew:\n0.277\nProb(JB):\n0.309\n\n\nKurtosis:\n3.906\nCond. No.\n187.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nComments: regression looks significant. \\(R^2\\) looks good. Fisher statistics, is conclusive (the hypothesis H0 that all coefficients are zero is rejected at a 3.82e-17 confidence level.. The student statistics are also quite high. For each coefficient, the hypothesis H0 that coefficient is zero is rejected at a 0.001 confidence level.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(res.resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.subplot(131)\nplt.plot(df['x'], df['y'],'o')\nplt.subplot(132)\nplt.plot(df['y'], df['z'],'o')\nplt.subplot(133)\nplt.plot(df['x'], df['z'],'o')\nplt.xlabel(\"x\")\nplt.ylabel(\"z\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nplt.plot( np.log(df['z']), np.log(df['x']), 'o' )\n\n\n\n\n\n\n\n\nApparently, there is a linear relationship between ‘log(x)’ and log(y)\n\nfrom numpy import log\n\n\nmodel = smf.ols('log(z) ~ log(x) + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlog(z)\nR-squared:\n0.957\n\n\nModel:\nOLS\nAdj. R-squared:\n0.955\n\n\nMethod:\nLeast Squares\nF-statistic:\n525.6\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n6.89e-33\n\n\nTime:\n10:41:56\nLog-Likelihood:\n44.223\n\n\nNo. Observations:\n50\nAIC:\n-82.45\n\n\nDf Residuals:\n47\nBIC:\n-76.71\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0128\n0.039\n0.326\n0.746\n-0.066\n0.092\n\n\nlog(x)\n0.2957\n0.010\n29.878\n0.000\n0.276\n0.316\n\n\ny\n0.6164\n0.048\n12.735\n0.000\n0.519\n0.714\n\n\n\n\n\n\nOmnibus:\n4.351\nDurbin-Watson:\n2.490\n\n\nProb(Omnibus):\n0.114\nJarque-Bera (JB):\n1.936\n\n\nSkew:\n0.089\nProb(JB):\n0.380\n\n\nKurtosis:\n2.052\nCond. No.\n12.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nplt.plot(res.resid, 'o')\n\n\n\n\n\n\n\n\n\n\n\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\n\nimport statsmodels\n\n## google: stats models macrodata\n## google: statsmodels datasets  -&gt; example in the tutorial\n\n# https://www.statsmodels.org/0.6.1/datasets/index.html\n# example about how to use lengley database\n\n\nimport statsmodels.api as sm\n\n\nsm.datasets.macrodata\n\n&lt;module 'statsmodels.datasets.macrodata' from '/home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages/statsmodels/datasets/macrodata/__init__.py'&gt;\n\n\n\nds = sm.datasets.macrodata.load_pandas()\n\n\ndf = ds.raw_data\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nCreate a database with all variables of interest including detrended gdp\n\ngdp = df['realgdp']\ninflation = df['infl']\nrealint = df['realint']\n\n\nddf = df # \n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nWe use the fisher relation: \\(r_t = i_t - \\pi_t\\)\n\nddf['ir'] = ddf['realint'] + ddf['infl']\n\nto detrend the gdp, we use hp-filter function from scipy google: hpfilter scipy\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\n\n\ncycle, trend = hpfilter(ddf['realgdp'])\n\n\nddf['gdp'] = cycle/trend*100 # nominal interest rate and inflation are in percent\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\nRun the basic regression\n\nfrom statsmodels.formula import api as sm\n\n\nmodel = sm.ols(\"ir ~ infl + gdp\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared:\n0.389\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.65\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n4.06e-22\n\n\nTime:\n11:54:15\nLog-Likelihood:\n-448.17\n\n\nNo. Observations:\n203\nAIC:\n902.3\n\n\nDf Residuals:\n200\nBIC:\n912.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2035\n0.252\n12.696\n0.000\n2.706\n3.701\n\n\ninfl\n0.5288\n0.050\n10.557\n0.000\n0.430\n0.628\n\n\ngdp\n0.0795\n0.105\n0.759\n0.449\n-0.127\n0.286\n\n\n\n\n\n\nOmnibus:\n30.222\nDurbin-Watson:\n0.417\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n50.662\n\n\nSkew:\n0.796\nProb(JB):\n9.98e-12\n\n\nKurtosis:\n4.858\nCond. No.\n8.56\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\n\nmodel = sm.ols(\"ir ~ infl + gdp + pop + unemp -1\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared (uncentered):\n0.884\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n380.2\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n5.64e-92\n\n\nTime:\n11:58:05\nLog-Likelihood:\n-432.84\n\n\nNo. Observations:\n203\nAIC:\n873.7\n\n\nDf Residuals:\n199\nBIC:\n886.9\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ninfl\n0.4380\n0.049\n8.895\n0.000\n0.341\n0.535\n\n\ngdp\n0.5710\n0.120\n4.739\n0.000\n0.333\n0.809\n\n\npop\n-0.0050\n0.002\n-2.068\n0.040\n-0.010\n-0.000\n\n\nunemp\n0.8064\n0.108\n7.458\n0.000\n0.593\n1.020\n\n\n\n\n\n\nOmnibus:\n5.307\nDurbin-Watson:\n0.391\n\n\nProb(Omnibus):\n0.070\nJarque-Bera (JB):\n7.501\n\n\nSkew:\n0.070\nProb(JB):\n0.0235\n\n\nKurtosis:\n3.931\nCond. No.\n247.\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAt confidence level 2.5% gdp is between 0.333 and 0.809.\nAt confidence level 2.5% infl is between 0.341 and 0.535.\nThe coefficients would be significantly different from 0.5 if 0.5 was not in the condifence interval."
  },
  {
    "objectID": "slides/session_4/Regressions_correction_2022.html#linear-regressions",
    "href": "slides/session_4/Regressions_correction_2022.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\n\n# skipped. requires the formula from the course\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\n# skipped. requires the formula from the course\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}  + \\beta_2  \\times \\text{prestige}\\). Comment regression statistics.\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "slides/session_4/Regressions_correction_2022.html#finding-the-right-model",
    "href": "slides/session_4/Regressions_correction_2022.html#finding-the-right-model",
    "title": "Regressions",
    "section": "",
    "text": "Import dataset from data.dta. Explore dataset (statistics, plots)\n\nimport pandas\n\n\ndf = pandas.read_stata('data.dta')\ndf.head()\n\n\n\n\n\n\n\n\nindex\nx\ny\nz\n\n\n\n\n0\n0\n1.504053\n0.543556\n1.917895\n\n\n1\n1\n43.619758\n0.543113\n4.058487\n\n\n2\n2\n1.226398\n0.736955\n1.785403\n\n\n3\n3\n89.103260\n0.996219\n6.321152\n\n\n4\n4\n32.117073\n0.140142\n3.445228\n\n\n\n\n\n\n\nOur goal is to explain z by x and y. Run a regression.\n\nfrom statsmodels.formula import api as smf\n\n\nmodel = smf.ols('z ~ x + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nz\nR-squared:\n0.800\n\n\nModel:\nOLS\nAdj. R-squared:\n0.791\n\n\nMethod:\nLeast Squares\nF-statistic:\n93.90\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n3.82e-17\n\n\nTime:\n10:41:10\nLog-Likelihood:\n-57.244\n\n\nNo. Observations:\n50\nAIC:\n120.5\n\n\nDf Residuals:\n47\nBIC:\n126.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.2177\n0.243\n5.019\n0.000\n0.730\n1.706\n\n\nx\n0.0356\n0.003\n12.235\n0.000\n0.030\n0.041\n\n\ny\n1.9128\n0.369\n5.177\n0.000\n1.169\n2.656\n\n\n\n\n\n\nOmnibus:\n3.205\nDurbin-Watson:\n1.859\n\n\nProb(Omnibus):\n0.201\nJarque-Bera (JB):\n2.349\n\n\nSkew:\n0.277\nProb(JB):\n0.309\n\n\nKurtosis:\n3.906\nCond. No.\n187.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nComments: regression looks significant. \\(R^2\\) looks good. Fisher statistics, is conclusive (the hypothesis H0 that all coefficients are zero is rejected at a 3.82e-17 confidence level.. The student statistics are also quite high. For each coefficient, the hypothesis H0 that coefficient is zero is rejected at a 0.001 confidence level.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(res.resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.subplot(131)\nplt.plot(df['x'], df['y'],'o')\nplt.subplot(132)\nplt.plot(df['y'], df['z'],'o')\nplt.subplot(133)\nplt.plot(df['x'], df['z'],'o')\nplt.xlabel(\"x\")\nplt.ylabel(\"z\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nplt.plot( np.log(df['z']), np.log(df['x']), 'o' )\n\n\n\n\n\n\n\n\nApparently, there is a linear relationship between ‘log(x)’ and log(y)\n\nfrom numpy import log\n\n\nmodel = smf.ols('log(z) ~ log(x) + y', data=df)\nres = model.fit()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlog(z)\nR-squared:\n0.957\n\n\nModel:\nOLS\nAdj. R-squared:\n0.955\n\n\nMethod:\nLeast Squares\nF-statistic:\n525.6\n\n\nDate:\nTue, 23 Feb 2021\nProb (F-statistic):\n6.89e-33\n\n\nTime:\n10:41:56\nLog-Likelihood:\n44.223\n\n\nNo. Observations:\n50\nAIC:\n-82.45\n\n\nDf Residuals:\n47\nBIC:\n-76.71\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0128\n0.039\n0.326\n0.746\n-0.066\n0.092\n\n\nlog(x)\n0.2957\n0.010\n29.878\n0.000\n0.276\n0.316\n\n\ny\n0.6164\n0.048\n12.735\n0.000\n0.519\n0.714\n\n\n\n\n\n\nOmnibus:\n4.351\nDurbin-Watson:\n2.490\n\n\nProb(Omnibus):\n0.114\nJarque-Bera (JB):\n1.936\n\n\nSkew:\n0.089\nProb(JB):\n0.380\n\n\nKurtosis:\n2.052\nCond. No.\n12.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nplt.plot(res.resid, 'o')"
  },
  {
    "objectID": "slides/session_4/Regressions_correction_2022.html#taylor-rule",
    "href": "slides/session_4/Regressions_correction_2022.html#taylor-rule",
    "title": "Regressions",
    "section": "",
    "text": "In 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\n\nimport statsmodels\n\n## google: stats models macrodata\n## google: statsmodels datasets  -&gt; example in the tutorial\n\n# https://www.statsmodels.org/0.6.1/datasets/index.html\n# example about how to use lengley database\n\n\nimport statsmodels.api as sm\n\n\nsm.datasets.macrodata\n\n&lt;module 'statsmodels.datasets.macrodata' from '/home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages/statsmodels/datasets/macrodata/__init__.py'&gt;\n\n\n\nds = sm.datasets.macrodata.load_pandas()\n\n\ndf = ds.raw_data\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nCreate a database with all variables of interest including detrended gdp\n\ngdp = df['realgdp']\ninflation = df['infl']\nrealint = df['realint']\n\n\nddf = df # \n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\nWe use the fisher relation: \\(r_t = i_t - \\pi_t\\)\n\nddf['ir'] = ddf['realint'] + ddf['infl']\n\nto detrend the gdp, we use hp-filter function from scipy google: hpfilter scipy\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\n\n\ncycle, trend = hpfilter(ddf['realgdp'])\n\n\nddf['gdp'] = cycle/trend*100 # nominal interest rate and inflation are in percent\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\nRun the basic regression\n\nfrom statsmodels.formula import api as sm\n\n\nmodel = sm.ols(\"ir ~ infl + gdp\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared:\n0.389\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.65\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n4.06e-22\n\n\nTime:\n11:54:15\nLog-Likelihood:\n-448.17\n\n\nNo. Observations:\n203\nAIC:\n902.3\n\n\nDf Residuals:\n200\nBIC:\n912.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2035\n0.252\n12.696\n0.000\n2.706\n3.701\n\n\ninfl\n0.5288\n0.050\n10.557\n0.000\n0.430\n0.628\n\n\ngdp\n0.0795\n0.105\n0.759\n0.449\n-0.127\n0.286\n\n\n\n\n\n\nOmnibus:\n30.222\nDurbin-Watson:\n0.417\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n50.662\n\n\nSkew:\n0.796\nProb(JB):\n9.98e-12\n\n\nKurtosis:\n4.858\nCond. No.\n8.56\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\nir\ngdp\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n0.00\n1.479383\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n3.08\n2.967657\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n3.83\n1.792534\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n4.33\n1.110571\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n3.50\n2.331547\n\n\n\n\n\n\n\n\nmodel = sm.ols(\"ir ~ infl + gdp + pop + unemp -1\", data=ddf) # no intercept\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nir\nR-squared (uncentered):\n0.884\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n380.2\n\n\nDate:\nTue, 02 Mar 2021\nProb (F-statistic):\n5.64e-92\n\n\nTime:\n11:58:05\nLog-Likelihood:\n-432.84\n\n\nNo. Observations:\n203\nAIC:\n873.7\n\n\nDf Residuals:\n199\nBIC:\n886.9\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ninfl\n0.4380\n0.049\n8.895\n0.000\n0.341\n0.535\n\n\ngdp\n0.5710\n0.120\n4.739\n0.000\n0.333\n0.809\n\n\npop\n-0.0050\n0.002\n-2.068\n0.040\n-0.010\n-0.000\n\n\nunemp\n0.8064\n0.108\n7.458\n0.000\n0.593\n1.020\n\n\n\n\n\n\nOmnibus:\n5.307\nDurbin-Watson:\n0.391\n\n\nProb(Omnibus):\n0.070\nJarque-Bera (JB):\n7.501\n\n\nSkew:\n0.070\nProb(JB):\n0.0235\n\n\nKurtosis:\n3.931\nCond. No.\n247.\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAt confidence level 2.5% gdp is between 0.333 and 0.809.\nAt confidence level 2.5% infl is between 0.341 and 0.535.\nThe coefficients would be significantly different from 0.5 if 0.5 was not in the condifence interval."
  },
  {
    "objectID": "slides/session_5/index.html#data",
    "href": "slides/session_5/index.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "Data",
    "text": "Data\nOur multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\nLike: \\[x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\]"
  },
  {
    "objectID": "slides/session_5/index.html#data-1",
    "href": "slides/session_5/index.html#data-1",
    "title": "Introduction to Instrumental Variables",
    "section": "Data",
    "text": "Data\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False\\}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))"
  },
  {
    "objectID": "slides/session_5/index.html#binary-variable",
    "href": "slides/session_5/index.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "Binary variable",
    "text": "Binary variable\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)"
  },
  {
    "objectID": "slides/session_5/index.html#categorical-variable",
    "href": "slides/session_5/index.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "Categorical variable",
    "text": "Categorical variable\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{banish cars}} \\]\nwhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Do you suport the banishment of petrol cars?.\n\n\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…"
  },
  {
    "objectID": "slides/session_5/index.html#hierarchical-index",
    "href": "slides/session_5/index.html#hierarchical-index",
    "title": "Introduction to Instrumental Variables",
    "section": "Hierarchical index",
    "text": "Hierarchical index\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/session_5/index.html#hierarchical-index-1",
    "href": "slides/session_5/index.html#hierarchical-index-1",
    "title": "Introduction to Instrumental Variables",
    "section": "Hierarchical index",
    "text": "Hierarchical index\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO"
  },
  {
    "objectID": "slides/session_5/index.html#hierarchical-index-2",
    "href": "slides/session_5/index.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "Hierarchical index (2)",
    "text": "Hierarchical index (2)\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group"
  },
  {
    "objectID": "slides/session_5/index.html#nonnumerical-variables",
    "href": "slides/session_5/index.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "Nonnumerical variables",
    "text": "Nonnumerical variables\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1"
  },
  {
    "objectID": "slides/session_5/index.html#hands-on",
    "href": "slides/session_5/index.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "Hands-on",
    "text": "Hands-on\nUse statsmodels/linearmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "slides/session_5/index.html#what-is-causality",
    "href": "slides/session_5/index.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it."
  },
  {
    "objectID": "slides/session_5/index.html#spurious-correlation",
    "href": "slides/session_5/index.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\nSpurious Correlation\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…"
  },
  {
    "objectID": "slides/session_5/index.html#definitions",
    "href": "slides/session_5/index.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\n\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]"
  },
  {
    "objectID": "slides/session_5/index.html#how-do-we-define-causality-1",
    "href": "slides/session_5/index.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]"
  },
  {
    "objectID": "slides/session_5/index.html#how-do-we-define-causality-2",
    "href": "slides/session_5/index.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…"
  },
  {
    "objectID": "slides/session_5/index.html#causality-in-statistics",
    "href": "slides/session_5/index.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if\n\n\\(A\\) and \\(B\\) are correlated\n\\(A\\) is known before \\(B\\)\ncorrelation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course"
  },
  {
    "objectID": "slides/session_5/index.html#factual-and-counterfactual",
    "href": "slides/session_5/index.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!"
  },
  {
    "objectID": "slides/session_5/index.html#scientific-experiment",
    "href": "slides/session_5/index.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n\n\nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual"
  },
  {
    "objectID": "slides/session_5/index.html#measuring-effect-of-treatment",
    "href": "slides/session_5/index.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?"
  },
  {
    "objectID": "slides/session_5/index.html#an-exmple-from-behavioural-economics",
    "href": "slides/session_5/index.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab (ENS Lyon)\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?"
  },
  {
    "objectID": "slides/session_5/index.html#formalisation-of-the-problem",
    "href": "slides/session_5/index.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables"
  },
  {
    "objectID": "slides/session_5/index.html#randomized-control-trial",
    "href": "slides/session_5/index.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent."
  },
  {
    "objectID": "slides/session_5/index.html#natural-experiment",
    "href": "slides/session_5/index.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n\nResult: yes, they get 1.5% less votes by right-wing voters\n\n\nWhat was the natural experiment?"
  },
  {
    "objectID": "slides/session_5/index.html#example",
    "href": "slides/session_5/index.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (unobserved selection bias)?\n\nProblem (for the economist):\n\nwe didn’t send people to war randomly\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n\nCan we use the Draft to generate randomness ?"
  },
  {
    "objectID": "slides/session_5/index.html#problem",
    "href": "slides/session_5/index.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be unobserved confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)"
  },
  {
    "objectID": "slides/session_5/index.html#reformulate-the-problem",
    "href": "slides/session_5/index.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)"
  },
  {
    "objectID": "slides/session_5/index.html#two-stage-regression",
    "href": "slides/session_5/index.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\)."
  },
  {
    "objectID": "slides/session_5/index.html#choosing-a-good-instrument",
    "href": "slides/session_5/index.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x.\nIn particular, it should be uncorrelated from any potential confounding factor (whether observed or unobserved)."
  },
  {
    "objectID": "slides/session_5/index.html#in-practice",
    "href": "slides/session_5/index.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)"
  },
  {
    "objectID": "slides/session_5/index_handout.html",
    "href": "slides/session_5/index_handout.html",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\nLike: \\[x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\]\n\n\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False\\}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))\n\n\n\n\n\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)\n\n\n\n\n\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{banish cars}} \\]\nwhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Do you suport the banishment of petrol cars?.\n\n\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…\n\n\n\n\n\n\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO\n\n\n\n\n\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group\n\n\n\n\n\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\n\n\n\n\nUse statsmodels/linearmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#data",
    "href": "slides/session_5/index_handout.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\nLike: \\[x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\]",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#data-1",
    "href": "slides/session_5/index_handout.html#data-1",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "How do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False\\}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#binary-variable",
    "href": "slides/session_5/index_handout.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Nothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#categorical-variable",
    "href": "slides/session_5/index_handout.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Look at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{banish cars}} \\]\nwhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Do you suport the banishment of petrol cars?.\n\n\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#hierarchical-index",
    "href": "slides/session_5/index_handout.html#hierarchical-index",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "We use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#hierarchical-index-1",
    "href": "slides/session_5/index_handout.html#hierarchical-index-1",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Values are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#hierarchical-index-2",
    "href": "slides/session_5/index_handout.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#nonnumerical-variables",
    "href": "slides/session_5/index_handout.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "What about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#hands-on",
    "href": "slides/session_5/index_handout.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Use statsmodels/linearmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#what-is-causality",
    "href": "slides/session_5/index_handout.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n. . .\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it.",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#spurious-correlation",
    "href": "slides/session_5/index_handout.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\n\n\nSpurious Correlation\n\n\n\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#definitions",
    "href": "slides/session_5/index_handout.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\n. . .\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#how-do-we-define-causality-1",
    "href": "slides/session_5/index_handout.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#how-do-we-define-causality-2",
    "href": "slides/session_5/index_handout.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n. . .\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#causality-in-statistics",
    "href": "slides/session_5/index_handout.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if\n\n\\(A\\) and \\(B\\) are correlated\n\\(A\\) is known before \\(B\\)\ncorrelation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#factual-and-counterfactual",
    "href": "slides/session_5/index_handout.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n. . .\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#scientific-experiment",
    "href": "slides/session_5/index_handout.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n\n\nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#measuring-effect-of-treatment",
    "href": "slides/session_5/index_handout.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n. . .\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#an-exmple-from-behavioural-economics",
    "href": "slides/session_5/index_handout.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab (ENS Lyon)\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#formalisation-of-the-problem",
    "href": "slides/session_5/index_handout.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#randomized-control-trial",
    "href": "slides/session_5/index_handout.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent.",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#natural-experiment",
    "href": "slides/session_5/index_handout.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n. . .\nResult: yes, they get 1.5% less votes by right-wing voters\n. . .\nWhat was the natural experiment?",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#example",
    "href": "slides/session_5/index_handout.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (unobserved selection bias)?\n\nProblem (for the economist):\n\nwe didn’t send people to war randomly\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n. . .\nCan we use the Draft to generate randomness ?",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#problem",
    "href": "slides/session_5/index_handout.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be unobserved confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#reformulate-the-problem",
    "href": "slides/session_5/index_handout.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#two-stage-regression",
    "href": "slides/session_5/index_handout.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\).",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#choosing-a-good-instrument",
    "href": "slides/session_5/index_handout.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x.\nIn particular, it should be uncorrelated from any potential confounding factor (whether observed or unobserved).",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/session_5/index_handout.html#in-practice",
    "href": "slides/session_5/index_handout.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)",
    "crumbs": [
      "lectures",
      "Introduction to Instrumental Variables"
    ]
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_elements.html",
    "href": "slides/pession_6/machine_learning_regressions_elements.html",
    "title": "Intro to sklearn",
    "section": "",
    "text": "Objectives:\n\ntrain a model with sklearn\nperform a validation test\n\n\n\nImport the diabetes dataset from sklearn. Describe it.\n\nfrom sklearn.datasets import load_diabetes\ndata = load_diabetes()\nX = data['data']\nY = data['target']\n\n\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, T-Cells (a type of white blood cells)\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, thyroid stimulating hormone\n      - s5      ltg, lamotrigine\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n\n\nX_train.shape\n\n(309, 10)\n\n\n\nX_test.shape\n\n(133, 10)\n\n\n\n133/(133+309)\n\n0.3009049773755656\n\n\nFeatures are already “centered and scaled”: no need to renormalize them\nTrain a linear model (with intercept) on the training set\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n# by default there is an intercept (check the doc: default value for fit_intercept is True)\n\n\nLinearRegression?\n\n\nInit signature:\nLinearRegression(\n    *,\n    fit_intercept=True,\n    normalize=False,\n    copy_X=True,\n    n_jobs=None,\n    positive=False,\n)\nDocstring:     \nOrdinary least squares Linear Regression.\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup for n_targets &gt; 1 and sufficient large problems.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`\n    for more details.\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n    .. versionadded:: 0.24\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; reg = LinearRegression().fit(X, y)\n&gt;&gt;&gt; reg.score(X, y)\n1.0\n&gt;&gt;&gt; reg.coef_\narray([1., 2.])\n&gt;&gt;&gt; reg.intercept_\n3.0000...\n&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))\narray([16.])\nFile:           ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:           ABCMeta\nSubclasses:     \n\n\n\n\n\nmodel.fit(X_train, Y_train)\n\nLinearRegression()\n\n\n\nmodel.fit?\n\n\nSignature: model.fit(X, y, sample_weight=None)\nDocstring:\nFit linear model.\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\nReturns\n-------\nself : returns an instance of self.\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:      method\n\n\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\npred = model.predict(X_test)\n\n\nY_test\n\narray([190., 225., 141., 281., 168.,  42., 116., 276., 281.,  51., 121.,\n       156., 163., 142., 187., 173.,  39., 229., 155., 332., 257., 261.,\n        42., 201., 103.,  47., 142., 172.,  71.,  96., 173., 127., 150.,\n       230., 185., 209.,  37.,  92., 235., 131.,  97.,  40., 279.,  97.,\n       245., 258., 102., 168.,  51., 248.,  88.,  91.,  58., 110., 308.,\n        88.,  60., 311., 246., 310., 214., 200., 220., 131.,  72.,  72.,\n       181.,  89., 163., 104.,  96.,  70., 217.,  55., 317., 259.,  50.,\n       118., 200.,  25., 124., 129., 179., 109.,  71., 102., 252., 189.,\n       190., 263., 174., 259., 111.,  85., 145.,  85., 252., 258., 274.,\n        83., 140., 196., 219., 200., 197.,  51.,  66.,  79., 275.,  78.,\n       257., 180., 202.,  71., 122., 136., 270.,  70., 146., 281., 114.,\n        59., 191.,  91.,  65., 143., 185., 243.,  53.,  99., 125., 139.,\n       292.])\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(Y_test, pred,'o')\nplt.xlabel(\"True value\")\nplt.ylabel(\"Prediction\")\nplt.title(\"Out of sample test\")\n\nText(0.5, 1.0, 'Out of sample test')\n\n\n\n\n\n\n\n\n\n\nmodel.score(X_test, Y_test)\n\n0.5249868646449161\n\n\n\nmodel.score?\n\n\nSignature: model.score(X, y, sample_weight=None)\nDocstring:\nReturn the coefficient of determination :math:`R^2` of the\nprediction.\nThe coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\nwhere :math:`u` is the residual sum of squares ``((y_true - y_pred)\n** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\ny_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\ncan be negative (because the model can be arbitrarily worse). A\nconstant model that always predicts the expected value of `y`,\ndisregarding the input features, would get a :math:`R^2` score of\n0.0.\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator.\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True values for `X`.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\nReturns\n-------\nscore : float\n    :math:`R^2` of ``self.predict(X)`` wrt. `y`.\nNotes\n-----\nThe :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/base.py\nType:      method\n\n\n\n\nShould we adjust the size of the test set? What would be the problem?\n\nfor values in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=values)\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    print(f\"Test Set {values:.2f}% | Score: {score:.3f}\")\n\nTest Set 0.05% | Score: 0.427\nTest Set 0.10% | Score: 0.537\nTest Set 0.20% | Score: 0.558\nTest Set 0.30% | Score: 0.451\nTest Set 0.40% | Score: 0.513\nTest Set 0.50% | Score: 0.475\nTest Set 0.60% | Score: 0.488\nTest Set 0.70% | Score: 0.439\n\n\nThere is a tradeoff between: - a big test set: score more accurate, but the fitting is less accurate (more bias) - a small test set: score more volatile, but the fitting is more accurate (more variance)\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nscores = []\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]\n    print(X_train.shape)\n\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    scores.append(score)\n    \n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test\n\n(294, 10)\n(295, 10)\n(295, 10)\n\n\n\nscores\n\n[0.4693057771290108, 0.48724993937707484, 0.5095525852352711]\n\n\n\nsum(scores)/3\n\n0.4887027672471189\n\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\ndata['feature_names']\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\nimport pandas\ndf = pandas.DataFrame(X, columns=data['feature_names'])\ndf['target'] = data['target']\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019908\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068330\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005671\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002864\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022692\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031991\n-0.046641\n135.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n178.0\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018118\n0.044485\n104.0\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017282\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046879\n0.015491\n132.0\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044528\n-0.025930\n220.0\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081414\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004220\n0.003064\n57.0\n\n\n\n\n442 rows × 11 columns\n\n\n\n\nfrom statsmodels.formula import api\n\n\nols_model = api.ols('target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6', df)\n\n\nresult = ols_model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntarget\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nWed, 16 Mar 2022\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n11:59:29\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0122\n59.749\n-0.168\n0.867\n-127.448\n107.424\n\n\nsex\n-239.8191\n61.222\n-3.917\n0.000\n-360.151\n-119.488\n\n\nbmi\n519.8398\n66.534\n7.813\n0.000\n389.069\n650.610\n\n\nbp\n324.3904\n65.422\n4.958\n0.000\n195.805\n452.976\n\n\ns1\n-792.1842\n416.684\n-1.901\n0.058\n-1611.169\n26.801\n\n\ns2\n476.7458\n339.035\n1.406\n0.160\n-189.621\n1143.113\n\n\ns3\n101.0446\n212.533\n0.475\n0.635\n-316.685\n518.774\n\n\ns4\n177.0642\n161.476\n1.097\n0.273\n-140.313\n494.442\n\n\ns5\n751.2793\n171.902\n4.370\n0.000\n413.409\n1089.150\n\n\ns6\n67.6254\n65.984\n1.025\n0.306\n-62.065\n197.316\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nImport the Boston House Price Dataset from sklearn. Describe it. Compute correlations.\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?\n\n\n\nSklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\nProperly train a linear logistic regression to predict cancer morbidity. (bonus: use k-fold validation)\nTry with other classifiers. Which one is best?"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_elements.html#diabetes-dataset",
    "href": "slides/pession_6/machine_learning_regressions_elements.html#diabetes-dataset",
    "title": "Intro to sklearn",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\n\nfrom sklearn.datasets import load_diabetes\ndata = load_diabetes()\nX = data['data']\nY = data['target']\n\n\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, T-Cells (a type of white blood cells)\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, thyroid stimulating hormone\n      - s5      ltg, lamotrigine\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3)\n\n\nX_train.shape\n\n(309, 10)\n\n\n\nX_test.shape\n\n(133, 10)\n\n\n\n133/(133+309)\n\n0.3009049773755656\n\n\nFeatures are already “centered and scaled”: no need to renormalize them\nTrain a linear model (with intercept) on the training set\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n# by default there is an intercept (check the doc: default value for fit_intercept is True)\n\n\nLinearRegression?\n\n\nInit signature:\nLinearRegression(\n    *,\n    fit_intercept=True,\n    normalize=False,\n    copy_X=True,\n    n_jobs=None,\n    positive=False,\n)\nDocstring:     \nOrdinary least squares Linear Regression.\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.\nParameters\n----------\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to False, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\ncopy_X : bool, default=True\n    If True, X will be copied; else, it may be overwritten.\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This will only provide\n    speedup for n_targets &gt; 1 and sufficient large problems.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`\n    for more details.\npositive : bool, default=False\n    When set to ``True``, forces the coefficients to be positive. This\n    option is only supported for dense arrays.\n    .. versionadded:: 0.24\nAttributes\n----------\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\n    Estimated coefficients for the linear regression problem.\n    If multiple targets are passed during the fit (y 2D), this\n    is a 2D array of shape (n_targets, n_features), while if only\n    one target is passed, this is a 1D array of length n_features.\nrank_ : int\n    Rank of matrix `X`. Only available when `X` is dense.\nsingular_ : array of shape (min(X, y),)\n    Singular values of `X`. Only available when `X` is dense.\nintercept_ : float or array of shape (n_targets,)\n    Independent term in the linear model. Set to 0.0 if\n    `fit_intercept = False`.\nSee Also\n--------\nRidge : Ridge regression addresses some of the\n    problems of Ordinary Least Squares by imposing a penalty on the\n    size of the coefficients with l2 regularization.\nLasso : The Lasso is a linear model that estimates\n    sparse coefficients with l1 regularization.\nElasticNet : Elastic-Net is a linear regression\n    model trained with both l1 and l2 -norm regularization of the\n    coefficients.\nNotes\n-----\nFrom the implementation point of view, this is just plain Ordinary\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n(scipy.optimize.nnls) wrapped as a predictor object.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; reg = LinearRegression().fit(X, y)\n&gt;&gt;&gt; reg.score(X, y)\n1.0\n&gt;&gt;&gt; reg.coef_\narray([1., 2.])\n&gt;&gt;&gt; reg.intercept_\n3.0000...\n&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))\narray([16.])\nFile:           ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:           ABCMeta\nSubclasses:     \n\n\n\n\n\nmodel.fit(X_train, Y_train)\n\nLinearRegression()\n\n\n\nmodel.fit?\n\n\nSignature: model.fit(X, y, sample_weight=None)\nDocstring:\nFit linear model.\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\nReturns\n-------\nself : returns an instance of self.\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/linear_model/_base.py\nType:      method\n\n\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\npred = model.predict(X_test)\n\n\nY_test\n\narray([190., 225., 141., 281., 168.,  42., 116., 276., 281.,  51., 121.,\n       156., 163., 142., 187., 173.,  39., 229., 155., 332., 257., 261.,\n        42., 201., 103.,  47., 142., 172.,  71.,  96., 173., 127., 150.,\n       230., 185., 209.,  37.,  92., 235., 131.,  97.,  40., 279.,  97.,\n       245., 258., 102., 168.,  51., 248.,  88.,  91.,  58., 110., 308.,\n        88.,  60., 311., 246., 310., 214., 200., 220., 131.,  72.,  72.,\n       181.,  89., 163., 104.,  96.,  70., 217.,  55., 317., 259.,  50.,\n       118., 200.,  25., 124., 129., 179., 109.,  71., 102., 252., 189.,\n       190., 263., 174., 259., 111.,  85., 145.,  85., 252., 258., 274.,\n        83., 140., 196., 219., 200., 197.,  51.,  66.,  79., 275.,  78.,\n       257., 180., 202.,  71., 122., 136., 270.,  70., 146., 281., 114.,\n        59., 191.,  91.,  65., 143., 185., 243.,  53.,  99., 125., 139.,\n       292.])\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(Y_test, pred,'o')\nplt.xlabel(\"True value\")\nplt.ylabel(\"Prediction\")\nplt.title(\"Out of sample test\")\n\nText(0.5, 1.0, 'Out of sample test')\n\n\n\n\n\n\n\n\n\n\nmodel.score(X_test, Y_test)\n\n0.5249868646449161\n\n\n\nmodel.score?\n\n\nSignature: model.score(X, y, sample_weight=None)\nDocstring:\nReturn the coefficient of determination :math:`R^2` of the\nprediction.\nThe coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\nwhere :math:`u` is the residual sum of squares ``((y_true - y_pred)\n** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\ny_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\ncan be negative (because the model can be arbitrarily worse). A\nconstant model that always predicts the expected value of `y`,\ndisregarding the input features, would get a :math:`R^2` score of\n0.0.\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples. For some estimators this may be a precomputed\n    kernel matrix or a list of generic objects instead with shape\n    ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n    is the number of samples used in the fitting for the estimator.\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True values for `X`.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\nReturns\n-------\nscore : float\n    :math:`R^2` of ``self.predict(X)`` wrt. `y`.\nNotes\n-----\nThe :math:`R^2` score used when calling ``score`` on a regressor uses\n``multioutput='uniform_average'`` from version 0.23 to keep consistent\nwith default value of :func:`~sklearn.metrics.r2_score`.\nThis influences the ``score`` method of all the multioutput\nregressors (except for\n:class:`~sklearn.multioutput.MultiOutputRegressor`).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/sklearn/base.py\nType:      method\n\n\n\n\nShould we adjust the size of the test set? What would be the problem?\n\nfor values in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=values)\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    print(f\"Test Set {values:.2f}% | Score: {score:.3f}\")\n\nTest Set 0.05% | Score: 0.427\nTest Set 0.10% | Score: 0.537\nTest Set 0.20% | Score: 0.558\nTest Set 0.30% | Score: 0.451\nTest Set 0.40% | Score: 0.513\nTest Set 0.50% | Score: 0.475\nTest Set 0.60% | Score: 0.488\nTest Set 0.70% | Score: 0.439\n\n\nThere is a tradeoff between: - a big test set: score more accurate, but the fitting is less accurate (more bias) - a small test set: score more volatile, but the fitting is more accurate (more variance)\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nscores = []\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = Y[train_index], Y[test_index]\n    print(X_train.shape)\n\n    \n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    \n    score = model.score(X_test, Y_test)\n    \n    scores.append(score)\n    \n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test\n\n(294, 10)\n(295, 10)\n(295, 10)\n\n\n\nscores\n\n[0.4693057771290108, 0.48724993937707484, 0.5095525852352711]\n\n\n\nsum(scores)/3\n\n0.4887027672471189\n\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\ndata['feature_names']\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\nimport pandas\ndf = pandas.DataFrame(X, columns=data['feature_names'])\ndf['target'] = data['target']\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019908\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068330\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005671\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002864\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022692\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031991\n-0.046641\n135.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n437\n0.041708\n0.050680\n0.019662\n0.059744\n-0.005697\n-0.002566\n-0.028674\n-0.002592\n0.031193\n0.007207\n178.0\n\n\n438\n-0.005515\n0.050680\n-0.015906\n-0.067642\n0.049341\n0.079165\n-0.028674\n0.034309\n-0.018118\n0.044485\n104.0\n\n\n439\n0.041708\n0.050680\n-0.015906\n0.017282\n-0.037344\n-0.013840\n-0.024993\n-0.011080\n-0.046879\n0.015491\n132.0\n\n\n440\n-0.045472\n-0.044642\n0.039062\n0.001215\n0.016318\n0.015283\n-0.028674\n0.026560\n0.044528\n-0.025930\n220.0\n\n\n441\n-0.045472\n-0.044642\n-0.073030\n-0.081414\n0.083740\n0.027809\n0.173816\n-0.039493\n-0.004220\n0.003064\n57.0\n\n\n\n\n442 rows × 11 columns\n\n\n\n\nfrom statsmodels.formula import api\n\n\nols_model = api.ols('target ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6', df)\n\n\nresult = ols_model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntarget\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nWed, 16 Mar 2022\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n11:59:29\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0122\n59.749\n-0.168\n0.867\n-127.448\n107.424\n\n\nsex\n-239.8191\n61.222\n-3.917\n0.000\n-360.151\n-119.488\n\n\nbmi\n519.8398\n66.534\n7.813\n0.000\n389.069\n650.610\n\n\nbp\n324.3904\n65.422\n4.958\n0.000\n195.805\n452.976\n\n\ns1\n-792.1842\n416.684\n-1.901\n0.058\n-1611.169\n26.801\n\n\ns2\n476.7458\n339.035\n1.406\n0.160\n-189.621\n1143.113\n\n\ns3\n101.0446\n212.533\n0.475\n0.635\n-316.685\n518.774\n\n\ns4\n177.0642\n161.476\n1.097\n0.273\n-140.313\n494.442\n\n\ns5\n751.2793\n171.902\n4.370\n0.000\n413.409\n1089.150\n\n\ns6\n67.6254\n65.984\n1.025\n0.306\n-62.065\n197.316\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_elements.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "slides/pession_6/machine_learning_regressions_elements.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Intro to sklearn",
    "section": "",
    "text": "Import the Boston House Price Dataset from sklearn. Describe it. Compute correlations.\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_elements.html#predicting-breast-cancer",
    "href": "slides/pession_6/machine_learning_regressions_elements.html#predicting-breast-cancer",
    "title": "Intro to sklearn",
    "section": "",
    "text": "Sklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\nProperly train a linear logistic regression to predict cancer morbidity. (bonus: use k-fold validation)\nTry with other classifiers. Which one is best?"
  },
  {
    "objectID": "slides/pession_6/index.html#what-is-machine-learning-1",
    "href": "slides/pession_6/index.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/pession_6/index.html#what-about-artificial-intelligence",
    "href": "slides/pession_6/index.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "slides/pession_6/index.html#econometrics-vs-machine-learning",
    "href": "slides/pession_6/index.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "slides/pession_6/index.html#data-types",
    "href": "slides/pession_6/index.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "slides/pession_6/index.html#tabular-data",
    "href": "slides/pession_6/index.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\n\ntabular data"
  },
  {
    "objectID": "slides/pession_6/index.html#networks",
    "href": "slides/pession_6/index.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "slides/pession_6/index.html#big-data-1",
    "href": "slides/pession_6/index.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "slides/pession_6/index.html#big-subfields-of-machine-learning",
    "href": "slides/pession_6/index.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "slides/pession_6/index.html#difference-with-traditional-regression",
    "href": "slides/pession_6/index.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "slides/pession_6/index.html#difference-with-traditional-regression-1",
    "href": "slides/pession_6/index.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n. . .\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "slides/pession_6/index.html#difference-with-traditional-regression-2",
    "href": "slides/pession_6/index.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "slides/pession_6/index.html#long-data",
    "href": "slides/pession_6/index.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "slides/pession_6/index.html#long-data-1",
    "href": "slides/pession_6/index.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "slides/pession_6/index.html#long-data-2",
    "href": "slides/pession_6/index.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "slides/pession_6/index.html#formalisation-a-typical-machine-learning-task",
    "href": "slides/pession_6/index.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "slides/pession_6/index.html#training-gradient-descent",
    "href": "slides/pession_6/index.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "slides/pession_6/index.html#not-everything-goes-wrong-all-the-time",
    "href": "slides/pession_6/index.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "slides/pession_6/index.html#wide-data",
    "href": "slides/pession_6/index.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n. . .\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "slides/pession_6/index.html#wide-data-regression",
    "href": "slides/pession_6/index.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "slides/pession_6/index.html#training",
    "href": "slides/pession_6/index.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "slides/pession_6/index.html#example-imf-challenge",
    "href": "slides/pession_6/index.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "slides/pession_6/index.html#nonlinear-regression-1",
    "href": "slides/pession_6/index.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "slides/pession_6/index.html#how-to-evaluate-the-machine-learning",
    "href": "slides/pession_6/index.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "slides/pession_6/index.html#how-to-evaluate-the-machine-learning-1",
    "href": "slides/pession_6/index.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "slides/pession_6/index.html#section",
    "href": "slides/pession_6/index.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\n\n\n. . .\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "slides/pession_6/index.html#how-to-choose-the-validation-set",
    "href": "slides/pession_6/index.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n. . .\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "slides/pession_6/index.html#how-to-choose-the-validation-set-1",
    "href": "slides/pession_6/index.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "slides/pession_6/index.html#wait",
    "href": "slides/pession_6/index.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "slides/pession_6/index.html#in-practice",
    "href": "slides/pession_6/index.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "slides/pession_6/index.html#k-fold-validation-with-sklearn",
    "href": "slides/pession_6/index.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "slides/pession_6/slides.html#regressions",
    "href": "slides/pession_6/slides.html#regressions",
    "title": "Introduction to Machine Learning",
    "section": "Regressions",
    "text": "Regressions"
  },
  {
    "objectID": "slides/pession_6/slides.html#what-is-machine-learning-1",
    "href": "slides/pession_6/slides.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/pession_6/slides.html#what-about-artificial-intelligence",
    "href": "slides/pession_6/slides.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "slides/pession_6/slides.html#econometrics-vs-machine-learning",
    "href": "slides/pession_6/slides.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "slides/pession_6/slides.html#data-types",
    "href": "slides/pession_6/slides.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "slides/pession_6/slides.html#tabular-data",
    "href": "slides/pession_6/slides.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\ntabular data"
  },
  {
    "objectID": "slides/pession_6/slides.html#networks",
    "href": "slides/pession_6/slides.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "slides/pession_6/slides.html#big-data-1",
    "href": "slides/pession_6/slides.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "slides/pession_6/slides.html#big-subfields-of-machine-learning",
    "href": "slides/pession_6/slides.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "slides/pession_6/slides.html#difference-with-traditional-regression",
    "href": "slides/pession_6/slides.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "slides/pession_6/slides.html#difference-with-traditional-regression-1",
    "href": "slides/pession_6/slides.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "slides/pession_6/slides.html#difference-with-traditional-regression-2",
    "href": "slides/pession_6/slides.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "slides/pession_6/slides.html#long-data",
    "href": "slides/pession_6/slides.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "slides/pession_6/slides.html#long-data-1",
    "href": "slides/pession_6/slides.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "slides/pession_6/slides.html#long-data-2",
    "href": "slides/pession_6/slides.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "slides/pession_6/slides.html#formalisation-a-typical-machine-learning-task",
    "href": "slides/pession_6/slides.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "slides/pession_6/slides.html#training-gradient-descent",
    "href": "slides/pession_6/slides.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "slides/pession_6/slides.html#not-everything-goes-wrong-all-the-time",
    "href": "slides/pession_6/slides.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "slides/pession_6/slides.html#wide-data",
    "href": "slides/pession_6/slides.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "slides/pession_6/slides.html#wide-data-regression",
    "href": "slides/pession_6/slides.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "slides/pession_6/slides.html#training",
    "href": "slides/pession_6/slides.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "slides/pession_6/slides.html#example-imf-challenge",
    "href": "slides/pession_6/slides.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "slides/pession_6/slides.html#nonlinear-regression-1",
    "href": "slides/pession_6/slides.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "slides/pession_6/slides.html#how-to-evaluate-the-machine-learning",
    "href": "slides/pession_6/slides.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "slides/pession_6/slides.html#how-to-evaluate-the-machine-learning-1",
    "href": "slides/pession_6/slides.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "slides/pession_6/slides.html#section",
    "href": "slides/pession_6/slides.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "slides/pession_6/slides.html#how-to-choose-the-validation-set",
    "href": "slides/pession_6/slides.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "slides/pession_6/slides.html#how-to-choose-the-validation-set-1",
    "href": "slides/pession_6/slides.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "slides/pession_6/slides.html#wait",
    "href": "slides/pession_6/slides.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "slides/pession_6/slides.html#in-practice",
    "href": "slides/pession_6/slides.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "slides/pession_6/slides.html#k-fold-validation-with-sklearn",
    "href": "slides/pession_6/slides.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "slides/pession_6/graphs/inference.html",
    "href": "slides/pession_6/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_correction.html",
    "href": "slides/pession_6/machine_learning_regressions_correction.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:\n\ncreate a training set and a validation set\ntrain a model with sklearn\nperform a validation test\n\n\n\nImport the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\nimport statsmodels\nfrom statsmodels.formula import api as smf\n\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n       'disease_progression'],\n      dtype='object')\n\n\n\nregmodel = smf.ols(formula=\"disease_progression ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6\", data=df)\nregresults = regmodel.fit()\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed\n\n\n\n\n!!! update: boston price dataset has been deprecated\n!!! use california_housing instead\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\n\nprint(dataset[\"DESCR\"])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nAn household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surpinsingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\n\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n\nTrain a lasso model to predict house prices. Compute the score on the test set.\n\n# we should check that the data is normalized, or normalize it ourselves\n\n\nfrom sklearn.linear_model import Lasso\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nmodel_lasso.score(X_test, y_test)\n\n0.28204855993177635\n\n\nTrain a ridge model to predict house prices. Which one is better?\n\nfrom sklearn.linear_model import Ridge\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nmodel_ridge.score(X_test, y_test)\n\n0.6060031802405054\n\n\nIt looks like the ridge model has a better fit (score). However, we should have left a test set appart and not used it at all during training phase. Here it has influenced the choice of the model (between ridge and lasso)."
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "href": "slides/pession_6/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\nimport statsmodels\nfrom statsmodels.formula import api as smf\n\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6',\n       'disease_progression'],\n      dtype='object')\n\n\n\nregmodel = smf.ols(formula=\"disease_progression ~ age + sex + bmi + bp + s1 + s2 + s3 + s4 + s5 + s6\", data=df)\nregresults = regmodel.fit()\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "slides/pession_6/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "!!! update: boston price dataset has been deprecated\n!!! use california_housing instead\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\n\nprint(dataset[\"DESCR\"])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nAn household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surpinsingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\n\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n\nTrain a lasso model to predict house prices. Compute the score on the test set.\n\n# we should check that the data is normalized, or normalize it ourselves\n\n\nfrom sklearn.linear_model import Lasso\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nmodel_lasso.score(X_test, y_test)\n\n0.28204855993177635\n\n\nTrain a ridge model to predict house prices. Which one is better?\n\nfrom sklearn.linear_model import Ridge\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nmodel_ridge.score(X_test, y_test)\n\n0.6060031802405054\n\n\nIt looks like the ridge model has a better fit (score). However, we should have left a test set appart and not used it at all during training phase. Here it has influenced the choice of the model (between ridge and lasso)."
  },
  {
    "objectID": "slides/pession_6/recap.html#important-points",
    "href": "slides/pession_6/recap.html#important-points",
    "title": "Quick Recap",
    "section": "Important points",
    "text": "Important points"
  },
  {
    "objectID": "slides/session_2/Numerical Python.html",
    "href": "slides/session_2/Numerical Python.html",
    "title": "Numerical Python",
    "section": "",
    "text": "Most python scientists, use the following libraries:\n\nnumpy: performant array library (vectors and matrices)\nmatplotlib: plotting library\nscipy: all kinds of mathematical routines\n\nIn the rest of the course, we’ll make some use of numpy and matplotlib\nThey are included in all python distributions like Anaconda Python\nAll additional libraries use numpy and matplotlib: pandas, statsmodels, sklearn\n\n\n\n\nIt is standard to import the libraries as np, and plt. We’ll follow this convention here.\n\n# these lines need to be run only once per program\nimport numpy as np\nimport matplotlib as plt\n\n\nprint(f\"Numpy version {np.__version__}\")\nprint(f\"Matplotlib version {plt.__version__}\")\n\nNumpy version 1.19.5\nMatplotlib version3.3.3\n\n\n\n\n\n\n\n\n\nVectors and matrices are created with the np.array(...) function.\nSpecial vectors can be created with np.zeros, np.ones, np.linspace\n\n\n# an array can be created from a list of numbers\nnp.array( [1.0, 2.0, 3.0] )\n\narray([1., 2., 3.])\n\n\n\n# or initialized by specifying the length of the array\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n\n# 10 regularly spaced points between 0 and 1\nnp.linspace(0, 1, 10)\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\n\n\n\nA matrix is a 2-dimensional array and is created with np.array\nFunction np.matrix() has been deprecated: do not use it.\nThere are functions to create specific matrices: np.eye, np.diag, …\n\n\n# an array can be created from a list of (equal size) lists\nnp.array([\n    [1.0, 2.0, 3.0],\n    [4  ,   5,   6] \n])\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\n# initialize an empty matrix with the dimensions as a tuple\nA = np.zeros( (2, 3) )\nA\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# matrix dimensions are contained in the shape attribute\nA.shape\n\n(2, 3)\n\n\n\n\n\nVector multiplications and Matrix multiplications can be performed using special sign @\n\nA = np.array([[1.0, 2.0], [2,4]])\nA\n\narray([[1., 2.],\n       [2., 4.]])\n\n\n\nB = np.array([1.0, 2.0])\nB\n\narray([1., 2.])\n\n\n\nA@B\n\narray([ 5., 10.])\n\n\n\nA@A\n\narray([[ 5., 10.],\n       [10., 20.]])\n\n\n\n\n\nNumpy arrays can contain data of several scalar types.\n\n[True, False, True]\n\n[True, False, True]\n\n\n\n# vector of boolean\nboolean_vector = np.array( [True, False, True] )\nprint(f\"type of scalar '{boolean_vector.dtype}'\")\nboolean_vector\n\ntype of scalar 'bool'\n\n\narray([ True, False,  True])\n\n\n\n# vector of integers\nint_vector = np.array([1, 2, 0])\nprint(f\"type of scalar '{int_vector.dtype}'\")\nint_vector\n\ntype of scalar 'int64'\n\n\narray([1, 2, 0])\n\n\n\n\n\n\nElements and subarrays, can be retrieved using the same syntax as lists and strings.\n\nRemember that indexing starts at 0.\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1])  # second element\n\n1.0\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1:3])  # second, third and fourth element\n\narray([1., 2.])\n\n\n\n\n\n\nElements and suvectors, can be assigned to new values, as long as they have the right dimensions.\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\nV[3] = 3.0\nV\n\narray([ 1.,  1.,  2.,  3.,  5.,  8., 13.])\n\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\n# V[1:4] = [1,2,3,4] # this doesn't work\nV[1:4] = [2,3,4] # this works\n\n\n\n\n\nIndexing generalizes to matrices: there are two indices istead of one: M[i,j]\nOne can extract a row, or a column (a slice) with M[i,:] or M[:,i]\nA submatrix is defining with two intervals: M[i:j, k:l] or M[i:j, :], …\n\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,1] # access element (1,2)\n\n2\n\n\n\nM[2,:] # third row\n\narray([7, 8, 9])\n\n\n\nM[:,1] # second column     # M[i,1] for any i\n\narray([2, 5, 8])\n\n\n\nM[1:3, :] # lines from 1 (included) to 3 (excluded) ; all columns\n\narray([[4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\n\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,0] = 0\nM\n\narray([[0, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[1:3, 1:3] = np.array([[0,1],[1,0]]) # dimensions must match\nM\n\narray([[0, 2, 3],\n       [4, 0, 1],\n       [7, 1, 0]])\n\n\n\n\n\n\nThe following algebraic operations are defined on arrays: +, -, *, /, **.\nComparisons operators (&lt;,&lt;=, &gt;, &gt;=, ==) are defined are return boolean arrays.\nThey operate element by element.\n\n\nA = np.array([1,2,3,4])\nB = np.array([4,3,2,1])\nA+B\n\narray([5, 5, 5, 5])\n\n\n\nA*B    # note the difference with A@B\n\narray([4, 6, 6, 4])\n\n\n\nA&gt;B\n\narray([False, False,  True,  True])\n\n\n\n\n\n\nThe following logical operations are defined element-wise on arrays: & (and), | (or), ~ (not)\n\n\nA = np.array([False, False, True, True])\nB = np.array([False, True, False, True])\n\n\n~A\n\narray([ True,  True, False, False])\n\n\n\nA | B\n\narray([False,  True,  True,  True])\n\n\n\nA & B\n\narray([False, False, False,  True])\n\n\n\n\n\n\nArrays can be indexed by boolean arrays instead of ranges.\nOnly elements corresponding to true are retrieved\n\n\nx = np.linspace(0,1,6)\nx\n\narray([0. , 0.2, 0.4, 0.6, 0.8, 1. ])\n\n\n\n# indexes such that (x^2) &gt; (x/2)\nx**2 &gt; (x/2)\n\narray([False, False, False,  True,  True,  True])\n\n\n\ncond = x**2 &gt; (x/2)\nx[ cond ] \n\narray([0.6, 0.8, 1. ])\n\n\n\n\n\n\nNumpy library has defined very consistent conventions, to match inconsistent dimensions.\nIgnore them for now…\n\n\nM = np.eye(4)\nM\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\nM[2:4, 2:4] = 0.5 # float\nM\n\narray([[1. , 0. , 0. , 0. ],\n       [0. , 1. , 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n\nM[:,:2] = np.array([[0.1, 0.2]])  # 1x2 array\nM\n\narray([[0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0.5, 0.5],\n       [0.1, 0.2, 0.5, 0.5]])\n\n\n\n\n\n\nOther useful functions (easy to google):\n\nnp.arange() regularly spaced integers\nnp.where() find elements in\n…\n\n\n\n\n\n\n\n\n\nmatplotlib is …\nobject oriented api optional Matlab-like syntax\nmain function is plt.plot(x,y) where x and y are vectors (or iterables like lists)\n\nlots of optional arguments\n\n\n\nfrom matplotlib import pyplot as plt\n\n\n\n\n\nx = np.linspace(-1,1,6)\n\n\ny = np.sin(x)/x # sinus cardinal\n\n\nplt.plot(x,y,'o')\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,100)\n\nfig = plt.figure() # keep a figure open to draw on it\nfor k in range(1,5):\n    y = np.sin(x*k)/(x*k)\n    plt.plot(x, y, label=f\"$sinc({k} x)$\") # label each line\nplt.plot(x, x*0, color='black', linestyle='--')\nplt.grid(True) # add a grid\nplt.title(\"Looking for the right hat.\")\nplt.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,100)\n\nplt.figure()\nplt.subplot(2,2,1) # create a 2x2 subplot and draw in first quadrant\nplt.plot(x,x)\nplt.subplot(2,2,2) # create a 2x2 subplot and draw in second quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,3) # create a 2x2 subplot and draw in third quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,4) # create a 2x2 subplot and draw in fourth quadrant\nplt.plot(x,x)\n\nplt.tight_layout() # save some space\n\n\n\n\n\n\n\n\n\n\n\n\nplotly (nice javascript graphs)\naltair (good for datavisualisation/interactivity)\n\npython wrapper to Vega-lite\n\n\n\n\n\n\n\nDataFrames and pandas"
  },
  {
    "objectID": "slides/session_2/Numerical Python.html#scientific-stack",
    "href": "slides/session_2/Numerical Python.html#scientific-stack",
    "title": "Numerical Python",
    "section": "",
    "text": "Most python scientists, use the following libraries:\n\nnumpy: performant array library (vectors and matrices)\nmatplotlib: plotting library\nscipy: all kinds of mathematical routines\n\nIn the rest of the course, we’ll make some use of numpy and matplotlib\nThey are included in all python distributions like Anaconda Python\nAll additional libraries use numpy and matplotlib: pandas, statsmodels, sklearn\n\n\n\n\nIt is standard to import the libraries as np, and plt. We’ll follow this convention here.\n\n# these lines need to be run only once per program\nimport numpy as np\nimport matplotlib as plt\n\n\nprint(f\"Numpy version {np.__version__}\")\nprint(f\"Matplotlib version {plt.__version__}\")\n\nNumpy version 1.19.5\nMatplotlib version3.3.3"
  },
  {
    "objectID": "slides/session_2/Numerical Python.html#numpy",
    "href": "slides/session_2/Numerical Python.html#numpy",
    "title": "Numerical Python",
    "section": "",
    "text": "Vectors and matrices are created with the np.array(...) function.\nSpecial vectors can be created with np.zeros, np.ones, np.linspace\n\n\n# an array can be created from a list of numbers\nnp.array( [1.0, 2.0, 3.0] )\n\narray([1., 2., 3.])\n\n\n\n# or initialized by specifying the length of the array\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n\n# 10 regularly spaced points between 0 and 1\nnp.linspace(0, 1, 10)\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\n\n\n\nA matrix is a 2-dimensional array and is created with np.array\nFunction np.matrix() has been deprecated: do not use it.\nThere are functions to create specific matrices: np.eye, np.diag, …\n\n\n# an array can be created from a list of (equal size) lists\nnp.array([\n    [1.0, 2.0, 3.0],\n    [4  ,   5,   6] \n])\n\narray([[1., 2., 3.],\n       [4., 5., 6.]])\n\n\n\n# initialize an empty matrix with the dimensions as a tuple\nA = np.zeros( (2, 3) )\nA\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# matrix dimensions are contained in the shape attribute\nA.shape\n\n(2, 3)\n\n\n\n\n\nVector multiplications and Matrix multiplications can be performed using special sign @\n\nA = np.array([[1.0, 2.0], [2,4]])\nA\n\narray([[1., 2.],\n       [2., 4.]])\n\n\n\nB = np.array([1.0, 2.0])\nB\n\narray([1., 2.])\n\n\n\nA@B\n\narray([ 5., 10.])\n\n\n\nA@A\n\narray([[ 5., 10.],\n       [10., 20.]])\n\n\n\n\n\nNumpy arrays can contain data of several scalar types.\n\n[True, False, True]\n\n[True, False, True]\n\n\n\n# vector of boolean\nboolean_vector = np.array( [True, False, True] )\nprint(f\"type of scalar '{boolean_vector.dtype}'\")\nboolean_vector\n\ntype of scalar 'bool'\n\n\narray([ True, False,  True])\n\n\n\n# vector of integers\nint_vector = np.array([1, 2, 0])\nprint(f\"type of scalar '{int_vector.dtype}'\")\nint_vector\n\ntype of scalar 'int64'\n\n\narray([1, 2, 0])\n\n\n\n\n\n\nElements and subarrays, can be retrieved using the same syntax as lists and strings.\n\nRemember that indexing starts at 0.\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1])  # second element\n\n1.0\n\n\n\nV = np.array([0., 1., 2., 3., 4.])\ndisplay(V[1:3])  # second, third and fourth element\n\narray([1., 2.])\n\n\n\n\n\n\nElements and suvectors, can be assigned to new values, as long as they have the right dimensions.\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\nV[3] = 3.0\nV\n\narray([ 1.,  1.,  2.,  3.,  5.,  8., 13.])\n\n\n\nV = np.array([1., 1., 2., 4., 5., 8., 13.])\n# V[1:4] = [1,2,3,4] # this doesn't work\nV[1:4] = [2,3,4] # this works\n\n\n\n\n\nIndexing generalizes to matrices: there are two indices istead of one: M[i,j]\nOne can extract a row, or a column (a slice) with M[i,:] or M[:,i]\nA submatrix is defining with two intervals: M[i:j, k:l] or M[i:j, :], …\n\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,1] # access element (1,2)\n\n2\n\n\n\nM[2,:] # third row\n\narray([7, 8, 9])\n\n\n\nM[:,1] # second column     # M[i,1] for any i\n\narray([2, 5, 8])\n\n\n\nM[1:3, :] # lines from 1 (included) to 3 (excluded) ; all columns\n\narray([[4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\n\n\nM = np.array([[1,2,3],[4,5,6],[7,8,9]])\nM\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[0,0] = 0\nM\n\narray([[0, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nM[1:3, 1:3] = np.array([[0,1],[1,0]]) # dimensions must match\nM\n\narray([[0, 2, 3],\n       [4, 0, 1],\n       [7, 1, 0]])\n\n\n\n\n\n\nThe following algebraic operations are defined on arrays: +, -, *, /, **.\nComparisons operators (&lt;,&lt;=, &gt;, &gt;=, ==) are defined are return boolean arrays.\nThey operate element by element.\n\n\nA = np.array([1,2,3,4])\nB = np.array([4,3,2,1])\nA+B\n\narray([5, 5, 5, 5])\n\n\n\nA*B    # note the difference with A@B\n\narray([4, 6, 6, 4])\n\n\n\nA&gt;B\n\narray([False, False,  True,  True])\n\n\n\n\n\n\nThe following logical operations are defined element-wise on arrays: & (and), | (or), ~ (not)\n\n\nA = np.array([False, False, True, True])\nB = np.array([False, True, False, True])\n\n\n~A\n\narray([ True,  True, False, False])\n\n\n\nA | B\n\narray([False,  True,  True,  True])\n\n\n\nA & B\n\narray([False, False, False,  True])\n\n\n\n\n\n\nArrays can be indexed by boolean arrays instead of ranges.\nOnly elements corresponding to true are retrieved\n\n\nx = np.linspace(0,1,6)\nx\n\narray([0. , 0.2, 0.4, 0.6, 0.8, 1. ])\n\n\n\n# indexes such that (x^2) &gt; (x/2)\nx**2 &gt; (x/2)\n\narray([False, False, False,  True,  True,  True])\n\n\n\ncond = x**2 &gt; (x/2)\nx[ cond ] \n\narray([0.6, 0.8, 1. ])\n\n\n\n\n\n\nNumpy library has defined very consistent conventions, to match inconsistent dimensions.\nIgnore them for now…\n\n\nM = np.eye(4)\nM\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\nM[2:4, 2:4] = 0.5 # float\nM\n\narray([[1. , 0. , 0. , 0. ],\n       [0. , 1. , 0. , 0. ],\n       [0. , 0. , 0.5, 0.5],\n       [0. , 0. , 0.5, 0.5]])\n\n\n\nM[:,:2] = np.array([[0.1, 0.2]])  # 1x2 array\nM\n\narray([[0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0. , 0. ],\n       [0.1, 0.2, 0.5, 0.5],\n       [0.1, 0.2, 0.5, 0.5]])\n\n\n\n\n\n\nOther useful functions (easy to google):\n\nnp.arange() regularly spaced integers\nnp.where() find elements in\n…"
  },
  {
    "objectID": "slides/session_2/Numerical Python.html#matplotlib",
    "href": "slides/session_2/Numerical Python.html#matplotlib",
    "title": "Numerical Python",
    "section": "",
    "text": "matplotlib is …\nobject oriented api optional Matlab-like syntax\nmain function is plt.plot(x,y) where x and y are vectors (or iterables like lists)\n\nlots of optional arguments\n\n\n\nfrom matplotlib import pyplot as plt\n\n\n\n\n\nx = np.linspace(-1,1,6)\n\n\ny = np.sin(x)/x # sinus cardinal\n\n\nplt.plot(x,y,'o')\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,100)\n\nfig = plt.figure() # keep a figure open to draw on it\nfor k in range(1,5):\n    y = np.sin(x*k)/(x*k)\n    plt.plot(x, y, label=f\"$sinc({k} x)$\") # label each line\nplt.plot(x, x*0, color='black', linestyle='--')\nplt.grid(True) # add a grid\nplt.title(\"Looking for the right hat.\")\nplt.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,100)\n\nplt.figure()\nplt.subplot(2,2,1) # create a 2x2 subplot and draw in first quadrant\nplt.plot(x,x)\nplt.subplot(2,2,2) # create a 2x2 subplot and draw in second quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,3) # create a 2x2 subplot and draw in third quadrant\nplt.plot(x,-x)\nplt.subplot(2,2,4) # create a 2x2 subplot and draw in fourth quadrant\nplt.plot(x,x)\n\nplt.tight_layout() # save some space\n\n\n\n\n\n\n\n\n\n\n\n\nplotly (nice javascript graphs)\naltair (good for datavisualisation/interactivity)\n\npython wrapper to Vega-lite"
  },
  {
    "objectID": "slides/session_2/Numerical Python.html#next",
    "href": "slides/session_2/Numerical Python.html#next",
    "title": "Numerical Python",
    "section": "",
    "text": "DataFrames and pandas"
  },
  {
    "objectID": "slides/session_2/index.html#dataframe",
    "href": "slides/session_2/index.html#dataframe",
    "title": "Dataframes",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nA DataFrame (aka a table) is a 2-D labeled data structure with columns\n\neach column has a specific type and a column name\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\nfirst goal of an econometrician: constitute a good dataframe\n\naka “cleaning the data”",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#dataframes-are-everywhere",
    "href": "slides/session_2/index.html#dataframes-are-everywhere",
    "title": "Dataframes",
    "section": "DataFrames are everywhere",
    "text": "DataFrames are everywhere\n\n\n\n\n\nsometimes data comes from several linked dataframes\n\nrelational database\ncan still be seen conceptually as one dataframe…\n… through a join operation\n\n\n\n\n\ndataframes / relational databases are so ubiquitous a language has been developed for them\n\nSQL\nin the 80s…\n\nprobably worth looking at if you have some “data” ambitions\nyou will see the shadow of SQL everywhere\nplenty of resources to learn (example: sqbolt)",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#pandas-1",
    "href": "slides/session_2/index.html#pandas-1",
    "title": "Dataframes",
    "section": "pandas",
    "text": "pandas\n\npandas = panel + datas\n\na python library created by WesMcKinney\nvery optimized\n\nessentially a dataframe object\nmany options but if in doubt:\n\nminimally sufficient pandas is asmall subset of pandas to do everything\n\ntons of online tutorials\n\nofficial documenation doc\nquantecon",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#creating-a-dataframe-1",
    "href": "slides/session_2/index.html#creating-a-dataframe-1",
    "title": "Dataframes",
    "section": "creating a dataframe (1)",
    "text": "creating a dataframe (1)\n\n\n\nImport pandas\n\npreferably with standard alias pd\n\nimport pandas as pd\nImport a dataframe\n\neach line a different entry in a dictionary\n\n# from a dictionary\nd = {\n  \"country\": [\"USA\", \"UK\", \"France\"],\n  \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\n\n\ncountry\n\n\ncomics\n\n\n\n\n\n\n0\n\n\nUSA\n\n\n13\n\n\n\n\n1\n\n\nUK\n\n\n10\n\n\n\n\n2\n\n\nFrance\n\n\n12",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#creating-a-dataframe-2",
    "href": "slides/session_2/index.html#creating-a-dataframe-2",
    "title": "Dataframes",
    "section": "creating a dataframe (2)",
    "text": "creating a dataframe (2)\n\nthere are many other ways to create a dataframe\n\nfor instance using a numpy matrix (numpy is a linear algebra library)\n\n# from a matrix\nimport numpy as np\nM = np.array(\n    [[18, 150],\n     [21, 200],\n     [29, 1500]]\n)   \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\n\nage\n\n\ntravel\n\n\n\n\n\n\n0\n\n\n18\n\n\n150\n\n\n\n\n1\n\n\n21\n\n\n200\n\n\n\n\n2\n\n\n29\n\n\n1500",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#common-file-formats",
    "href": "slides/session_2/index.html#common-file-formats",
    "title": "Dataframes",
    "section": "Common file formats",
    "text": "Common file formats\n\ncomma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\nnote that excel does not store a dataframe (each cell is potentially different)\npostprocessing is needed",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#comma-separated-file",
    "href": "slides/session_2/index.html#comma-separated-file",
    "title": "Dataframes",
    "section": "Comma separated file",
    "text": "Comma separated file\n\none can actually a file from python\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\nand import it\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#annoying-comma-separated-file",
    "href": "slides/session_2/index.html#annoying-comma-separated-file",
    "title": "Dataframes",
    "section": "“Annoying” Comma Separated File",
    "text": "“Annoying” Comma Separated File\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\n\nthe kind of separator, whether there are quotes…\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\nadd relevant options to pd.read_csv and check result\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#exporting-a-dataframe",
    "href": "slides/session_2/index.html#exporting-a-dataframe",
    "title": "Dataframes",
    "section": "Exporting a DataFrame",
    "text": "Exporting a DataFrame\n\npandas can export to many formats: df.to_...\nto (standard) CSV\n\nprint( df.to_csv() )\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\nor to stata\n\ndf.to_stata('dummy_example.dta')",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#types-of-data-sources",
    "href": "slides/session_2/index.html#types-of-data-sources",
    "title": "Dataframes",
    "section": "Types of Data Sources",
    "text": "Types of Data Sources\n\n\n\nWhere can we get data from ?\n\ncheck one of the databases lists kaggle, econ network\n\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#data-providers",
    "href": "slides/session_2/index.html#data-providers",
    "title": "Dataframes",
    "section": "Data providers",
    "text": "Data providers\n\ncommercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones available as a python library\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n145\n\n\n6.7\n\n\n3.0\n\n\n5.2\n\n\n2.3\n\n\nvirginica\n\n\n\n\n146\n\n\n6.3\n\n\n2.5\n\n\n5.0\n\n\n1.9\n\n\nvirginica\n\n\n\n\n147\n\n\n6.5\n\n\n3.0\n\n\n5.2\n\n\n2.0\n\n\nvirginica\n\n\n\n\n148\n\n\n6.2\n\n\n3.4\n\n\n5.4\n\n\n2.3\n\n\nvirginica\n\n\n\n\n149\n\n\n5.9\n\n\n3.0\n\n\n5.1\n\n\n1.8\n\n\nvirginica\n\n\n\n\n\n150 rows × 5 columns",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#dbnomics-example",
    "href": "slides/session_2/index.html#dbnomics-example",
    "title": "Dataframes",
    "section": "DBnomics example",
    "text": "DBnomics example\n\nDBnomics aggregates time series from various public sources\ndata is organized as provider/database/series\ntry to find the identifer of one or several series\n\nimport dbnomics\ndf = dbnomics.fetch_series('AMECO/ZUTN/EA19.1.0.0.0.ZUTN')\n\ntip: in case one python package is missing, it can be installed on the fly as in\n\n!pip install dbnomics",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#inspecting-data",
    "href": "slides/session_2/index.html#inspecting-data",
    "title": "Dataframes",
    "section": "Inspecting data",
    "text": "Inspecting data\n\nonce the data is loaded as df, we want to look at some basic properties:\ngeneral\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # general summary\n\ncentral tendency\n\ndf.mean() # average\ndf.median() # median\n\nspread\n\ndf.std() # standard deviations\ndf.var() # variance\ndf.min(), df.max() # bounds\n\ncounts (for categorical variable\n\ndf.count()\n\n\n\ndf.head(2)\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\n\n\nsepalWidth\n\n\npetalLength\n\n\npetalWidth\n\n\n\n\n\n\ncount\n\n\n150.000000\n\n\n150.000000\n\n\n150.000000\n\n\n150.000000\n\n\n\n\nmean\n\n\n5.843333\n\n\n3.057333\n\n\n3.758000\n\n\n1.199333\n\n\n\n\nstd\n\n\n0.828066\n\n\n0.435866\n\n\n1.765298\n\n\n0.762238\n\n\n\n\nmin\n\n\n4.300000\n\n\n2.000000\n\n\n1.000000\n\n\n0.100000\n\n\n\n\n25%\n\n\n5.100000\n\n\n2.800000\n\n\n1.600000\n\n\n0.300000\n\n\n\n\n50%\n\n\n5.800000\n\n\n3.000000\n\n\n4.350000\n\n\n1.300000\n\n\n\n\n75%\n\n\n6.400000\n\n\n3.300000\n\n\n5.100000\n\n\n1.800000\n\n\n\n\nmax\n\n\n7.900000\n\n\n4.400000\n\n\n6.900000\n\n\n2.500000",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#changing-names-of-columns",
    "href": "slides/session_2/index.html#changing-names-of-columns",
    "title": "Dataframes",
    "section": "Changing names of columns",
    "text": "Changing names of columns\n\nColumns are defined by property df.columns\n\ndf.columns\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\nThis property can be set with a list of the right length\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#indexing-a-column",
    "href": "slides/session_2/index.html#indexing-a-column",
    "title": "Dataframes",
    "section": "Indexing a column",
    "text": "Indexing a column\n\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n0      3.5\n1      3.0\n      ... \n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\nThe result is a series object (typed values with a name and an index)\nIt has its own set of methods\n\ntry:\n\nseries.mean(), series.std()\nseries.plot()\nseries.diff()\n\ncreates \\(y_t = x_t-x_{t-1}\\)\n\nseries.pct_change()\n\ncreates \\(y_t = \\frac{x_t-x_{t-1}}{x_{t-1}}\\)",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#creating-a-new-column",
    "href": "slides/session_2/index.html#creating-a-new-column",
    "title": "Dataframes",
    "section": "Creating a new column",
    "text": "Creating a new column\n\nIt is possible to create a new column by combining existing ones\n\ndf['totalLength'] = df['pLength'] + df['sLength']\n# this would also work\ndf['totalLength'] = 0.5*df['pLength'] + 0.5*df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.5\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.3",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#replacing-a-column",
    "href": "slides/session_2/index.html#replacing-a-column",
    "title": "Dataframes",
    "section": "Replacing a column",
    "text": "Replacing a column\n\nAn existing column can be replaced with the same syntax.\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.95\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.85",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#selecting-several-columns",
    "href": "slides/session_2/index.html#selecting-several-columns",
    "title": "Dataframes",
    "section": "Selecting several columns",
    "text": "Selecting several columns\n\nIndex with a list of column names\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\npLength\n\n\nsLength\n\n\n\n\n\n\n0\n\n\n1.4\n\n\n5.1\n\n\n\n\n1\n\n\n1.4\n\n\n4.9\n\n\n\n\n2\n\n\n1.3\n\n\n4.7",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#selecting-lines-1",
    "href": "slides/session_2/index.html#selecting-lines-1",
    "title": "Dataframes",
    "section": "Selecting lines (1)",
    "text": "Selecting lines (1)\n\nuse index range\n\n☡: in Python the end of a range is not included !\n\ndf[2:4]\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n3.65\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n3.80",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#selecting-lines-2",
    "href": "slides/session_2/index.html#selecting-lines-2",
    "title": "Dataframes",
    "section": "Selecting lines (2)",
    "text": "Selecting lines (2)\n\nlet’s look at unique species\n\ndf['species'].unique()\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\nwe would like to keep only the lines with virginica\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\nthe result is a boolean series, where each element tells whether a line should be kept or not\n\ne = df[ bool_ind ]\ne.head(4)\n\nif you want you can keep the recipe:\n\ndf[df['species'] == 'virginica']\n\nto keep lines where species is equal to virginica\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n100\n\n\n6.3\n\n\n3.3\n\n\n6.0\n\n\n2.5\n\n\nvirginica\n\n\n9.15\n\n\n\n\n101\n\n\n5.8\n\n\n2.7\n\n\n5.1\n\n\n1.9\n\n\nvirginica\n\n\n8.00\n\n\n\n\n102\n\n\n7.1\n\n\n3.0\n\n\n5.9\n\n\n2.1\n\n\nvirginica\n\n\n9.45\n\n\n\n\n103\n\n\n6.3\n\n\n2.9\n\n\n5.6\n\n\n1.8\n\n\nvirginica\n\n\n8.75",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#selecting-lines-and-columns",
    "href": "slides/session_2/index.html#selecting-lines-and-columns",
    "title": "Dataframes",
    "section": "Selecting lines and columns",
    "text": "Selecting lines and columns\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\ndf.loc[0:4, 'species']\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#combine-everything",
    "href": "slides/session_2/index.html#combine-everything",
    "title": "Dataframes",
    "section": "Combine everything",
    "text": "Combine everything\n\nHere is an example combiing serveral techniques\n\nLet’s change the way totalLength is computed, but only for ‘virginica’\n\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#section-1",
    "href": "slides/session_2/index.html#section-1",
    "title": "Dataframes",
    "section": "",
    "text": "The following code creates two example databases.\ntxt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n71\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n136\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#wide-vs-long-format-1",
    "href": "slides/session_2/index.html#wide-vs-long-format-1",
    "title": "Dataframes",
    "section": "Wide vs Long format (1)",
    "text": "Wide vs Long format (1)\nCompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\nyear\n\n\nfrance\n\n\nusa\n\n\n\n\n\n\n0\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n1\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#wide-vs-long-format-2",
    "href": "slides/session_2/index.html#wide-vs-long-format-2",
    "title": "Dataframes",
    "section": "Wide vs Long format (2)",
    "text": "Wide vs Long format (2)\n\nin long format: each line is an independent observation\n\ntwo lines may belong to the same category (year, or country)\nall values are given in the same column\ntheir types/categories are given in another column\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\nvalues of different kinds are in different columns\nthe types/categories are stored as column names\n\nboth representations are useful",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#tidy-data",
    "href": "slides/session_2/index.html#tidy-data",
    "title": "Dataframes",
    "section": "Tidy data:",
    "text": "Tidy data:\n\ntidy data:\n\nevery column is a variable.\nevery row is an observation.\nevery cell is a single value.\n\na very good format for:\n\nquick visualization\ndata analysis",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#converting-from-wide-to-long",
    "href": "slides/session_2/index.html#converting-from-wide-to-long",
    "title": "Dataframes",
    "section": "Converting from Wide to Long",
    "text": "Converting from Wide to Long\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\nyear\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#converting-from-long-to-wide",
    "href": "slides/session_2/index.html#converting-from-long-to-wide",
    "title": "Dataframes",
    "section": "Converting from Long to Wide",
    "text": "Converting from Long to Wide\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\nmeasure\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#groupby",
    "href": "slides/session_2/index.html#groupby",
    "title": "Dataframes",
    "section": "groupby",
    "text": "groupby\n\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\nfor instance to compute averages per country\n\ndf_long.groupby(\"country\").mean()\n\n\n\n\n\n\n\n\n\nyear\n\n\nmeasure\n\n\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\nfrance\n\n\n2019.0\n\n\n970.000000\n\n\n\n\nusa\n\n\n2019.0\n\n\n2316.666667\n\n\n\n\n\n\nYou can perform several aggregations at the same time:\n\ndf_long.groupby(\"country\").agg(['mean','std'])",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#merging-two-dataframes",
    "href": "slides/session_2/index.html#merging-two-dataframes",
    "title": "Dataframes",
    "section": "Merging two dataframes",
    "text": "Merging two dataframes\n\nSuppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcat if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time\n\n\n\ntxt_long_1 = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen(\"dummy_long_1.csv\",'w').write(txt_long_1)\ntxt_long_2 = \"\"\"year,country,recipient\n2018,\"france\",maxime\n2019,\"france\",mauricette\n2020,\"france\",mathilde\n2018,\"usa\",sherlock\n2019,\"usa\",watson\n2020,\"usa\",moriarty\n\"\"\"\nopen(\"dummy_long_2.csv\",'w').write(txt_long_2)\ndf_long_1 = pd.read_csv('dummy_long_1.csv')\ndf_long_2 = pd.read_csv('dummy_long_2.csv')",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/index.html#merging-two-dataframes-with-pandas",
    "href": "slides/session_2/index.html#merging-two-dataframes-with-pandas",
    "title": "Dataframes",
    "section": "Merging two DataFrames with pandas",
    "text": "Merging two DataFrames with pandas\ndf_long_1.merge(df_long_2)\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\nrecipient\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\nmaxime\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\nmauricette\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\nmathilde\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\nsherlock\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\nwatson\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\nmoriarty",
    "crumbs": [
      "lectures",
      "Dataframes"
    ]
  },
  {
    "objectID": "slides/session_2/slides.html#dataframe",
    "href": "slides/session_2/slides.html#dataframe",
    "title": "Dataframes",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nA DataFrame (aka a table) is a 2-D labeled data structure with columns\n\neach column has a specific type and a column name\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\nfirst goal of an econometrician: constitute a good dataframe\n\naka “cleaning the data”"
  },
  {
    "objectID": "slides/session_2/slides.html#dataframes-are-everywhere",
    "href": "slides/session_2/slides.html#dataframes-are-everywhere",
    "title": "Dataframes",
    "section": "DataFrames are everywhere",
    "text": "DataFrames are everywhere\n\n\n\n\n\nsometimes data comes from several linked dataframes\n\nrelational database\ncan still be seen conceptually as one dataframe…\n… through a join operation\n\n\n\n\n\ndataframes / relational databases are so ubiquitous a language has been developed for them\n\nSQL\nin the 80s…\n\nprobably worth looking at if you have some “data” ambitions\nyou will see the shadow of SQL everywhere\nplenty of resources to learn (example: sqbolt)"
  },
  {
    "objectID": "slides/session_2/slides.html#pandas-1",
    "href": "slides/session_2/slides.html#pandas-1",
    "title": "Dataframes",
    "section": "pandas",
    "text": "pandas\n\npandas = panel + datas\n\na python library created by WesMcKinney\nvery optimized\n\nessentially a dataframe object\nmany options but if in doubt:\n\nminimally sufficient pandas is asmall subset of pandas to do everything\n\ntons of online tutorials\n\nofficial documenation doc\nquantecon"
  },
  {
    "objectID": "slides/session_2/slides.html#creating-a-dataframe-1",
    "href": "slides/session_2/slides.html#creating-a-dataframe-1",
    "title": "Dataframes",
    "section": "creating a dataframe (1)",
    "text": "creating a dataframe (1)\n\n\n\nImport pandas\n\npreferably with standard alias pd\n\nimport pandas as pd\nImport a dataframe\n\neach line a different entry in a dictionary\n\n# from a dictionary\nd = {\n  \"country\": [\"USA\", \"UK\", \"France\"],\n  \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\n\n\ncountry\n\n\ncomics\n\n\n\n\n\n\n0\n\n\nUSA\n\n\n13\n\n\n\n\n1\n\n\nUK\n\n\n10\n\n\n\n\n2\n\n\nFrance\n\n\n12"
  },
  {
    "objectID": "slides/session_2/slides.html#creating-a-dataframe-2",
    "href": "slides/session_2/slides.html#creating-a-dataframe-2",
    "title": "Dataframes",
    "section": "creating a dataframe (2)",
    "text": "creating a dataframe (2)\n\nthere are many other ways to create a dataframe\n\nfor instance using a numpy matrix (numpy is a linear algebra library)\n\n# from a matrix\nimport numpy as np\nM = np.array(\n    [[18, 150],\n     [21, 200],\n     [29, 1500]]\n)   \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\n\nage\n\n\ntravel\n\n\n\n\n\n\n0\n\n\n18\n\n\n150\n\n\n\n\n1\n\n\n21\n\n\n200\n\n\n\n\n2\n\n\n29\n\n\n1500"
  },
  {
    "objectID": "slides/session_2/slides.html#common-file-formats",
    "href": "slides/session_2/slides.html#common-file-formats",
    "title": "Dataframes",
    "section": "Common file formats",
    "text": "Common file formats\n\ncomma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\nnote that excel does not store a dataframe (each cell is potentially different)\npostprocessing is needed"
  },
  {
    "objectID": "slides/session_2/slides.html#comma-separated-file",
    "href": "slides/session_2/slides.html#comma-separated-file",
    "title": "Dataframes",
    "section": "Comma separated file",
    "text": "Comma separated file\n\none can actually a file from python\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\nand import it\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0"
  },
  {
    "objectID": "slides/session_2/slides.html#annoying-comma-separated-file",
    "href": "slides/session_2/slides.html#annoying-comma-separated-file",
    "title": "Dataframes",
    "section": "“Annoying” Comma Separated File",
    "text": "“Annoying” Comma Separated File\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\n\nthe kind of separator, whether there are quotes…\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\nadd relevant options to pd.read_csv and check result\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0"
  },
  {
    "objectID": "slides/session_2/slides.html#exporting-a-dataframe",
    "href": "slides/session_2/slides.html#exporting-a-dataframe",
    "title": "Dataframes",
    "section": "Exporting a DataFrame",
    "text": "Exporting a DataFrame\n\npandas can export to many formats: df.to_...\nto (standard) CSV\n\nprint( df.to_csv() )\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\nor to stata\n\ndf.to_stata('dummy_example.dta')"
  },
  {
    "objectID": "slides/session_2/slides.html#types-of-data-sources",
    "href": "slides/session_2/slides.html#types-of-data-sources",
    "title": "Dataframes",
    "section": "Types of Data Sources",
    "text": "Types of Data Sources\n\n\n\nWhere can we get data from ?\n\ncheck one of the databases lists kaggle, econ network\n\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)"
  },
  {
    "objectID": "slides/session_2/slides.html#data-providers",
    "href": "slides/session_2/slides.html#data-providers",
    "title": "Dataframes",
    "section": "Data providers",
    "text": "Data providers\n\ncommercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones available as a python library\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair"
  },
  {
    "objectID": "slides/session_2/slides.html#dbnomics-example",
    "href": "slides/session_2/slides.html#dbnomics-example",
    "title": "Dataframes",
    "section": "DBnomics example",
    "text": "DBnomics example\n\nDBnomics aggregates time series from various public sources\ndata is organized as provider/database/series\ntry to find the identifer of one or several series\n\nimport dbnomics\ndf = dbnomics.fetch_series('AMECO/ZUTN/EA19.1.0.0.0.ZUTN')\n\ntip: in case one python package is missing, it can be installed on the fly as in\n\n!pip install dbnomics"
  },
  {
    "objectID": "slides/session_2/slides.html#inspecting-data",
    "href": "slides/session_2/slides.html#inspecting-data",
    "title": "Dataframes",
    "section": "Inspecting data",
    "text": "Inspecting data\n\nonce the data is loaded as df, we want to look at some basic properties:\ngeneral\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # general summary\n\ncentral tendency\n\ndf.mean() # average\ndf.median() # median\n\nspread\n\ndf.std() # standard deviations\ndf.var() # variance\ndf.min(), df.max() # bounds\n\ncounts (for categorical variable\n\ndf.count()"
  },
  {
    "objectID": "slides/session_2/slides.html#changing-names-of-columns",
    "href": "slides/session_2/slides.html#changing-names-of-columns",
    "title": "Dataframes",
    "section": "Changing names of columns",
    "text": "Changing names of columns\n\nColumns are defined by property df.columns\n\ndf.columns\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\nThis property can be set with a list of the right length\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa"
  },
  {
    "objectID": "slides/session_2/slides.html#indexing-a-column",
    "href": "slides/session_2/slides.html#indexing-a-column",
    "title": "Dataframes",
    "section": "Indexing a column",
    "text": "Indexing a column\n\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n0      3.5\n1      3.0\n      ... \n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\nThe result is a series object (typed values with a name and an index)\nIt has its own set of methods\n\ntry:\n\nseries.mean(), series.std()\nseries.plot()\nseries.diff()\n\ncreates \\(y_t = x_t-x_{t-1}\\)\n\nseries.pct_change()\n\ncreates \\(y_t = \\frac{x_t-x_{t-1}}{x_{t-1}}\\)"
  },
  {
    "objectID": "slides/session_2/slides.html#creating-a-new-column",
    "href": "slides/session_2/slides.html#creating-a-new-column",
    "title": "Dataframes",
    "section": "Creating a new column",
    "text": "Creating a new column\n\nIt is possible to create a new column by combining existing ones\n\ndf['totalLength'] = df['pLength'] + df['sLength']\n# this would also work\ndf['totalLength'] = 0.5*df['pLength'] + 0.5*df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.5\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n6.3"
  },
  {
    "objectID": "slides/session_2/slides.html#replacing-a-column",
    "href": "slides/session_2/slides.html#replacing-a-column",
    "title": "Dataframes",
    "section": "Replacing a column",
    "text": "Replacing a column\n\nAn existing column can be replaced with the same syntax.\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.95\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n3.85"
  },
  {
    "objectID": "slides/session_2/slides.html#selecting-several-columns",
    "href": "slides/session_2/slides.html#selecting-several-columns",
    "title": "Dataframes",
    "section": "Selecting several columns",
    "text": "Selecting several columns\n\nIndex with a list of column names\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\npLength\n\n\nsLength\n\n\n\n\n\n\n0\n\n\n1.4\n\n\n5.1\n\n\n\n\n1\n\n\n1.4\n\n\n4.9\n\n\n\n\n2\n\n\n1.3\n\n\n4.7"
  },
  {
    "objectID": "slides/session_2/slides.html#selecting-lines-1",
    "href": "slides/session_2/slides.html#selecting-lines-1",
    "title": "Dataframes",
    "section": "Selecting lines (1)",
    "text": "Selecting lines (1)\n\nuse index range\n\n☡: in Python the end of a range is not included !\n\ndf[2:4]\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n3.65\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n3.80"
  },
  {
    "objectID": "slides/session_2/slides.html#selecting-lines-2",
    "href": "slides/session_2/slides.html#selecting-lines-2",
    "title": "Dataframes",
    "section": "Selecting lines (2)",
    "text": "Selecting lines (2)\n\nlet’s look at unique species\n\ndf['species'].unique()\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\nwe would like to keep only the lines with virginica\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\nthe result is a boolean series, where each element tells whether a line should be kept or not\n\ne = df[ bool_ind ]\ne.head(4)\n\nif you want you can keep the recipe:\n\ndf[df['species'] == 'virginica']\n\nto keep lines where species is equal to virginica\n\n\n\n\n\n\n\n\n\nsLength\n\n\nsWidth\n\n\npLength\n\n\npWidth\n\n\nspecies\n\n\ntotalLength\n\n\n\n\n\n\n100\n\n\n6.3\n\n\n3.3\n\n\n6.0\n\n\n2.5\n\n\nvirginica\n\n\n9.15\n\n\n\n\n101\n\n\n5.8\n\n\n2.7\n\n\n5.1\n\n\n1.9\n\n\nvirginica\n\n\n8.00\n\n\n\n\n102\n\n\n7.1\n\n\n3.0\n\n\n5.9\n\n\n2.1\n\n\nvirginica\n\n\n9.45\n\n\n\n\n103\n\n\n6.3\n\n\n2.9\n\n\n5.6\n\n\n1.8\n\n\nvirginica\n\n\n8.75"
  },
  {
    "objectID": "slides/session_2/slides.html#selecting-lines-and-columns",
    "href": "slides/session_2/slides.html#selecting-lines-and-columns",
    "title": "Dataframes",
    "section": "Selecting lines and columns",
    "text": "Selecting lines and columns\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\ndf.loc[0:4, 'species']\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object"
  },
  {
    "objectID": "slides/session_2/slides.html#combine-everything",
    "href": "slides/session_2/slides.html#combine-everything",
    "title": "Dataframes",
    "section": "Combine everything",
    "text": "Combine everything\n\nHere is an example combiing serveral techniques\n\nLet’s change the way totalLength is computed, but only for ‘virginica’\n\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']"
  },
  {
    "objectID": "slides/session_2/slides.html#section-1",
    "href": "slides/session_2/slides.html#section-1",
    "title": "Dataframes",
    "section": "",
    "text": "The following code creates two example databases.\ntxt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n71\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n136\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")"
  },
  {
    "objectID": "slides/session_2/slides.html#wide-vs-long-format-1",
    "href": "slides/session_2/slides.html#wide-vs-long-format-1",
    "title": "Dataframes",
    "section": "Wide vs Long format (1)",
    "text": "Wide vs Long format (1)\nCompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\nyear\n\n\nfrance\n\n\nusa\n\n\n\n\n\n\n0\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n1\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0"
  },
  {
    "objectID": "slides/session_2/slides.html#wide-vs-long-format-2",
    "href": "slides/session_2/slides.html#wide-vs-long-format-2",
    "title": "Dataframes",
    "section": "Wide vs Long format (2)",
    "text": "Wide vs Long format (2)\n\nin long format: each line is an independent observation\n\ntwo lines may belong to the same category (year, or country)\nall values are given in the same column\ntheir types/categories are given in another column\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\nvalues of different kinds are in different columns\nthe types/categories are stored as column names\n\nboth representations are useful"
  },
  {
    "objectID": "slides/session_2/slides.html#tidy-data",
    "href": "slides/session_2/slides.html#tidy-data",
    "title": "Dataframes",
    "section": "Tidy data:",
    "text": "Tidy data:\n\ntidy data:\n\nevery column is a variable.\nevery row is an observation.\nevery cell is a single value.\n\na very good format for:\n\nquick visualization\ndata analysis"
  },
  {
    "objectID": "slides/session_2/slides.html#converting-from-wide-to-long",
    "href": "slides/session_2/slides.html#converting-from-wide-to-long",
    "title": "Dataframes",
    "section": "Converting from Wide to Long",
    "text": "Converting from Wide to Long\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\nyear\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0"
  },
  {
    "objectID": "slides/session_2/slides.html#converting-from-long-to-wide",
    "href": "slides/session_2/slides.html#converting-from-long-to-wide",
    "title": "Dataframes",
    "section": "Converting from Long to Wide",
    "text": "Converting from Long to Wide\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\nmeasure\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\ncountry\n\n\nfrance\n\n\nusa\n\n\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n950.0\n\n\n2500.0\n\n\n\n\n2019\n\n\n960.0\n\n\n2150.0\n\n\n\n\n2020\n\n\n1000.0\n\n\n2300.0"
  },
  {
    "objectID": "slides/session_2/slides.html#groupby",
    "href": "slides/session_2/slides.html#groupby",
    "title": "Dataframes",
    "section": "groupby",
    "text": "groupby\n\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\nfor instance to compute averages per country\n\ndf_long.groupby(\"country\").mean()\n\n\n\n\n\n\n\n\n\nyear\n\n\nmeasure\n\n\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\nfrance\n\n\n2019.0\n\n\n970.000000\n\n\n\n\nusa\n\n\n2019.0\n\n\n2316.666667\n\n\n\n\n\n\nYou can perform several aggregations at the same time:\n\ndf_long.groupby(\"country\").agg(['mean','std'])"
  },
  {
    "objectID": "slides/session_2/slides.html#merging-two-dataframes",
    "href": "slides/session_2/slides.html#merging-two-dataframes",
    "title": "Dataframes",
    "section": "Merging two dataframes",
    "text": "Merging two dataframes\n\nSuppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcat if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time"
  },
  {
    "objectID": "slides/session_2/slides.html#merging-two-dataframes-with-pandas",
    "href": "slides/session_2/slides.html#merging-two-dataframes-with-pandas",
    "title": "Dataframes",
    "section": "Merging two DataFrames with pandas",
    "text": "Merging two DataFrames with pandas\ndf_long_1.merge(df_long_2)\n\n\n\n\n\n\n\n\nyear\n\n\ncountry\n\n\nmeasure\n\n\nrecipient\n\n\n\n\n\n\n0\n\n\n2018\n\n\nfrance\n\n\n950.0\n\n\nmaxime\n\n\n\n\n1\n\n\n2019\n\n\nfrance\n\n\n960.0\n\n\nmauricette\n\n\n\n\n2\n\n\n2020\n\n\nfrance\n\n\n1000.0\n\n\nmathilde\n\n\n\n\n3\n\n\n2018\n\n\nusa\n\n\n2500.0\n\n\nsherlock\n\n\n\n\n4\n\n\n2019\n\n\nusa\n\n\n2150.0\n\n\nwatson\n\n\n\n\n5\n\n\n2020\n\n\nusa\n\n\n2300.0\n\n\nmoriarty"
  },
  {
    "objectID": "slides/session_2/Exercises.html",
    "href": "slides/session_2/Exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\nDefine a vector y representing \\(y=sin(x)\\)\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\nPlot y, y1, y2 against x\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\nCreate an empty vector x = np.zeros(T)\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\nUse function hpfilter from statsmodels (google it). It returns a and a residual\nPlot the simulated series, the filtered series and the residual\n\n\n\n\n\nYou will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\nPrint statistics (mean, std), by flower, for each characteristics.\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\nConvert the database to long format\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations.\n\n\n\n\nIf needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\nCompute growth rate of gdp.\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output)."
  },
  {
    "objectID": "slides/session_2/Exercises.html#numerical-python",
    "href": "slides/session_2/Exercises.html#numerical-python",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\nDefine a vector y representing \\(y=sin(x)\\)\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\nPlot y, y1, y2 against x\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\nCreate an empty vector x = np.zeros(T)\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\nUse function hpfilter from statsmodels (google it). It returns a and a residual\nPlot the simulated series, the filtered series and the residual"
  },
  {
    "objectID": "slides/session_2/Exercises.html#iris-data-set",
    "href": "slides/session_2/Exercises.html#iris-data-set",
    "title": "Exercises",
    "section": "",
    "text": "You will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\nPrint statistics (mean, std), by flower, for each characteristics.\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\nConvert the database to long format\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations."
  },
  {
    "objectID": "slides/session_2/Exercises.html#philips-curve-and-okuns-law",
    "href": "slides/session_2/Exercises.html#philips-curve-and-okuns-law",
    "title": "Exercises",
    "section": "",
    "text": "If needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\nCompute growth rate of gdp.\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output)."
  },
  {
    "objectID": "homework/2024/homework.html",
    "href": "homework/2024/homework.html",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Students (up to 3):\n\n\n\n\nWhen working on the questions below, don’t hesitate to take some initiatives. In particular, if you don’t find how to answer a particular question (and you have asked 😉), feel free to propose a workaround.\nYour work will be evaluated in the following dimensions:\n\nWhether your notebook is replicable. When grading it, I should be able to run it from start to finish without error.\nWhether it is well written and clear. There should always be legible text to explain what you do, and make it a nice read. Imagine that the document was meant to be published as an online tutorial.\nWhether you have successfully solved the various theoretical and practical problems that are asked below.\nWhether you have shown some sense of initiative in approaching the various problems, in making the plots or in proposing extensions.\n\n\nThis homework is partly based on one of the projects from the excellent Doing economics section of the Core Econ website.\nDon’t hesitate to browse the website to get more context. You can even check the solutions in R to get an idea of what you can do.\n\n\nThe data originates from the EVS project (https://search.gesis.org/research_data/ZA4804). It has been used in the article Employment status and subjective well-being: the role of the social norm to work.\nLoosely following the spirit of the authors work, our goal will be to investigate one channel through wich unemployement can lead to lower life satisfaction. The general hypothesis is that social norms are key to the subjective disutility of being unemployed. In this notebook we focus on one particular called work ethic.\n\nimport pandas as pd\n\nQuestion 1\nOpen the economics.xlsx workbook with excel. Fill the first sheet using the ZA4804_EVS_VariableCorrespondence.pdf file. Upload the resulting file and run the following code to import the data.\n\n# the following imports the variable correspondances.\nvariables = pd.read_excel(\"economics.xlsx\", sheet_name=0)\n\n\n# check the variable correspondances\nvariables\n\n\n\n\n\n\n\n\nVariable\nNew name\nVariable description\n\n\n\n\n0\nS002EVS\nNaN\nNaN\n\n\n1\nS003\nNaN\nNaN\n\n\n2\nS006\nNaN\nNaN\n\n\n3\nS009\nNaN\nNaN\n\n\n4\nA009\nNaN\nNaN\n\n\n5\nA170\nNaN\nNaN\n\n\n6\nC036\nNaN\nNaN\n\n\n7\nC037\nNaN\nNaN\n\n\n8\nC038\nNaN\nNaN\n\n\n9\nC039\nNaN\nNaN\n\n\n10\nC041\nNaN\nNaN\n\n\n11\nX001\nNaN\nNaN\n\n\n12\nX003\nNaN\nNaN\n\n\n13\nX007\nNaN\nNaN\n\n\n14\nX011_01\nNaN\nNaN\n\n\n15\nX025A\nNaN\nNaN\n\n\n16\nX028\nNaN\nNaN\n\n\n17\nX047D\nNaN\nNaN\n\n\n\n\n\n\n\n\n# we now open the datasets corresponding to the various waves of the study\nsheets = {\n   '8184': pd.read_excel(\"economics.xlsx\", sheet_name=1), # wave 1\n   '9093': pd.read_excel(\"economics.xlsx\", sheet_name=2), # wave 2\n   '9901': pd.read_excel(\"economics.xlsx\", sheet_name=3), # wave 3\n   '0810': pd.read_excel(\"economics.xlsx\", sheet_name=4) # wave 4\n}\n\n\n# we concatenate all sheets into a single dataframe\ndf = pd.concat(sheets, names=['wave'], ignore_index=True)\n\nQuestion 2: describe the dataset. Intuitively, which variables would you associate with a higher disutility of unemployment?\n\n\n\nOptional: Change the colum names of df into more meaningful identifiers\nThis will make the code easier to read.\nNote that in the rest of the notebook we use indifferently the variable codes or the extended names.\nQuestion 3: Perform the following cleaning operations:\n\nCurrently all missing values are coded as “.a”. Replace them by pd.NA\nVariable A170 (life satisfaction) is currently a mixture of numbers (2 to 9) and words (‘Satisfied’ and ‘Dissatisfied’), but we would like it to be all numbers. Replace the word ‘Dissatisfied’ with the number 1, and the word ‘Satisfied’ with the number 10.\nVariable X011_01 (number of children) has recorded no children as a word rather than a number. Replace ‘No children’ with the number 0.\nThe variables C036 to C041 should be replaced with numbers ranging from 1 (‘Strongly disagree’) to 5 (‘Strongly agree’) so we can take averages of them later. Similarly, variable A009 should be recoded as 1 = ‘Very poor’, 2 = ‘Poor’, 3 = ‘Fair’, 4 = ‘Good’, 5 = ‘Very good’.\nbonus: Split X025A into two variables, one for the number before the colon, and the other containing the words after the colon.\n\n\ndf[df==\".a\"] = pd.NA\n\n\ndf.loc[df['A170']=='Satisfied','A170'] = 10\ndf.loc[df['A170']=='Dissatisfied','A170'] = 1\n\n\ndf.loc[df['X011_01']=='No children','X011_01'] = 0\n\n\ndf[df==\"Strongly disagree\"] = 1\ndf[df==\"Disagree\"] = 2\ndf[df==\"Neither agree nor disagree\"] = 3\ndf[df==\"Agree\"] = 4\ndf[df==\"Strongly agree\"] = 5\n\n\ndf[df==\"Very poor\"] = 1\ndf[df==\"Poor\"] = 2\ndf[df==\"Fair\"] = 3\ndf[df==\"Good\"] = 4\ndf[df==\"Very good\"] = 5\n\n\ndf['A009'].unique()\n\narray([3, 5, 2, 4, 1, &lt;NA&gt;], dtype=object)\n\n\nQuestion 4: Remove missing values\nIn your dataset, remove all rows in all waves that have missing data for A170. Do the same for:\n\nX003, X028, X007 and X001 in all waves\nA009 in Waves 1, 2, and 4 only\nC036, C037, C038, C039, C041 and X047D in Waves 3 and 4 only\nX011_01 and X025A, in Wave 4.\n\nQuestion 5: Create a new variable work_ethic as the average of columns C036 to C041.\nBonus: Create a variable relative_income which contains income divided by the average income from the relevant country. Why is that a better variable than raw income?\n\n\n\nQuestion 6: First cross-country comparison.\nCreate a table showing the breakdown of each country’s population according to employment status, with country (S003) as the row variable, and employment status (X028) as the column variable. Express the values as percentages of the row total rather than as frequencies. Discuss any differences or similarities between countries that you find interesting.\nQuestion 7: Create a table with descriptive statistics aranged after the following layout. Comment.\n\n\n\n\nQuestion 8: the evolution of work ethic.\nUse the data from Wave 3 and Wave 4 only, for one country of your choice: - For this country create a frequency table that contains the frequency of each unique value of the work ethic scores. Also include the percentage of individuals at each value, grouped by Wave 3 and Wave 4 separately. - Plot a column chart showing the distribution of work ethic scores in Wave 3, with the percentage of individuals on the vertical axis and the range of work ethic scores on the horizontal axis. Plot the distribution of scores in Wave 4 on top of the Wave 3 distribution. - Based on your chart does it appear that the attitudes towards work in each country of your choice have changed over time?\nBonus: do the same for another country of your choice and compare.\nQuestion 9: Replicate the same analysis for life satisfaction.\nQuestion 10: for Wave 4 only, compute the correlations of the main variables with life satisfaction and work ethic. It should correspond to the following pattern:\n\nFor employment status and gender, you will need to create new variables: full-time employment should be equal to 1 if full-time employed and 0 if unemployed, and treated as missing data (left as a blank cell) otherwise. Gender should be 0 if male and 1 if female.\nInterpret the coefficients.\nQuestion 11 Using the data from Wave 4, carry out the following\n\nCreate a table showing the average life satisfaction according to employment status (showing the full-time employed, retired, and unemployed categories only) with country (S003) as the row variable, and employment status (X028) as the column variable. Comment on any differences in average life satisfaction between these three groups, and whether social norms is a plausible explanation for these differences.\nUse the table from Question 4(a) to calculate the difference in average life satisfaction (full-time employed minus unemployed, and full-time employed minus retired).\nMake a separate scatterplot for each of these differences in life satisfaction, with average work ethic on the horizontal axis and difference in life satisfaction on the vertical axis.\nFor each difference (employed vs unemployed, employed vs retired), calculate and interpret the correlation coefficient between average work ethic and difference in life satisfaction.\n\n\n\n\nQuestion 12: using only wave 4, we consider first the following linear regression\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n\\)\nRun this regression without any other regressor. Comment on the result.\nQuestion 13: we consider now the regression\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n +  \\text{other regressors}\\)\nWhich regressors would you choose? Can you find the combination that maximizes the predictive power of the regression?\nQuestion 14: the effect of work ethics\nFrom the preceding regression can we cannot conclude about the interaction between work ethics and unemployment.\nTry to answer that question by running the following steps - create one variable high_work_ethic, equal to 1, if work ethic is above average - run the regression from before for two different dataset, one where high_work_ethic is equal to 1, and another where it is equal to 0 - compare the values obtained for \\(\\beta\\) in both cases - interpret - bonus: make a graphical representation\nQuestion 15: interaction term\nAnother common approach consists in running a regression with an interacion term, that is\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n +\\delta  \\text{unemployment}_n \\times \\text{work ethic}_n+  \\text{other regressors}\\)\nThen if \\(\\delta\\) is negative, higher work ethic implies a stronger effect of unemployment.\nRun that regression and comment.\n(hint: for statsmodels formula, a*b where a and b are regressors is interpreted as a regression part \\(c_1 a+ c_2 b+ c_3 a b\\) where \\(c_1,c_2,c_3\\) are unknown coefficients)\n\n\n\nQuestion 16: propose any another data work using the same dataframe that you find interesting (graph, econometric analysis, machine learning)"
  },
  {
    "objectID": "homework/2024/homework.html#import-the",
    "href": "homework/2024/homework.html#import-the",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "The data originates from the EVS project (https://search.gesis.org/research_data/ZA4804). It has been used in the article Employment status and subjective well-being: the role of the social norm to work.\nLoosely following the spirit of the authors work, our goal will be to investigate one channel through wich unemployement can lead to lower life satisfaction. The general hypothesis is that social norms are key to the subjective disutility of being unemployed. In this notebook we focus on one particular called work ethic.\n\nimport pandas as pd\n\nQuestion 1\nOpen the economics.xlsx workbook with excel. Fill the first sheet using the ZA4804_EVS_VariableCorrespondence.pdf file. Upload the resulting file and run the following code to import the data.\n\n# the following imports the variable correspondances.\nvariables = pd.read_excel(\"economics.xlsx\", sheet_name=0)\n\n\n# check the variable correspondances\nvariables\n\n\n\n\n\n\n\n\nVariable\nNew name\nVariable description\n\n\n\n\n0\nS002EVS\nNaN\nNaN\n\n\n1\nS003\nNaN\nNaN\n\n\n2\nS006\nNaN\nNaN\n\n\n3\nS009\nNaN\nNaN\n\n\n4\nA009\nNaN\nNaN\n\n\n5\nA170\nNaN\nNaN\n\n\n6\nC036\nNaN\nNaN\n\n\n7\nC037\nNaN\nNaN\n\n\n8\nC038\nNaN\nNaN\n\n\n9\nC039\nNaN\nNaN\n\n\n10\nC041\nNaN\nNaN\n\n\n11\nX001\nNaN\nNaN\n\n\n12\nX003\nNaN\nNaN\n\n\n13\nX007\nNaN\nNaN\n\n\n14\nX011_01\nNaN\nNaN\n\n\n15\nX025A\nNaN\nNaN\n\n\n16\nX028\nNaN\nNaN\n\n\n17\nX047D\nNaN\nNaN\n\n\n\n\n\n\n\n\n# we now open the datasets corresponding to the various waves of the study\nsheets = {\n   '8184': pd.read_excel(\"economics.xlsx\", sheet_name=1), # wave 1\n   '9093': pd.read_excel(\"economics.xlsx\", sheet_name=2), # wave 2\n   '9901': pd.read_excel(\"economics.xlsx\", sheet_name=3), # wave 3\n   '0810': pd.read_excel(\"economics.xlsx\", sheet_name=4) # wave 4\n}\n\n\n# we concatenate all sheets into a single dataframe\ndf = pd.concat(sheets, names=['wave'], ignore_index=True)\n\nQuestion 2: describe the dataset. Intuitively, which variables would you associate with a higher disutility of unemployment?"
  },
  {
    "objectID": "homework/2024/homework.html#prepare-the-data",
    "href": "homework/2024/homework.html#prepare-the-data",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Optional: Change the colum names of df into more meaningful identifiers\nThis will make the code easier to read.\nNote that in the rest of the notebook we use indifferently the variable codes or the extended names.\nQuestion 3: Perform the following cleaning operations:\n\nCurrently all missing values are coded as “.a”. Replace them by pd.NA\nVariable A170 (life satisfaction) is currently a mixture of numbers (2 to 9) and words (‘Satisfied’ and ‘Dissatisfied’), but we would like it to be all numbers. Replace the word ‘Dissatisfied’ with the number 1, and the word ‘Satisfied’ with the number 10.\nVariable X011_01 (number of children) has recorded no children as a word rather than a number. Replace ‘No children’ with the number 0.\nThe variables C036 to C041 should be replaced with numbers ranging from 1 (‘Strongly disagree’) to 5 (‘Strongly agree’) so we can take averages of them later. Similarly, variable A009 should be recoded as 1 = ‘Very poor’, 2 = ‘Poor’, 3 = ‘Fair’, 4 = ‘Good’, 5 = ‘Very good’.\nbonus: Split X025A into two variables, one for the number before the colon, and the other containing the words after the colon.\n\n\ndf[df==\".a\"] = pd.NA\n\n\ndf.loc[df['A170']=='Satisfied','A170'] = 10\ndf.loc[df['A170']=='Dissatisfied','A170'] = 1\n\n\ndf.loc[df['X011_01']=='No children','X011_01'] = 0\n\n\ndf[df==\"Strongly disagree\"] = 1\ndf[df==\"Disagree\"] = 2\ndf[df==\"Neither agree nor disagree\"] = 3\ndf[df==\"Agree\"] = 4\ndf[df==\"Strongly agree\"] = 5\n\n\ndf[df==\"Very poor\"] = 1\ndf[df==\"Poor\"] = 2\ndf[df==\"Fair\"] = 3\ndf[df==\"Good\"] = 4\ndf[df==\"Very good\"] = 5\n\n\ndf['A009'].unique()\n\narray([3, 5, 2, 4, 1, &lt;NA&gt;], dtype=object)\n\n\nQuestion 4: Remove missing values\nIn your dataset, remove all rows in all waves that have missing data for A170. Do the same for:\n\nX003, X028, X007 and X001 in all waves\nA009 in Waves 1, 2, and 4 only\nC036, C037, C038, C039, C041 and X047D in Waves 3 and 4 only\nX011_01 and X025A, in Wave 4.\n\nQuestion 5: Create a new variable work_ethic as the average of columns C036 to C041.\nBonus: Create a variable relative_income which contains income divided by the average income from the relevant country. Why is that a better variable than raw income?"
  },
  {
    "objectID": "homework/2024/homework.html#summary-statistics",
    "href": "homework/2024/homework.html#summary-statistics",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Question 6: First cross-country comparison.\nCreate a table showing the breakdown of each country’s population according to employment status, with country (S003) as the row variable, and employment status (X028) as the column variable. Express the values as percentages of the row total rather than as frequencies. Discuss any differences or similarities between countries that you find interesting.\nQuestion 7: Create a table with descriptive statistics aranged after the following layout. Comment."
  },
  {
    "objectID": "homework/2024/homework.html#visualizing-the-data",
    "href": "homework/2024/homework.html#visualizing-the-data",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Question 8: the evolution of work ethic.\nUse the data from Wave 3 and Wave 4 only, for one country of your choice: - For this country create a frequency table that contains the frequency of each unique value of the work ethic scores. Also include the percentage of individuals at each value, grouped by Wave 3 and Wave 4 separately. - Plot a column chart showing the distribution of work ethic scores in Wave 3, with the percentage of individuals on the vertical axis and the range of work ethic scores on the horizontal axis. Plot the distribution of scores in Wave 4 on top of the Wave 3 distribution. - Based on your chart does it appear that the attitudes towards work in each country of your choice have changed over time?\nBonus: do the same for another country of your choice and compare.\nQuestion 9: Replicate the same analysis for life satisfaction.\nQuestion 10: for Wave 4 only, compute the correlations of the main variables with life satisfaction and work ethic. It should correspond to the following pattern:\n\nFor employment status and gender, you will need to create new variables: full-time employment should be equal to 1 if full-time employed and 0 if unemployed, and treated as missing data (left as a blank cell) otherwise. Gender should be 0 if male and 1 if female.\nInterpret the coefficients.\nQuestion 11 Using the data from Wave 4, carry out the following\n\nCreate a table showing the average life satisfaction according to employment status (showing the full-time employed, retired, and unemployed categories only) with country (S003) as the row variable, and employment status (X028) as the column variable. Comment on any differences in average life satisfaction between these three groups, and whether social norms is a plausible explanation for these differences.\nUse the table from Question 4(a) to calculate the difference in average life satisfaction (full-time employed minus unemployed, and full-time employed minus retired).\nMake a separate scatterplot for each of these differences in life satisfaction, with average work ethic on the horizontal axis and difference in life satisfaction on the vertical axis.\nFor each difference (employed vs unemployed, employed vs retired), calculate and interpret the correlation coefficient between average work ethic and difference in life satisfaction."
  },
  {
    "objectID": "homework/2024/homework.html#measuring-the-non-monetary-cost-of-unemployment",
    "href": "homework/2024/homework.html#measuring-the-non-monetary-cost-of-unemployment",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Question 12: using only wave 4, we consider first the following linear regression\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n\\)\nRun this regression without any other regressor. Comment on the result.\nQuestion 13: we consider now the regression\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n +  \\text{other regressors}\\)\nWhich regressors would you choose? Can you find the combination that maximizes the predictive power of the regression?\nQuestion 14: the effect of work ethics\nFrom the preceding regression can we cannot conclude about the interaction between work ethics and unemployment.\nTry to answer that question by running the following steps - create one variable high_work_ethic, equal to 1, if work ethic is above average - run the regression from before for two different dataset, one where high_work_ethic is equal to 1, and another where it is equal to 0 - compare the values obtained for \\(\\beta\\) in both cases - interpret - bonus: make a graphical representation\nQuestion 15: interaction term\nAnother common approach consists in running a regression with an interacion term, that is\n\\(\\text{life satisfaction}_n =  \\alpha + \\beta \\text{unemployment}_n + \\gamma \\text{work ethic}_n +\\delta  \\text{unemployment}_n \\times \\text{work ethic}_n+  \\text{other regressors}\\)\nThen if \\(\\delta\\) is negative, higher work ethic implies a stronger effect of unemployment.\nRun that regression and comment.\n(hint: for statsmodels formula, a*b where a and b are regressors is interpreted as a regression part \\(c_1 a+ c_2 b+ c_3 a b\\) where \\(c_1,c_2,c_3\\) are unknown coefficients)"
  },
  {
    "objectID": "homework/2024/homework.html#open-question",
    "href": "homework/2024/homework.html#open-question",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Question 16: propose any another data work using the same dataframe that you find interesting (graph, econometric analysis, machine learning)"
  },
  {
    "objectID": "slides/session_2/DataFrames.html",
    "href": "slides/session_2/DataFrames.html",
    "title": "Data Frames",
    "section": "",
    "text": "A DataFrame (aka a table) is a 2-D labeled data structure with columns of potentially different types.\n\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\n\n\n\n\n\n\nfirst goal of an econometrician: constitute a good dataframe\n\n“cleaning the data”\n\nsometimes data comes from several linked dataframes\n\nrelational database\n\ndataframes / relational databases are so ubiquitous a language has been developed for them: SQL\n\n\n\n\n\n\n\nimport pandas as pd\n\n\n\n\npandas = panel + datas\ncreated by WesMcKinsey, very optimized\nmany options\nif in doubt: minimally sufficient pandas\n\nsmall subset of pandas to do everything\n\ntons of online tutorials ex: doc\n\n\n\n\n\n# from a dictionary\nd = {\n    \"country\": [\"USA\", \"UK\", \"France\"],\n    \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\ncountry\ncomics\n\n\n\n\n0\nUSA\n13\n\n\n1\nUK\n10\n\n\n2\nFrance\n12\n\n\n\n\n\n\n\n\n\n\n\n# from a matrix\nimport numpy as np\nM = np.array([\n    [18, 150],\n    [21, 200],\n    [29, 1500]\n])\n    \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\nage\ntravel\n\n\n\n\n0\n18\n150\n\n\n1\n21\n200\n\n\n2\n29\n1500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncomma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\n\n\n\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\nadd options to pd.read_csv\n\n\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\npandas can export to many formats: df.to_...\n\n\nprint( df.to_csv() )\n\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\n\n\n\ndf.to_stata('dummy_example.dta')\n\n\n\n\n\n\n\n\nWhere can we get data from ?\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)\n\n\n\n\n\n\n\n\ncommercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones:\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair\ncovid*: lots of datasets…\n\nreminder: python packages, can be installed in the notebook with\n\n!pip install ...\n\n\n\n!pip install vega_datasets\n\nRequirement already satisfied: vega_datasets in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (0.9.0)\nRequirement already satisfied: pandas in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from vega_datasets) (1.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2020.5)\nRequirement already satisfied: numpy&gt;=1.16.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (1.19.5)\nRequirement already satisfied: six&gt;=1.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;vega_datasets) (1.15.0)\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\n\nonce the data is loaded as df, we want to look at some basic properties:\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # summary\ndf.mean() # averages\ndf.std() # standard deviations\n\n\n\ndf.head(2)\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumns are defined by attribute df.columns\n\ndf.columns\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\n\nThis attribute can be set\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n\n0      3.5\n1      3.0\n2      3.2\n3      3.1\n4      3.6\n      ... \n145    3.0\n146    2.5\n147    3.0\n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\n\n\nseries.plot()\n\n\n\n\n\n\n\n\n\n\n\n\ndf['totalLength'] = df['pLength'] + df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n6.5\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n6.3\n\n\n\n\n\n\n\n\n\n\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n3.95\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n3.85\n\n\n\n\n\n\n\n\n\n\n\nIndex with a list of column names\n\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\npLength\nsLength\n\n\n\n\n0\n1.4\n5.1\n\n\n1\n1.4\n4.9\n\n\n2\n1.3\n4.7\n\n\n\n\n\n\n\n\n\n\n\nuse index range\n\n\ndf[2:4]\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n3.65\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n3.80\n\n\n\n\n\n\n\n\n\n\n\nuse boolean\n\n\ndf['species'].unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\n\ne = df[ bool_ind ]\ne.head(4)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\nvirginica\n9.15\n\n\n101\n5.8\n2.7\n5.1\n1.9\nvirginica\n8.00\n\n\n102\n7.1\n3.0\n5.9\n2.1\nvirginica\n9.45\n\n\n103\n6.3\n2.9\n5.6\n1.8\nvirginica\n8.75\n\n\n\n\n\n\n\n\n\n\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\n\ndf.loc[0:4, 'species']\n\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object\n\n\n\n\n\n\n# Let's change the way totalLength is computed, only for 'virginica'\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']\n\n\n\n\n\n\ntxt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n\n71\n\n\n\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n\n136\n\n\n\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")\n\n\n\n\ncompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\nyear\nfrance\nusa\n\n\n\n\n0\n2018\n950.0\n2500.0\n\n\n1\n2019\n960.0\n2150.0\n\n\n2\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\nin long format: each line is an independent observation\n\ntwo lines mayb belong to the same category (year, or country)\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\n\nboth representations are useful\n\n\n\n\n\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\nyear\nvariable\nvalue\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\nmeasure\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n\n\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\ndf_long.groupby(\"country\").agg('mean')\n\nNameError: name 'df_long' is not defined\n\n\n\n\n\n\n\n\n\nSuppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcatenate if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time\n\n\n\ntxt_long_1 = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen(\"dummy_long_1.csv\",'w').write(txt_long_1)\n\n136\n\n\n\ntxt_long_2 = \"\"\"year,country,recipient\n2018,\"france\",maxime\n2019,\"france\",mauricette\n2020,\"france\",mathilde\n2018,\"usa\",sherlock\n2019,\"usa\",watson\n2020,\"usa\",moriarty\n\"\"\"\nopen(\"dummy_long_2.csv\",'w').write(txt_long_2)\n\n150\n\n\n\ndf_long_1 = pd.read_csv('dummy_long_1.csv')\ndf_long_2 = pd.read_csv('dummy_long_2.csv')\n\n\n\n\n\ndf_long_1.merge(df_long_2)\n\nNameError: name 'df_long_1' is not defined"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#tabular-data",
    "href": "slides/session_2/DataFrames.html#tabular-data",
    "title": "Data Frames",
    "section": "",
    "text": "A DataFrame (aka a table) is a 2-D labeled data structure with columns of potentially different types.\n\ntypes: quantitative, qualitative (ordered, non-ordered, …)\n\nFirst column is special: the index\n\n\n\n\n\n\nfirst goal of an econometrician: constitute a good dataframe\n\n“cleaning the data”\n\nsometimes data comes from several linked dataframes\n\nrelational database\n\ndataframes / relational databases are so ubiquitous a language has been developed for them: SQL"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#pandas",
    "href": "slides/session_2/DataFrames.html#pandas",
    "title": "Data Frames",
    "section": "",
    "text": "import pandas as pd\n\n\n\n\npandas = panel + datas\ncreated by WesMcKinsey, very optimized\nmany options\nif in doubt: minimally sufficient pandas\n\nsmall subset of pandas to do everything\n\ntons of online tutorials ex: doc\n\n\n\n\n\n# from a dictionary\nd = {\n    \"country\": [\"USA\", \"UK\", \"France\"],\n    \"comics\": [13, 10, 12]   \n}\npd.DataFrame(d)\n\n\n\n\n\n\n\n\ncountry\ncomics\n\n\n\n\n0\nUSA\n13\n\n\n1\nUK\n10\n\n\n2\nFrance\n12\n\n\n\n\n\n\n\n\n\n\n\n# from a matrix\nimport numpy as np\nM = np.array([\n    [18, 150],\n    [21, 200],\n    [29, 1500]\n])\n    \ndf = pd.DataFrame( M, columns=[\"age\", \"travel\"] )\ndf\n\n\n\n\n\n\n\n\nage\ntravel\n\n\n\n\n0\n18\n150\n\n\n1\n21\n200\n\n\n2\n29\n1500"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#file-formats",
    "href": "slides/session_2/DataFrames.html#file-formats",
    "title": "Data Frames",
    "section": "",
    "text": "comma separated files: csv file\n\noften distributed online\ncan be exported easily from Excel or LibreOffice\n\nstata files: use pd.read_dta()\nexcel files: use pd.read_excel() or xlsreader if unlucky\n\n\n\n\n\ntxt = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\ndf = pd.read_csv('dummy_file.csv') # what index should we use ?\ndf\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\nSometimes, comma-separated files, are not quite comma-separated…\n\ninspect the file with a text editor to see what it contains\nadd options to pd.read_csv\n\n\n\ntxt = \"\"\"year;country;measure\n2018;\"france\";950.0\n2019;\"france\";960.0\n2020;\"france\";1000.0\n2018;\"usa\";2500.0\n2019;\"usa\";2150.0\n2020;\"usa\";2300.0\n\"\"\"\nopen('annoying_dummy_file.csv','w').write(txt) # we write it to a file\n\n136\n\n\n\npd.read_csv(\"annoying_dummy_file.csv\", sep=\";\")\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\npandas can export to many formats: df.to_...\n\n\nprint( df.to_csv() )\n\n,year,country,measure\n0,2018,france,950.0\n1,2019,france,960.0\n2,2020,france,1000.0\n3,2018,usa,2500.0\n4,2019,usa,2150.0\n5,2020,usa,2300.0\n\n\n\n\ndf.to_stata('dummy_example.dta')"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#data-sources",
    "href": "slides/session_2/DataFrames.html#data-sources",
    "title": "Data Frames",
    "section": "",
    "text": "Where can we get data from ?\nOfficial websites\n\noften in csv form\nunpractical applications\nsometimes unavoidable\nopen data trend: more unstructured data\n\nData providers\n\nsupply an API (i.e. easy to use function)"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#data-providers",
    "href": "slides/session_2/DataFrames.html#data-providers",
    "title": "Data Frames",
    "section": "",
    "text": "commercial ones:\n\nbloomberg, macrobond, factsets, quandl …\n\nfree ones:\n\ndbnomics: many official time-series\nqeds: databases used by quantecon\nvega-datasets: distributed with altair\ncovid*: lots of datasets…\n\nreminder: python packages, can be installed in the notebook with\n\n!pip install ...\n\n\n\n!pip install vega_datasets\n\nRequirement already satisfied: vega_datasets in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (0.9.0)\nRequirement already satisfied: pandas in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from vega_datasets) (1.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (2020.5)\nRequirement already satisfied: numpy&gt;=1.16.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from pandas-&gt;vega_datasets) (1.19.5)\nRequirement already satisfied: six&gt;=1.5 in /home/pablo/.local/opt/miniconda3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;vega_datasets) (1.15.0)\n\n\n\nimport vega_datasets\ndf = vega_datasets.data('iris')\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\n\nonce the data is loaded as df, we want to look at some basic properties:\n\ndf.head(5) # 5 first lines\ndf.tail(5) # 5 first lines\ndf.describe() # summary\ndf.mean() # averages\ndf.std() # standard deviations\n\n\n\ndf.head(2)\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#manipulating-dataframes",
    "href": "slides/session_2/DataFrames.html#manipulating-dataframes",
    "title": "Data Frames",
    "section": "",
    "text": "Columns are defined by attribute df.columns\n\ndf.columns\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species'], dtype='object')\n\n\nThis attribute can be set\n\ndf.columns = ['sLength', 'sWidth', 'pLength', 'pWidth', 'species']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\nA column can be extracted using its name as in a dictionary (like df['sLength'])\n\nseries = df['sWidth'] # note the resulting object: a series\nseries\n\n0      3.5\n1      3.0\n2      3.2\n3      3.1\n4      3.6\n      ... \n145    3.0\n146    2.5\n147    3.0\n148    3.4\n149    3.0\nName: sWidth, Length: 150, dtype: float64\n\n\n\nseries.plot()\n\n\n\n\n\n\n\n\n\n\n\n\ndf['totalLength'] = df['pLength'] + df['sLength']\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n6.5\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n6.3\n\n\n\n\n\n\n\n\n\n\n\ndf['totalLength'] = df['pLength'] + df['sLength']*0.5\ndf.head(2)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n3.95\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n3.85\n\n\n\n\n\n\n\n\n\n\n\nIndex with a list of column names\n\n\ne = df[ ['pLength', 'sLength'] ]\ne.head(3)\n\n\n\n\n\n\n\n\npLength\nsLength\n\n\n\n\n0\n1.4\n5.1\n\n\n1\n1.4\n4.9\n\n\n2\n1.3\n4.7\n\n\n\n\n\n\n\n\n\n\n\nuse index range\n\n\ndf[2:4]\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n3.65\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n3.80\n\n\n\n\n\n\n\n\n\n\n\nuse boolean\n\n\ndf['species'].unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nbool_ind = df['species'] == 'virginica' # this is a boolean serie\n\n\ne = df[ bool_ind ]\ne.head(4)\n\n\n\n\n\n\n\n\nsLength\nsWidth\npLength\npWidth\nspecies\ntotalLength\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\nvirginica\n9.15\n\n\n101\n5.8\n2.7\n5.1\n1.9\nvirginica\n8.00\n\n\n102\n7.1\n3.0\n5.9\n2.1\nvirginica\n9.45\n\n\n103\n6.3\n2.9\n5.6\n1.8\nvirginica\n8.75\n\n\n\n\n\n\n\n\n\n\n\nsometimes, one wants finer control about which lines and columns to select:\n\nuse df.loc[...] which can be indexed as a matrix\n\n\n\ndf.loc[0:4, 'species']\n\n0    setosa\n1    setosa\n2    setosa\n3    setosa\n4    setosa\nName: species, dtype: object\n\n\n\n\n\n\n# Let's change the way totalLength is computed, only for 'virginica'\nindex = (df['species']=='virginica')\ndf.loc[index,'totalLength'] = df.loc[index,'sLength'] + 1.5*df[index]['pLength']"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#reshaping-dataframes",
    "href": "slides/session_2/DataFrames.html#reshaping-dataframes",
    "title": "Data Frames",
    "section": "",
    "text": "txt_wide = \"\"\"year,france,usa\n2018,950.0,2500.0\n2019,960.0,2150.0\n2020,1000.0,2300.0\n\"\"\"\nopen('dummy_file_wide.csv','w').write(txt_wide) # we write it to a file\n\n71\n\n\n\ntxt_long = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen('dummy_file_long.csv','w').write(txt_long) # we write it to a file\n\n136\n\n\n\ndf_long = pd.read_csv(\"dummy_file_long.csv\")\ndf_wide = pd.read_csv(\"dummy_file_wide.csv\")\n\n\n\n\ncompare the following tables\n\n\ndf_wide\n\n\n\n\n\n\n\n\nyear\nfrance\nusa\n\n\n\n\n0\n2018\n950.0\n2500.0\n\n\n1\n2019\n960.0\n2150.0\n\n\n2\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nyear\ncountry\nmeasure\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\nin long format: each line is an independent observation\n\ntwo lines mayb belong to the same category (year, or country)\n\nin wide format: some observations are grouped\n\nin the example it is grouped by year\n\nboth representations are useful\n\n\n\n\n\ndf_wide.melt(id_vars='year')\n\n\n\n\n\n\n\n\nyear\nvariable\nvalue\n\n\n\n\n0\n2018\nfrance\n950.0\n\n\n1\n2019\nfrance\n960.0\n\n\n2\n2020\nfrance\n1000.0\n\n\n3\n2018\nusa\n2500.0\n\n\n4\n2019\nusa\n2150.0\n\n\n5\n2020\nusa\n2300.0\n\n\n\n\n\n\n\n\n\n\n\ndf_ = df_long.pivot(index='year', columns='country')\ndf_\n\n\n\n\n\n\n\n\nmeasure\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n# the result of pivot has a \"hierarchical index\"\n# let's change columns names\ndf_.columns = df_.columns.get_level_values(1)\ndf_\n\n\n\n\n\n\n\ncountry\nfrance\nusa\n\n\nyear\n\n\n\n\n\n\n2018\n950.0\n2500.0\n\n\n2019\n960.0\n2150.0\n\n\n2020\n1000.0\n2300.0\n\n\n\n\n\n\n\n\n\n\ngroupby is a very powerful function which can be used to work directly on data in the long format.\n\ndf_long.groupby(\"country\").agg('mean')\n\nNameError: name 'df_long' is not defined"
  },
  {
    "objectID": "slides/session_2/DataFrames.html#merging",
    "href": "slides/session_2/DataFrames.html#merging",
    "title": "Data Frames",
    "section": "",
    "text": "Suppose we have two dataframes, with related observations\nHow can we construct one single database with all informations?\nAnswer:\n\nconcatenate if long format\nmerge databases if wide format\n\nLots of subtleties when data gets complicated\n\nwe’ll see them in due time\n\n\n\ntxt_long_1 = \"\"\"year,country,measure\n2018,\"france\",950.0\n2019,\"france\",960.0\n2020,\"france\",1000.0\n2018,\"usa\",2500.0\n2019,\"usa\",2150.0\n2020,\"usa\",2300.0\n\"\"\"\nopen(\"dummy_long_1.csv\",'w').write(txt_long_1)\n\n136\n\n\n\ntxt_long_2 = \"\"\"year,country,recipient\n2018,\"france\",maxime\n2019,\"france\",mauricette\n2020,\"france\",mathilde\n2018,\"usa\",sherlock\n2019,\"usa\",watson\n2020,\"usa\",moriarty\n\"\"\"\nopen(\"dummy_long_2.csv\",'w').write(txt_long_2)\n\n150\n\n\n\ndf_long_1 = pd.read_csv('dummy_long_1.csv')\ndf_long_2 = pd.read_csv('dummy_long_2.csv')\n\n\n\n\n\ndf_long_1.merge(df_long_2)\n\nNameError: name 'df_long_1' is not defined"
  },
  {
    "objectID": "slides/session_2/Exercises_correction.html",
    "href": "slides/session_2/Exercises_correction.html",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\n\n\nimport numpy as np\nx = np.linspace(0,10, 1000)\n\n\nDefine a vector y representing \\(y=sin(x)\\)\n\n\ny = np.sin(x)\n\n\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\n\n\ny1 = np.abs(np.sin(x))\n\n\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\n\n\ncond = (y&gt;0.1)\n\n\ny2 = np.sin(x)*cond + 0.1*(~cond)\n\n\nPlot y, y1, y2 against x\n\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(x,y, label=\"y\")\nplt.plot(x,y1, label='y1')\nplt.plot(x,y2, label='y2')\nplt.grid(True)\nplt.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\n\n\nT = 100\nrho = 0.9\nsigma = 0.01\n\n\nCreate an empty vector x = np.zeros(T)\n\n\nimport numpy as np\nx = np.zeros(T)\nx\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\n\n\nnp.random.normal(scale=sigma)\nnp.random.randn()*sigma\n\n-0.012179445299749143\n\n\n\nfor t in range(1, T):\n    # press tabulation to indent\n    ϵ = np.random.randn()*sigma # random normal variable with standard deviation sigma (google numpy random variable)\n    x[t] = rho*x[t-1] + ϵ\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x)\n\n\n\n\n\n\n\n\n\nUse function hpfilter from statsmodels (google it). It returns a and a residual\n\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\ncycle, trend = hpfilter(x)\n\n#cycle is residual\n\n\nPlot the simulated series, the filtered series and the residual\n\n\n\nplt.subplot(2,1,1)\nplt.plot(x, label='data')\nplt.plot(trend, label='trend')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(cycle, label='cycle (residual)')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\nYou will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\n# uncomment and run the following if vega_datasets is not already installed\n# !pip install vega_datasets # on linux\n# pip install vega_datasets # try if the former doesn't work\n\n\n# uncomment and run the following if altair is not already installed\n#!pip install altair\n\n\nPrint statistics (mean, std), by flower, for each characteristics.\n\n\n# we start by importing the library\nimport vega_datasets\n\n\ndf = vega_datasets.data.iris()\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# we can print a summary for the whole database\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n# but this mixes all kinds of flowers\n# here is how we do it for the 'setosa' type:\ndf[df['species']=='setosa'].describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n50.00000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.00600\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.35249\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.30000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.80000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.00000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.20000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.80000\n4.400000\n1.900000\n0.600000\n\n\n\n\n\n\n\n\n# or we can print the statistics for all species (here we do it for the mean)\nfor spec in ['setosa', 'virginica', 'versicolor']:\n    print(f\"\\nMean for: '{spec}'\")\n    m = df[df['species']==spec].mean()\n    print(m)\n\n\nMean for: 'setosa'\nsepalLength    5.006\nsepalWidth     3.428\npetalLength    1.462\npetalWidth     0.246\ndtype: float64\n\nMean for: 'virginica'\nsepalLength    6.588\nsepalWidth     2.974\npetalLength    5.552\npetalWidth     2.026\ndtype: float64\n\nMean for: 'versicolor'\nsepalLength    5.936\nsepalWidth     2.770\npetalLength    4.260\npetalWidth     1.326\ndtype: float64\n\n\n\n# the same result can be obtained using pandas' groubpy function\ndf.groupby('species').apply( lambda x: x.mean())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\n# same for the standard deviation\ndf.groupby('species').apply( lambda x: x.std())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nversicolor\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nvirginica\n0.635880\n0.322497\n0.551895\n0.274650\n\n\n\n\n\n\n\n\n# we can get all statistics at once, by group, with .describe\ndf.groupby('species').apply( lambda x: x.describe())\n\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\n\nsetosa\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.006000\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.300000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.800000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.000000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.200000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.800000\n4.400000\n1.900000\n0.600000\n\n\nversicolor\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.936000\n2.770000\n4.260000\n1.326000\n\n\nstd\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nmin\n4.900000\n2.000000\n3.000000\n1.000000\n\n\n25%\n5.600000\n2.525000\n4.000000\n1.200000\n\n\n50%\n5.900000\n2.800000\n4.350000\n1.300000\n\n\n75%\n6.300000\n3.000000\n4.600000\n1.500000\n\n\nmax\n7.000000\n3.400000\n5.100000\n1.800000\n\n\nvirginica\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n6.588000\n2.974000\n5.552000\n2.026000\n\n\nstd\n0.635880\n0.322497\n0.551895\n0.274650\n\n\nmin\n4.900000\n2.200000\n4.500000\n1.400000\n\n\n25%\n6.225000\n2.800000\n5.100000\n1.800000\n\n\n50%\n6.500000\n3.000000\n5.550000\n2.000000\n\n\n75%\n6.900000\n3.175000\n5.875000\n2.300000\n\n\nmax\n7.900000\n3.800000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\n\nFirst, let’s do the correlation plot for one pair of two characteristics and one species type.\n\n# we need to import the plotting library:\nfrom matplotlib import pyplot as plt\n\n\n# we do it for setosa\nddf = df[df['species']=='setosa'] # extract subdataframe where species=='setosa'\nplt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.xlabel(\"sepalLength\")\nplt.ylabel(\"sepalWidth\")\n\nText(0, 0.5, 'sepalWidth')\n\n\n\n\n\n\n\n\n\nHere is how we can plot the same plot for all species on the same graph. Not that matplotlib chooses a new color by default, for each new call to function plot()\n\n# let's get a list of all species\nspecies = df['species'].unique()\nspecies\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nfor spec in species:\n    ddf = df[df['species']==spec]\n    plt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.legend()\n\n\n\n\n\n\n\n\nNow we can produce the full graph. We use the subplots function to arrange the graphs on a 4x4 grid.\n\n# we compute the list of characteristics from the columns of the tables \ncharacteristics = df.columns[:4] # we ignore the 5th column which is 'species'\ncharacteristics\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth'], dtype='object')\n\n\n\nplt.figure(figsize=(16,16))\n# we loop over lines (i from 0 to 3)\nfor i in range(4):\n    ch_i = characteristics[i]\n    # we loop over columns (j from 0 to 3)\n    for j in range(4):\n        ch_j = characteristics[j]\n\n        # create the subplot \n        # we compute the position of the current subplot (goes from 1 to 16)\n        position = i*4 + j + 1\n        plt.subplot(4, 4, position)\n        for spec in species:\n            ddf = df[df['species']==spec]\n            plt.plot(ddf[ch_i], ddf[ch_j], 'o', label=spec)\n            plt.xlabel(ch_i)\n            plt.ylabel(ch_j)\nplt.tight_layout()\n#         plt.legend()\n\n\n\n\n\n\n\n\n\nConvert the database to long format\n\n\ndf_long = df.melt(value_vars=['sepalLength','sepalWidth','petalLength','petalWidth'], \n                  id_vars=\"species\" )\ndf_long.head()\n\n\n\n\n\n\n\n\nspecies\nvariable\nvalue\n\n\n\n\n0\nsetosa\nsepalLength\n5.1\n\n\n1\nsetosa\nsepalLength\n4.9\n\n\n2\nsetosa\nsepalLength\n4.7\n\n\n3\nsetosa\nsepalLength\n4.6\n\n\n4\nsetosa\nsepalLength\n5.0\n\n\n\n\n\n\n\n\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations.\n\nAn introduction about how to use altair is on youtube: Altair Otherwise, the online doc is very useful and complete. It has many demos that can be adapted to your need: demos\n\nimport altair as alt\n\n\nch = alt.Chart(df).mark_point().encode(\n    x='sepalWidth',\n    y='petalWidth',\n    color='species',\n)\nch\n\n\n\n\n\n\nTo plot all correlations, best practice is to use altair’s repeat function. Note that the result is an interactive graph where all subplots move in a synchronized way. This is a typical feature of “visualization” libraries.\n\nalt.Chart(df).mark_point().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color='species:N'\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=['petalLength', 'petalWidth','sepalLength', 'sepalWidth'],\n    column=['petalLength', 'petalWidth','sepalLength', 'sepalWidth']\n).interactive()\n\n\n\n\n\n\n\n\n\nIf needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\n\nThere is a clear tutorial on how to use dbnomics available from Quantecon. There are two ways to import a dbnomics series:\n\nuse api link\nuse the organization/database/series identifiers\n\nDownload with the API was broken when we tried in class (the website returned an incorrect json file), so we will use the second method. (update: this seems to be fixed now)\nOn the dbnomics website we search for “inflation france” and decide to use OECD database. We eventually obtain the following page:\n\nFrom this page we obtain the series identifier: OECD/MEI/FRA.CPALTT01.CTGY.M\nIt is split in three parts: - organization: OECD - database: KEY (Key Economic Indicators) - series: CPALTT01.FRA.GY.A\nWe use these elements to import a series with dbnomics:\n\nimport dbnomics\ndf_inflation = dbnomics.fetch_series('OECD', 'KEI', 'CPALTT01.FRA.GY.A')\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# equivalent:\n# df_inflation = dbnomics.fetch_series_by_api_link(...)\n# df_inflation\n\n\n# let's check it is not empty:\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# we see the column associated with the values is called 'value'\n# the one associated with date is called `period`\ndisplay( df_inflation['value'].head() )\ndisplay( df_inflation['period'].head() )\n\n0     1.897315\n1     3.057669\n2    15.260526\n3     5.815255\n4     4.139938\nName: value, dtype: float64\n\n\n0   1956-01-01\n1   1957-01-01\n2   1958-01-01\n3   1959-01-01\n4   1960-01-01\nName: period, dtype: datetime64[ns]\n\n\n\nplt.plot(df_inflation['period'],df_inflation['value'])\n\n\n\n\n\n\n\n\nWe follow the same steps for unemployment and gdp. For the sake of simplicity, we choose annual frequency for all series. Not that the series on unemployment starts on only in 2004. That will be enough for the current purpose.\n\n# we proceed similarly for unemployment and gdp\ndf_gdp = dbnomics.fetch_series('OECD', 'MEI', 'FRA.NAEXCP01.STSA.A')\ndf_unemployment = dbnomics.fetch_series('OECD', 'CSPCUBE', 'UNEMPLRT_T1C.FRA')\n\n\n# let's look at what we have\nplt.figure(figsize=(10,5))\nplt.subplot(131)\nplt.plot(df_unemployment['period'], df_unemployment['value'])\nplt.title('unemployment')\nplt.subplot(132)\nplt.plot(df_gdp['period'], df_gdp['value'])\nplt.title('gdp')\nplt.subplot(133)\nplt.plot(df_inflation['period'], df_inflation['value'])\nplt.title(\"inflation\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# before we proceed, let's create some new columns to avoid conflicts\ndf_inflation['inflation'] = df_inflation['value']\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\ninflation\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n1.897315\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n3.057669\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n15.260526\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n5.815255\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n4.139938\n\n\n\n\n\n\n\n\ndf_gdp['gdp'] = df_gdp['value']\ndf_unemployment['unemployment'] = df_unemployment['value']\n\n\n# the following table contains both gdp and inflation\n# note that when there was any ambiguity_x suffixes were added to the gdp table, and _y suffixes added to inflation table\n# this is why we added another column with the good name.\n# as for the period column, since it had the same meaning for both tables, it is not renamed\nddf = df_gdp.merge(df_inflation, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nvalue_y\nSUBJECT_y\nLOCATION_y\nMEASURE_y\nFREQUENCY_y\nSubject_y\nCountry_y\nMeasure_y\nFrequency_y\ninflation\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1980\n1980-01-01\n451.772\n451.772\n...\n13.562578\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.562578\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1981\n1981-01-01\n509.984\n509.984\n...\n13.314400\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.314400\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1982\n1982-01-01\n585.990\n585.990\n...\n11.978476\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n11.978476\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1983\n1983-01-01\n650.514\n650.514\n...\n9.459548\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n9.459548\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1984\n1984-01-01\n707.030\n707.030\n...\n7.673803\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n7.673803\n\n\n\n\n5 rows × 37 columns\n\n\n\n\nddf.columns\n\nIndex(['@frequency_x', 'provider_code_x', 'dataset_code_x', 'dataset_name_x',\n       'series_code_x', 'series_name_x', 'original_period_x', 'period',\n       'original_value_x', 'value_x', 'LOCATION_x', 'SUBJECT_x', 'MEASURE_x',\n       'FREQUENCY_x', 'Country_x', 'Subject_x', 'Measure_x', 'Frequency_x',\n       'gdp', '@frequency_y', 'provider_code_y', 'dataset_code_y',\n       'dataset_name_y', 'series_code_y', 'series_name_y', 'original_period_y',\n       'original_value_y', 'value_y', 'SUBJECT_y', 'LOCATION_y', 'MEASURE_y',\n       'FREQUENCY_y', 'Subject_y', 'Country_y', 'Measure_y', 'Frequency_y',\n       'inflation'],\n      dtype='object')\n\n\n\n# plot time series:\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(ddf['period'], ddf['gdp'])\nplt.title(\"gdp\")\nplt.subplot(122)\nplt.plot(ddf['period'], ddf['inflation'])\nplt.title('inflation')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# let's add unemployment too\nddf = ddf.merge(df_unemployment, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nseries_code\nseries_name\noriginal_period\noriginal_value\nvalue\nSUB\nLOCATION\nSubject\nCountry\nunemployment\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2003\n2003-01-01\n1630.666\n1630.666\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2003\n8.099563\n8.099563\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.099563\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2004\n2004-01-01\n1704.017\n1704.017\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2004\n8.468398\n8.468398\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.468398\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2005\n2005-01-01\n1765.903\n1765.903\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2005\n8.493855\n8.493855\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.493855\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2006\n2006-01-01\n1848.150\n1848.150\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2006\n8.449007\n8.449007\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.449007\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2007\n2007-01-01\n1941.361\n1941.361\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2007\n7.658579\n7.658579\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n7.658579\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n# let's keep only what we need\nddf = ddf[['period', 'inflation', 'gdp', 'unemployment']]\n\n\n# let the period be the index of the dataframe\nddf.index = ddf['period']\nddf.head()\n\n\n\n\n\n\n\n\nperiod\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n\n2003-01-01\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\n\n# to keep things tidy, we can remove the period column\nddf = ddf.drop(columns=['period'])\nddf.head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\nNow we’ve got a nice, easy to use, dataframe !\n\nCompute growth rate of gdp.\n\n\n#a new series with the observations from period before can be obtained using .shift()(\n# note the missing value for the initial date\nddf.shift(1).head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\nNaN\nNaN\nNaN\n\n\n2004-01-01\n2.098472\n1630.666\n8.099563\n\n\n2005-01-01\n2.142090\n1704.017\n8.468398\n\n\n2006-01-01\n1.745869\n1765.903\n8.493855\n\n\n2007-01-01\n1.675124\n1848.150\n8.449007\n\n\n\n\n\n\n\n\n# now we can compute growth rates\nddf['gdp_growth'] = (ddf['gdp']-ddf['gdp'].shift(1))/(ddf['gdp'].shift(1))*100\n\n\nddf['gdp_growth'].head()\n\nperiod\n2003-01-01         NaN\n2004-01-01    4.498223\n2005-01-01    3.631771\n2006-01-01    4.657504\n2007-01-01    5.043476\nName: gdp_growth, dtype: float64\n\n\n\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output).\n\n\nddf.columns\n\nIndex(['inflation', 'gdp', 'unemployment', 'gdp_growth'], dtype='object')\n\n\n\nplt.plot(ddf['unemployment'], ddf['inflation'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation (%)\")\nplt.title(\"Phillips curve (2004-2020)\")\n\nText(0.5, 1.0, 'Phillips curve (2004-2020)')\n\n\n\n\n\n\n\n\n\nWithout any econometric, work, it would seem that the Phillips relationship holds pretty well in France from 2004 to 2020.\n\nplt.plot(ddf['unemployment'], ddf['gdp_growth'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"GDP growth (%)\")\nplt.title(\"Okun's law (France: 2004-2020)\")\n\nText(0.5, 1.0, \"Okun's law (France: 2004-2020)\")\n\n\n\n\n\n\n\n\n\nAs for Okun’s law, again, the negeative relationship between GDP growth and unemployment holds fairly well, save for one very abnormal point.\n\nBonus: alternative solution to import the data\n\nIt is possible to import all series at once, by supplying all identifiers to the ‘fetch_series’ method.\n\nfull_df = dbnomics.fetch_series(['OECD/KEI/CPALTT01.USA.GP.A', 'OECD/MEI/FRA.NAEXCP01.STSA.A', 'OECD/CSPCUBE/UNEMPLRT_T1C.FRA'])\nfull_df.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\n...\nSubject\nCountry\nSubject\nCountry\nMeasure\nFrequency\nCountry\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1956\n1956-01-01\n1.525054\n1.525054\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1957\n1957-01-01\n3.341508\n3.341508\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1958\n1958-01-01\n2.729160\n2.729160\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1959\n1959-01-01\n1.010684\n1.010684\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1960\n1960-01-01\n1.457976\n1.457976\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nfull_df['series_name'].unique()\n\narray(['Consumer prices: all items – United States – Growth previous period – Annual',\n       'France – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual',\n       'Unemployment rates: total – France'], dtype=object)\n\n\nIn the result, each line corresponds to an observation. The column series_name contains the relevant observation. Let’s keep only the relevant column to get a clearer view.\n\ndf_long = full_df[['period', 'series_name','value']]\n\nThis is essentially the long format. We can use it as is, or convert to the wide format.\n\ndf_long.columns\n\nIndex(['period', 'series_name', 'value'], dtype='object')\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nperiod\nseries_name\nvalue\n\n\n\n\n0\n1956-01-01\nConsumer prices: all items – United States – G...\n1.525054\n\n\n1\n1957-01-01\nConsumer prices: all items – United States – G...\n3.341508\n\n\n2\n1958-01-01\nConsumer prices: all items – United States – G...\n2.729160\n\n\n3\n1959-01-01\nConsumer prices: all items – United States – G...\n1.010684\n\n\n4\n1960-01-01\nConsumer prices: all items – United States – G...\n1.457976\n\n\n...\n...\n...\n...\n\n\n11\n2014-01-01\nUnemployment rates: total – France\n10.291710\n\n\n12\n2015-01-01\nUnemployment rates: total – France\n10.359810\n\n\n13\n2016-01-01\nUnemployment rates: total – France\n10.056610\n\n\n14\n2017-01-01\nUnemployment rates: total – France\n9.398605\n\n\n15\n2018-01-01\nUnemployment rates: total – France\n9.059228\n\n\n\n\n120 rows × 3 columns\n\n\n\nTo convert it to the wide format, use the pivot function.\n\ndf_wide = df_long.pivot(index='period', columns=['series_name'])\n\n\ndf_wide\n\n\n\n\n\n\n\n\nvalue\n\n\nseries_name\nConsumer prices: all items – United States – Growth previous period – Annual\nFrance – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual\nUnemployment rates: total – France\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns\n\n\n\n\n# rename columns\ndf_wide.columns = ['inflation','gdp', 'unemployment']\n\n\n# and here is our tidy dataframe !\ndf_wide\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns"
  },
  {
    "objectID": "slides/session_2/Exercises_correction.html#numerical-python",
    "href": "slides/session_2/Exercises_correction.html#numerical-python",
    "title": "Exercises",
    "section": "",
    "text": "Define a vector x with 1000 regularly spaced elements between 0 and 10\n\n\nimport numpy as np\nx = np.linspace(0,10, 1000)\n\n\nDefine a vector y representing \\(y=sin(x)\\)\n\n\ny = np.sin(x)\n\n\nDefine a vector y1 representing \\(y=abs(sin(x))\\)\n\n\ny1 = np.abs(np.sin(x))\n\n\nDefine a vector y2 representing \\(y=sin(x) \\text{if} y&gt;0.1 \\text{else} 0.1\\)\n\n\ncond = (y&gt;0.1)\n\n\ny2 = np.sin(x)*cond + 0.1*(~cond)\n\n\nPlot y, y1, y2 against x\n\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(x,y, label=\"y\")\nplt.plot(x,y1, label='y1')\nplt.plot(x,y2, label='y2')\nplt.grid(True)\nplt.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n\n\n\nSet T=100, rho=0.9, sigma=0.01. We consider an autoregressive process \\(x_t=\\rho x_{t-1} + \\epsilon_t\\) where \\(\\epsilon_t\\) is normally distributed with standard deviation \\(\\sigma\\)\n\n\nT = 100\nrho = 0.9\nsigma = 0.01\n\n\nCreate an empty vector x = np.zeros(T)\n\n\nimport numpy as np\nx = np.zeros(T)\nx\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nLoop over t&gt;0 and fill x[t] so that x represents a simulation for t periods of process \\(x_t\\)\n\n\nnp.random.normal(scale=sigma)\nnp.random.randn()*sigma\n\n-0.012179445299749143\n\n\n\nfor t in range(1, T):\n    # press tabulation to indent\n    ϵ = np.random.randn()*sigma # random normal variable with standard deviation sigma (google numpy random variable)\n    x[t] = rho*x[t-1] + ϵ\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x)\n\n\n\n\n\n\n\n\n\nUse function hpfilter from statsmodels (google it). It returns a and a residual\n\n\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\ncycle, trend = hpfilter(x)\n\n#cycle is residual\n\n\nPlot the simulated series, the filtered series and the residual\n\n\n\nplt.subplot(2,1,1)\nplt.plot(x, label='data')\nplt.plot(trend, label='trend')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(cycle, label='cycle (residual)')\nplt.legend()"
  },
  {
    "objectID": "slides/session_2/Exercises_correction.html#iris-data-set",
    "href": "slides/session_2/Exercises_correction.html#iris-data-set",
    "title": "Exercises",
    "section": "",
    "text": "You will need the library vega_datasets and the altair library. You can install them with !pip install vega_datasets and !pip install altair Load the iris database.\n\n# uncomment and run the following if vega_datasets is not already installed\n# !pip install vega_datasets # on linux\n# pip install vega_datasets # try if the former doesn't work\n\n\n# uncomment and run the following if altair is not already installed\n#!pip install altair\n\n\nPrint statistics (mean, std), by flower, for each characteristics.\n\n\n# we start by importing the library\nimport vega_datasets\n\n\ndf = vega_datasets.data.iris()\ndf\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# we can print a summary for the whole database\ndf.describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n# but this mixes all kinds of flowers\n# here is how we do it for the 'setosa' type:\ndf[df['species']=='setosa'].describe()\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\n\n\ncount\n50.00000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.00600\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.35249\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.30000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.80000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.00000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.20000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.80000\n4.400000\n1.900000\n0.600000\n\n\n\n\n\n\n\n\n# or we can print the statistics for all species (here we do it for the mean)\nfor spec in ['setosa', 'virginica', 'versicolor']:\n    print(f\"\\nMean for: '{spec}'\")\n    m = df[df['species']==spec].mean()\n    print(m)\n\n\nMean for: 'setosa'\nsepalLength    5.006\nsepalWidth     3.428\npetalLength    1.462\npetalWidth     0.246\ndtype: float64\n\nMean for: 'virginica'\nsepalLength    6.588\nsepalWidth     2.974\npetalLength    5.552\npetalWidth     2.026\ndtype: float64\n\nMean for: 'versicolor'\nsepalLength    5.936\nsepalWidth     2.770\npetalLength    4.260\npetalWidth     1.326\ndtype: float64\n\n\n\n# the same result can be obtained using pandas' groubpy function\ndf.groupby('species').apply( lambda x: x.mean())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\n# same for the standard deviation\ndf.groupby('species').apply( lambda x: x.std())\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nversicolor\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nvirginica\n0.635880\n0.322497\n0.551895\n0.274650\n\n\n\n\n\n\n\n\n# we can get all statistics at once, by group, with .describe\ndf.groupby('species').apply( lambda x: x.describe())\n\n\n\n\n\n\n\n\n\nsepalLength\nsepalWidth\npetalLength\npetalWidth\n\n\nspecies\n\n\n\n\n\n\n\n\n\nsetosa\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.006000\n3.428000\n1.462000\n0.246000\n\n\nstd\n0.352490\n0.379064\n0.173664\n0.105386\n\n\nmin\n4.300000\n2.300000\n1.000000\n0.100000\n\n\n25%\n4.800000\n3.200000\n1.400000\n0.200000\n\n\n50%\n5.000000\n3.400000\n1.500000\n0.200000\n\n\n75%\n5.200000\n3.675000\n1.575000\n0.300000\n\n\nmax\n5.800000\n4.400000\n1.900000\n0.600000\n\n\nversicolor\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.936000\n2.770000\n4.260000\n1.326000\n\n\nstd\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nmin\n4.900000\n2.000000\n3.000000\n1.000000\n\n\n25%\n5.600000\n2.525000\n4.000000\n1.200000\n\n\n50%\n5.900000\n2.800000\n4.350000\n1.300000\n\n\n75%\n6.300000\n3.000000\n4.600000\n1.500000\n\n\nmax\n7.000000\n3.400000\n5.100000\n1.800000\n\n\nvirginica\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n6.588000\n2.974000\n5.552000\n2.026000\n\n\nstd\n0.635880\n0.322497\n0.551895\n0.274650\n\n\nmin\n4.900000\n2.200000\n4.500000\n1.400000\n\n\n25%\n6.225000\n2.800000\n5.100000\n1.800000\n\n\n50%\n6.500000\n3.000000\n5.550000\n2.000000\n\n\n75%\n6.900000\n3.175000\n5.875000\n2.300000\n\n\nmax\n7.900000\n3.800000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nUse matplotlib to make correlation plots, betwen flowers characteristics. (for instance, plot sepalWidth against sepalLength. Ideally, use different shapes or colors for various flowers.\n\nFirst, let’s do the correlation plot for one pair of two characteristics and one species type.\n\n# we need to import the plotting library:\nfrom matplotlib import pyplot as plt\n\n\n# we do it for setosa\nddf = df[df['species']=='setosa'] # extract subdataframe where species=='setosa'\nplt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.xlabel(\"sepalLength\")\nplt.ylabel(\"sepalWidth\")\n\nText(0, 0.5, 'sepalWidth')\n\n\n\n\n\n\n\n\n\nHere is how we can plot the same plot for all species on the same graph. Not that matplotlib chooses a new color by default, for each new call to function plot()\n\n# let's get a list of all species\nspecies = df['species'].unique()\nspecies\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nfor spec in species:\n    ddf = df[df['species']==spec]\n    plt.plot(ddf['sepalLength'], ddf['sepalWidth'], 'o', label=spec)\nplt.legend()\n\n\n\n\n\n\n\n\nNow we can produce the full graph. We use the subplots function to arrange the graphs on a 4x4 grid.\n\n# we compute the list of characteristics from the columns of the tables \ncharacteristics = df.columns[:4] # we ignore the 5th column which is 'species'\ncharacteristics\n\nIndex(['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth'], dtype='object')\n\n\n\nplt.figure(figsize=(16,16))\n# we loop over lines (i from 0 to 3)\nfor i in range(4):\n    ch_i = characteristics[i]\n    # we loop over columns (j from 0 to 3)\n    for j in range(4):\n        ch_j = characteristics[j]\n\n        # create the subplot \n        # we compute the position of the current subplot (goes from 1 to 16)\n        position = i*4 + j + 1\n        plt.subplot(4, 4, position)\n        for spec in species:\n            ddf = df[df['species']==spec]\n            plt.plot(ddf[ch_i], ddf[ch_j], 'o', label=spec)\n            plt.xlabel(ch_i)\n            plt.ylabel(ch_j)\nplt.tight_layout()\n#         plt.legend()\n\n\n\n\n\n\n\n\n\nConvert the database to long format\n\n\ndf_long = df.melt(value_vars=['sepalLength','sepalWidth','petalLength','petalWidth'], \n                  id_vars=\"species\" )\ndf_long.head()\n\n\n\n\n\n\n\n\nspecies\nvariable\nvalue\n\n\n\n\n0\nsetosa\nsepalLength\n5.1\n\n\n1\nsetosa\nsepalLength\n4.9\n\n\n2\nsetosa\nsepalLength\n4.7\n\n\n3\nsetosa\nsepalLength\n4.6\n\n\n4\nsetosa\nsepalLength\n5.0\n\n\n\n\n\n\n\n\nUse altair, to plot correlation between two characteristics, with different color for each flower. Plot all correlations.\n\nAn introduction about how to use altair is on youtube: Altair Otherwise, the online doc is very useful and complete. It has many demos that can be adapted to your need: demos\n\nimport altair as alt\n\n\nch = alt.Chart(df).mark_point().encode(\n    x='sepalWidth',\n    y='petalWidth',\n    color='species',\n)\nch\n\n\n\n\n\n\nTo plot all correlations, best practice is to use altair’s repeat function. Note that the result is an interactive graph where all subplots move in a synchronized way. This is a typical feature of “visualization” libraries.\n\nalt.Chart(df).mark_point().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color='species:N'\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=['petalLength', 'petalWidth','sepalLength', 'sepalWidth'],\n    column=['petalLength', 'petalWidth','sepalLength', 'sepalWidth']\n).interactive()"
  },
  {
    "objectID": "slides/session_2/Exercises_correction.html#philips-curve-and-okuns-law",
    "href": "slides/session_2/Exercises_correction.html#philips-curve-and-okuns-law",
    "title": "Exercises",
    "section": "",
    "text": "If needed, install dbnomics with !pip install dbnomics.\n\nDownload inflation, unemployment and gdp series from France.\n\nThere is a clear tutorial on how to use dbnomics available from Quantecon. There are two ways to import a dbnomics series:\n\nuse api link\nuse the organization/database/series identifiers\n\nDownload with the API was broken when we tried in class (the website returned an incorrect json file), so we will use the second method. (update: this seems to be fixed now)\nOn the dbnomics website we search for “inflation france” and decide to use OECD database. We eventually obtain the following page:\n\nFrom this page we obtain the series identifier: OECD/MEI/FRA.CPALTT01.CTGY.M\nIt is split in three parts: - organization: OECD - database: KEY (Key Economic Indicators) - series: CPALTT01.FRA.GY.A\nWe use these elements to import a series with dbnomics:\n\nimport dbnomics\ndf_inflation = dbnomics.fetch_series('OECD', 'KEI', 'CPALTT01.FRA.GY.A')\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# equivalent:\n# df_inflation = dbnomics.fetch_series_by_api_link(...)\n# df_inflation\n\n\n# let's check it is not empty:\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n\n\n\n\n\n\n\n\n# we see the column associated with the values is called 'value'\n# the one associated with date is called `period`\ndisplay( df_inflation['value'].head() )\ndisplay( df_inflation['period'].head() )\n\n0     1.897315\n1     3.057669\n2    15.260526\n3     5.815255\n4     4.139938\nName: value, dtype: float64\n\n\n0   1956-01-01\n1   1957-01-01\n2   1958-01-01\n3   1959-01-01\n4   1960-01-01\nName: period, dtype: datetime64[ns]\n\n\n\nplt.plot(df_inflation['period'],df_inflation['value'])\n\n\n\n\n\n\n\n\nWe follow the same steps for unemployment and gdp. For the sake of simplicity, we choose annual frequency for all series. Not that the series on unemployment starts on only in 2004. That will be enough for the current purpose.\n\n# we proceed similarly for unemployment and gdp\ndf_gdp = dbnomics.fetch_series('OECD', 'MEI', 'FRA.NAEXCP01.STSA.A')\ndf_unemployment = dbnomics.fetch_series('OECD', 'CSPCUBE', 'UNEMPLRT_T1C.FRA')\n\n\n# let's look at what we have\nplt.figure(figsize=(10,5))\nplt.subplot(131)\nplt.plot(df_unemployment['period'], df_unemployment['value'])\nplt.title('unemployment')\nplt.subplot(132)\nplt.plot(df_gdp['period'], df_gdp['value'])\nplt.title('gdp')\nplt.subplot(133)\nplt.plot(df_inflation['period'], df_inflation['value'])\nplt.title(\"inflation\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# before we proceed, let's create some new columns to avoid conflicts\ndf_inflation['inflation'] = df_inflation['value']\ndf_inflation.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nSUBJECT\nLOCATION\nMEASURE\nFREQUENCY\nSubject\nCountry\nMeasure\nFrequency\ninflation\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1956\n1956-01-01\n1.897315\n1.897315\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n1.897315\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1957\n1957-01-01\n3.057669\n3.057669\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n3.057669\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1958\n1958-01-01\n15.260526\n15.260526\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n15.260526\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1959\n1959-01-01\n5.815255\n5.815255\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n5.815255\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.FRA.GY.A\nConsumer prices: all items – France – Growth o...\n1960\n1960-01-01\n4.139938\n4.139938\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n4.139938\n\n\n\n\n\n\n\n\ndf_gdp['gdp'] = df_gdp['value']\ndf_unemployment['unemployment'] = df_unemployment['value']\n\n\n# the following table contains both gdp and inflation\n# note that when there was any ambiguity_x suffixes were added to the gdp table, and _y suffixes added to inflation table\n# this is why we added another column with the good name.\n# as for the period column, since it had the same meaning for both tables, it is not renamed\nddf = df_gdp.merge(df_inflation, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nvalue_y\nSUBJECT_y\nLOCATION_y\nMEASURE_y\nFREQUENCY_y\nSubject_y\nCountry_y\nMeasure_y\nFrequency_y\ninflation\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1980\n1980-01-01\n451.772\n451.772\n...\n13.562578\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.562578\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1981\n1981-01-01\n509.984\n509.984\n...\n13.314400\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n13.314400\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1982\n1982-01-01\n585.990\n585.990\n...\n11.978476\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n11.978476\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1983\n1983-01-01\n650.514\n650.514\n...\n9.459548\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n9.459548\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n1984\n1984-01-01\n707.030\n707.030\n...\n7.673803\nCPALTT01\nFRA\nGY\nA\nConsumer prices: all items\nFrance\nGrowth on the same period of the previous year\nAnnual\n7.673803\n\n\n\n\n5 rows × 37 columns\n\n\n\n\nddf.columns\n\nIndex(['@frequency_x', 'provider_code_x', 'dataset_code_x', 'dataset_name_x',\n       'series_code_x', 'series_name_x', 'original_period_x', 'period',\n       'original_value_x', 'value_x', 'LOCATION_x', 'SUBJECT_x', 'MEASURE_x',\n       'FREQUENCY_x', 'Country_x', 'Subject_x', 'Measure_x', 'Frequency_x',\n       'gdp', '@frequency_y', 'provider_code_y', 'dataset_code_y',\n       'dataset_name_y', 'series_code_y', 'series_name_y', 'original_period_y',\n       'original_value_y', 'value_y', 'SUBJECT_y', 'LOCATION_y', 'MEASURE_y',\n       'FREQUENCY_y', 'Subject_y', 'Country_y', 'Measure_y', 'Frequency_y',\n       'inflation'],\n      dtype='object')\n\n\n\n# plot time series:\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplt.plot(ddf['period'], ddf['gdp'])\nplt.title(\"gdp\")\nplt.subplot(122)\nplt.plot(ddf['period'], ddf['inflation'])\nplt.title('inflation')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# let's add unemployment too\nddf = ddf.merge(df_unemployment, on='period')\nddf.head()\n\n\n\n\n\n\n\n\n@frequency_x\nprovider_code_x\ndataset_code_x\ndataset_name_x\nseries_code_x\nseries_name_x\noriginal_period_x\nperiod\noriginal_value_x\nvalue_x\n...\nseries_code\nseries_name\noriginal_period\noriginal_value\nvalue\nSUB\nLOCATION\nSubject\nCountry\nunemployment\n\n\n\n\n0\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2003\n2003-01-01\n1630.666\n1630.666\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2003\n8.099563\n8.099563\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.099563\n\n\n1\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2004\n2004-01-01\n1704.017\n1704.017\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2004\n8.468398\n8.468398\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.468398\n\n\n2\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2005\n2005-01-01\n1765.903\n1765.903\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2005\n8.493855\n8.493855\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.493855\n\n\n3\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2006\n2006-01-01\n1848.150\n1848.150\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2006\n8.449007\n8.449007\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n8.449007\n\n\n4\nannual\nOECD\nMEI\nMain Economic Indicators Publication\nFRA.NAEXCP01.STSA.A\nFrance – National Accounts &gt; GDP by Expenditur...\n2007\n2007-01-01\n1941.361\n1941.361\n...\nUNEMPLRT_T1C.FRA\nUnemployment rates: total – France\n2007\n7.658579\n7.658579\nUNEMPLRT_T1C\nFRA\nUnemployment rates: total\nFrance\n7.658579\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n# let's keep only what we need\nddf = ddf[['period', 'inflation', 'gdp', 'unemployment']]\n\n\n# let the period be the index of the dataframe\nddf.index = ddf['period']\nddf.head()\n\n\n\n\n\n\n\n\nperiod\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n\n2003-01-01\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\n\n# to keep things tidy, we can remove the period column\nddf = ddf.drop(columns=['period'])\nddf.head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\n2.098472\n1630.666\n8.099563\n\n\n2004-01-01\n2.142090\n1704.017\n8.468398\n\n\n2005-01-01\n1.745869\n1765.903\n8.493855\n\n\n2006-01-01\n1.675124\n1848.150\n8.449007\n\n\n2007-01-01\n1.487998\n1941.361\n7.658579\n\n\n\n\n\n\n\nNow we’ve got a nice, easy to use, dataframe !\n\nCompute growth rate of gdp.\n\n\n#a new series with the observations from period before can be obtained using .shift()(\n# note the missing value for the initial date\nddf.shift(1).head()\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n2003-01-01\nNaN\nNaN\nNaN\n\n\n2004-01-01\n2.098472\n1630.666\n8.099563\n\n\n2005-01-01\n2.142090\n1704.017\n8.468398\n\n\n2006-01-01\n1.745869\n1765.903\n8.493855\n\n\n2007-01-01\n1.675124\n1848.150\n8.449007\n\n\n\n\n\n\n\n\n# now we can compute growth rates\nddf['gdp_growth'] = (ddf['gdp']-ddf['gdp'].shift(1))/(ddf['gdp'].shift(1))*100\n\n\nddf['gdp_growth'].head()\n\nperiod\n2003-01-01         NaN\n2004-01-01    4.498223\n2005-01-01    3.631771\n2006-01-01    4.657504\n2007-01-01    5.043476\nName: gdp_growth, dtype: float64\n\n\n\nPlot two graphs two verify graphically the Phillips curve (unemployment against inflation) and Okun’s law (unemployment against output).\n\n\nddf.columns\n\nIndex(['inflation', 'gdp', 'unemployment', 'gdp_growth'], dtype='object')\n\n\n\nplt.plot(ddf['unemployment'], ddf['inflation'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation (%)\")\nplt.title(\"Phillips curve (2004-2020)\")\n\nText(0.5, 1.0, 'Phillips curve (2004-2020)')\n\n\n\n\n\n\n\n\n\nWithout any econometric, work, it would seem that the Phillips relationship holds pretty well in France from 2004 to 2020.\n\nplt.plot(ddf['unemployment'], ddf['gdp_growth'], 'o')\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"GDP growth (%)\")\nplt.title(\"Okun's law (France: 2004-2020)\")\n\nText(0.5, 1.0, \"Okun's law (France: 2004-2020)\")\n\n\n\n\n\n\n\n\n\nAs for Okun’s law, again, the negeative relationship between GDP growth and unemployment holds fairly well, save for one very abnormal point.\n\nBonus: alternative solution to import the data\n\nIt is possible to import all series at once, by supplying all identifiers to the ‘fetch_series’ method.\n\nfull_df = dbnomics.fetch_series(['OECD/KEI/CPALTT01.USA.GP.A', 'OECD/MEI/FRA.NAEXCP01.STSA.A', 'OECD/CSPCUBE/UNEMPLRT_T1C.FRA'])\nfull_df.head()\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\n...\nSubject\nCountry\nSubject\nCountry\nMeasure\nFrequency\nCountry\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1956\n1956-01-01\n1.525054\n1.525054\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n1\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1957\n1957-01-01\n3.341508\n3.341508\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n2\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1958\n1958-01-01\n2.729160\n2.729160\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n3\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1959\n1959-01-01\n1.010684\n1.010684\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n4\nannual\nOECD\nKEI\nKey Short-Term Economic Indicators\nCPALTT01.USA.GP.A\nConsumer prices: all items – United States – G...\n1960\n1960-01-01\n1.457976\n1.457976\n...\nConsumer prices: all items\nUnited States\nConsumer prices: all items\nUnited States\nGrowth previous period\nAnnual\nUnited States\nConsumer prices: all items\nGrowth previous period\nAnnual\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nfull_df['series_name'].unique()\n\narray(['Consumer prices: all items – United States – Growth previous period – Annual',\n       'France – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual',\n       'Unemployment rates: total – France'], dtype=object)\n\n\nIn the result, each line corresponds to an observation. The column series_name contains the relevant observation. Let’s keep only the relevant column to get a clearer view.\n\ndf_long = full_df[['period', 'series_name','value']]\n\nThis is essentially the long format. We can use it as is, or convert to the wide format.\n\ndf_long.columns\n\nIndex(['period', 'series_name', 'value'], dtype='object')\n\n\n\ndf_long\n\n\n\n\n\n\n\n\nperiod\nseries_name\nvalue\n\n\n\n\n0\n1956-01-01\nConsumer prices: all items – United States – G...\n1.525054\n\n\n1\n1957-01-01\nConsumer prices: all items – United States – G...\n3.341508\n\n\n2\n1958-01-01\nConsumer prices: all items – United States – G...\n2.729160\n\n\n3\n1959-01-01\nConsumer prices: all items – United States – G...\n1.010684\n\n\n4\n1960-01-01\nConsumer prices: all items – United States – G...\n1.457976\n\n\n...\n...\n...\n...\n\n\n11\n2014-01-01\nUnemployment rates: total – France\n10.291710\n\n\n12\n2015-01-01\nUnemployment rates: total – France\n10.359810\n\n\n13\n2016-01-01\nUnemployment rates: total – France\n10.056610\n\n\n14\n2017-01-01\nUnemployment rates: total – France\n9.398605\n\n\n15\n2018-01-01\nUnemployment rates: total – France\n9.059228\n\n\n\n\n120 rows × 3 columns\n\n\n\nTo convert it to the wide format, use the pivot function.\n\ndf_wide = df_long.pivot(index='period', columns=['series_name'])\n\n\ndf_wide\n\n\n\n\n\n\n\n\nvalue\n\n\nseries_name\nConsumer prices: all items – United States – Growth previous period – Annual\nFrance – National Accounts &gt; GDP by Expenditure &gt; Current Prices &gt; Gross Domestic Product - Total – Level, rate or national currency, s.a. – Annual\nUnemployment rates: total – France\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns\n\n\n\n\n# rename columns\ndf_wide.columns = ['inflation','gdp', 'unemployment']\n\n\n# and here is our tidy dataframe !\ndf_wide\n\n\n\n\n\n\n\n\ninflation\ngdp\nunemployment\n\n\nperiod\n\n\n\n\n\n\n\n1956-01-01\n1.525054\nNaN\nNaN\n\n\n1957-01-01\n3.341508\nNaN\nNaN\n\n\n1958-01-01\n2.729160\nNaN\nNaN\n\n\n1959-01-01\n1.010684\nNaN\nNaN\n\n\n1960-01-01\n1.457976\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2015-01-01\n0.118627\n2198.432\n10.359810\n\n\n2016-01-01\n1.261583\n2234.129\n10.056610\n\n\n2017-01-01\n2.130110\n2297.244\n9.398605\n\n\n2018-01-01\n2.442583\n2360.686\n9.059228\n\n\n2019-01-01\n1.812210\n2425.710\nNaN\n\n\n\n\n64 rows × 3 columns"
  },
  {
    "objectID": "slides/session_9/index_handout.html",
    "href": "slides/session_9/index_handout.html",
    "title": "Large Language Models for Finance",
    "section": "",
    "text": "Come back later!."
  },
  {
    "objectID": "slides/pession_6/transcript.html#what-is-machine-learning-1",
    "href": "slides/pession_6/transcript.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/pession_6/transcript.html#what-about-artificial-intelligence",
    "href": "slides/pession_6/transcript.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "slides/pession_6/transcript.html#econometrics-vs-machine-learning",
    "href": "slides/pession_6/transcript.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "slides/pession_6/transcript.html#data-types",
    "href": "slides/pession_6/transcript.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "slides/pession_6/transcript.html#tabular-data",
    "href": "slides/pession_6/transcript.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\n\ntabular data"
  },
  {
    "objectID": "slides/pession_6/transcript.html#networks",
    "href": "slides/pession_6/transcript.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "slides/pession_6/transcript.html#big-data-1",
    "href": "slides/pession_6/transcript.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "slides/pession_6/transcript.html#big-subfields-of-machine-learning",
    "href": "slides/pession_6/transcript.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\n\nregression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\n\nsupervised: regression\n\n\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\n\nclassification\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "slides/pession_6/transcript.html#difference-with-traditional-regression",
    "href": "slides/pession_6/transcript.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "slides/pession_6/transcript.html#difference-with-traditional-regression-1",
    "href": "slides/pession_6/transcript.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n. . .\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "slides/pession_6/transcript.html#difference-with-traditional-regression-2",
    "href": "slides/pession_6/transcript.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "slides/pession_6/transcript.html#long-data",
    "href": "slides/pession_6/transcript.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data"
  },
  {
    "objectID": "slides/pession_6/transcript.html#long-data-1",
    "href": "slides/pession_6/transcript.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations."
  },
  {
    "objectID": "slides/pession_6/transcript.html#long-data-2",
    "href": "slides/pession_6/transcript.html#long-data-2",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "slides/pession_6/transcript.html#formalisation-a-typical-machine-learning-task",
    "href": "slides/pession_6/transcript.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "slides/pession_6/transcript.html#training-gradient-descent",
    "href": "slides/pession_6/transcript.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "slides/pession_6/transcript.html#not-everything-goes-wrong-all-the-time",
    "href": "slides/pession_6/transcript.html#not-everything-goes-wrong-all-the-time",
    "title": "Introduction to Machine Learning",
    "section": "Not everything goes wrong all the time",
    "text": "Not everything goes wrong all the time\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "slides/pession_6/transcript.html#wide-data",
    "href": "slides/pession_6/transcript.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n. . .\nProblem: - with many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "slides/pession_6/transcript.html#wide-data-regression",
    "href": "slides/pession_6/transcript.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\n\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "slides/pession_6/transcript.html#training",
    "href": "slides/pession_6/transcript.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "slides/pession_6/transcript.html#example-imf-challenge",
    "href": "slides/pession_6/transcript.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "slides/pession_6/transcript.html#nonlinear-regression-1",
    "href": "slides/pession_6/transcript.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "slides/pession_6/transcript.html#how-to-evaluate-the-machine-learning",
    "href": "slides/pession_6/transcript.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "slides/pession_6/transcript.html#how-to-evaluate-the-machine-learning-1",
    "href": "slides/pession_6/transcript.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "slides/pession_6/transcript.html#section",
    "href": "slides/pession_6/transcript.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\n\n\n. . .\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "slides/pession_6/transcript.html#how-to-choose-the-validation-set",
    "href": "slides/pession_6/transcript.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\n\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n. . .\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "slides/pession_6/transcript.html#how-to-choose-the-validation-set-1",
    "href": "slides/pession_6/transcript.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "slides/pession_6/transcript.html#wait",
    "href": "slides/pession_6/transcript.html#wait",
    "title": "Introduction to Machine Learning",
    "section": "Wait",
    "text": "Wait\n\nAnother library to do regression ?\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "slides/pession_6/transcript.html#in-practice",
    "href": "slides/pession_6/transcript.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)"
  },
  {
    "objectID": "slides/pession_6/transcript.html#k-fold-validation-with-sklearn",
    "href": "slides/pession_6/transcript.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "slides/pession_6/graphs/Untitled1.html",
    "href": "slides/pession_6/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions.html",
    "href": "slides/pession_6/machine_learning_regressions.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Objectives:\n\ncreate a training set and a validation set\ntrain a model with sklearn\nperform a validation test\n\n\n\nImport the diabetes dataset from sklearn. Describe it.\nSplit the dataset into a training set (70%) and a test set (30%)\nTrain a linear model (with intercept) on the training set\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\nShould we adjust the size of the test set? What would be the problem?\nImplement \\(k\\)-fold model with \\(k=3\\).\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?\n\n\n\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "href": "slides/pession_6/machine_learning_regressions.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\nSplit the dataset into a training set (70%) and a test set (30%)\nTrain a linear model (with intercept) on the training set\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\nShould we adjust the size of the test set? What would be the problem?\nImplement \\(k\\)-fold model with \\(k=3\\).\nBonus: use statsmodels (or linearmodels) to estimate the same linear model on the full sample. Is it always a superior method?"
  },
  {
    "objectID": "slides/pession_6/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "slides/pession_6/machine_learning_regressions.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\nSplit the dataset into a training set (70%) and a test set (30%).\nTrain a lasso model to predict house prices. Compute the score on the test set.\nTrain a ridge model to predict house prices. Which one is better?\n(bonus) Use statsmodels to build a model predicting house prices. What is the problem?"
  },
  {
    "objectID": "slides/session_5/transcript.html",
    "href": "slides/session_5/transcript.html",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))\n\n\n\n\n\n\n\nNothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)\n\n\n\n\n\n\nLook at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…\n\n\n\n\n\n\nWe use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO\n\n\n\n\n\n\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group\n\n\n\n\n\n\n\n\n\nWhat about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\n\n\n\nUse statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "slides/session_5/transcript.html#data",
    "href": "slides/session_5/transcript.html#data",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Our multilinear regression: \\[y = \\alpha + \\beta x_1 + \\cdots + \\beta x_n\\]\nSo far, we have only considered real variables: (\\(x_i \\in \\mathbb{R}\\)).\n\nex: \\(x_{\\text{gdp}} = \\alpha + \\beta_1 x_{\\text{unemployment}} + \\beta_2 x_{\\text{inflation}}\\)\n\nHow do we deal with the following cases?\n\nbinary variable: \\(x\\in \\{0,1\\}\\) (or \\(\\{True, False}\\))\n\nex: \\(\\text{gonetowar}\\), \\(\\text{hasdegree}\\)\n\ncategorical variable:\n\nex: survey result (0: I don’t know, 1: I strongly disagree, 2: I disagree, 3: I agree, 4: I strongly agree)\nthere is no ranking of answers\nwhen there is ranking: hierarchical index\n\nnonnumerical variables:\n\nex: (flower type: \\(x\\in \\text{myosotis}, \\text{rose}, ...\\))"
  },
  {
    "objectID": "slides/session_5/transcript.html#binary-variable",
    "href": "slides/session_5/transcript.html#binary-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Nothing to be done: just make sure variables take values 0 or 1. \\[y_\\text{salary} = \\alpha + \\beta x_{\\text{gonetowar}}\\]\nInterpretation:\n\nhaving gone to war is associated with a \\(\\beta\\) increase (or decrease?) in salary (still no causality)"
  },
  {
    "objectID": "slides/session_5/transcript.html#categorical-variable",
    "href": "slides/session_5/transcript.html#categorical-variable",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Look at the model: \\[y_{\\text{CO2 emission}} = \\alpha + \\beta x_{\\text{yellow vest support}} \\]\nWhere \\(y_{\\text{CO2 emission}}\\) is an individual’s CO2 emissions and \\(x_{\\text{yellow vest support}}\\) is the response the the question Are you in agreement with the yellow vests demands?.\nResponse is coded up as:\n\n0: Strongly disagree\n1: Disagree\n2: Neutral\n3: Agree\n4: Strongly agree\n\nIf the variable was used directly, how would you intepret the coefficient \\(\\beta\\) ?\n\nindex is hierarchical\nbut the distances between 1 and 2 or 2 and 3 are not comparable…"
  },
  {
    "objectID": "slides/session_5/transcript.html#hierarchical-index-2",
    "href": "slides/session_5/transcript.html#hierarchical-index-2",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "We use one dummy variable per possible answer.\n\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{Strongly Disagree}}\\)\n\\(D_{\\text{Disagree}}\\)\n\\(D_{\\text{Neutral}}\\)\n\\(D_{\\text{Agree}}\\)\n\\(D_{\\text{Strongly Agree}}\\)\n\n\n\n\n1\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n\n\nValues are linked by the specific dummy coding.\n\nthe choice of the reference group (with 0) is not completely neutral\n\nfor linear regressions, we can ignore its implications\n\nit must be frequent enough in the data\neffects coding: reference group takes -1 instead of 0\n\nNote that hierarchy is lost. The same treatment can be applied to non-hierachical variables\nNow our variables are perfectly colinear:\n\nwe can deduce one from all the others\nwe drop one from the regression: the reference group TODO"
  },
  {
    "objectID": "slides/session_5/transcript.html#hierarchical-index-3",
    "href": "slides/session_5/transcript.html#hierarchical-index-3",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "\\[y_{\\text{CO2 emission}} = \\alpha + \\beta_1 x_{\\text{strdis}} + \\beta_2 x_{\\text{dis}} + \\beta_3 x_{\\text{agr}} + \\beta_4 x_{\\text{stragr}}\\]\n\nInterpretation:\n\nbeing in the group which strongly agrees to the yellow vest’s claim is associated with an additional \\(\\beta_4\\) increase in CO2 consumption compared with members of the neutral group"
  },
  {
    "objectID": "slides/session_5/transcript.html#nonnumerical-variables",
    "href": "slides/session_5/transcript.html#nonnumerical-variables",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "What about nonnumerical variables?\n\nWhen variables take nonnumerical variables, we convert them to numerical variables.\n\nExample:\n\n\n\n\nactivity\ncode\n\n\n\n\nmassage therapist\n1\n\n\nmortician\n2\n\n\narcheologist\n3\n\n\nfinancial clerks\n4\n\n\n\n\n\n\nThen we convert to dummy variables exactly like hierarchical indices\n\nhere \\(\\text{massage therapist}\\) is taken as reference\n\n\n\n\n\n\n\n\n\n\n\\(D_{\\text{mortician}}\\)\n\\(D_{\\text{archeologist}}\\)\n\\(D_{\\text{financial clerks}}\\)\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1"
  },
  {
    "objectID": "slides/session_5/transcript.html#hands-on",
    "href": "slides/session_5/transcript.html#hands-on",
    "title": "Introduction to Instrumental Variables",
    "section": "",
    "text": "Use statsmodels to create dummy variables with formula API.\n\nReplace\n\nsalary ~ activity\n\nby:\n\nsalary ~ C(activity)\nThere is an options to choose the reference group\nsalary ~ C(activity, Treatment(reference=\"archeologist\"))"
  },
  {
    "objectID": "slides/session_5/transcript.html#what-is-causality",
    "href": "slides/session_5/transcript.html#what-is-causality",
    "title": "Introduction to Instrumental Variables",
    "section": "What is causality?",
    "text": "What is causality?\n. . .\n\n\n\nGroucho Marx\n\n\nClear? Huh! Why a four-year-old child could understand this report! Run out and find me a four-year-old child, I can’t make head or tail of it."
  },
  {
    "objectID": "slides/session_5/transcript.html#spurious-correlation",
    "href": "slides/session_5/transcript.html#spurious-correlation",
    "title": "Introduction to Instrumental Variables",
    "section": "Spurious correlation",
    "text": "Spurious correlation\n\n\n\nSpurious Correlation\n\n\n\nWe have seen spurious correlation before\n\nit happens when two series comove without being actually correlated\n\nAlso, two series might be correlated without one causing the other\n\nex: countries eating more chocolate have more nobel prices…"
  },
  {
    "objectID": "slides/session_5/transcript.html#definitions",
    "href": "slides/session_5/transcript.html#definitions",
    "title": "Introduction to Instrumental Variables",
    "section": "Definitions?",
    "text": "Definitions?\nBut how do we define\n\ncorrelation\ncausality\n\n?\nBoth concepts are actually hard to define:\n\nin statistics (and econometrices) they refer to the generating process\nif the data was generated again, would you observe the same relations?\n\nFor instance correlation between \\(X\\) and \\(Y\\) is just the average correlation taken over many draws \\(\\omega\\) of the data: \\[E_{\\omega}\\left[ (X-E[X])(Y-E[Y])\\right]\\]"
  },
  {
    "objectID": "slides/session_5/transcript.html#how-do-we-define-causality-1",
    "href": "slides/session_5/transcript.html#how-do-we-define-causality-1",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (1)",
    "text": "How do we define causality (1)\n\nIn math, we have implication: \\(A \\implies B\\)\n\napplies to statements that can be either true or false\ngiven \\(A\\) and \\(B\\), \\(A\\) implies \\(B\\) unless \\(A\\) is true and \\(B\\) is false\nparadox of the drinker: at any time, there exists a person such that: if this person drinks, then everybody drinks\n\nIn a mathematical universe taking values \\(\\omega\\), we can define causality between statement \\(A(\\omega)\\) and \\(B(\\omega)\\) as : \\[\\forall \\omega, A(\\omega) \\implies B(\\omega)\\]"
  },
  {
    "objectID": "slides/session_5/transcript.html#how-do-we-define-causality-2",
    "href": "slides/session_5/transcript.html#how-do-we-define-causality-2",
    "title": "Introduction to Instrumental Variables",
    "section": "How do we define causality (2)",
    "text": "How do we define causality (2)\nBut causality in the real world is problematic\nUsually, we observe \\(A(\\omega)\\) only once…\n. . .\n\n\nExample:\n\nstate of the world \\(\\omega\\): 2008, big financial crisis, …\nA: Ben Bernanke chairman of the Fed\nB: successful economic interventions\nWas Ben Bernanke a good central banker?\nImpossible to say.\n\n\n\n\n\n\n\nThen there is the uncertain concept of time… But let’s take it as granted to not overcomplicate…"
  },
  {
    "objectID": "slides/session_5/transcript.html#causality-in-statistics",
    "href": "slides/session_5/transcript.html#causality-in-statistics",
    "title": "Introduction to Instrumental Variables",
    "section": "Causality in Statistics",
    "text": "Causality in Statistics\n\n\n\n\n\n\nStatistical definition of causality\n\n\n\nVariable \\(A\\) causes \\(B\\) in a statistical sense if - \\(A\\) and \\(B\\) are correlated - \\(A\\) is known before \\(B\\) - correlation between \\(A\\) and \\(B\\) is unaffected by other variables\n\n\n\nThere are other related statistical definitions:\n\nlike Granger causality…\n… but not for this course"
  },
  {
    "objectID": "slides/session_5/transcript.html#factual-and-counterfactual",
    "href": "slides/session_5/transcript.html#factual-and-counterfactual",
    "title": "Introduction to Instrumental Variables",
    "section": "Factual and counterfactual",
    "text": "Factual and counterfactual\n\n\n \n\n\n\n\n\n\n\nSuppose we observe an event A\n\nA: a patient is administered a drug, government closes all schools during Covid\n\nWe observe a another event B\n\nB: the patient recovers, virus circulation decreases\n\n\n\n\n\nTo interpret B as a consequence of A, we would like to consider the counter-factual:\n\na patient is not administered a drug, government doesn’t close schools\npatient does not recover, virus circulation is stable\n\n\n\n\n\n. . .\nAn important task in econometrics is to construct a counter-factual\n\nas the name suggests is it sometimes never observed!"
  },
  {
    "objectID": "slides/session_5/transcript.html#scientific-experiment",
    "href": "slides/session_5/transcript.html#scientific-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Scientific Experiment",
    "text": "Scientific Experiment\n\n\n\n \n\n\n\nIn science we establish causality by performing experiments\n\nand create the counterfactual\n\nA good experiment is reproducible\n\nsame variables\nsame state of the world (other variables)\nreproduce several times (in case output is noisy or random)\n\nChange one factor at a time\n\nto create a counter-factual"
  },
  {
    "objectID": "slides/session_5/transcript.html#measuring-effect-of-treatment",
    "href": "slides/session_5/transcript.html#measuring-effect-of-treatment",
    "title": "Introduction to Instrumental Variables",
    "section": "Measuring effect of treatment",
    "text": "Measuring effect of treatment\n\n\n\n\n\n\n\n\n\n\n\nAssume we have discovered two medications: R and B\n\n\n\n\nGive one of them (R) to a patient and observe the outcome\n\n\n\n\nWould would have been the effect of (B) on the same patient?\n\n????\n\n\n\n\n\nWhat if we had many patients and let them choose the medication?\n\n\n\n\n. . .\nMaybe the effect would be the consequence of the choice of patients rather than of the medication?"
  },
  {
    "objectID": "slides/session_5/transcript.html#an-exmple-from-behavioural-economics",
    "href": "slides/session_5/transcript.html#an-exmple-from-behavioural-economics",
    "title": "Introduction to Instrumental Variables",
    "section": "An exmple from behavioural economics",
    "text": "An exmple from behavioural economics\n\n\n\nExample: cognitive dissonance\n\nExperiment in GATE Lab\nVolunteers play an investment game.\nThey are asked beforehand whether they support OM, PSG, or none.\n\n\n\n\n\nExperiment 1:\n\nBefore the experiment, randomly selected volunteers are given a football shirt of their preferred team (treatment 1)\nOther volunteers receive nothing (treatment 0)\n\nResult:\n\nhaving a football shirt seems to boost investment performance…\n\n\n\n\n\nExperiment 2: subjects are given randomly a shirt of either Olympique de Marseille or PSG.\nResult:\n\nHaving the good shirt improves performance.\nHaving the wrong one deteriorates it badly.\n\n\n\n\n\nHow would you code up this experiment?\nCan we conclude on some form of causality?"
  },
  {
    "objectID": "slides/session_5/transcript.html#formalisation-of-the-problem",
    "href": "slides/session_5/transcript.html#formalisation-of-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Formalisation of the problem",
    "text": "Formalisation of the problem\n\n\n\n\n\n\nCause (A): two groups of people\n\nthose given a shirt (treatment 1)\nthose not given a shirt (treatment 0)\n\nPossible consequence (B): performance\nTake a given agent Alice: she performs well with a PSG shirt.\n\nmaybe she is a good investor?\nor maybe she is playing for her team?\n\nLet’s try to have her play again without the football shirt\n\nnow the experiment has changed: she has gained experience, is more tired, misses the shirt…\nit is impossible to get a perfect counterfactual (i.e. where only A changes)\n\n\n\n\nLet’s take somebody else then? Bob was really bad without a PSG shirt.\n\nhe might be a bad investor? or he didn’t understand the rules?\nsome other variables have changed, not only the treatment\n\nHow to make a perfect experiment?\n\nChoose randomly whether assigning a shirt or not\nby construction the treatment will not be correlated with other variables"
  },
  {
    "objectID": "slides/session_5/transcript.html#randomized-control-trial",
    "href": "slides/session_5/transcript.html#randomized-control-trial",
    "title": "Introduction to Instrumental Variables",
    "section": "Randomized Control Trial",
    "text": "Randomized Control Trial\n\n\n\n\n\n\nRandomized Control Trial (RCT)\n\n\n\nThe best way to ensure that treatment is independent from other factors is to randomize it.\n\n\n\n\n\nIn medecine\n\nsome patients receive the treatment (red pill)\nsome other receive the control treatment (blue pill / placebo)\n\nIn economics:\n\nrandomized field experiments\nrandomized phase-ins for new policies\n\nvery useful for policy evaluation\n\n\n\n\n\n\n\nEsther Duflo\n\n\n\n\n\n\nIt is common in economics, instead of assigning treatments randomly, we often say that we assign individuals randomly to the treatment and to the control group. It is equivalent."
  },
  {
    "objectID": "slides/session_5/transcript.html#natural-experiment",
    "href": "slides/session_5/transcript.html#natural-experiment",
    "title": "Introduction to Instrumental Variables",
    "section": "Natural experiment",
    "text": "Natural experiment\n\n\n\n\n\n\nNatural Experiment\n\n\n\nA natural experiment satisfies conditions that treatment is assigned randomly\n\nwithout interference by the econometrician\n\n\n\n\nAn exemple of a Natural Experiment:\n\ngender bias in french local elections (jean-pierre eymeoud, paul vertier) link\nare women discriminated against by voters in local elections?\n\n\n. . .\n\nResult: yes, they get 1.5% less votes by right-wing voters\n\n. . .\n\nWhat was the natural experiment"
  },
  {
    "objectID": "slides/session_5/transcript.html#example",
    "href": "slides/session_5/transcript.html#example",
    "title": "Introduction to Instrumental Variables",
    "section": "Example",
    "text": "Example\nLifetime Earnings and the Vietnam Era Draft Lottery, by JD Angrist\n\n\n\nFact:\n\nveterans of the vietnam war (55-75) earn (in the 80s) an income that is 15% less in average than those who didn’t go to the war.\nWhat can we conclude?\nHard to say: maybe those sent to the war came back with lower productivity (because of PTSD, public stigma, …)? maybe they were not the most productive in the first place (selection bias)?\n\nProblem (for the economist):\n\nwe didn’t sent people to war randomly\n\n\n\n\nGenius idea:\n\nhere is a variable which randomly affected whether people were sent: the Draft\n\n\nbetween 1947, and 1973, a lottery was run to determine who would go to war\n\nthe draft number was determined, based on date of birth, and first letters of name\n\nand was correlated with the probability that a given person would go to war\nand it was so to say random or at least independent from anything relevant to the problem\n\n\n\n\n. . .\nCan we use the Draft to generate randomness ?"
  },
  {
    "objectID": "slides/session_5/transcript.html#problem",
    "href": "slides/session_5/transcript.html#problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Problem",
    "text": "Problem\n\nTake the linear regression: \\[y = \\alpha + \\beta x + \\epsilon\\]\n\n\\(y\\): salary\n\\(x\\): went to war\n\nWe want to establish causality from x to y\n\nwe would like to interpret \\(x\\) as the “treatment”\n\nBut there can be confounding factors:\n\nvariable \\(z\\) which causes both x and y\nexemple: socio-economic background, IQ, …\n\nIf we could identify \\(z\\) we could control for it: \\[y = \\alpha + \\beta_1 x + \\beta_2 z + \\epsilon\\]\n\nwe would get a better predictor of \\(y\\) but more uncertainty about \\(\\beta_1\\) (\\(x\\) and \\(z\\) are correlated)"
  },
  {
    "objectID": "slides/session_5/transcript.html#reformulate-the-problem",
    "href": "slides/session_5/transcript.html#reformulate-the-problem",
    "title": "Introduction to Instrumental Variables",
    "section": "Reformulate the problem",
    "text": "Reformulate the problem\n\n\n\nLet’s assume treatment \\(x\\) is a binary variable \\(\\in{0,1}\\)\nWe want to estimate \\[y = \\alpha + \\beta x + z + \\epsilon\\] where \\(z\\) is potentially correlated to \\(x\\) and \\(y\\)\nThere are two groups:\n\nthose who receive the treatment \\[y = \\alpha + \\beta + z_{T=1} + \\epsilon\\]\nthe others \\[y = \\alpha + 0 +  z_{T=0} + \\epsilon\\]\n\n\n\n\nProblem:\n\nif \\(z\\) is higher in the treatment group, its effect can’t be separated from the treatment effect.\n\nIntuition: what if we make groups differently?\n\ncompletely independent from \\(z\\) (and \\(\\epsilon\\))\nnot independently from \\(x\\) so that one group will receive more treatment than the other\n\nTo make this group we need a new variable \\(q\\) that is:\n\ncorrelated with \\(x\\) so that it will correspond to some treatment effect\nuncorrelated to \\(z\\) or \\(\\epsilon\\) (exogenous)"
  },
  {
    "objectID": "slides/session_5/transcript.html#two-stage-regression",
    "href": "slides/session_5/transcript.html#two-stage-regression",
    "title": "Introduction to Instrumental Variables",
    "section": "Two stage regression",
    "text": "Two stage regression\n\n\n\nWe would like to redo the treatment groups in a way that is independent from \\(z\\) (and everything contained in \\(\\epsilon\\))\n\n\\(q\\) is a binary variable: drafted or not\n\n\nFirst stage: regress group assignment on the instrument: \\[x = \\alpha_0 + \\beta_0 q + \\eta\\]\n\nwe can now predict group assignment in a way that is independent from \\(z\\) (and everything in \\(\\epsilon\\)) \\[\\tilde{x} = \\alpha_0 + \\beta_0 q\\]\n\n\nSecond stage: use the predicted value instead of the original one \\[y = \\alpha + \\beta_1 \\tilde{x} + z + \\epsilon\\]\n\n\n\n\nResult:\n\nIf \\(\\beta_1\\) is significantly nonzero, there is a causal effect between \\(x\\) and \\(y\\).\nNote that \\(\\tilde{x}\\) is imperfectly correlated with the treatment: \\(\\beta_1\\) can’t be interpreted directly\nThe actual effect will be \\(\\frac{\\beta_1}{\\beta_0}\\) (in 1d)\n\n\nWe say that we instrument \\(x\\) by \\(q\\)."
  },
  {
    "objectID": "slides/session_5/transcript.html#choosing-a-good-instrument",
    "href": "slides/session_5/transcript.html#choosing-a-good-instrument",
    "title": "Introduction to Instrumental Variables",
    "section": "Choosing a good instrument",
    "text": "Choosing a good instrument\n\n\n\n\n\n\n\nChoosing an instrumental variable\n\n\n\nA good instrument when trying to explain y by x, is a variable that is correlated to the treatment (x) but does not have any effect on the outcome of interest (y), appart from its effect through x."
  },
  {
    "objectID": "slides/session_5/transcript.html#in-practice",
    "href": "slides/session_5/transcript.html#in-practice",
    "title": "Introduction to Instrumental Variables",
    "section": "In practice",
    "text": "In practice\n\nBoth statsmodels and linearmodels support instrumental variables\n\nlibrary (look for IV2SLS)\n\nLibrary linearmodels has a handy formula syntax: salary ~ 1 + [war ~ draft]\n\nAPI is similar but not exactly identical to statsmodels\nfor instance linearmodels does not include constants by default\n\nExample from the doc\n\nformula = (\n    \"np.log(drugexp) ~ 1 + totchr + age + linc + blhisp + [hi_empunion ~ ssiratio]\"\n)\nols = IV2SLS.from_formula(formula, data)\nols_res = ols.fit(cov_type=\"robust\")\nprint(ols_res)"
  },
  {
    "objectID": "slides/session_11/index_handout.html",
    "href": "slides/session_11/index_handout.html",
    "title": "Final Exam 🤞",
    "section": "",
    "text": "Come back later!."
  },
  {
    "objectID": "slides/session_4/transcript.html",
    "href": "slides/session_4/transcript.html",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?\n\n\n\n\n\n\n\n\n\nif the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education\n\n\n\n\n\n\nWhat about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)\n\n\n\n\n\n\nNow we are trying to fit a plane to a cloud of points.\n \n\n\n\n\n\nTake all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula\n\n\n\n\n\n\n\n\n\n\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]\n\n\n\n\n\n\n\nResult: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "slides/session_4/transcript.html#remember-dataset-from-last-time",
    "href": "slides/session_4/transcript.html#remember-dataset-from-last-time",
    "title": "Multiple Regressions",
    "section": "",
    "text": "type\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\nLast week we “ran” a linear regression: \\(y = \\alpha + \\beta x\\). Result: \\[\\text{income} = xx + 0.72 \\text{education}\\]\nShould we have looked at “prestige” instead ? \\[\\text{income} = xx + 0.83 \\text{prestige}\\]\nWhich one is better?"
  },
  {
    "objectID": "slides/session_4/transcript.html#prestige-or-education",
    "href": "slides/session_4/transcript.html#prestige-or-education",
    "title": "Multiple Regressions",
    "section": "",
    "text": "if the goal is to predict: the one with higher explained variance\n\nprestige has higher \\(R^2\\) (\\(0.83^2\\))\n\nunless we are interested in the effect of education"
  },
  {
    "objectID": "slides/session_4/transcript.html#multiple-regression",
    "href": "slides/session_4/transcript.html#multiple-regression",
    "title": "Multiple Regressions",
    "section": "",
    "text": "What about using both?\n\n2 variables model: \\[\\text{income} = \\alpha + \\beta_1 \\text{education} + \\beta_2 \\text{prestige}\\]\nwill probably improve prediction power (explained variance)\n\\(\\beta_1\\) might not be meaningful on its own anymore (education and prestige are correlated)"
  },
  {
    "objectID": "slides/session_4/transcript.html#fitting-a-model",
    "href": "slides/session_4/transcript.html#fitting-a-model",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Now we are trying to fit a plane to a cloud of points."
  },
  {
    "objectID": "slides/session_4/transcript.html#minimization-criterium",
    "href": "slides/session_4/transcript.html#minimization-criterium",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Take all observations: \\((\\text{income}\\_n,\\text{education}\\_n,\\text{prestige}\\_n)\\_{n\\in[0,N]}\\)\nObjective: sum of squares \\[ L(\\alpha, \\beta_1, \\beta_2) = \\sum_i \\left( \\underbrace{ \\alpha + \\beta_1 \\text{education}\\_n + \\beta_2 \\text{prestige}\\_n - \\text{income}\\_n }\\_{e_n=\\text{prediction error} }\\right)^2 \\]\nMinimize loss function in \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\)\nAgain, we can perform numerical optimization (machine learning approach)\n\n… but there is an explicit formula"
  },
  {
    "objectID": "slides/session_4/transcript.html#ordinary-least-square",
    "href": "slides/session_4/transcript.html#ordinary-least-square",
    "title": "Multiple Regressions",
    "section": "",
    "text": "\\[Y = \\begin{bmatrix}\n\\text{income}_1 \\\\\\\\\n\\vdots \\\\\\\\\n\\text{income}_N\n\\end{bmatrix}\\] \\[X = \\begin{bmatrix}\n1 & \\text{education}_1 & \\text{prestige}_1 \\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 &\\text{education}_N & \\text{prestige}_N\n\\end{bmatrix}\\]\n\n\nMatrix Version (look for \\(B = \\left( \\alpha,  \\beta_1 , \\beta_2 \\right)\\)): \\[Y =  X B + E\\]\nNote that constant can be interpreted as a “variable”\nLoss function \\[L(A,B) = (Y - X B)' (Y - X B)\\]\nResult of minimization \\(\\min_{(A,B)} L(A,B)\\) : \\[\\begin{bmatrix}\\alpha & \\beta_1 & \\beta_2 \\end{bmatrix} = (X'X)^{-1} X' Y \\]"
  },
  {
    "objectID": "slides/session_4/transcript.html#solution",
    "href": "slides/session_4/transcript.html#solution",
    "title": "Multiple Regressions",
    "section": "",
    "text": "Result: \\[\\text{income} = 10.43  + 0.03 \\times \\text{education} + 0.62 \\times \\text{prestige}\\]\nQuestions:\n\nis it a better regression than the other?\nis the coefficient in front of education significant?\nhow do we interpret it?\ncan we build confidence intervals?"
  },
  {
    "objectID": "slides/session_4/transcript.html#explained-variance-1",
    "href": "slides/session_4/transcript.html#explained-variance-1",
    "title": "Multiple Regressions",
    "section": "Explained Variance",
    "text": "Explained Variance\n\nAs in the 1d case we can compare:\n\nthe variability of the model predictions (\\(MSS\\))\nthe variance of the data (\\(TSS\\), T for total)\n\nCoefficient of determination: \\[R^2 = \\frac{MSS}{TSS}\\]\nOr: \\[R^2 = 1-\\frac{RSS}{SST}\\] where \\(RSS\\) is the non explained variance"
  },
  {
    "objectID": "slides/session_4/transcript.html#adjusted-r-squared",
    "href": "slides/session_4/transcript.html#adjusted-r-squared",
    "title": "Multiple Regressions",
    "section": "Adjusted R squared",
    "text": "Adjusted R squared\n\n\n\nIn our example:\n\n\n\n\n\n\n\n\n\nRegression\n\\(R^2\\)\n \\(R^2_{adj}\\) \n\n\n\n\neducation\n0.525\n 0.514 \n\n\nprestige\n0.702\n 0.695 \n\n\neducation + prestige\n0.7022\n 0.688 \n\n\n\n\n\n\nFact:\n\nadding more regressors always improve \\(R^2\\)\nwhy not throw everything in? (kitchen sink regressions)\n\ntwo many regressors: overfitting the data\n\n\nPenalise additional regressors: adjusted R^2\n\nexample formula:\n\n\\(N\\): number of observations\n\\(p\\) number of variables \\[R^2_{adj} = 1-(1-R^2)\\frac{N-1}{N-p-1}\\]"
  },
  {
    "objectID": "slides/session_4/transcript.html#making-a-regression-with-statsmodels",
    "href": "slides/session_4/transcript.html#making-a-regression-with-statsmodels",
    "title": "Multiple Regressions",
    "section": "Making a regression with statsmodels",
    "text": "Making a regression with statsmodels\nimport statsmodels\nWe use a special API inspired by R:\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "slides/session_4/transcript.html#performing-a-regression",
    "href": "slides/session_4/transcript.html#performing-a-regression",
    "title": "Multiple Regressions",
    "section": "Performing a regression",
    "text": "Performing a regression\n\nRunning a regression with statsmodels\n\nmodel = smf.ols('income ~ education',  df)  # model\nres = model.fit()  # perform the regression\nres.describe()\n\n‘income ~ education’ is the model formula\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     47.51\nDate:                Tue, 02 Feb 2021   Prob (F-statistic):           1.84e-08\nTime:                        05:21:25   Log-Likelihood:                -190.42\nNo. Observations:                  45   AIC:                             384.8\nDf Residuals:                      43   BIC:                             388.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n==============================================================================\nIntercept     10.6035      5.198      2.040      0.048       0.120      21.087\neducation      0.5949      0.086      6.893      0.000       0.421       0.769\n==============================================================================\nOmnibus:                        9.841   Durbin-Watson:                   1.736\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               10.609\nSkew:                           0.776   Prob(JB):                      0.00497\nKurtosis:                       4.802   Cond. No.                         123.\n=============================================================================="
  },
  {
    "objectID": "slides/session_4/transcript.html#formula-mini-language",
    "href": "slides/session_4/transcript.html#formula-mini-language",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nWith statsmodels formulas, can be supplied with R-style syntax\nExamples:\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nincome ~ education\n\\(\\text{income}_i = \\alpha + \\beta \\text{education}_i\\)\n\n\nincome ~ prestige\n\\(\\text{income}_i = \\alpha + \\beta \\text{prestige}_i\\)\n\n\nincome ~ prestige - 1\n\\(\\text{income}_i = \\beta \\text{prestige}_i\\) (no intercept)\n\n\nincome ~ education + prestige\n\\(\\text{income}_i = \\alpha + \\beta_1 \\text{education}_i + \\beta_2 \\text{prestige}_i\\)"
  },
  {
    "objectID": "slides/session_4/transcript.html#formula-mini-language-1",
    "href": "slides/session_4/transcript.html#formula-mini-language-1",
    "title": "Multiple Regressions",
    "section": "Formula mini-language",
    "text": "Formula mini-language\n\nOne can use formulas to apply transformations to variables\n\n\n\n\n\n\n\n\nFormula\nModel\n\n\n\n\nlog(P) ~ log(M) + log(Y)\n\\(\\log(P_i) = \\alpha + \\alpha_1 \\log(M_i) + \\alpha_2 \\log(Y_i)\\) (log-log)\n\n\nlog(Y) ~ i\n\\(\\log(P_i) = \\alpha + i_i\\) (semi-logs)\n\n\n\n\nThis is useful if the true relationship is nonlinear\nAlso useful, to interpret the coefficients"
  },
  {
    "objectID": "slides/session_4/transcript.html#coefficients-interpetation",
    "href": "slides/session_4/transcript.html#coefficients-interpetation",
    "title": "Multiple Regressions",
    "section": "Coefficients interpetation",
    "text": "Coefficients interpetation\n\nExample:\n\n(police_spending and prevention_policies in million dollars) \\[\\text{number_or_crimes} = 0.005\\% - 0.001 \\text{pol_spend} - 0.005 \\text{prev_pol} + 0.002 \\text{population density}\\]\n\nreads: when holding other variables constant a 0.1 million increase in police spending reduces crime rate by 0.001%\ninterpretation?\n\nproblematic because variables have different units\nwe can say that prevention policies are more efficient than police spending ceteris paribus\n\nTake logs: \\[\\log(\\text{number_or_crimes}) = 0.005\\% - 0.15 \\log(\\text{pol_spend}) - 0.4 \\log(\\text{prev_pol}) + 0.2 \\log(\\text{population density})\\]\n\nnow we have an estimate of elasticities\na \\(1\\%\\) increase in police spending leads to a \\(0.15\\%\\) decrease in the number of crimes"
  },
  {
    "objectID": "slides/session_4/transcript.html#hypotheses",
    "href": "slides/session_4/transcript.html#hypotheses",
    "title": "Multiple Regressions",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\n\n\nRecall what we do:\n\nwe have the data \\(X,Y\\)\nwe choose a model: \\[ Y = \\alpha + X \\beta \\]\nfrom the data we compute estimates: \\[\\hat{\\beta}  = (X'X)^{-1} X' Y \\] \\[\\hat{\\alpha} = Y- X \\beta \\]\nestimates are a precise function of data\n\nexact formula not important here\n\n\n\n\n\n\nWe make some hypotheses on the data generation process:\n\n\\(Y = X \\beta + \\epsilon\\)\n\\(\\mathbb{E}\\left[ \\epsilon \\right] = 0\\)\n\\(\\epsilon\\) multivariate normal with covariance matrix \\(\\sigma^2 I_n\\)\n\n\\(\\forall i, \\sigma(\\epsilon_i) = \\sigma\\)\n\\(\\forall i,j, cov(\\epsilon_i, \\epsilon_j) = 0\\)\n\n\n\nUnder these hypotheses:\n\n\\(\\hat{\\beta}\\) is an unbiased estimate of true parameter \\(\\beta\\)\n\ni.e. \\(\\mathbb{E} [\\hat{\\beta}] = \\beta\\)\n\none can prove \\(Var(\\hat{\\beta}) = \\sigma^2 I_n\\)\n\\(\\sigma\\) can be estimated by \\(\\hat{\\sigma}=S\\frac{\\sum_i (y_i-{pred}_i)^2}{N-p}\\)\n\n\\(N-p\\): degrees of freedoms\n\none can estimate: \\(\\sigma(\\hat{\\beta_k})\\)\n\nit is the \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)"
  },
  {
    "objectID": "slides/session_4/transcript.html#is-the-regression-significant",
    "href": "slides/session_4/transcript.html#is-the-regression-significant",
    "title": "Multiple Regressions",
    "section": "Is the regression significant?",
    "text": "Is the regression significant?\n\n\n\nApproach is very similar to the one-dimensional case\nFisher criterium (F-test):\n\n\\(H0\\): all coeficients are 0\n\ni.e. true model is \\(y=\\alpha + \\epsilon\\)\n\n\\(H1\\): some coefficients are not 0\n\nStatistics: \\[F=\\frac{MSR}{MSE}\\]\n\n\\(MSR\\): mean-squared error of constant model\n\\(MSE\\): mean-squared error of full model\n\n\n\n\n\nUnder:\n\nthe model assumptions about the data generation process\nthe H0 hypothesis\n\n… the distribution of \\(F\\) is known\nIt is remarkable that it doesn’t depend on \\(\\sigma\\) !\n\nOne can produce a p-value.\n\nprobability to obtain this statistics given hypothesis H0\nif very low, H0 is rejected"
  },
  {
    "objectID": "slides/session_4/transcript.html#is-each-coefficient-significant",
    "href": "slides/session_4/transcript.html#is-each-coefficient-significant",
    "title": "Multiple Regressions",
    "section": "Is each coefficient significant ?",
    "text": "Is each coefficient significant ?\n\nStudent test. Given a coefficient \\(\\beta_k\\):\n\n\\(H0\\): coefficient is 0\n\\(H1\\): coefficient is not zero\n\nStatistics: \\(t = \\frac{\\hat{\\beta_k}}{\\hat{\\sigma}(\\hat{\\beta_k})}\\)\n\nwhere \\(\\hat{\\sigma}(\\beta_k)\\) is \\(i\\)-th diagonal element of \\(\\hat{\\sigma}^2 X'X\\)\nit compares the estimated value of a coefficient to its estimated standard deviation\n\nUnder the inference hypotheses, distribution of \\(t\\) is known.\n\nit is a student distribution\n\nProcedure:\n\nCompute \\(t\\). Check acceptance threshold \\(t*\\) at probability \\(\\alpha\\) (ex 5%)\nCoefficient is significant with probability \\(1-\\alpha\\) if \\(t&gt;t*\\)\nOr just look at the \\(p-value\\): probability that \\(t\\) would be as high as it is, assuming \\(H0\\)"
  },
  {
    "objectID": "slides/session_4/transcript.html#confidence-intervals",
    "href": "slides/session_4/transcript.html#confidence-intervals",
    "title": "Multiple Regressions",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nSame as in the 1d case.\nTake estimate \\(\\color{red}{\\beta_i}\\) with an estimate of its standard deviation \\(\\color{red}{\\hat{\\sigma}(\\beta_i)}\\)\nCompute student \\(\\color{red}{t^{\\star}}\\) at \\(\\color{red}{\\alpha}\\) confidence level (ex: \\(\\alpha=5\\\\%\\)) such that:\n\n\\(P(|t|&gt;t^{\\star})&lt;\\alpha\\)\n\nProduce confidence intervals at \\(\\alpha\\) confidence level:\n\n\\([\\color{red}{\\beta_i} - t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}, \\color{red}{\\beta_i} + t^{\\star} \\color{red}{\\hat{\\sigma}(\\beta_i)}]\\)\n\nInterpretation:\n\nfor a given confidence interval at confidence level \\(\\alpha\\)…\nthe probability that our coefficient was obtained, if the true coefficient were outside of it, is smaller than \\(\\alpha\\)"
  },
  {
    "objectID": "slides/session_4/transcript.html#other-tests",
    "href": "slides/session_4/transcript.html#other-tests",
    "title": "Multiple Regressions",
    "section": "Other tests",
    "text": "Other tests\n\nThe tests seen so far rely on strong statistical assumptions (normality, homoscedasticity, etc..)\nSome tests can be used to test these assumptions:\n\nJarque-Bera: is the distribution of data truly normal\nDurbin-Watson: are residuals autocorrelated (makes sense for time-series)\n…\n\nIn case assumptions are not met…\n\n… still possible to do econometrics\n… but beyond the scope of this course"
  },
  {
    "objectID": "slides/session_4/transcript.html#variable-selection-1",
    "href": "slides/session_4/transcript.html#variable-selection-1",
    "title": "Multiple Regressions",
    "section": "Variable selection",
    "text": "Variable selection\n\n\n\nI’ve got plenty of data:\n\n\\(y\\): gdp\n\\(x_1\\): investment\n\\(x_2\\): inflation\n\\(x_3\\): education\n\\(x_4\\): unemployment\n…\n\n\n\n\nMany possible regressions:\n\n\\(y = α + \\beta_1 x_1\\)\n\\(y = α + \\beta_2 x_2 + \\beta_3 x_4\\)\n…\n\n\n\n\n. . .\n\nWhich one do I choose ?\n\nputting everything together is not an option (kitchen sink regression)"
  },
  {
    "objectID": "slides/session_4/transcript.html#not-enough-coefficients",
    "href": "slides/session_4/transcript.html#not-enough-coefficients",
    "title": "Multiple Regressions",
    "section": "Not enough coefficients",
    "text": "Not enough coefficients\n\nSuppose you run a regression: \\[y = \\alpha + \\beta_1 x_1 + \\epsilon\\] and are genuinely interested in coefficient \\(\\beta_1\\)\nBut unknowingly to you, the actual model is \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\eta\\]\nThe residual \\(y - \\alpha - \\beta_1 x_1\\) is not white noise\n\nspecification hypotheses are violated\nestimate \\(\\hat{\\beta_1}\\) will have a bias (omitted variable bias)\nto correct the bias we add \\(x_2\\)\n\neven though we are not interested in \\(x_2\\) by itself\nwe control for \\(x_2\\))"
  },
  {
    "objectID": "slides/session_4/transcript.html#example",
    "href": "slides/session_4/transcript.html#example",
    "title": "Multiple Regressions",
    "section": "Example",
    "text": "Example\n\n\nSuppose I want to check Okun’s law. I consider the following model: \\[\\text{gdp_growth} = \\alpha + \\beta \\times \\text{unemployment}\\]\nI obtain: \\[\\text{gdp_growth} = 0.01 - 0.1 \\times \\text{unemployment} + e_i\\]\nThen I inspect visually the residuals: not normal at all!\nConclusion: my regression is misspecified, \\(0.1\\) is a biased (useless) estimate\nI need to control for additional variables. For instance: \\[\\text{gdp_growth} = \\alpha + \\beta_1 \\text{unemployment} + \\beta_2 \\text{interest rate}\\]\nUntil the residuals are actually white noise"
  },
  {
    "objectID": "slides/session_4/transcript.html#colinear-regressors",
    "href": "slides/session_4/transcript.html#colinear-regressors",
    "title": "Multiple Regressions",
    "section": "Colinear regressors",
    "text": "Colinear regressors\n\n\nWhat happens if two regressors are (almost) colinear? \\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\] where \\(x_2 = \\kappa x_1\\)\nIntuitively: parameters are not unique\n\nif \\(y = \\alpha + \\beta_1 x_1\\) is the right model…\nthen \\(y = \\alpha + \\beta_1 \\lambda x_1 + \\beta_2 (1-\\lambda) \\frac{1}{\\kappa} x_2\\) is exactly as good…\n\nMathematically: \\((X'X)\\) is not invertible.\nWhen regressors are almost colinear, coefficients can have a lot of variability.\nTest:\n\ncorrelation statistics\ncorrelation plot"
  },
  {
    "objectID": "slides/session_4/transcript.html#choosing-regressors",
    "href": "slides/session_4/transcript.html#choosing-regressors",
    "title": "Multiple Regressions",
    "section": "Choosing regressors",
    "text": "Choosing regressors\n\\[y = \\alpha + \\beta_1 x_1 + ... \\beta_n x_n\\]\nWhich regressors to choose ?\n\nMethod 1 : remove coefficients with lowest t (less significant) to maximize adjusted R-squared\n\nremove regressors with lowest t\n\nnot the one you are interested in ;)\n\nregress again\nsee if adjusted \\(R^2\\) is decreasing\n\nif so continue\notherwise cancel last step and stop\n\n\nMethod 2 : choose combination to maximize Akaike Information Criterium\n\nAIC: \\(p - log(L)\\)\n\\(L\\) is likelihood\ncomputed by all good econometric softwares"
  },
  {
    "objectID": "slides/session_4/graphs/inference.html",
    "href": "slides/session_4/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_4/Regressions.html",
    "href": "slides/session_4/Regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\n# \n\n__Use statsmodels to estimate $ = + + _2 $. Comment regression statistics.__\nWHich model would you recommend? For which purpose?\nPlot the regression with prestige\nCheck visually normality of residuals\n\n\n\nImport dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\n\n\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\n\n\nImport macrodata dataset from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html). Describe briefly its content using the metadata.\n\nimport statsmodels.api as sm\ndataset = sm.datasets.macrodata.load_pandas()\n\n\n# the dataset object contains some data on the dataset: explore them (dataset.+Tab)\n\nExtract the dataframe from the dataset object. Print first lines and summary statistics.\n\ndf = dataset.data\ndf\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.980\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.150\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.350\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.370\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.540\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198\n2008.0\n3.0\n13324.600\n9267.7\n1990.693\n991.551\n9838.3\n216.889\n1474.7\n1.17\n6.0\n305.270\n-3.16\n4.33\n\n\n199\n2008.0\n4.0\n13141.920\n9195.3\n1857.661\n1007.273\n9920.4\n212.174\n1576.5\n0.12\n6.9\n305.952\n-8.79\n8.91\n\n\n200\n2009.0\n1.0\n12925.410\n9209.2\n1558.494\n996.287\n9926.4\n212.671\n1592.8\n0.22\n8.1\n306.547\n0.94\n-0.71\n\n\n201\n2009.0\n2.0\n12901.504\n9189.0\n1456.678\n1023.528\n10077.5\n214.469\n1653.6\n0.18\n9.2\n307.226\n3.37\n-3.19\n\n\n202\n2009.0\n3.0\n12990.341\n9256.0\n1486.398\n1044.088\n10040.6\n216.385\n1673.9\n0.12\n9.6\n308.013\n3.56\n-3.44\n\n\n\n\n203 rows × 14 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\n\n\nCompute inflation as the growth and store it in the dataframe as variable π.\nAdd nominal interest rate to the database (use the Fisher relation).\nDetrend GDP using Hodrick-Prescott filter. If needed, Check wikipedia and the documentation. The result is a trend tau and a residual epsilon. Store log(tau/residual) as y\n\n\n\n\nRun the basic regression. Interpret the results.\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "slides/session_4/Regressions.html#linear-regressions",
    "href": "slides/session_4/Regressions.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\nCompute total, explained, unexplained variance. Compute R^2 statistics\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ffad5b135e0&gt;\n\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:29:50\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\n# \n\n__Use statsmodels to estimate $ = + + _2 $. Comment regression statistics.__\nWHich model would you recommend? For which purpose?\nPlot the regression with prestige\nCheck visually normality of residuals"
  },
  {
    "objectID": "slides/session_4/Regressions.html#finding-the-right-model",
    "href": "slides/session_4/Regressions.html#finding-the-right-model",
    "title": "Regressions",
    "section": "",
    "text": "Import dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?"
  },
  {
    "objectID": "slides/session_4/Regressions.html#taylor-rule",
    "href": "slides/session_4/Regressions.html#taylor-rule",
    "title": "Regressions",
    "section": "",
    "text": "In 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\n\n\nImport macrodata dataset from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html). Describe briefly its content using the metadata.\n\nimport statsmodels.api as sm\ndataset = sm.datasets.macrodata.load_pandas()\n\n\n# the dataset object contains some data on the dataset: explore them (dataset.+Tab)\n\nExtract the dataframe from the dataset object. Print first lines and summary statistics.\n\ndf = dataset.data\ndf\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.980\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.150\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.350\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.370\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.540\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198\n2008.0\n3.0\n13324.600\n9267.7\n1990.693\n991.551\n9838.3\n216.889\n1474.7\n1.17\n6.0\n305.270\n-3.16\n4.33\n\n\n199\n2008.0\n4.0\n13141.920\n9195.3\n1857.661\n1007.273\n9920.4\n212.174\n1576.5\n0.12\n6.9\n305.952\n-8.79\n8.91\n\n\n200\n2009.0\n1.0\n12925.410\n9209.2\n1558.494\n996.287\n9926.4\n212.671\n1592.8\n0.22\n8.1\n306.547\n0.94\n-0.71\n\n\n201\n2009.0\n2.0\n12901.504\n9189.0\n1456.678\n1023.528\n10077.5\n214.469\n1653.6\n0.18\n9.2\n307.226\n3.37\n-3.19\n\n\n202\n2009.0\n3.0\n12990.341\n9256.0\n1486.398\n1044.088\n10040.6\n216.385\n1673.9\n0.12\n9.6\n308.013\n3.56\n-3.44\n\n\n\n\n203 rows × 14 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\n\n\nCompute inflation as the growth and store it in the dataframe as variable π.\nAdd nominal interest rate to the database (use the Fisher relation).\nDetrend GDP using Hodrick-Prescott filter. If needed, Check wikipedia and the documentation. The result is a trend tau and a residual epsilon. Store log(tau/residual) as y"
  },
  {
    "objectID": "slides/session_4/Regressions.html#run-the-regression",
    "href": "slides/session_4/Regressions.html#run-the-regression",
    "title": "Regressions",
    "section": "",
    "text": "Run the basic regression. Interpret the results.\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "slides/session_8/index_archive.html#classification",
    "href": "slides/session_8/index_archive.html#classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/session_8/index_archive.html#classification-problems",
    "href": "slides/session_8/index_archive.html#classification-problems",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification Problems",
    "text": "Classification Problems"
  },
  {
    "objectID": "slides/session_8/index_archive.html#other-classifiers",
    "href": "slides/session_8/index_archive.html#other-classifiers",
    "title": "Introduction to Machine Learning (2)",
    "section": "Other Classifiers",
    "text": "Other Classifiers"
  },
  {
    "objectID": "slides/session_8/index_archive.html#validation",
    "href": "slides/session_8/index_archive.html#validation",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validation",
    "text": "Validation"
  },
  {
    "objectID": "slides/session_8/graphs/inference.html",
    "href": "slides/session_8/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_3/graphs/Untitled1.html",
    "href": "slides/session_3/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_3/Untitled.html",
    "href": "slides/session_3/Untitled.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "!pip install statsmodels\n\nCollecting statsmodels\n  Downloading statsmodels-0.13.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 5.1 MB/s eta 0:00:00m eta 0:00:01[36m0:00:01\nRequirement already satisfied: pandas&gt;=0.25 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.4.4)\nRequirement already satisfied: numpy&gt;=1.17 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.22.4)\nRequirement already satisfied: scipy&gt;=1.3 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (1.9.1)\nCollecting patsy&gt;=0.5.2\n  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 4.7 MB/s eta 0:00:00 MB/s eta 0:00:01\nRequirement already satisfied: packaging&gt;=21.3 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from statsmodels) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from packaging&gt;=21.3-&gt;statsmodels) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.25-&gt;statsmodels) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from pandas&gt;=0.25-&gt;statsmodels) (2022.2.1)\nRequirement already satisfied: six in /home/pablo/.local/opt/mambaforge/lib/python3.10/site-packages (from patsy&gt;=0.5.2-&gt;statsmodels) (1.16.0)\nInstalling collected packages: patsy, statsmodels\nSuccessfully installed patsy-0.5.3 statsmodels-0.13.5\n\n\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\ndfs = df.sort_values('income', ascending=False).head()\n\n\ndf.mean()\n\n/tmp/ipykernel_20796/3698961737.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  df.mean()\n\n\nincome       41.866667\neducation    52.555556\nprestige     47.688889\ndtype: float64\n\n\n\ndf.median()\n\n/tmp/ipykernel_20796/530051474.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n  df.median()\n\n\nincome       42.0\neducation    45.0\nprestige     41.0\ndtype: float64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndfs['income'].plot()\n\n\n\n\n\n\n\n\n\ndf['income'].hist()\n\n\n\n\n\n\n\n\n\ndf['education'].hist()\n\n\n\n\n\n\n\n\n\ndf['prestige'].hist()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(df['education'], df['income'],'o')\n\n\n\n\n\n\n\n\n\nplt.plot(df['prestige'], df['income'],'o')\n\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\n\nimport statsmodels.formula.api as sm\n\n\nmodel = sm.ols(formula=\"income ~ education\", data=df)\n\n\nresult = model.fit()\n\n\nresult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nWed, 01 Feb 2023\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n09:38:54\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\"income ~ education\"   ~~   income = α + β * education"
  },
  {
    "objectID": "slides/session_7/graphs/Untitled1.html",
    "href": "slides/session_7/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_7/index_handout.html",
    "href": "slides/session_7/index_handout.html",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Binary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values\n\n\n\n\n\n\n\n\nGiven a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)\n\n\n\n\n\n\n\n\n\nThe linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1.\n\n\n\n\n\n\n\nIf there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#classification-problem",
    "href": "slides/session_7/index_handout.html#classification-problem",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Binary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#logistic-regression",
    "href": "slides/session_7/index_handout.html#logistic-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "Given a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#logistic-regression-1",
    "href": "slides/session_7/index_handout.html#logistic-regression-1",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "The linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#multinomial-regression",
    "href": "slides/session_7/index_handout.html#multinomial-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "",
    "text": "If there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#common-classification-algorithms",
    "href": "slides/session_7/index_handout.html#common-classification-algorithms",
    "title": "Introduction to Machine Learning (2)",
    "section": "Common classification algorithms",
    "text": "Common classification algorithms\nThere are many:\n\nLogistic Regression\nNaive Bayes Classifier\nNearest Distance\nneural networks (replace score in sigmoid by n.n.)\nDecision Trees\nSupport Vector Machines",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#nearest-distance",
    "href": "slides/session_7/index_handout.html#nearest-distance",
    "title": "Introduction to Machine Learning (2)",
    "section": "Nearest distance",
    "text": "Nearest distance\n\n\n\nIdea:\n\nin order to predict category \\(c\\) corresponding to \\(x\\) find the closest point \\(x_0\\) in the training set\nAssign to \\(x\\) the same category as \\(x_0\\)\n\nBut this would be very susceptible to noise\nAmended idea: \\(k-nearest\\) neighbours\n\nlook for the \\(k\\) points closest to \\(x\\)\nlabel \\(x\\) with the same category as the majority of them\n\nRemark: this algorithm uses Euclidean distance. This is why it is important to normalize the dataset.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#decision-tree-random-forests",
    "href": "slides/session_7/index_handout.html#decision-tree-random-forests",
    "title": "Introduction to Machine Learning (2)",
    "section": "Decision Tree / Random Forests",
    "text": "Decision Tree / Random Forests\n\n\n\nDecision Tree\n\nrecursively find simple criteria to subdivide dataset\n\nProblems:\n\nGreedy: algorithm does not simplify branches\neasily overfits\n\nExtension : random tree forest\n\nuses several (randomly generated) trees to generate a prediction\nsolves the overfitting problem",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#support-vector-classification",
    "href": "slides/session_7/index_handout.html#support-vector-classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Support Vector Classification",
    "text": "Support Vector Classification\n\n\n\n\nSeparates data by one line (hyperplane).\n\nChooses the largest margin according to support vectors\n\nCan use a nonlinear kernel.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#all-these-algorithms-are-super-easy-to-use",
    "href": "slides/session_7/index_handout.html#all-these-algorithms-are-super-easy-to-use",
    "title": "Introduction to Machine Learning (2)",
    "section": "All these algorithms are super easy to use!",
    "text": "All these algorithms are super easy to use!\nExamples:\n\nDecision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n. . .\n\nSupport Vector\n\nfrom sklearn.svm import SVC\nclf = SVC(random_state=0)\n. . .\n\nRidge Regression\n\nfrom sklearn.linear_model import Ridge\nclf = Ridge(random_state=0)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#validity-of-a-classification-algorithm",
    "href": "slides/session_7/index_handout.html#validity-of-a-classification-algorithm",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validity of a classification algorithm",
    "text": "Validity of a classification algorithm\n\nIndependently of how the classification is made, its validity can be assessed with a similar procedure as in the regression.\nSeparate training set and test set\n\ndo not touch test set at all during the training\n\nCompute score: number of correctly identified categories\n\nnote that this is not the same as the loss function minimized by the training",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#classification-matrix",
    "href": "slides/session_7/index_handout.html#classification-matrix",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification matrix",
    "text": "Classification matrix\n\nFor binary classification, we focus on the classification matrix or confusion matrix.\n\n\n\n\nPredicted\n(0) Actual\n(1) Actual\n\n\n\n\n0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\n1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\n. . .\nWe can then define different measures:\n\nSensitivity aka True Positive Rate (TPR): \\(\\frac{TP}{FP+TP}\\)\nFalse Positive Rate (FPR): \\(\\frac{FP}{TN+FP}\\)\nOverall accuracy: \\(\\frac{\\text{TN}+\\text{TP}}{\\text{total}}\\)\n\n. . .\nWhich one to favour depends on the use case",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#example-london-police",
    "href": "slides/session_7/index_handout.html#example-london-police",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example: London Police",
    "text": "Example: London Police\n\n\n\nPolice cameras in London\n\n\nAccording to London Police the cameras in London have\n\nTrue Positive Identification rate of over 80% at a fixed number of False Positive Alerts.29 nov. 2022\n\n. . .\nInterpretation? Is failure rate too high?",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#example",
    "href": "slides/session_7/index_handout.html#example",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example",
    "text": "Example\n\n\n\nIn-sample confusion matrix\n\n\nBased on consumer data, an algorithm tries to predict the credit score from.\nCan you calculate: FPR, TPR and overall accuracy?",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index_handout.html#confusion-matrix-with-sklearn",
    "href": "slides/session_7/index_handout.html#confusion-matrix-with-sklearn",
    "title": "Introduction to Machine Learning (2)",
    "section": "Confusion matrix with sklearn",
    "text": "Confusion matrix with sklearn\n\nPredict on the test set:\n\ny_pred = model.predict(x_test)\n\nCompute confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning (2)"
    ]
  },
  {
    "objectID": "slides/session_7/index.html#classification-problem",
    "href": "slides/session_7/index.html#classification-problem",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification problem",
    "text": "Classification problem\n\nBinary Classification\n\nGoal is to make a prediction \\(c_n = f(x_{1,1}, ... x_{k,n})\\) …\n…where \\(c_i\\) is a binary variable (\\(\\in\\{0,1\\}\\))\n… and \\((x_{i,n})_k\\), \\(k\\) different features to predict \\(c_n\\)\n\nMulticategory Classification\n\nThe variable to predict takes values in a non ordered set with \\(p\\) different values"
  },
  {
    "objectID": "slides/session_7/index.html#logistic-regression",
    "href": "slides/session_7/index.html#logistic-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\nGiven a regression model (a linear predictor) \\[ a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n \\]\none can build a classification model: \\[ f(x_1, ..., x_n) = \\sigma( a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n )\\] where \\(\\sigma(x)=\\frac{1}{1+\\exp(-x)}\\) is the logistic function a.k.a. sigmoid\nThe loss function to minimize is: \\[L() = \\sum_n (c_n - \\sigma( a_{0} + a_1 x_{1,n} + a_2 x_{2,n} + \\cdots a_k x_{k,n} ) )^2\\]\nThis works for any regression model (LASSO, RIDGE, nonlinear…)"
  },
  {
    "objectID": "slides/session_7/index.html#logistic-regression-1",
    "href": "slides/session_7/index.html#logistic-regression-1",
    "title": "Introduction to Machine Learning (2)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nThe linear model predicts an intensity/score (not a category) \\[ f(x_1, ..., x_n) = \\sigma( \\underbrace{a_0 + a_1 x_1 + a_2 x_2 + \\cdots a_n x_n }_{\\text{score}})\\]\nTo make a prediction: round to 0 or 1."
  },
  {
    "objectID": "slides/session_7/index.html#multinomial-regression",
    "href": "slides/session_7/index.html#multinomial-regression",
    "title": "Introduction to Machine Learning (2)",
    "section": "Multinomial regression",
    "text": "Multinomial regression\n\n\nIf there are \\(P\\) categories to predict:\n\nbuild a linear predictor \\(f_p\\) for each category \\(p\\)\nlinear predictor is also called score\n\nTo predict:\n\nevaluate the score of all categories\nchoose the one with highest score\n\nTo train the model:\n\ntrain separately all scores (works for any predictor, not just linear)\n… there are more subtle approaches (not here)"
  },
  {
    "objectID": "slides/session_7/index.html#common-classification-algorithms",
    "href": "slides/session_7/index.html#common-classification-algorithms",
    "title": "Introduction to Machine Learning (2)",
    "section": "Common classification algorithms",
    "text": "Common classification algorithms\nThere are many:\n\nLogistic Regression\nNaive Bayes Classifier\nNearest Distance\nneural networks (replace score in sigmoid by n.n.)\nDecision Trees\nSupport Vector Machines"
  },
  {
    "objectID": "slides/session_7/index.html#nearest-distance",
    "href": "slides/session_7/index.html#nearest-distance",
    "title": "Introduction to Machine Learning (2)",
    "section": "Nearest distance",
    "text": "Nearest distance\n\n\n\nIdea:\n\nin order to predict category \\(c\\) corresponding to \\(x\\) find the closest point \\(x_0\\) in the training set\nAssign to \\(x\\) the same category as \\(x_0\\)\n\nBut this would be very susceptible to noise\nAmended idea: \\(k-nearest\\) neighbours\n\nlook for the \\(k\\) points closest to \\(x\\)\nlabel \\(x\\) with the same category as the majority of them\n\nRemark: this algorithm uses Euclidean distance. This is why it is important to normalize the dataset."
  },
  {
    "objectID": "slides/session_7/index.html#decision-tree-random-forests",
    "href": "slides/session_7/index.html#decision-tree-random-forests",
    "title": "Introduction to Machine Learning (2)",
    "section": "Decision Tree / Random Forests",
    "text": "Decision Tree / Random Forests\n\n\n\nDecision Tree\n\nrecursively find simple criteria to subdivide dataset\n\nProblems:\n\nGreedy: algorithm does not simplify branches\neasily overfits\n\nExtension : random tree forest\n\nuses several (randomly generated) trees to generate a prediction\nsolves the overfitting problem"
  },
  {
    "objectID": "slides/session_7/index.html#support-vector-classification",
    "href": "slides/session_7/index.html#support-vector-classification",
    "title": "Introduction to Machine Learning (2)",
    "section": "Support Vector Classification",
    "text": "Support Vector Classification\n\n\n\n\nSeparates data by one line (hyperplane).\n\nChooses the largest margin according to support vectors\n\nCan use a nonlinear kernel."
  },
  {
    "objectID": "slides/session_7/index.html#all-these-algorithms-are-super-easy-to-use",
    "href": "slides/session_7/index.html#all-these-algorithms-are-super-easy-to-use",
    "title": "Introduction to Machine Learning (2)",
    "section": "All these algorithms are super easy to use!",
    "text": "All these algorithms are super easy to use!\nExamples:\n\nDecision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n\n\nSupport Vector\n\nfrom sklearn.svm import SVC\nclf = SVC(random_state=0)\n\n\n\nRidge Regression\n\nfrom sklearn.linear_model import Ridge\nclf = Ridge(random_state=0)"
  },
  {
    "objectID": "slides/session_7/index.html#validity-of-a-classification-algorithm",
    "href": "slides/session_7/index.html#validity-of-a-classification-algorithm",
    "title": "Introduction to Machine Learning (2)",
    "section": "Validity of a classification algorithm",
    "text": "Validity of a classification algorithm\n\nIndependently of how the classification is made, its validity can be assessed with a similar procedure as in the regression.\nSeparate training set and test set\n\ndo not touch test set at all during the training\n\nCompute score: number of correctly identified categories\n\nnote that this is not the same as the loss function minimized by the training"
  },
  {
    "objectID": "slides/session_7/index.html#classification-matrix",
    "href": "slides/session_7/index.html#classification-matrix",
    "title": "Introduction to Machine Learning (2)",
    "section": "Classification matrix",
    "text": "Classification matrix\n\nFor binary classification, we focus on the classification matrix or confusion matrix.\n\n\n\n\nPredicted\n(0) Actual\n(1) Actual\n\n\n\n\n0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\n1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\n\nWe can then define different measures:\n\nSensitivity aka True Positive Rate (TPR): \\(\\frac{TP}{FP+TP}\\)\nFalse Positive Rate (FPR): \\(\\frac{FP}{TN+FP}\\)\nOverall accuracy: \\(\\frac{\\text{TN}+\\text{TP}}{\\text{total}}\\)\n\n\n\nWhich one to favour depends on the use case"
  },
  {
    "objectID": "slides/session_7/index.html#example-london-police",
    "href": "slides/session_7/index.html#example-london-police",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example: London Police",
    "text": "Example: London Police\n\nPolice cameras in LondonAccording to London Police the cameras in London have\n\nTrue Positive Identification rate of over 80% at a fixed number of False Positive Alerts.29 nov. 2022\n\n\nInterpretation? Is failure rate too high?"
  },
  {
    "objectID": "slides/session_7/index.html#example",
    "href": "slides/session_7/index.html#example",
    "title": "Introduction to Machine Learning (2)",
    "section": "Example",
    "text": "Example\n\nIn-sample confusion matrixBased on consumer data, an algorithm tries to predict the credit score from.\nCan you calculate: FPR, TPR and overall accuracy?"
  },
  {
    "objectID": "slides/session_7/index.html#confusion-matrix-with-sklearn",
    "href": "slides/session_7/index.html#confusion-matrix-with-sklearn",
    "title": "Introduction to Machine Learning (2)",
    "section": "Confusion matrix with sklearn",
    "text": "Confusion matrix with sklearn\n\nPredict on the test set:\n\ny_pred = model.predict(x_test)\n\nCompute confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"
  },
  {
    "objectID": "slides/session_1/index.html#general-1",
    "href": "slides/session_1/index.html#general-1",
    "title": "Introduction",
    "section": "General (1)",
    "text": "General (1)\n\nYour instructors:\n\nPablo Winadnt: pwinant@escp.eu (course and tutorials)\nRayane Hanifi: rayane.hanifi@edu.escp.eu (tutorials)\nÉmilie Rannou: (session on LLMs)\n\nHint: start your mail subject by [dbe]",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#general-2",
    "href": "slides/session_1/index.html#general-2",
    "title": "Introduction",
    "section": "General (2)",
    "text": "General (2)\n\nAll course material on www.mosphere.fr/dbe\nTutorials on Nuvolos\n\na datascience platform\nyou’ll be able to keep a full backup afterwards\n\nCollaboration between students is strongly encouraged",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#so-what-will-we-do",
    "href": "slides/session_1/index.html#so-what-will-we-do",
    "title": "Introduction",
    "section": "So what will we do ?",
    "text": "So what will we do ?\n\n\nProgramming\nEconometrics / Machine Learning\nTalk about economics",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#data-based-economics-1",
    "href": "slides/session_1/index.html#data-based-economics-1",
    "title": "Introduction",
    "section": "Data-based economics (1)",
    "text": "Data-based economics (1)\n\nMost economists use data all the time\n\nto illustrate facts\nto test theories",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#what-do-economists-do",
    "href": "slides/session_1/index.html#what-do-economists-do",
    "title": "Introduction",
    "section": "What do economists do ?",
    "text": "What do economists do ?\n\n\nimport data\nclean the data\n\ndeal with heterogenous sources, missing data, abnormal observerations\nsuper time consuming\nwe’ll make this part easy for you\n\ndescribe the data (statistics), visualize it\ninterpret it using a model\npresent results",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#econometricks",
    "href": "slides/session_1/index.html#econometricks",
    "title": "Introduction",
    "section": "Econometricks",
    "text": "Econometricks\n\nAn art invented by economists: \\[\\underbrace{y}_{\\text{dependent variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]\n\n\n\nMain challenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\) while controlling for \\(b\\)\nunderstand robustness of results\npredict new values of \\(y\\) for new values of \\(x\\)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#econometricks-example-1",
    "href": "slides/session_1/index.html#econometricks-example-1",
    "title": "Introduction",
    "section": "Econometricks: Example 1",
    "text": "Econometricks: Example 1\nCheck out the following website: How happy are you?\n\nWhat is \\(x\\) ? What is \\(y\\) ?",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#econometricks-example-2",
    "href": "slides/session_1/index.html#econometricks-example-2",
    "title": "Introduction",
    "section": "Econometricks: Example 2",
    "text": "Econometricks: Example 2\n\\[\\underbrace{y}_{\\text{dependent variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]\n\n\nA famous study:\n\nyoung men who go to war receive in average lower wages when they return than men who didn’t go to war\n… is it because they skipped college?\n… or did they choose to go to war because they were less skilled for college?\n\nHow to decide?",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#econometricks-example-2-1",
    "href": "slides/session_1/index.html#econometricks-example-2-1",
    "title": "Introduction",
    "section": "Econometricks: Example 2",
    "text": "Econometricks: Example 2\n\\[\\underbrace{y}_{\\text{dependent variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]\n\n\nHow to decide?\n\nfind a way to extract causality\ninstrumental variables\nThis was worth a Nobel Prize! (D. Card, J. Angrist, G.W. Imbens)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#big-data-era-and-machine-learning-1",
    "href": "slides/session_1/index.html#big-data-era-and-machine-learning-1",
    "title": "Introduction",
    "section": "Big Data Era and Machine Learning (1)",
    "text": "Big Data Era and Machine Learning (1)\n\nData has become very abundant\nLarge amounts of data of all kinds\n\nstructured (tables, …)\nunstructured (text, images, …)\n\nMachine learning:\n\na set of powerful algorithms…\n… so powerful some call it artificial intelligence\n\nthey learn by processing data\n\n… to extract information and relations in large data sets",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#big-data-era-and-machine-learning-2",
    "href": "slides/session_1/index.html#big-data-era-and-machine-learning-2",
    "title": "Introduction",
    "section": "Big Data Era and Machine Learning (2)",
    "text": "Big Data Era and Machine Learning (2)\n\nMachine learning:\n\na set of powerful algorithms…\n… so powerful some call it artificial intelligence\n\nthey learn by processing data\n\n… to extract information and relations in large data sets\n…\n\nComparison with econometrics\n\nML has it own, partially redundant, jargon\nharder to study causality, standard deviation (precision)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#machine-learning",
    "href": "slides/session_1/index.html#machine-learning",
    "title": "Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\n\\[\\underbrace{y}_{\\text{predicted variable}} = f( \\underbrace{x}_{\\text{feature}} , a)\\]\n\nChallenge:\n\ngiven dataset \\((x_i, y_i)\\)\nfind \\(a\\), that is find a nonlinear relationship between \\(a\\) and \\(b\\)\npredict new values of \\(y\\) given new values of \\(x\\)\n\nWhat is the difference with econometrics?",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#big-data-era-and-machine-learning-1-1",
    "href": "slides/session_1/index.html#big-data-era-and-machine-learning-1-1",
    "title": "Introduction",
    "section": "Big Data Era and Machine Learning (1)",
    "text": "Big Data Era and Machine Learning (1)\n\nSentiment analysis: predict population’s optimism by analyzing tweets.\nCheck sentiment viz",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#big-data-era-and-machine-learning-2-1",
    "href": "slides/session_1/index.html#big-data-era-and-machine-learning-2-1",
    "title": "Introduction",
    "section": "Big Data Era and Machine Learning (2)",
    "text": "Big Data Era and Machine Learning (2)\n\nBeautiful people (from NVIDIA presentation)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#big-data-era-and-machine-learning-2-2",
    "href": "slides/session_1/index.html#big-data-era-and-machine-learning-2-2",
    "title": "Introduction",
    "section": "Big Data Era and Machine Learning (2)",
    "text": "Big Data Era and Machine Learning (2)\n\nBeautiful people (from NVIDIA presentation)Task: predict second and third columns from the first one.\nSolution: deep learning with artificial neural nets",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#why-program-in-python",
    "href": "slides/session_1/index.html#why-program-in-python",
    "title": "Introduction",
    "section": "Why program in Python?",
    "text": "Why program in Python?\n\nWhy learn and use Python?\nAnd not \n\nR\nSPSS\nStata\nMatlab\nC\nJavascript\nSQL\n…",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#because",
    "href": "slides/session_1/index.html#because",
    "title": "Introduction",
    "section": "Because",
    "text": "Because\nPython is: Easy",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#because-1",
    "href": "slides/session_1/index.html#because-1",
    "title": "Introduction",
    "section": "Because",
    "text": "Because\nPython is: Free",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#because-2",
    "href": "slides/session_1/index.html#because-2",
    "title": "Introduction",
    "section": "Because",
    "text": "Because\nPython is: Popular\n\n(TIOBE Index)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#because-3",
    "href": "slides/session_1/index.html#because-3",
    "title": "Introduction",
    "section": "Because",
    "text": "Because\nPython has:\n\na lively community\nlots of online ressources\nlibraries for virtually anything",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#because-4",
    "href": "slides/session_1/index.html#because-4",
    "title": "Introduction",
    "section": "Because",
    "text": "Because\n   \n\nThe lingua Franca of Machine learning\n\nAll major machine learning softwares are written or interface with Python",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#why-learn-programming-1",
    "href": "slides/session_1/index.html#why-learn-programming-1",
    "title": "Introduction",
    "section": "Why learn programming ? (1)",
    "text": "Why learn programming ? (1)\n\n\nResearchers (econometricians or data scientists) spend 80% of their time writing code.\nPresentation (plots, interactive apps) is key and relies on\n\n… programming\n\nInteraction with code becomes unavoidable in business environment\n\nfixing the website\nquerying the database, …\n…",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#why-learn-programming-2",
    "href": "slides/session_1/index.html#why-learn-programming-2",
    "title": "Introduction",
    "section": "Why learn programming ? (2)",
    "text": "Why learn programming ? (2)\n\n\n\nWorth investing a bit of time to learn it\n\nyou can easily become an expert\nand can do anything\n\nPlus it’s fun\n\n\n\n\n\nimport antigravity",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#why-should-you-learn-programming-2",
    "href": "slides/session_1/index.html#why-should-you-learn-programming-2",
    "title": "Introduction",
    "section": "Why should you learn programming ? (2)",
    "text": "Why should you learn programming ? (2)\n\nimport antigravity",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#how-good-should-you-program",
    "href": "slides/session_1/index.html#how-good-should-you-program",
    "title": "Introduction",
    "section": "How good should you program ?",
    "text": "How good should you program ?\n\n\n\n\n\n\n\n\n\n\n\nWe will “assume” everybody as some prior experience with Python\nEven though some of you have possibly never touched it\nWe’ll do some catchup today\nAnd count on you to find the resources to learn what you need when you need it\nOf course you can always ask questions",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#additional-resources",
    "href": "slides/session_1/index.html#additional-resources",
    "title": "Introduction",
    "section": "Additional resources",
    "text": "Additional resources\nPlenty of online resources to learn python/econometrics/machine learning\n\nlearnpython sponsored by datacamp\nquantecon: designed for economists, good examples of projects\nPython Data Science Handbook: by Jake Van der Plas, very complete. Online free version.\nIntroduction to Econometrics with R, in R but very clear (beginner and advanced versions)",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_1/index.html#quantecon",
    "href": "slides/session_1/index.html#quantecon",
    "title": "Introduction",
    "section": "Quantecon",
    "text": "Quantecon\n\n\n\n\n\n\nQuantEcon\n\n\n\n\n\n\n\nTom Sargent\n\n\n\n\n\n\n\nJohn Stachurski\n\n\n\n\n\n\n \n\nQuantecon: free online lectures to learn python programming and (advanced) economics\n\nnow with a section on datascience\nit is excellent!\nwe will use some of it today",
    "crumbs": [
      "lectures",
      "Introduction"
    ]
  },
  {
    "objectID": "slides/session_10/graphs/inference.html",
    "href": "slides/session_10/graphs/inference.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef generate_dataset(μ1, μ2, α, β, σ, N=10):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    return pd.DataFrame({'x': xvec, 'y': yvec})\n\n\ndf = generate_dataset(0.0, 1.0, 0.1, 0.8, 0.1)\n\n\nplt.plot(df['x'], df['y'], 'o')\nplt.grid()\n\n\n\n\n\n\n\n\n\ndef plot_distribution(α, β, σ, N=100000, μ1=0.0, μ2=1.0):\n    xvec = np.random.uniform(μ1, μ2, N)\n    yvec = α + β*xvec + np.random.normal(size=N)*σ\n    plt.plot(xvec, yvec, '.r', alpha=0.005)\n    plt.plot(xvec, α + β*xvec, color='black')\n\n# missing ridge line\n\n\nimport statsmodels\n\n\nμ1 = 0\nμ2 = 1.0\nα = 0.1\nβ = 0.8\nσ = 0.2\nN = 20\nK = 1000\n\n\nimport statsmodels.formula.api as smf\n\n\ndf = generate_dataset(μ1, μ2, α, β, σ, N=N)\n\n\nres = smf.ols(formula='y ~ x + 1', data=df).fit()\nparams = res.params\nαhat = params['Intercept']\nβhat = params['x']\nσhat = res.resid.std()\n\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.692\n\n\nModel:\nOLS\nAdj. R-squared:\n0.675\n\n\nMethod:\nLeast Squares\nF-statistic:\n40.48\n\n\nDate:\nTue, 26 Jan 2021\nProb (F-statistic):\n5.41e-06\n\n\nTime:\n04:02:36\nLog-Likelihood:\n7.6662\n\n\nNo. Observations:\n20\nAIC:\n-11.33\n\n\nDf Residuals:\n18\nBIC:\n-9.341\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.1210\n0.077\n1.565\n0.135\n-0.041\n0.283\n\n\nx\n0.7941\n0.125\n6.362\n0.000\n0.532\n1.056\n\n\n\n\n\n\nOmnibus:\n1.410\nDurbin-Watson:\n1.507\n\n\nProb(Omnibus):\n0.494\nJarque-Bera (JB):\n0.890\n\n\nSkew:\n-0.081\nProb(JB):\n0.641\n\n\nKurtosis:\n1.979\nCond. No.\n4.20\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nres.predict(df['x'])\n\n0     0.326200\n1     0.211704\n2     0.798819\n3     0.603306\n4     0.573319\n5     0.823919\n6     0.740622\n7     0.503227\n8     0.292622\n9     0.489566\n10    0.138720\n11    0.355157\n12    0.594171\n13    0.883917\n14    0.266229\n15    0.827021\n16    0.912376\n17    0.163088\n18    0.684858\n19    0.732782\ndtype: float64\n\n\n\nfor i in [1,2,3]:\n    \n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {α:.2f} + {β:.2f} x + {σ:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n\n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    if i&gt;=3:\n        plt.plot(df['x'], res.predict(), label=f'$\\hat{{α}}={αhat:.2f}; \\hat{{β}}={βhat:.2f}$')\n        plt.legend(loc='lower right')\n    plt.title(\"Random Draw\")\n    plt.grid()\n    \n    plt.savefig(f\"regression_uncertainty_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats\n\n\ndatasets = [generate_dataset(μ1, μ2, αhat, βhat, σhat, N=N) for i in range(K)]\nall_params = [smf.ols(formula='x ~ y + 1', data=df).fit() for df in datasets]\nαvec = np.array( [e.params['Intercept'] for e in all_params] )\nβvec = np.array( [e.params['y'] for e in all_params] )\n\n\ngkd = scipy.stats.kde.gaussian_kde(βvec)\n\n\nfor i in [1,2,3,4,5,6,7,8,9,10,100]:\n\n    fig = plt.figure(figsize=(10,14))\n    plt.subplot(311)\n    plot_distribution(0.1, 0.8, 0.2)\n    plt.grid()\n    plt.title(f\"True Distribution: $y = {αhat:.2f} + {βhat:.2f} x + {σhat:.2f} u$\")\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    \n    plt.subplot(312)\n    plt.xlim(0,1)\n    plt.ylim(-0.5, 1.5)\n    df = datasets[i]\n    if i&gt;=2:\n        plt.plot(df['x'], df['y'], 'o')\n    plt.title(\"Random Draw\")\n    plt.grid()\n\n    plt.subplot(313)\n    if i==3:\n        plt.plot(βvec[i], βvec[i]*0, 'o')\n    if i&gt;4:\n        plt.plot(βvec[3:i], βvec[3:i]*0, 'o')\n    if i&gt;10:\n        xx = np.linspace(0.2, 1.4, 10000)\n        plt.plot( βvec, gkd.pdf(βvec), '.')\n    plt.title(\"Distribution of β\")\n    plt.xlim(0.2, 1.4)\n    plt.ylim(-0.1, 4)\n    plt.grid()\n\n    plt.tight_layout()\n\n    plt.savefig(f\"random_estimates_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.plot( βvec, βvec*0, 'o')"
  },
  {
    "objectID": "slides/session_6/graphs/Untitled1.html",
    "href": "slides/session_6/graphs/Untitled1.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\n\ndf.cov()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n597.072727\n526.871212\n645.071212\n\n\neducation\n526.871212\n885.707071\n798.904040\n\n\nprestige\n645.071212\n798.904040\n992.901010\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(8,6))\nplt.plot(df['education'],df['income'],'o')\nplt.grid()\nplt.xlabel(\"x (Education)\")\nplt.ylabel(\"y (Income)\")\nplt.savefig(\"data_description.png\")\n\n\n\n\n\n\n\n\n\nfor i in [1,2,3]:\n    xvec = np.linspace(10,100)\n\n    plt.figure(figsize=(12,8))\n    plt.plot(df['education'],df['income'],'o')\n\n    plt.plot(xvec, xvec * 0 + 50)\n    if i&gt;=2:\n        plt.plot(xvec, xvec )\n    if i&gt;=3:\n        plt.plot(xvec,  90- 0.6*xvec )\n\n    plt.grid()\n    plt.xlabel(\"x (Education)\")\n    plt.ylabel(\"y (Income)\")\n    plt.savefig(f\"which_line_{i}.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\nimport matplotlib.patches as patches\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nplt.vlines(x, y+h, y, color='red')\n\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"error_0.png\")\n\n\n\n\n\n\n\n\n\nplt.vlines?\n\n\nSignature:\nplt.vlines(\n    x,\n    ymin,\n    ymax,\n    colors=None,\n    linestyles='solid',\n    label='',\n    *,\n    data=None,\n    **kwargs,\n)\nDocstring:\nPlot vertical lines.\nPlot vertical lines at each *x* from *ymin* to *ymax*.\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have same length.\ncolors : list of colors, default: :rc:`lines.color`\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\nlabel : str, default: ''\nReturns\n-------\n`~matplotlib.collections.LineCollection`\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.LineCollection` properties.\nSee Also\n--------\nhlines : horizontal lines\naxvline: vertical line across the axes\nNotes\n-----\n.. note::\n    In addition to the above described arguments, this function can take\n    a *data* keyword argument. If such a *data* argument is given,\n    the following arguments can also be string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception):\n    *x*, *ymin*, *ymax*, *colors*.\n    Objects passed as **data** must support item access (``data[s]``) and\n    membership test (``s in data``).\nFile:      ~/.local/opt/miniconda/lib/python3.8/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\n\n\na = 0.1\nb = 1.0\nind = 23\n\n\napprox =  a + b*xvec\n\n# Create figure and axes\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\nplt.plot(df['education'],df['income'],'o')\nplt.plot(xvec, approx, color='red')\n\nx, y = df['education'][ind], df['income'][ind]\nplt.plot(x, y, 'o', color='red' )\np = a+b*x\nplt.grid(True)\nh = abs(p-y)\nif p-y&gt;0:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n    ax.add_patch(rect)\n    \nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.savefig(f\"errors_{1}.png\")\n\n\n\n\n\n\n\n\n\ndef L(a,b):\n    Δ = a + b*df['education'] - df['income']\n    return (Δ**2).sum()\n\n\na = 0.1\nb = 0.8\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_2.png\")\n\n\n\n\n\n\n\n\n\na = 90\nb = -0.6\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_3.png\")\n\n\n\n\n\n\n\n\n\nimport scipy.optimize\n\n\nscipy.optimize.minimize(lambda x: L(x[0], x[1]),np.array([0.5, 0.5]))\n\n      fun: 12480.970174488397\n hess_inv: array([[ 7.14169839e-09, -3.91281920e-09],\n       [-3.91281920e-09,  2.46663613e-09]])\n      jac: array([0.00024414, 0.00012207])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 57\n      nit: 7\n     njev: 19\n   status: 2\n  success: False\n        x: array([10.60350224,  0.59485938])\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red')\n\nplt.grid(True)\nfor ind in range(df.shape[0]):\n    \n    x, y = df['education'][ind], df['income'][ind]\n    p = a+b*x\n\n    h = abs(p-y)\n    if p-y&gt;0:\n        # Create a Rectangle patch\n        rect = patches.Rectangle((x,y),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\n    else:\n        rect = patches.Rectangle((x,y-h),h,h,linewidth=1, color='red', fill=True, alpha=0.05)\n        ax.add_patch(rect)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"errors_4.png\")\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  a + b*xvec\n\n\n# plt.figure(figsize=(8,6))\nfig,ax = plt.subplots(1,figsize=(12,8))\n\n\nplt.plot(df['education'],df['income'],'o', label=f\"L({a,b})={L(a,b)}\")\nplt.plot(xvec, approx, color='red', alpha=0.5)\n\nplt.plot(60, a + b*60, 'o', color='red',)\n\nprint(a+b*60)\nplt.xlim(0,140)\nplt.ylim(0,100)\nplt.legend(loc='upper right')\nplt.savefig(f\"prediction.png\")\n\n45.4\n\n\n\n\n\n\n\n\n\n\na = 10\nb = 0.59\n\napprox =  (a + b*df['education'] - df['income'])\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(121)\nplt.plot(approx)\nplt.grid(False)\nplt.title(\"Residuals\")\n\n\nplt.subplot(122)\ndistplot(approx)\nplt.title(\"Distribution of residuals\")\nplt.grid()\n\nplt.savefig(\"residuals.png\")\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n(a + b*df['education'] - df['income']).std()\n\n16.842782676352154\n\n\n\n\n\n/home/pablo/.local/opt/miniconda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f\n\n\nf(0.3)\n\nTypeError: _parse_args() missing 1 required positional argument: 'dfd'\n\n\n\nnp.rand\n\n\nK = 100\nxvec = np.linspace(0,1,K)\ne1 = np.random.randn(K)*0.1\nyvec = 0.1 + xvec*0.4 + e1\ne2 = np.random.randn(K)*0.05\nyvec2 = 0.1 + xvec*(xvec-1)/2 + e2\ne3 = np.random.randn(K)*xvec/2\nyvec3 = 0.1 + xvec + e3\n\nyvec4 = 0.1 + np.sin(xvec*6) + np.random.randn(K)*xvec/2\n\n\nfrom dolo.numeric.processes import VAR1\n\n\nsim = VAR1( ρ=0.8, Σ=0.001).simulate(N=1,T=100)\nyvec4 = 0.1 + xvec*0.4 + sim.ravel()\n\n\nplt.figure(figsize=(18,6))\nplt.subplot(241)\nplt.plot(xvec, yvec,'o')\nplt.plot(xvec, 0.1 + xvec*0.4 )\nplt.ylabel(\"Series\")\nplt.title(\"white noise\")\nplt.subplot(242)\nplt.plot(xvec, yvec2, 'o')\nplt.plot(xvec, yvec2*0)\nplt.title('nonlinear')\nplt.subplot(243)\nplt.plot(xvec, yvec3,'o')\nplt.plot(xvec, 0.1 + xvec)\nplt.title('heteroskedastic')\nplt.subplot(244)\nplt.plot(xvec, yvec4,'o')\nplt.plot(xvec, xvec*0.6)\n\nplt.title('correlated')\n\n\nplt.subplot(245)\nplt.plot(xvec, e1,'o')\nplt.ylabel(\"Residuals\")\nplt.subplot(246)\nplt.plot(xvec, yvec2-0.075, 'o')\n\nplt.subplot(247)\nplt.plot(xvec, e3,'o')\nplt.subplot(248)\nplt.plot(xvec, sim.ravel(),'o')\n\nplt.tight_layout()\n\nplt.savefig(\"residuals_circus.png\")"
  },
  {
    "objectID": "slides/session_6/index_handout.html#what-is-machine-learning-1",
    "href": "slides/session_6/index_handout.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#what-about-artificial-intelligence",
    "href": "slides/session_6/index_handout.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#econometrics-vs-machine-learning",
    "href": "slides/session_6/index_handout.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#data-types",
    "href": "slides/session_6/index_handout.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#tabular-data",
    "href": "slides/session_6/index_handout.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\n\n\ntabular data",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#networks",
    "href": "slides/session_6/index_handout.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#big-data-1",
    "href": "slides/session_6/index_handout.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#big-subfields-of-machine-learning",
    "href": "slides/session_6/index_handout.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\nsupervised: regression:\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\nsupervised: regression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\nsupervised: regression\n\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\nclassification\n\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nunsupervised\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#difference-with-traditional-regression",
    "href": "slides/session_6/index_handout.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#difference-with-traditional-regression-1",
    "href": "slides/session_6/index_handout.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n. . .\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#difference-with-traditional-regression-2",
    "href": "slides/session_6/index_handout.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#long-data",
    "href": "slides/session_6/index_handout.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations.\n\n\n\n\n\nModern society is gathering a lot of data.\n\nin doesn’t fit in the computer memory so we can’t run a basic regression\n\nIn some cases we would also like to update our model continuously:\n\nincremental regression\n\n\n\nWe need a way to fit a model on a subset of the data at a time.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#long-data-1",
    "href": "slides/session_6/index_handout.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#formalisation-a-typical-machine-learning-task",
    "href": "slides/session_6/index_handout.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#training-gradient-descent",
    "href": "slides/session_6/index_handout.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#some-possible-issues",
    "href": "slides/session_6/index_handout.html#some-possible-issues",
    "title": "Introduction to Machine Learning",
    "section": "Some possible issues",
    "text": "Some possible issues\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#wide-data",
    "href": "slides/session_6/index_handout.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n. . .\nProblem:\n\nwith many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#wide-data-regression",
    "href": "slides/session_6/index_handout.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\nMain Idea: penalize non-zero coefficients to encourage scarcity\n. . .\n\n\n\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\n\n\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#training",
    "href": "slides/session_6/index_handout.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#example-imf-challenge",
    "href": "slides/session_6/index_handout.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#nonlinear-regression-1",
    "href": "slides/session_6/index_handout.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#how-to-evaluate-the-machine-learning",
    "href": "slides/session_6/index_handout.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n. . .\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\n. . .\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#how-to-evaluate-the-machine-learning-1",
    "href": "slides/session_6/index_handout.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter).",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#section",
    "href": "slides/session_6/index_handout.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\n\n\n. . .\nThe test set reveals that orange model is overfitting.",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#how-to-choose-the-validation-set",
    "href": "slides/session_6/index_handout.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\n. . .\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n. . .\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#how-to-choose-the-validation-set-1",
    "href": "slides/session_6/index_handout.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#another-library-to-perform-regressions",
    "href": "slides/session_6/index_handout.html#another-library-to-perform-regressions",
    "title": "Introduction to Machine Learning",
    "section": "Another library to perform regressions",
    "text": "Another library to perform regressions\n\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#in-practice",
    "href": "slides/session_6/index_handout.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index_handout.html#k-fold-validation-with-sklearn",
    "href": "slides/session_6/index_handout.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test",
    "crumbs": [
      "lectures",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "slides/session_6/index.html#regressions",
    "href": "slides/session_6/index.html#regressions",
    "title": "Introduction to Machine Learning",
    "section": "Regressions",
    "text": "Regressions"
  },
  {
    "objectID": "slides/session_6/index.html#what-is-machine-learning-1",
    "href": "slides/session_6/index.html#what-is-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine learning?",
    "text": "What is Machine learning?\nDefinition Candidates:\nArthur Samuel: Field of study that gives computers the ability to learn without being explicitly programmed\nTom Mitchell: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/session_6/index.html#what-about-artificial-intelligence",
    "href": "slides/session_6/index.html#what-about-artificial-intelligence",
    "title": "Introduction to Machine Learning",
    "section": "What about artificial intelligence ?",
    "text": "What about artificial intelligence ?\n\n\n\nAIs\n\nthink and learn\nmimmic human cognition"
  },
  {
    "objectID": "slides/session_6/index.html#econometrics-vs-machine-learning",
    "href": "slides/session_6/index.html#econometrics-vs-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Econometrics vs Machine Learning",
    "text": "Econometrics vs Machine Learning\n\nEconometrics is essentially a subfield of machine learning with a different jargon and a focus on:\n\nstudying properties and validity of results\n\ndata is scarce\ninference\n\nsingling out effects of specific explanatory variables\nestablishing causality\n\nMachine learning:\n\nstructure data\nmake predictions (interpolate data)"
  },
  {
    "objectID": "slides/session_6/index.html#data-types",
    "href": "slides/session_6/index.html#data-types",
    "title": "Introduction to Machine Learning",
    "section": "Data types",
    "text": "Data types\n\nstructured:\n\ntabular\n\nlong\nwide\n\n\nunstructured:\n\nfiles\nnetworks\ntext, mails\nimages, sound"
  },
  {
    "objectID": "slides/session_6/index.html#tabular-data",
    "href": "slides/session_6/index.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\ntabular data"
  },
  {
    "objectID": "slides/session_6/index.html#networks",
    "href": "slides/session_6/index.html#networks",
    "title": "Introduction to Machine Learning",
    "section": "Networks",
    "text": "Networks\n\nBanking networks\nProduction network"
  },
  {
    "objectID": "slides/session_6/index.html#big-data-1",
    "href": "slides/session_6/index.html#big-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nBig data:\n\nwide data (K&gt;&gt;N)\nlong data (N&gt;&gt;K)\nheterogenous, unstructured data\n\nMight not even fit in memory\n\nout of core computations\nlearn from a subset of the data"
  },
  {
    "objectID": "slides/session_6/index.html#big-subfields-of-machine-learning",
    "href": "slides/session_6/index.html#big-subfields-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Big Subfields of Machine Learning",
    "text": "Big Subfields of Machine Learning\n\n\n\nTraditional classification\n\nsupervised (labelled data)\n\nregression: predict quantity\nclassification: predict index (categorical variable)\n\nunsupervised (no labels)\n\ndimension reduction\nclustering\n\nsemi-supervised / self-supervised\nreinforcement learning\n\nBazillions of different algorithms: https://scikit-learn.org/stable/user_guide.html\n\n\n\n\nsupervised: regression:\n\n\n\nAge\n\n\nActivity\n\n\nSalary\n\n\n\n\n23\n\n\nExplorer\n\n\n1200\n\n\n\n\n40\n\n\nMortician\n\n\n2000\n\n\n\n\n45\n\n\nMortician\n\n\n2500\n\n\n\n\n33\n\n\nMovie Star\n\n\n3000\n\n\n\n\n35\n\n\nExplorer\n\n\n???\n\n\n\n\n\nsupervised: regression:\n\nPredict: \\(y = f(x; \\theta)\\)\n\n\n\n\nsupervised: regression\n\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nsupervised: classification\n\nOutput is discrete\nRegular trick: \\(\\sigma(f(x; \\theta))\\) where \\(\\sigma(x)=\\frac{1}{1-e^{-x}}\\)\n\n\n\n\nclassification\n\n\n\n\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\n???\n\n\n\n\n\nunsupervised\n\n\n\nAge\n\n\nSalary\n\n\nActivity\n\n\n\n\n23\n\n\n1200\n\n\nExplorer\n\n\n\n\n40\n\n\n2000\n\n\nMortician\n\n\n\n\n45\n\n\n2500\n\n\nMortician\n\n\n\n\n33\n\n\n3000\n\n\nMovie Star\n\n\n\n\n35\n\n\n3000\n\n\nExplorer\n\n\n\n\n\nunsupervised\n\norganize data without labels\n\ndimension reduction: describe data with less parameters\nclustering: sort data into “similar groups” (exemple)\n\n\n\n\nunsupervised: clustering\n\n\n\nkmeansclustering\n\n\n\n\nunsupervised: clustering\nWomen buying dresses during the year:"
  },
  {
    "objectID": "slides/session_6/index.html#difference-with-traditional-regression",
    "href": "slides/session_6/index.html#difference-with-traditional-regression",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{explained variable}} = a \\underbrace{x}_{\\text{explanatory variable}} + b\\]"
  },
  {
    "objectID": "slides/session_6/index.html#difference-with-traditional-regression-1",
    "href": "slides/session_6/index.html#difference-with-traditional-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\\[\\underbrace{y}_{\\text{labels}} = a \\underbrace{x}_{\\text{features}} + b\\]\n\n\n\n\n\n\n\n\nEconometrics\nMachine learning\n\n\n\n\nRegressand / independent variable / explanatory variable\nFeatures\n\n\nRegressor / dependent variable / explained variable\nLabels\n\n\nRegression\nModel Training"
  },
  {
    "objectID": "slides/session_6/index.html#difference-with-traditional-regression-2",
    "href": "slides/session_6/index.html#difference-with-traditional-regression-2",
    "title": "Introduction to Machine Learning",
    "section": "Difference with traditional regression",
    "text": "Difference with traditional regression\n\nBig data requires other means to process the data:\n\ndata is long: so many observations \\(x\\) doesn’t fit in the memory\n\nneed to use incremental training method to use only a subsample at a time\n\ndata is wide: so many features, the model is crudely overspecified\n\nneed to build dimension reduction into the objective\n\ndata is nonlinear:\n\nuse nonlinear model (and nonlinear training)\n\ndata is not a simple vector…\n\nsame as nonlinear"
  },
  {
    "objectID": "slides/session_6/index.html#long-data",
    "href": "slides/session_6/index.html#long-data",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\nLong data is characterized by a high number of observations.\n\n\n\n\n\nModern society is gathering a lot of data.\n\nin doesn’t fit in the computer memory so we can’t run a basic regression\n\nIn some cases we would also like to update our model continuously:\n\nincremental regression\n\n\n\nWe need a way to fit a model on a subset of the data at a time."
  },
  {
    "objectID": "slides/session_6/index.html#long-data-1",
    "href": "slides/session_6/index.html#long-data-1",
    "title": "Introduction to Machine Learning",
    "section": "Long data",
    "text": "Long data\n\n\nTraditional regression:\n\nfull sample \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nOLS: \\(\\min_{a,b} \\sum_{i=1}^N (a x_i + b - y_i)^2\\)\nclosed-form solution: \\(a = X^{\\prime}X Y\\) and \\(b= ...\\)\nhard to compute if \\(X\\) is very big\n\n\n\nIncremental learning:\n\ngiven initial \\(a_n\\), \\(b_n\\)\npick \\(N\\) random observations (the batch)\n\nregress them to get new estimate \\(a\\), \\(b\\)\nthis minimizes the square of errors\n\nupdate with learning rate \\(\\beta\\):\n\n\\(a_{n+1} \\leftarrow a_n (1-\\beta_n) + \\beta_n a\\)\n\\(b_{n+1} \\leftarrow b_n (1-\\beta_n) + \\beta_n b\\)\n\nprocess is not biased (that is \\(a\\) converges to the true value) as long as one decreases \\(\\beta\\) sufficiently fast over time (ex: \\(\\beta_n=\\frac{1}{n}\\))"
  },
  {
    "objectID": "slides/session_6/index.html#formalisation-a-typical-machine-learning-task",
    "href": "slides/session_6/index.html#formalisation-a-typical-machine-learning-task",
    "title": "Introduction to Machine Learning",
    "section": "Formalisation: a typical machine learning task",
    "text": "Formalisation: a typical machine learning task\n\nvector of unknowns: \\(\\theta=(a,b)\\)\ndataset \\(X,Y=(x_i,y_i)_{i=1:N}\\)\nfor a random draw \\(\\omega = (a_{\\sigma(i)}, b_{\\sigma(i)})_{i=[1,N]} \\subset (X,Y)\\)\n\n\\(\\omega\\) is just a random batch of size \\(N\\)\n\ndefine the empirical risk (or empirical cost) \\[\\xi(\\theta, \\omega) = \\sum_{(x,y) \\in \\omega} (y - (a x + b))^2\\]\nwe want to minimize theoretical risk: \\[\\Xi(\\theta) = \\mathbb{E} \\left[ \\xi(\\theta, \\omega)\\right]\\]"
  },
  {
    "objectID": "slides/session_6/index.html#training-gradient-descent",
    "href": "slides/session_6/index.html#training-gradient-descent",
    "title": "Introduction to Machine Learning",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\n\n\nHow do we minimize a function \\(f(a,b)\\)?\nGradient descent:\n\n\\(a_k, b_k\\) given\ncompute the gradient (slope) \\(\\nabla_{a,b} f = \\begin{bmatrix} \\frac{\\partial f}{\\partial a} \\\\\\\\ \\frac{\\partial f}{\\partial b}\\end{bmatrix}\\)\nfollow the steepest slope: (Newton Algorithm)\n\n\\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} - \\nabla_{a,b} f\\]\n\nbut not too fast: use learning rate \\(\\lambda\\): \\[ \\begin{bmatrix} a_{k+1} \\\\\\\\ b_{k+1} \\end{bmatrix} \\leftarrow  (1-\\lambda) \\begin{bmatrix} a_k \\\\\\\\ b_k \\end{bmatrix} + \\lambda (- \\nabla_{a,b} f )\\]"
  },
  {
    "objectID": "slides/session_6/index.html#some-possible-issues",
    "href": "slides/session_6/index.html#some-possible-issues",
    "title": "Introduction to Machine Learning",
    "section": "Some possible issues",
    "text": "Some possible issues\n \n\nIn practice, choosing the right learning rate \\(\\lambda\\) is crucial\n\\(\\lambda\\) is a metaparameter of the model training."
  },
  {
    "objectID": "slides/session_6/index.html#wide-data",
    "href": "slides/session_6/index.html#wide-data",
    "title": "Introduction to Machine Learning",
    "section": "Wide data",
    "text": "Wide data\n\nWide Data is characterized by a high number of features compared to the number of observations.\n\n\nProblem:\n\nwith many independent variables \\(x_1, ... x_K\\), \\(K&gt;&gt;N\\) and one dependent variable \\(y\\) the regression \\[y = a_1 x_1 + a_2 x_2 + \\cdots + a_N x_N + b\\] is grossly overidentified."
  },
  {
    "objectID": "slides/session_6/index.html#wide-data-regression",
    "href": "slides/session_6/index.html#wide-data-regression",
    "title": "Introduction to Machine Learning",
    "section": "Wide data regression",
    "text": "Wide data regression\nMain Idea: penalize non-zero coefficients to encourage scarcity\n\n\n\n\n\nRidge: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N ( \\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|^2\\]\n\nshrinks parameters towards zero\nclosed form\n\nLasso: \\[\\Xi(a,b) = \\min_{a,b} \\sum_{i=1}^N (\\sum_j a_j x_j + b - y_i)^2 + \\mu \\sum_i |a_i|\\]\n\neliminates zero coefficients\n\nElastic: Ridge + Lasso\n\n\n\n\nRemarks:\n\n\\(\\mu\\) is called a regularization term.\nit is a hyperparameter\n\\(\\mu \\uparrow\\), bias increases, variance decreases"
  },
  {
    "objectID": "slides/session_6/index.html#training",
    "href": "slides/session_6/index.html#training",
    "title": "Introduction to Machine Learning",
    "section": "Training",
    "text": "Training\nTo perform Lasso and ridge regression:\n\nAI approach:\n\nminimize objective \\(\\Xi(a,b)\\) directly.\napproach is known as (stochastic) Gradient Descent\n\nUse special algorithms"
  },
  {
    "objectID": "slides/session_6/index.html#example-imf-challenge",
    "href": "slides/session_6/index.html#example-imf-challenge",
    "title": "Introduction to Machine Learning",
    "section": "Example: IMF challenge",
    "text": "Example: IMF challenge\n\nAn internal IMF challenge to predict crises in countries\nLots of different approaches\nLots of data:\n\nwhich one is relevant\nmachine must select relevant informations\n\nExample: Lasso Regressions and Forecasting Models in Applied Stress Testing by Jorge A. Chan-Lau\n\nin a given developing country\ntries to predict probability of default in various sectors"
  },
  {
    "objectID": "slides/session_6/index.html#nonlinear-regression-1",
    "href": "slides/session_6/index.html#nonlinear-regression-1",
    "title": "Introduction to Machine Learning",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\n\nSo far, we have assumed,\n\n\\(y_i = a + b x_i\\)\n\\(y_i = a + b x_i + μ_1 (a^2 + b^2) + μ_2 (|a| + |b|)\\)\ndefined \\(\\Xi(a,b)\\) and tried to minimize it\n\nSame approach works for fully nonlinear models\n\n\\(y_i = a x_i + a^2 x_i^2 + c\\)\n\\(y_i = \\varphi(x; \\theta)\\) ()\n\nSpecial case: neural network:\n\nprimer tensor playground"
  },
  {
    "objectID": "slides/session_6/index.html#how-to-evaluate-the-machine-learning",
    "href": "slides/session_6/index.html#how-to-evaluate-the-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn machine learning we can’t perform statistical inference easily. How do we assess the validity of a model?\n\nBasic idea (independent of how complex the algorithm is)\n\nseparate data in\n\ntraining set (in-sample)\ntest set (out of sample)\n\ntrain using only the training set\nevaluate performance on the test set\n\n\n\nPerformance can be:\n\nfitness, number of classification errors (false positive, false negative)"
  },
  {
    "objectID": "slides/session_6/index.html#how-to-evaluate-the-machine-learning-1",
    "href": "slides/session_6/index.html#how-to-evaluate-the-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "how to evaluate the machine learning",
    "text": "how to evaluate the machine learning\nIn case the training method depends itself on many parameters (the hyperparameters) we make three samples instead:\n\ntraining set (in-sample)\nvalidation set (to update hyperparameters)\ntest set (out of sample)\n\nGolden Rule: the test set should not be used to estimate the model, and should not affect the choice any training parameter (hyperparameter)."
  },
  {
    "objectID": "slides/session_6/index.html#section",
    "href": "slides/session_6/index.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Traintest\nThe test set reveals that orange model is overfitting."
  },
  {
    "objectID": "slides/session_6/index.html#how-to-choose-the-validation-set",
    "href": "slides/session_6/index.html#how-to-choose-the-validation-set",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nHoldout validation approach:\n\nkeeps x% of the data for the training, (100-x)% for the test\n\n\nHow to choose the sizes of the subsets?\n\nsmall dataset: 90-10\nbig data set: 70-30 (we can afford to waste more training data for the test)\n\n\n\n\nProblem:\n\nare we sure the validation size is correct? Are the results determined by an (un-) lucky draw?\na problem for smaller datasets"
  },
  {
    "objectID": "slides/session_6/index.html#how-to-choose-the-validation-set-1",
    "href": "slides/session_6/index.html#how-to-choose-the-validation-set-1",
    "title": "Introduction to Machine Learning",
    "section": "How to choose the validation set?",
    "text": "How to choose the validation set?\nA more robust solution: \\(k\\)-fold validation\n\n\n\nsplit dataset randomly in \\(K\\) subsets of equal size \\(S_1, ... S_K\\)\nuse subset \\(S_i\\) as test set, the rest as training set, compute the score\ncompare the scores obtained for all \\(i\\in[1,K]\\)\n\nthey should be similar (compute standard deviation)\n\naverage them"
  },
  {
    "objectID": "slides/session_6/index.html#another-library-to-perform-regressions",
    "href": "slides/session_6/index.html#another-library-to-perform-regressions",
    "title": "Introduction to Machine Learning",
    "section": "Another library to perform regressions",
    "text": "Another library to perform regressions\n\nstatsmodels:\n\nexplanatory analysis\nstatistical tests\nformula interface for many estimation algorithms\n\nstateless approach (model.fit() returns another object)\n\n\nlinearmodels\n\nextends statsmodels (very similar interface)\n\n(panel models, IV, systems…)\n\n\nsklearn:\n\nprediction\nfaster for big datasets\ncommon interface for several machine learning tasks\n\nstateful approach (model is modified by .fit operation)\n\ndefacto standard for machine learning"
  },
  {
    "objectID": "slides/session_6/index.html#in-practice",
    "href": "slides/session_6/index.html#in-practice",
    "title": "Introduction to Machine Learning",
    "section": "In practice",
    "text": "In practice\n\n\nBasic sklearn workflow:\n\n\nimport data\n\nfeatures: a matrix X (2d numpy array)\nlabels: a vector y (1d numpy array)\n\nsplit the data, between training and test datasets\n\nsplit needs to be random to avoid any bias\n\nnormalize the data\n\nmost ML algorithm are sensitive to scale\n\ncreate a model (independent from data)\ntrain the model on training dataset\nevaluate accuracy on test dataset (here \\(R^2\\))\nuse the model to make predictions\n\n\n\n\nfrom sklearn.datasets import load_diabetes\ndataset = load_diabetes()\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict(X_new)\n\n\nThe workflow is always the same, no matter what the model is\n\ntry sklearn.linear_model.Lasso instead of LinearRegression"
  },
  {
    "objectID": "slides/session_6/index.html#k-fold-validation-with-sklearn",
    "href": "slides/session_6/index.html#k-fold-validation-with-sklearn",
    "title": "Introduction to Machine Learning",
    "section": "\\(k\\)-fold validation with sklearn",
    "text": "\\(k\\)-fold validation with sklearn\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\n\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   ## train a model in X_train, y_train\n   ## test it on X_test, y_test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data-Based Economics",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJan 17, 2024\n\n\nIntroduction\n\n\n\n\nJan 24, 2024\n\n\nDataframes\n\n\n\n\nJan 31, 2024\n\n\nLinear Regression\n\n\n\n\nFeb 7, 2024\n\n\nMultiple Regression\n\n\n\n\nFeb 7, 2024\n\n\nIntroduction to Instrumental Variables\n\n\n\n\nFeb 28, 2024\n\n\nIntroduction to Machine Learning\n\n\n\n\nMar 12, 2024\n\n\nIntroduction to Machine Learning (2)\n\n\n\n\nMar 19, 2024\n\n\nText Analysis\n\n\n\n\nMar 26, 2024\n\n\nLarge Language Models for Finance\n\n\n\n\nApr 2, 2024\n\n\nSome Important Points\n\n\n\n\nApr 9, 2024\n\n\nFinal Exam 🤞\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Data-Based Economics",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nJan 17, 2024\n\n\nIntroduction\n\n\n\n\nJan 24, 2024\n\n\nDataframes\n\n\n\n\nJan 31, 2024\n\n\nLinear Regression\n\n\n\n\nFeb 7, 2024\n\n\nMultiple Regression\n\n\n\n\nFeb 7, 2024\n\n\nIntroduction to Instrumental Variables\n\n\n\n\nFeb 28, 2024\n\n\nIntroduction to Machine Learning\n\n\n\n\nMar 12, 2024\n\n\nIntroduction to Machine Learning (2)\n\n\n\n\nMar 19, 2024\n\n\nText Analysis\n\n\n\n\nMar 26, 2024\n\n\nLarge Language Models for Finance\n\n\n\n\nApr 2, 2024\n\n\nSome Important Points\n\n\n\n\nApr 9, 2024\n\n\nFinal Exam 🤞\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Data-Based Economics",
    "section": "Evaluation",
    "text": "Evaluation\n\nCoursework (50%)\n\na little replication exercise to do in small groups (2/3 students max)\n\nFinal exam (in-class and on computers) (50%)\n\nan MCQ part about the theory\nsimple coding exercises"
  },
  {
    "objectID": "tutorials/session_5/happiness_regression.html",
    "href": "tutorials/session_5/happiness_regression.html",
    "title": "More Regressions",
    "section": "",
    "text": "We will analyse those data to find relationships between the happiness score and economy, family, health, freedom, trust, perception of corruption, generosity…\nThe dataset contains the following variables:\n\nCountry : Country name\nOverall rank : Country ranking based on happiness score\nScore : Individual personal happiness rating from 0 to 10.\nGDP per capita : GDP per capita of each country in terms of purchasing power parity (PPP) (in USD)\nSocial support : Individual rating that determines whether, when you have problems, your family or friends would help you. Binary responses (0 or 1).\nHealthy life expectancy : Healthy life expectancy at birth is based on data from the World Health Organization (WHO)\nFreedom to make life choices : Individual rating that determines whether you are atisfied or dissatisfied with your freedom to choose hat you do with your life. Binary responses (0 or 1).\nGenerosity : Generosity is the residual from the regression of the national mean of responses to the question “Have you donated money to a charity in the last month?” on GDP per capita.\nPerceptions of corruption : Average of binary responses to two GWP questions: corruption in government and corruption in business.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.formula import api as smf\n\nOpen the csv “happiness_index_2019”\nExplore the dataset by using head function.\nCompute descriptive statistics using a Pandas function\nPlot variables that may have a positive correlation using matplotlib\nPlot the correlation matrix of the main variables using heatmap function of Seaborn package. It should already be installed on your Nuvolos instance (use escpython kernel).\nComment?\n\n\n\n\n\n\n\nPerform various linear regressions to predict the Happiness score using one of the variables available in the dataset.\n\n\n\nPerform various linear regressions to predict the Happiness score using several of the variables available in the dataset.\nWhat would be the best model?"
  },
  {
    "objectID": "tutorials/session_5/happiness_regression.html#the-world-happiness-report.",
    "href": "tutorials/session_5/happiness_regression.html#the-world-happiness-report.",
    "title": "More Regressions",
    "section": "",
    "text": "We will analyse those data to find relationships between the happiness score and economy, family, health, freedom, trust, perception of corruption, generosity…\nThe dataset contains the following variables:\n\nCountry : Country name\nOverall rank : Country ranking based on happiness score\nScore : Individual personal happiness rating from 0 to 10.\nGDP per capita : GDP per capita of each country in terms of purchasing power parity (PPP) (in USD)\nSocial support : Individual rating that determines whether, when you have problems, your family or friends would help you. Binary responses (0 or 1).\nHealthy life expectancy : Healthy life expectancy at birth is based on data from the World Health Organization (WHO)\nFreedom to make life choices : Individual rating that determines whether you are atisfied or dissatisfied with your freedom to choose hat you do with your life. Binary responses (0 or 1).\nGenerosity : Generosity is the residual from the regression of the national mean of responses to the question “Have you donated money to a charity in the last month?” on GDP per capita.\nPerceptions of corruption : Average of binary responses to two GWP questions: corruption in government and corruption in business.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.formula import api as smf\n\nOpen the csv “happiness_index_2019”\nExplore the dataset by using head function.\nCompute descriptive statistics using a Pandas function\nPlot variables that may have a positive correlation using matplotlib\nPlot the correlation matrix of the main variables using heatmap function of Seaborn package. It should already be installed on your Nuvolos instance (use escpython kernel).\nComment?"
  },
  {
    "objectID": "tutorials/session_5/happiness_regression.html#a-simple-linear-regression",
    "href": "tutorials/session_5/happiness_regression.html#a-simple-linear-regression",
    "title": "More Regressions",
    "section": "",
    "text": "Perform various linear regressions to predict the Happiness score using one of the variables available in the dataset."
  },
  {
    "objectID": "tutorials/session_5/happiness_regression.html#b-multiple-linear-regression",
    "href": "tutorials/session_5/happiness_regression.html#b-multiple-linear-regression",
    "title": "More Regressions",
    "section": "",
    "text": "Perform various linear regressions to predict the Happiness score using several of the variables available in the dataset.\nWhat would be the best model?"
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables_correction.html",
    "href": "tutorials/session_5/instrumental_variables_correction.html",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\nA problem arises when the confounding factor \\(z\\) is not observed. In that case, we can estimate the direct effect of \\(x\\) on \\(y\\) by using \\(q\\) as an instrument.\nRun the follwing code to create a mock dataset.\n\nimport numpy as np\nimport pandas as pd\n\n\nN = 100000\nϵ_z = np.random.randn(N)*0.1\nϵ_x = np.random.randn(N)*0.1\nϵ_q = np.random.randn(N)*0.01\nϵ_y = np.random.randn(N)*0.01\n\n\nz = 0.1 + ϵ_z\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\n# here we must change the definition so that q affects x:\n# x = 0.1 + z + ϵ_x\nx = 0.1 + z + q + ϵ_x\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the dataframe. Compute the correlations between the variables. Are they compatible with the hypotheses for IV?\nHere are the results from the database:\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.700385\n1.670457\n0.100148\n0.500003\n\n\nstd\n0.150426\n0.165036\n0.099989\n0.015830\n\n\nmin\n0.094732\n1.001409\n-0.365794\n0.432416\n\n\n25%\n0.598821\n1.559262\n0.032510\n0.489311\n\n\n50%\n0.700130\n1.669901\n0.100245\n0.499985\n\n\n75%\n0.801383\n1.781243\n0.167538\n0.510676\n\n\nmax\n1.308864\n2.358062\n0.553339\n0.571994\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.981609\n0.664405\n0.617227\n\n\ny\n0.981609\n1.000000\n0.787542\n0.505723\n\n\nz\n0.664405\n0.787542\n1.000000\n-0.003407\n\n\nq\n0.617227\n0.505723\n-0.003407\n1.000000\n\n\n\n\n\n\n\nWe observe: - cor(q, x) non zero: the instrument is relevant - close to zero: might be a weak instrument (we would need to check significance) - cor(q, z) = 0 : the instrument is really exogenous\n\n\n\nUse linearmodels to run a regression estimating the effect of \\(x\\) on \\(y\\) (note the slight API change w.r.t. statsmodels). Comment.\nWhat is the problem with this regression? How can it be detected?\n\nfrom linearmodels import OLS\n\nmodel = OLS.from_formula(\"y ~ x\", df)\nres = model.fit()\nres.summary\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9636\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9636\n\n\nNo. Observations:\n100000\nF-statistic:\n2.64e+06\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:10:57\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9162\n0.0005\n1928.2\n0.0000\n0.9153\n0.9171\n\n\nx\n1.0769\n0.0007\n1624.8\n0.0000\n1.0756\n1.0782\n\n\n\n\n\nRegression is globally significant (p-value for Fisher test &lt; 0.00001). The coefficient \\(\\beta=1.0999\\) in front of \\(x\\) is also very significant at a 0.001% level but does not match the true model for \\(y\\) (y  = 1.0 + 0.9*x + 0.4*z + ϵ_y).\n\n\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"y ~ x + z\"\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9964\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9964\n\n\nNo. Observations:\n100000\nF-statistic:\n2.729e+07\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:10:59\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9998\n0.0002\n5755.9\n0.0000\n0.9995\n1.0002\n\n\nx\n0.9003\n0.0003\n3219.2\n0.0000\n0.8998\n0.9008\n\n\nz\n0.4000\n0.0004\n949.12\n0.0000\n0.3991\n0.4008\n\n\n\nid: 0x7f4a249d7ef0\n\n\nNow we see that the coefficient in front of x is the correct one (that is 0.9).\n\n\n\nNow we try to run a regression without knowing the value of z.\nMake a causality graph, summarizing what you know from the equations.\nThe information about the model’s information structure can be summarized as:\n\ngraph TD; X–&gt;Y; Z–&gt;X; Z–&gt;Y; Q–&gt;X;\n\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ 1 + [x ~ q]\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9372\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.9372\n\n\nNo. Observations:\n100000\nF-statistic:\n4.067e+05\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:16:05\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0409\n0.0010\n1046.4\n0.0000\n1.0389\n1.0428\n\n\nx\n0.8989\n0.0014\n637.73\n0.0000\n0.8962\n0.9017\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f4a249d63c0\n\n\nWe observe that the result is, again, the correct one. This is especially impressive since we didn’t have access to the confounding factor z and couldn’t add it to the regression. Instead, we had another source of randomness q that we used to instrument the regression.",
    "crumbs": [
      "tutorials",
      "Instrumental variables"
    ]
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables_correction.html#iv-example-on-mock-dataset",
    "href": "tutorials/session_5/instrumental_variables_correction.html#iv-example-on-mock-dataset",
    "title": "Instrumental variables",
    "section": "",
    "text": "Create four random series of length \\(N=1000\\)\n\n\\(x\\): education\n\\(y\\): salary\n\\(z\\): ambition\n\\(q\\): early smoking\n\nsuch that:\n\n\\(x\\) and \\(z\\) cause \\(y\\)\n\\(z\\) causes \\(x\\)\n\\(q\\) is correlated with \\(x\\), not with \\(z\\)\n\nA problem arises when the confounding factor \\(z\\) is not observed. In that case, we can estimate the direct effect of \\(x\\) on \\(y\\) by using \\(q\\) as an instrument.\nRun the follwing code to create a mock dataset.\n\nimport numpy as np\nimport pandas as pd\n\n\nN = 100000\nϵ_z = np.random.randn(N)*0.1\nϵ_x = np.random.randn(N)*0.1\nϵ_q = np.random.randn(N)*0.01\nϵ_y = np.random.randn(N)*0.01\n\n\nz = 0.1 + ϵ_z\nq = 0.5 + 0.1234*ϵ_x + ϵ_q\n# here we must change the definition so that q affects x:\n# x = 0.1 + z + ϵ_x\nx = 0.1 + z + q + ϵ_x\ny  = 1.0 + 0.9*x + 0.4*z + ϵ_y\n\n\ndf = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"z\": z,\n    \"q\": q\n})\n\nDescribe the dataframe. Compute the correlations between the variables. Are they compatible with the hypotheses for IV?\nHere are the results from the database:\n\ndf.describe()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n0.700385\n1.670457\n0.100148\n0.500003\n\n\nstd\n0.150426\n0.165036\n0.099989\n0.015830\n\n\nmin\n0.094732\n1.001409\n-0.365794\n0.432416\n\n\n25%\n0.598821\n1.559262\n0.032510\n0.489311\n\n\n50%\n0.700130\n1.669901\n0.100245\n0.499985\n\n\n75%\n0.801383\n1.781243\n0.167538\n0.510676\n\n\nmax\n1.308864\n2.358062\n0.553339\n0.571994\n\n\n\n\n\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nx\ny\nz\nq\n\n\n\n\nx\n1.000000\n0.981609\n0.664405\n0.617227\n\n\ny\n0.981609\n1.000000\n0.787542\n0.505723\n\n\nz\n0.664405\n0.787542\n1.000000\n-0.003407\n\n\nq\n0.617227\n0.505723\n-0.003407\n1.000000\n\n\n\n\n\n\n\nWe observe: - cor(q, x) non zero: the instrument is relevant - close to zero: might be a weak instrument (we would need to check significance) - cor(q, z) = 0 : the instrument is really exogenous\n\n\n\nUse linearmodels to run a regression estimating the effect of \\(x\\) on \\(y\\) (note the slight API change w.r.t. statsmodels). Comment.\nWhat is the problem with this regression? How can it be detected?\n\nfrom linearmodels import OLS\n\nmodel = OLS.from_formula(\"y ~ x\", df)\nres = model.fit()\nres.summary\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9636\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9636\n\n\nNo. Observations:\n100000\nF-statistic:\n2.64e+06\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:10:57\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9162\n0.0005\n1928.2\n0.0000\n0.9153\n0.9171\n\n\nx\n1.0769\n0.0007\n1624.8\n0.0000\n1.0756\n1.0782\n\n\n\n\n\nRegression is globally significant (p-value for Fisher test &lt; 0.00001). The coefficient \\(\\beta=1.0999\\) in front of \\(x\\) is also very significant at a 0.001% level but does not match the true model for \\(y\\) (y  = 1.0 + 0.9*x + 0.4*z + ϵ_y).\n\n\n\n\nfrom linearmodels import IV2SLS\nformula = (\n    \"y ~ x + z\"\n)\nmod = IV2SLS.from_formula(formula, df)\nres = mod.fit()\nres\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9964\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.9964\n\n\nNo. Observations:\n100000\nF-statistic:\n2.729e+07\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:10:59\nDistribution:\nchi2(2)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.9998\n0.0002\n5755.9\n0.0000\n0.9995\n1.0002\n\n\nx\n0.9003\n0.0003\n3219.2\n0.0000\n0.8998\n0.9008\n\n\nz\n0.4000\n0.0004\n949.12\n0.0000\n0.3991\n0.4008\n\n\n\nid: 0x7f4a249d7ef0\n\n\nNow we see that the coefficient in front of x is the correct one (that is 0.9).\n\n\n\nNow we try to run a regression without knowing the value of z.\nMake a causality graph, summarizing what you know from the equations.\nThe information about the model’s information structure can be summarized as:\n\ngraph TD; X–&gt;Y; Z–&gt;X; Z–&gt;Y; Q–&gt;X;\n\nUse \\(q\\) to instrument the effect of x on y. Comment.\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"y ~ 1 + [x ~ q]\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\ny\nR-squared:\n0.9372\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n0.9372\n\n\nNo. Observations:\n100000\nF-statistic:\n4.067e+05\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:16:05\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.0409\n0.0010\n1046.4\n0.0000\n1.0389\n1.0428\n\n\nx\n0.8989\n0.0014\n637.73\n0.0000\n0.8962\n0.9017\n\n\n\nEndogenous: xInstruments: qRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f4a249d63c0\n\n\nWe observe that the result is, again, the correct one. This is especially impressive since we didn’t have access to the confounding factor z and couldn’t add it to the regression. Instead, we had another source of randomness q that we used to instrument the regression.",
    "crumbs": [
      "tutorials",
      "Instrumental variables"
    ]
  },
  {
    "objectID": "tutorials/session_5/instrumental_variables_correction.html#return-on-education",
    "href": "tutorials/session_5/instrumental_variables_correction.html#return-on-education",
    "title": "Instrumental variables",
    "section": "Return on Education",
    "text": "Return on Education\nWe follow the excellent R tutorial from the (excellent) Econometrics with R book.\nThe goal is to measure the effect of schooling on earnings, while correcting the endogeneity bias by using distance to college as an instrument.\nDownload the college distance using get_dataset function and make a nice dataframe. Describe the dataset. Plot a histogram of distance (you can use matplotlib’s hist function or seaborn).\nhttps://vincentarelbundock.github.io/Rdatasets/datasets.html\n\nimport statsmodels.api as sm\nds = sm.datasets.get_rdataset(\"CollegeDistance\", \"AER\")\n\n\n# the dataframe must be retrieved from the dataset object\ndf = ds.data\n\n\ndf.head()\n\n\n\n\n\n\n\n\ngender\nethnicity\nscore\nfcollege\nmcollege\nhome\nurban\nunemp\nwage\ndistance\ntuition\neducation\nincome\nregion\n\n\nrownames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nmale\nother\n39.150002\nyes\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nhigh\nother\n\n\n2\nfemale\nother\n48.869999\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n3\nmale\nother\n48.740002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n4\nmale\nafam\n40.400002\nno\nno\nyes\nyes\n6.2\n8.09\n0.2\n0.88915\n12\nlow\nother\n\n\n5\nfemale\nother\n40.480000\nno\nno\nno\nyes\n5.6\n8.09\n0.4\n0.88915\n13\nlow\nother\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nscore\nunemp\nwage\ndistance\ntuition\neducation\n\n\n\n\ncount\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n4739.000000\n\n\nmean\n50.889029\n7.597215\n9.500506\n1.802870\n0.814608\n13.807765\n\n\nstd\n8.701910\n2.763581\n1.343067\n2.297128\n0.339504\n1.789107\n\n\nmin\n28.950001\n1.400000\n6.590000\n0.000000\n0.257510\n12.000000\n\n\n25%\n43.924999\n5.900000\n8.850000\n0.400000\n0.484990\n12.000000\n\n\n50%\n51.189999\n7.100000\n9.680000\n1.000000\n0.824480\n13.000000\n\n\n75%\n57.769999\n8.900000\n10.150000\n2.500000\n1.127020\n16.000000\n\n\nmax\n72.809998\n24.900000\n12.960000\n20.000000\n1.404160\n18.000000\n\n\n\n\n\n\n\nHow is education encoded? Create a binary variable education_binary to replace it.\n\ndf['income'].unique()\n# education variable takes string values (\"high\" or \"low\"). \n\narray(['high', 'low'], dtype=object)\n\n\n\n# we need to convert them into 1 and 0 first\ndf['income_binary'] = (df['income'] == \"high\")*1\n\n\n# other option 1\ndf['incomeb'] = df['income'].map({'high' : 1, 'low': 0})\n\n\n# other option 2\ndf['incomeb'] = (df['income'] == 'high')*1\n\nPlot an histogram of distance to college.\n\nfrom matplotlib import pyplot as plt\nplt.hist(df['distance'])\n\n(array([3241.,  831.,  399.,  156.,   45.,   17.,   10.,   19.,   17.,\n           4.]),\n array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n# same with seaborn\nimport seaborn as sns\nsns.histplot(df['distance'])\n\n\n\n\n\n\n\n\nRun the naive regression \\(\\text{incomeb}=\\beta_0 + \\beta_1 \\text{education} + u\\) using linearmodels. Comment.\n\nfrom linearmodels.iv import IV2SLS # we can use IV2SLS instead of OLS to run regular regressions\n\nformula = \"incomeb ~ education\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n0.0480\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0478\n\n\nNo. Observations:\n4739\nF-statistic:\n227.43\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:17:44\nDistribution:\nchi2(1)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4780\n0.0499\n-9.5702\n0.0000\n-0.5759\n-0.3801\n\n\neducation\n0.0555\n0.0037\n15.081\n0.0000\n0.0483\n0.0627\n\n\n\nid: 0x7f4a182e7da0\n\n\nWe find that education explains higher income with a significant, but low coefficient 0.05.\nAugment the regression with unemp, hispanic, af-am, female and urban. Notice that categorical variables are encoded automatically. What are the treatment values? Change it using the syntax (C(var,Treatment='ref'))\n\nfrom linearmodels.iv import IV2SLS\n\nformula = \"incomeb ~ education + unemp + gender + ethnicity\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n0.0811\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0802\n\n\nNo. Observations:\n4739\nF-statistic:\n443.38\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:17:47\nDistribution:\nchi2(5)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.4361\n0.0533\n-8.1797\n0.0000\n-0.5406\n-0.3316\n\n\neducation\n0.0511\n0.0037\n13.982\n0.0000\n0.0439\n0.0582\n\n\nunemp\n-0.0111\n0.0022\n-4.9609\n0.0000\n-0.0155\n-0.0067\n\n\ngender[T.male]\n0.0484\n0.0128\n3.7866\n0.0002\n0.0234\n0.0735\n\n\nethnicity[T.hispanic]\n-0.0249\n0.0185\n-1.3425\n0.1794\n-0.0612\n0.0114\n\n\nethnicity[T.other]\n0.1347\n0.0163\n8.2871\n0.0000\n0.1029\n0.1666\n\n\n\nid: 0x7f4a18106ff0\n\n\n\ndf['ethnicity'].unique()\n\narray(['other', 'afam', 'hispanic'], dtype=object)\n\n\nIn the regression above, some variables have been created for each value of the categorical vairables (save for the reference value which doesn’t appear). In the case of variable ethnicity, we see that hispanic and other are two of the three values taken by variable ethnicity. This means that value afam was taken as reference.\nWe can change the reference variable as follows:\n\n# needed only if you use the function Treatment in the formulas\nfrom linearmodels.iv import IV2SLS\n\nformula = \"incomeb ~ education + unemp + C(gender) + C(ethnicity,Treatment(reference='other'))\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nOLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n0.0811\n\n\nEstimator:\nOLS\nAdj. R-squared:\n0.0802\n\n\nNo. Observations:\n4739\nF-statistic:\n443.38\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:17:51\nDistribution:\nchi2(5)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n-0.3014\n0.0540\n-5.5788\n0.0000\n-0.4073\n-0.1955\n\n\neducation\n0.0511\n0.0037\n13.982\n0.0000\n0.0439\n0.0582\n\n\nunemp\n-0.0111\n0.0022\n-4.9609\n0.0000\n-0.0155\n-0.0067\n\n\nC(gender)[T.male]\n0.0484\n0.0128\n3.7866\n0.0002\n0.0234\n0.0735\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.1347\n0.0163\n-8.2871\n0.0000\n-0.1666\n-0.1029\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1596\n0.0149\n-10.710\n0.0000\n-0.1888\n-0.1304\n\n\n\nid: 0x7f4a181d6630\n\n\nComment the results and explain the endogeneity problem\nAdding additional regressors has increased the fit (adj. R^2 from 0.04 to 0.08) without changing the coefficient on the education level. This would imply that regression is robust.\nHowever, we might have an endogeneity issue with some potential other factors explaining both income level and salary (cf many discussions in the course).\nExplain why distance to college might be used to instrument the effect of schooling.\nAssuming that the decision to live in a given county does not depend on the presence of a college nearby, the distance to college should be exogenous.\nThe distance to college is probably correlated with the decision to go so the instrument should have some power (opposite of weak)\nRun an IV regression, where distance is used to instrument schooling.\nlook at: https://bashtage.github.io/linearmodels/ (two-stage least squares)\n\nfrom linearmodels.iv import IV2SLS\n \nformula = \"incomeb ~ [education ~ distance] + unemp + C(gender) + C(ethnicity, Treatment(reference='other'))\"\nmodel = IV2SLS.from_formula(formula, df)\nresult = model.fit()\nresult\n\n\nIV-2SLS Estimation Summary\n\n\nDep. Variable:\nincomeb\nR-squared:\n-0.1339\n\n\nEstimator:\nIV-2SLS\nAdj. R-squared:\n-0.1351\n\n\nNo. Observations:\n4739\nF-statistic:\n1748.5\n\n\nDate:\nWed, Feb 21 2024\nP-value (F-stat)\n0.0000\n\n\nTime:\n15:18:14\nDistribution:\nchi2(6)\n\n\nCov. Estimator:\nrobust\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nunemp\n-0.0100\n0.0025\n-3.9704\n0.0001\n-0.0149\n-0.0051\n\n\nC(gender)[T.female]\n-1.9566\n0.5039\n-3.8830\n0.0001\n-2.9441\n-0.9690\n\n\nC(gender)[T.male]\n-1.9109\n0.5042\n-3.7897\n0.0002\n-2.8991\n-0.9226\n\n\nC(ethnicity, Treatment(reference='other'))[T.afam]\n-0.0753\n0.0259\n-2.9005\n0.0037\n-0.1261\n-0.0244\n\n\nC(ethnicity, Treatment(reference='other'))[T.hispanic]\n-0.1247\n0.0203\n-6.1432\n0.0000\n-0.1645\n-0.0849\n\n\neducation\n0.1692\n0.0358\n4.7261\n0.0000\n0.0990\n0.2393\n\n\n\nEndogenous: educationInstruments: distanceRobust Covariance (Heteroskedastic)Debiased: Falseid: 0x7f4a17f31190\n\n\nComment the results.\nThe estimate we get for the return on education is three times higher than without the instrument and highly significant.",
    "crumbs": [
      "tutorials",
      "Instrumental variables"
    ]
  },
  {
    "objectID": "tutorials/session_4/Regressions.html",
    "href": "tutorials/session_4/Regressions.html",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\nrownames\n\n\n\n\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\n\nΣ = df[ ['income', 'education'] ].cov()\n\n\nβ = Σ.loc['income','education'] / Σ.loc['education','education']\n\n\nβ\n\n0.5948594400410561\n\n\n\nμ = df[ ['income', 'education'] ].mean()\nμ\n\nincome       41.866667\neducation    52.555556\ndtype: float64\n\n\n\nα = μ['income'] - β*μ['education']\n\n\nα\n\n10.603498317842273\n\n\n\nprediction = α + β*df['education']\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'], df['income'], '.')\nplt.plot(df['education'], prediction)\n\n\n\n\n\n\n\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\ndf['prediction'] = α + β*df['education']\ndf['error_term'] =  df['income'] - prediction\n\n\nSigma = df[['income', 'education', 'prediction', 'error_term']].cov()\n\n\ntotal_variance = Sigma.loc['income','income'] \n\n\nprediction_variance = Sigma.loc['prediction','prediction']\n\n\nerror_variance = Sigma.loc['error_term', 'error_term']\n\n\ntotal_variance\n\n597.0727272727273\n\n\n\nprediction_variance\n\n313.4143142161768\n\n\n\nerror_variance\n\n283.6584130565506\n\n\n\nprediction_variance + error_variance\n\n597.0727272727274\n\n\n\nmyRsquared = 1 - error_variance/total_variance\n\n\nmyRsquared\n\n0.5249181546907554\n\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nWed, 07 Feb 2024\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:30:20\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\n\nImport dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?\n\n\n\nIn 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\nCreate a database with all variables of interest including detrended gdp\nRun the basic regression\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "tutorials/session_4/Regressions.html#linear-regressions",
    "href": "tutorials/session_4/Regressions.html#linear-regressions",
    "title": "Regressions",
    "section": "",
    "text": "Import the Duncan/carData dataset\n\nimport statsmodels.api as sm\ndataset = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\ndf = dataset.data\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\nrownames\n\n\n\n\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\ncount\n45.000000\n45.000000\n45.000000\n\n\nmean\n41.866667\n52.555556\n47.688889\n\n\nstd\n24.435072\n29.760831\n31.510332\n\n\nmin\n7.000000\n7.000000\n3.000000\n\n\n25%\n21.000000\n26.000000\n16.000000\n\n\n50%\n42.000000\n45.000000\n41.000000\n\n\n75%\n64.000000\n84.000000\n81.000000\n\n\nmax\n81.000000\n100.000000\n97.000000\n\n\n\n\n\n\n\nEstimate by hand the model \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\) . Plot.\n\nΣ = df[ ['income', 'education'] ].cov()\n\n\nβ = Σ.loc['income','education'] / Σ.loc['education','education']\n\n\nβ\n\n0.5948594400410561\n\n\n\nμ = df[ ['income', 'education'] ].mean()\nμ\n\nincome       41.866667\neducation    52.555556\ndtype: float64\n\n\n\nα = μ['income'] - β*μ['education']\n\n\nα\n\n10.603498317842273\n\n\n\nprediction = α + β*df['education']\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(df['education'], df['income'], '.')\nplt.plot(df['education'], prediction)\n\n\n\n\n\n\n\n\nCompute total, explained, unexplained variance. Compute R^2 statistics\n\ndf['prediction'] = α + β*df['education']\ndf['error_term'] =  df['income'] - prediction\n\n\nSigma = df[['income', 'education', 'prediction', 'error_term']].cov()\n\n\ntotal_variance = Sigma.loc['income','income'] \n\n\nprediction_variance = Sigma.loc['prediction','prediction']\n\n\nerror_variance = Sigma.loc['error_term', 'error_term']\n\n\ntotal_variance\n\n597.0727272727273\n\n\n\nprediction_variance\n\n313.4143142161768\n\n\n\nerror_variance\n\n283.6584130565506\n\n\n\nprediction_variance + error_variance\n\n597.0727272727274\n\n\n\nmyRsquared = 1 - error_variance/total_variance\n\n\nmyRsquared\n\n0.5249181546907554\n\n\nUse statsmodels (formula API) to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{education}\\). Comment regression statistics.\n\n#https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html\n\nfrom statsmodels.formula import api as smf\n\nmodel_1 = smf.ols(\"income ~ education\", df)\nres_1 = model_1.fit()\n\n\nres_1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n47.51\n\n\nDate:\nWed, 07 Feb 2024\nProb (F-statistic):\n1.84e-08\n\n\nTime:\n11:30:20\nLog-Likelihood:\n-190.42\n\n\nNo. Observations:\n45\nAIC:\n384.8\n\n\nDf Residuals:\n43\nBIC:\n388.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.6035\n5.198\n2.040\n0.048\n0.120\n21.087\n\n\neducation\n0.5949\n0.086\n6.893\n0.000\n0.421\n0.769\n\n\n\n\n\n\nOmnibus:\n9.841\nDurbin-Watson:\n1.736\n\n\nProb(Omnibus):\n0.007\nJarque-Bera (JB):\n10.609\n\n\nSkew:\n0.776\nProb(JB):\n0.00497\n\n\nKurtosis:\n4.802\nCond. No.\n123.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 5% p-value level both the intercept and the coefficient are significant. R-squared is 0.52: the model explains half of the variance.\nUse statsmodels to estimate \\(\\text{income} = \\alpha + \\beta  \\times \\text{prestige}\\). Comment regression statistics.\n\nformula = \"income ~ education\"\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntype\nincome\neducation\nprestige\n\n\n\n\naccountant\nprof\n62\n86\n82\n\n\npilot\nprof\n72\n76\n83\n\n\narchitect\nprof\n75\n92\n90\n\n\nauthor\nprof\n55\n90\n76\n\n\nchemist\nprof\n64\n86\n90\n\n\n\n\n\n\n\n\nmodel_2 = smf.ols(\"income ~ prestige\", df)\nres_2 = model_2.fit()\n\n\nres_2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.695\n\n\nMethod:\nLeast Squares\nF-statistic:\n101.3\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n7.14e-13\n\n\nTime:\n11:55:59\nLog-Likelihood:\n-179.93\n\n\nNo. Observations:\n45\nAIC:\n363.9\n\n\nDf Residuals:\n43\nBIC:\n367.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.8840\n3.678\n2.959\n0.005\n3.467\n18.301\n\n\nprestige\n0.6497\n0.065\n10.062\n0.000\n0.519\n0.780\n\n\n\n\n\n\nOmnibus:\n8.893\nDurbin-Watson:\n2.048\n\n\nProb(Omnibus):\n0.012\nJarque-Bera (JB):\n19.848\n\n\nSkew:\n0.047\nProb(JB):\n4.90e-05\n\n\nKurtosis:\n6.252\nCond. No.\n104.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is income = 10.6 + 0.59 education. At a 0.5% p-value level both the intercept and the coefficient are significant. R-squared is 0.70: the model predicts income better than the former one.\n__Use statsmodels to estimate $ = + + _2 + $. Comment regression statistics.__\n\nmodel_3 = smf.ols(\"income ~ education + prestige\", df)\nres_3 = model_3.fit()\n\n\nres_3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.688\n\n\nMethod:\nLeast Squares\nF-statistic:\n49.55\n\n\nDate:\nTue, 02 Feb 2021\nProb (F-statistic):\n8.88e-12\n\n\nTime:\n11:56:03\nLog-Likelihood:\n-179.90\n\n\nNo. Observations:\n45\nAIC:\n365.8\n\n\nDf Residuals:\n42\nBIC:\n371.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.4264\n4.164\n2.504\n0.016\n2.024\n18.829\n\n\neducation\n0.0323\n0.132\n0.244\n0.808\n-0.234\n0.299\n\n\nprestige\n0.6237\n0.125\n5.003\n0.000\n0.372\n0.875\n\n\n\n\n\n\nOmnibus:\n9.200\nDurbin-Watson:\n2.053\n\n\nProb(Omnibus):\n0.010\nJarque-Bera (JB):\n21.265\n\n\nSkew:\n0.075\nProb(JB):\n2.41e-05\n\n\nKurtosis:\n6.364\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe \\(R^2\\) is only slightly higher than last model, but adjusted \\(R^2\\) is actually lower: the model has less predictive power.\nThe coefficient for education is not significant. It should be dropped from the regresssion.\nThis might happen, because education and prestige are correlated. Let’s check it:\n\ndf.corr()\n\n\n\n\n\n\n\n\nincome\neducation\nprestige\n\n\n\n\nincome\n1.000000\n0.724512\n0.837801\n\n\neducation\n0.724512\n1.000000\n0.851916\n\n\nprestige\n0.837801\n0.851916\n1.000000\n\n\n\n\n\n\n\nEducation and prestige are correlated at 83%. It makes no sense keeping the two in the same regression.\nWHich model would you recommend? For which purpose?\nIf the goal is to predict income, the one with prestige only, has the highest prediction power. If we are interested in the effect of education, we keep only education.\nPlot the regression with prestige\n\na = res_2.params.Intercept\nb = res_2.params.prestige\n\n\nx = df['prestige']\n\n\ny = a + b*x\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(x, df['income'],'o')\nplt.plot(x, y)\nplt.xlabel(\"prestige\")\nplt.xlabel(\"income\")\n\nText(0.5, 0, 'income')\n\n\n\n\n\n\n\n\n\nCheck visually normality of residuals\n\npred = a + b*x\nactual = df['income']\nresid = actual - pred  # same as res_2.resid\n\n\nplt.plot(x,resid, 'o')\n\n\n\n\n\n\n\n\n\nplt.hist(resid)\n\n(array([0.00255915, 0.        , 0.        , 0.00511829, 0.0486238 ,\n        0.02815062, 0.01535488, 0.01023659, 0.00255915, 0.00255915]),\n array([-46.40643935, -37.72299114, -29.03954294, -20.35609473,\n        -11.67264653,  -2.98919832,   5.69424989,  14.37769809,\n         23.0611463 ,  31.74459451,  40.42804271]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "tutorials/session_4/Regressions.html#finding-the-right-model",
    "href": "tutorials/session_4/Regressions.html#finding-the-right-model",
    "title": "Regressions",
    "section": "",
    "text": "Import dataset from data.dta. Explore dataset (statistics, plots)\nOur goal is to explain z by x and y. Run a regression.\nExamine the residuals of the regression. What’s wrong? Remedy?"
  },
  {
    "objectID": "tutorials/session_4/Regressions.html#taylor-rule",
    "href": "tutorials/session_4/Regressions.html#taylor-rule",
    "title": "Regressions",
    "section": "",
    "text": "In 1993, John taylor, estimated, using US data the regression: \\(i_t = i^{\\star} + \\alpha_{\\pi} \\pi_t + \\alpha_{\\pi} y_t\\) where \\(\\pi_t\\) is inflation and \\(y_t\\) the output gap (let’s say deviation from real gdp from the trend). He found that both coefficients were not significantly different from \\(0.5\\). Our goal, is to replicate the same analysis.\nImport macro data from statsmodels (https://www.statsmodels.org/devel/datasets/generated/macrodata.html)\nCreate a database with all variables of interest including detrended gdp\nRun the basic regression\nWhich control variables would you propose to add? Does it increase prediction power? How do you interpret that?"
  },
  {
    "objectID": "tutorials/session_8/old_homework.html",
    "href": "tutorials/session_8/old_homework.html",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "Students (up to 3):\n\nx1 Gabrielle Labat\nx2 Manal Bendjedid\n\nWhen working on the questions below, don’t hesitate to take some initiatives. In particular, if you don’t find how to answer a particular question (and you have asked 😉), feel free to propose a workaround.\nYour work will be evaluated in the following dimensions:\n\nWhether your notebook is replicable. When grading it, I should be able to run it from start to finish without error.\nWhether it is well written and clear. There should always be legible text to explain what you do, and make it a nice read. Imagine that the document was meant to be published as an online tutorial.\nWhether you have successfully solved the various theoretical and practical problems that are asked below.\nWhether you have shown some sense of initiative in approaching the various problems, in making the plots or in proposing extensions.\n\n\n\nIn this exercise, you must use data from the WorldBank to check whether CO2 Emissions can be explained by income per capita.\n\n\nImport the data from file data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv as a dataframe df_wide. (This table was downloaded from the worldbank website)\nHints: check the documentation from pandas.read_csv() to avoid the import error. You can check the first few lines of the file by typing in a cell: !data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv\nHint2: Check that all columns are well defined.\n\nimport pandas as pd\ndf_wide = pd.read_csv('data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv', skiprows=4)\n!data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv\n\n/bin/bash: line 1: data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv: Permission denied\n\n\nDescribe briefly the data.\nThe data contains information on the level of carbon dioxide (CO2) emissions per capita (in metric tons) for different countries and regions over the years 1960 to 2019. The data is sourced from the World Bank’s World Development Indicators and is reported annually.\nThe dataset contains 264 rows (corresponding to different countries and regions) and 61 columns (corresponding to the years 1960 to 2019 and additional columns with country and region information). The data is in a wide format, with each row representing a country/region and each column representing a year.\nThe data is useful for analyzing trends in CO2 emissions over time and comparing emissions across countries and regions. It can also be used for exploring the relationship between CO2 emissions and various economic and environmental factors.\n\ndf_wide.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n0.0\n0.0\n0.0\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n4.288766\n4.194242\n4.141210\n4.140690\n4.148584\n4.128191\n4.086604\nNaN\nNaN\nNaN\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n5.002821\n4.862196\n4.731198\n4.653372\n4.585664\n4.489473\n4.474616\nNaN\nNaN\nNaN\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.024987\n0.027090\n0.037289\n0.029718\n0.033815\n0.035826\n0.035704\nNaN\nNaN\nNaN\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.720891\n0.785259\n0.771880\n0.769068\n0.802179\n0.797275\n0.790333\nNaN\nNaN\nNaN\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.726281\n2.888558\n2.827949\n2.760181\n2.699348\n2.979403\n2.981762\nNaN\nNaN\nNaN\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n6.264131\n6.028664\n5.792812\n5.828931\n6.021844\n5.950694\n5.890683\nNaN\nNaN\nNaN\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n37.420762\n36.875726\n35.111798\n33.493040\n32.281678\n31.235406\n32.761775\nNaN\nNaN\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\n\n\nConvert the data into the long format using the function pandas.melt(). The columns of the new table df_long should be: [\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\", \"Date\", \"Emissions\"]\n\ndf_long=pd.melt(df_wide,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"Emissions\")\ndf_long\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nEmissions\n\n\n\n\n0\nAruba\nABW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n1\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n2\nAfghanistan\nAFG\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n3\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n4\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16753\nKosovo\nXKX\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16754\nYemen, Rep.\nYEM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16755\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16756\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16757\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n\n\n16758 rows × 6 columns\n\n\n\nEliminate all rows for which no emission data is available\n\ndf_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nEmissions\n\n\n\n\n7981\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.982136\n\n\n7982\nAfghanistan\nAFG\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.222538\n\n\n7983\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.473669\n\n\n7984\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.554586\n\n\n7985\nAlbania\nALB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n1.819542\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n15954\nSamoa\nWSM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n1.415729\n\n\n15956\nYemen, Rep.\nYEM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.351859\n\n\n15957\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n7.568640\n\n\n15958\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.369958\n\n\n15959\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.765894\n\n\n\n\n7147 rows × 6 columns\n\n\n\nConvert the Date column into a date format.\nHint: look for pandas.to_datetime() (This is not absolutely mandatory but makes graphs nicer)\n\ndf_long['Date'] = pd.to_datetime(df_long['Date'], format='%Y')\nprint(df_long['Date'].dtype)\n\nValueError: time data 'Unnamed: 66' does not match format '%Y' (match)\n\n\n\n\n\nPlot the evolution over time of total recorded carbon emissions (ommiting dates where no information is available).\nHint: use groupby()\n\ntotal_emissions=df_long[df_long['Emissions'].notnull()].groupby(df_long['Date'])['Emissions'].sum()\ntotal_emissions\n\nDate\n1990     992.579902\n1991     971.488470\n1992     971.204768\n1993     955.914952\n1994     949.595751\n1995     948.978410\n1996     964.076804\n1997     970.839153\n1998     970.257143\n1999     964.693322\n2000     967.857097\n2001     984.230281\n2002     993.251894\n2003    1022.691228\n2004    1038.318099\n2005    1048.812274\n2006    1064.012494\n2007    1060.075753\n2008    1053.788436\n2009    1069.598447\n2010    1033.216694\n2011    1037.102602\n2012    1047.851826\n2013    1025.015035\n2014    1002.423899\n2015     989.749130\n2016     989.624913\n2017     991.511483\n2018     986.637647\n2019     976.698404\nName: Emissions, dtype: float64\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(total_emissions)\nplt.xlabel(\"Years\")\nplt.ylabel(\"Emissions as metric tons per capita\")\nplt.title(\"Evolution of total CO2 emissions over time\")\nplt.show\n\n\n\n\n\n\n\n\nPlot cumulative carbon emissions\n\nimport numpy as np\nplt.plot(total_emissions.cumsum())\nplt.xlabel(\"Years\")\nplt.ylabel(\"Emissions as metric tons per capita\")\nplt.title(\"Evolution of cumulated CO2 emissions over time\")\nplt.show\n\n\n\n\n\n\n\n\nPropose some plots to visualize the contribution of the main contributors to yearly carbon emissions and to cumulative carbon emissions. You can get inspiration from ourworldindata.\nWe could use a pie Chart of Global Emissions by Top 10 Emitters. This plot would show us the percentage of global carbon emissions of the top 10 emitters, with each country’s contribution shown as a colored segment of a pie chart in order to provide the relative contributions of each country to global emissions. We could also use an area chart of cumulative emissions by top emitters: This plot shows the cumulative carbon emissions over time for the top emitters, with each country’s contribution shown as a colored area of a stacked area chart. This plot allows for easy comparison of the contributions of different countries to cumulative emissions.\n\n\n\nWe are now interested in the relation between carbon emission and economic development. To this purpose, we would like to run a simple regression\n\\[\\frac{ \\text{emissions}_{i t} }{ \\text{population}_{i t} } = \\alpha + \\beta*\\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} + \\text{other factors}_{i t}\\]\nwhere \\(i\\) is the country index and \\(t\\) the time index. In a first step we will simply assume that the total effect of all other factors is normally distributed.\nWe would then like to consider variants of this regression in order to test the environment Kuznets curve hypothesis.\nBriefly summarize the environmental Kuznets curve hypothesis\n(hint: perform a small websearch. Try to identify your sources)\nThe Environmental Kuznets Curve hypothesis postulates an inverted-U-shaped relationship between different pollutants and per capita income, i.e., environmental pressure increases up to a certain level as income goes up; after that, it decreases. The environmental pollution increases at the beginning of economic growth. However, when it passes a certain level of income, the economic growth allows environmental remediation. from Environmental Kuznets Curve: The Evidence from BSEC Countries* - / EGE ACADEMIC REVIEW and “Environmental Kuznets Curve Hypothesis: A Survey” from Ecological Economics\nPrepare the data\nFrom the world bank website, download data for historical real gdp and population.\nPerform the same steps as in the first part for both series then merge the resulting long databases “on” the Country Code and Date columns.\nYou should obtain one single database data containing the columns [\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\",\"Date\", \"Emissions\", \"GDP\", \"Population\"]\n\ndf_gdp = pd.read_csv('data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_5358352.csv', skiprows=4)\ndf_gdp.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n1.340000e+02\n1.360000e+02\n1.380000e+02\n1.380000e+02\n1.380000e+02\n1.490000e+02\n1.520000e+02\n1.550000e+02\n1.600000e+02\n1.600000e+02\n...\n2.590000e+02\n2.600000e+02\n2.580000e+02\n2.570000e+02\n2.570000e+02\n2.570000e+02\n2.550000e+02\n2.520000e+02\n2.450000e+02\n0.0\n\n\nmean\n7.103309e+10\n7.186603e+10\n7.549842e+10\n8.144478e+10\n8.941392e+10\n9.084572e+10\n1.010890e+11\n1.048751e+11\n1.100032e+11\n1.216457e+11\n...\n2.477220e+12\n2.538873e+12\n2.400201e+12\n2.444669e+12\n2.619922e+12\n2.783541e+12\n2.845284e+12\n2.790295e+12\n3.276616e+12\nNaN\n\n\nstd\n2.132401e+11\n2.208573e+11\n2.354226e+11\n2.532708e+11\n2.769939e+11\n2.909741e+11\n3.185999e+11\n3.369853e+11\n3.589008e+11\n3.950250e+11\n...\n8.385432e+12\n8.597524e+12\n8.158733e+12\n8.314435e+12\n8.847933e+12\n9.417930e+12\n9.586893e+12\n9.387492e+12\n1.079655e+13\nNaN\n\n\nmin\n1.201201e+07\n1.159201e+07\n9.122751e+06\n1.084010e+07\n1.271247e+07\n1.359393e+07\n1.446908e+07\n1.583518e+07\n1.460000e+07\n1.585000e+07\n...\n3.861749e+07\n3.875969e+07\n3.681166e+07\n4.162950e+07\n4.521766e+07\n4.781829e+07\n5.422315e+07\n5.505471e+07\n6.310096e+07\nNaN\n\n\n25%\n5.151683e+08\n5.215510e+08\n5.354690e+08\n5.336845e+08\n5.526379e+08\n5.929812e+08\n6.427026e+08\n6.264909e+08\n6.454036e+08\n6.912229e+08\n...\n8.488220e+09\n9.029027e+09\n8.589120e+09\n8.595956e+09\n9.252834e+09\n9.880676e+09\n1.088080e+10\n1.016305e+10\n1.226939e+10\nNaN\n\n\n50%\n2.976974e+09\n2.966849e+09\n3.050700e+09\n3.570681e+09\n3.432183e+09\n3.120871e+09\n3.549759e+09\n3.384063e+09\n4.064739e+09\n4.759106e+09\n...\n5.094967e+10\n5.339963e+10\n4.966795e+10\n4.984049e+10\n5.472660e+10\n5.700369e+10\n6.113687e+10\n5.715932e+10\n6.740429e+10\nNaN\n\n\n75%\n2.976519e+10\n2.822553e+10\n2.788859e+10\n3.229580e+10\n2.974366e+10\n2.834471e+10\n2.947510e+10\n3.076382e+10\n3.420277e+10\n3.741948e+10\n...\n5.373933e+11\n5.478617e+11\n5.020845e+11\n5.156547e+11\n5.492678e+11\n5.554554e+11\n5.700678e+11\n6.254289e+11\n8.190352e+11\nNaN\n\n\nmax\n1.392273e+12\n1.448622e+12\n1.550544e+12\n1.671610e+12\n1.830287e+12\n1.993900e+12\n2.163894e+12\n2.302529e+12\n2.485213e+12\n2.741172e+12\n...\n7.760623e+13\n7.973264e+13\n7.518636e+13\n7.646936e+13\n8.140950e+13\n8.646696e+13\n8.765425e+13\n8.511634e+13\n9.652743e+13\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\ndf_pop=pd.read_csv('data/API_SP.POP.TOTL_DS2_en_csv_v2_5358404.csv', skiprows=4)\ndf_pop.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n...\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n0.0\n\n\nmean\n1.172187e+08\n1.188268e+08\n1.209957e+08\n1.236763e+08\n1.263792e+08\n1.291211e+08\n1.319789e+08\n1.348350e+08\n1.377713e+08\n1.408128e+08\n...\n2.925870e+08\n2.964856e+08\n3.003545e+08\n3.042155e+08\n3.080704e+08\n3.118393e+08\n3.155191e+08\n3.190983e+08\n3.223248e+08\nNaN\n\n\nstd\n3.693371e+08\n3.738947e+08\n3.806132e+08\n3.893042e+08\n3.980380e+08\n4.069033e+08\n4.162308e+08\n4.255158e+08\n4.350875e+08\n4.450496e+08\n...\n9.184860e+08\n9.299558e+08\n9.412781e+08\n9.525129e+08\n9.637185e+08\n9.745580e+08\n9.850384e+08\n9.950997e+08\n1.004098e+09\nNaN\n\n\nmin\n2.646000e+03\n2.888000e+03\n3.171000e+03\n3.481000e+03\n3.811000e+03\n4.161000e+03\n4.531000e+03\n4.930000e+03\n5.354000e+03\n5.646000e+03\n...\n1.069400e+04\n1.089900e+04\n1.087700e+04\n1.085200e+04\n1.082800e+04\n1.086500e+04\n1.095600e+04\n1.106900e+04\n1.120400e+04\nNaN\n\n\n25%\n5.132212e+05\n5.231345e+05\n5.337595e+05\n5.449288e+05\n5.566630e+05\n5.651150e+05\n5.691470e+05\n5.773872e+05\n5.832700e+05\n5.875942e+05\n...\n1.697753e+06\n1.743309e+06\n1.788196e+06\n1.777557e+06\n1.791003e+06\n1.797085e+06\n1.788878e+06\n1.790133e+06\n1.786038e+06\nNaN\n\n\n50%\n3.757486e+06\n3.887144e+06\n4.023896e+06\n4.139356e+06\n4.224612e+06\n4.277636e+06\n4.331825e+06\n4.385700e+06\n4.450934e+06\n4.530800e+06\n...\n1.014958e+07\n1.028212e+07\n1.035808e+07\n1.032545e+07\n1.030030e+07\n1.039533e+07\n1.044767e+07\n1.060623e+07\n1.050577e+07\nNaN\n\n\n75%\n2.670606e+07\n2.748694e+07\n2.830289e+07\n2.914708e+07\n3.001684e+07\n3.084892e+07\n3.163010e+07\n3.209247e+07\n3.249927e+07\n3.277149e+07\n...\n6.023395e+07\n6.078914e+07\n6.073058e+07\n6.062750e+07\n6.053671e+07\n6.042176e+07\n5.987258e+07\n6.170452e+07\n6.358833e+07\nNaN\n\n\nmax\n3.031565e+09\n3.072511e+09\n3.126935e+09\n3.193509e+09\n3.260518e+09\n3.328285e+09\n3.398561e+09\n3.468457e+09\n3.540255e+09\n3.614669e+09\n...\n7.229185e+09\n7.317509e+09\n7.404911e+09\n7.491934e+09\n7.578158e+09\n7.661776e+09\n7.742682e+09\n7.820982e+09\n7.888409e+09\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\ndf2_long=pd.melt(df_gdp,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"GDP\")\ndf2_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nGDP\n\n\n\n\n1\nAfrica Eastern and Southern\nAFE\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n2.129152e+10\n\n\n2\nAfghanistan\nAFG\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n5.377778e+08\n\n\n3\nAfrica Western and Central\nAFW\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n1.040414e+10\n\n\n13\nAustralia\nAUS\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n1.860567e+10\n\n\n14\nAustria\nAUT\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n6.592694e+09\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16486\nSamoa\nWSM\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n8.438424e+08\n\n\n16487\nKosovo\nXKX\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n9.412034e+09\n\n\n16489\nSouth Africa\nZAF\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n4.190150e+11\n\n\n16490\nZambia\nZMB\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n2.214763e+10\n\n\n16491\nZimbabwe\nZWE\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n2.837124e+10\n\n\n\n\n13156 rows × 6 columns\n\n\n\n\ndf3_long=pd.melt(df_pop,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"Population\")\ndf3_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nPopulation\n\n\n\n\n0\nAruba\nABW\nPopulation, total\nSP.POP.TOTL\n1960\n54608.0\n\n\n1\nAfrica Eastern and Southern\nAFE\nPopulation, total\nSP.POP.TOTL\n1960\n130692579.0\n\n\n2\nAfghanistan\nAFG\nPopulation, total\nSP.POP.TOTL\n1960\n8622466.0\n\n\n3\nAfrica Western and Central\nAFW\nPopulation, total\nSP.POP.TOTL\n1960\n97256290.0\n\n\n4\nAngola\nAGO\nPopulation, total\nSP.POP.TOTL\n1960\n5357195.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16487\nKosovo\nXKX\nPopulation, total\nSP.POP.TOTL\n2021\n1786038.0\n\n\n16488\nYemen, Rep.\nYEM\nPopulation, total\nSP.POP.TOTL\n2021\n32981641.0\n\n\n16489\nSouth Africa\nZAF\nPopulation, total\nSP.POP.TOTL\n2021\n59392255.0\n\n\n16490\nZambia\nZMB\nPopulation, total\nSP.POP.TOTL\n2021\n19473125.0\n\n\n16491\nZimbabwe\nZWE\nPopulation, total\nSP.POP.TOTL\n2021\n15993524.0\n\n\n\n\n16400 rows × 6 columns\n\n\n\n\ndf_kuznetz= df_long.merge(df2_long,on=[\"Date\",\"Country Name\",\"Country Code\"]).merge(df3_long, on=[\"Date\",\"Country Name\",\"Country Code\"])\ndf_kuznetz.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name_x\nIndicator Code_x\nDate\nEmissions\nIndicator Name_y\nIndicator Code_y\nGDP\nIndicator Name\nIndicator Code\nPopulation\n\n\n\n\n7981\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.982136\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.532352e+11\nPopulation, total\nSP.POP.TOTL\n3.098907e+08\n\n\n7983\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.473669\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.218021e+11\nPopulation, total\nSP.POP.TOTL\n2.067390e+08\n\n\n7984\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.554586\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.122876e+10\nPopulation, total\nSP.POP.TOTL\n1.182864e+07\n\n\n7985\nAlbania\nALB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n1.819542\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.028554e+09\nPopulation, total\nSP.POP.TOTL\n3.286542e+06\n\n\n7986\nAndorra\nAND\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n7.653680\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.029048e+09\nPopulation, total\nSP.POP.TOTL\n5.356900e+04\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15953\nWorld\nWLD\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n4.435673\nGDP (current US$)\nNY.GDP.MKTP.CD\n8.765425e+13\nPopulation, total\nSP.POP.TOTL\n7.742682e+09\n\n\n15954\nSamoa\nWSM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n1.415729\nGDP (current US$)\nNY.GDP.MKTP.CD\n9.129445e+08\nPopulation, total\nSP.POP.TOTL\n2.119050e+05\n\n\n15957\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n7.568640\nGDP (current US$)\nNY.GDP.MKTP.CD\n3.885320e+11\nPopulation, total\nSP.POP.TOTL\n5.808706e+07\n\n\n15958\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.369958\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.330867e+10\nPopulation, total\nSP.POP.TOTL\n1.838048e+07\n\n\n15959\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.765894\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.183223e+10\nPopulation, total\nSP.POP.TOTL\n1.535461e+07\n\n\n\n\n6937 rows × 12 columns\n\n\n\nRegress per capita emissions on gdp per capita and comment.\n\nfrom statsmodels.formula import api as smf\ndf_kuznetz['GDP_per_capita']=df_kuznetz['GDP']/1000/df_kuznetz['Population']#on divise le PIB par mille car c'est en million et la population est en milliers\n\n\nmodel=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz)\nres=model.fit()\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.334\n\n\nModel:\nOLS\nAdj. R-squared:\n0.334\n\n\nMethod:\nLeast Squares\nF-statistic:\n3473.\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n12:41:31\nLog-Likelihood:\n-19885.\n\n\nNo. Observations:\n6937\nAIC:\n3.977e+04\n\n\nDf Residuals:\n6935\nBIC:\n3.979e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4500\n0.059\n41.326\n0.000\n2.334\n2.566\n\n\nGDP_per_capita\n0.1852\n0.003\n58.931\n0.000\n0.179\n0.191\n\n\n\n\n\n\nOmnibus:\n3880.879\nDurbin-Watson:\n1.836\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n96298.037\n\n\nSkew:\n2.186\nProb(JB):\n0.00\n\n\nKurtosis:\n20.721\nCond. No.\n21.9\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is emissions per capita = 2,45+ 0,1852 gdp per capita. Both the intercept and the coefficient are significant, indicating that higher GDP per capita is associated with higher per capita emissions, holding other factors constant. The R-squared of the model is relatively low at 0.334, suggesting that other factors not included in the model may also be important in explaining per capita emissions.\nSplit the sample into the three quantiles, based on gdp/capita in the last period. Run the same regression in all three subsamples. Comment.\n\ndf_kuznetz['Quantile']= pd.qcut(df_kuznetz['GDP_per_capita'],q = 3, labels=['low', 'medium', 'high'])\nmodel_low=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"low\"])\nres_low=model_low.fit()\nres_low.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.044\n\n\nModel:\nOLS\nAdj. R-squared:\n0.043\n\n\nMethod:\nLeast Squares\nF-statistic:\n68.42\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n2.89e-16\n\n\nTime:\n13:37:32\nLog-Likelihood:\n-2221.9\n\n\nNo. Observations:\n1493\nAIC:\n4448.\n\n\nDf Residuals:\n1491\nBIC:\n4458.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0444\n0.073\n0.605\n0.545\n-0.100\n0.189\n\n\nGDP_per_capita\n1.2173\n0.147\n8.272\n0.000\n0.929\n1.506\n\n\n\n\n\n\nOmnibus:\n1261.416\nDurbin-Watson:\n1.783\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n26143.353\n\n\nSkew:\n4.011\nProb(JB):\n0.00\n\n\nKurtosis:\n21.865\nCond. No.\n6.47\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_medium=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"medium\"])\nres_medium=model_medium.fit()\nres_medium.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.096\n\n\nModel:\nOLS\nAdj. R-squared:\n0.095\n\n\nMethod:\nLeast Squares\nF-statistic:\n274.8\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n9.77e-59\n\n\nTime:\n13:38:08\nLog-Likelihood:\n-5698.0\n\n\nNo. Observations:\n2600\nAIC:\n1.140e+04\n\n\nDf Residuals:\n2598\nBIC:\n1.141e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.7320\n0.099\n7.418\n0.000\n0.539\n0.926\n\n\nGDP_per_capita\n0.6774\n0.041\n16.576\n0.000\n0.597\n0.758\n\n\n\n\n\n\nOmnibus:\n1316.986\nDurbin-Watson:\n1.691\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n7900.449\n\n\nSkew:\n2.395\nProb(JB):\n0.00\n\n\nKurtosis:\n10.071\nCond. No.\n6.42\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_high=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"high\"])\nres_high=model_high.fit()\nres_high.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.131\n\n\nModel:\nOLS\nAdj. R-squared:\n0.130\n\n\nMethod:\nLeast Squares\nF-statistic:\n427.5\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n1.37e-88\n\n\nTime:\n13:38:39\nLog-Likelihood:\n-8941.7\n\n\nNo. Observations:\n2844\nAIC:\n1.789e+04\n\n\nDf Residuals:\n2842\nBIC:\n1.790e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n5.7100\n0.152\n37.652\n0.000\n5.413\n6.007\n\n\nGDP_per_capita\n0.1068\n0.005\n20.676\n0.000\n0.097\n0.117\n\n\n\n\n\n\nOmnibus:\n1447.885\nDurbin-Watson:\n1.902\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12763.970\n\n\nSkew:\n2.245\nProb(JB):\n0.00\n\n\nKurtosis:\n12.357\nCond. No.\n42.3\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThese three linear regressions show us that the higher the GDP per capita of countries, the higher the R-squared. This means that the model is more predictive for countries with a “high” GDP, the higher their GDP per capita, the higher their per capita emissions. This partly confirms the Kuznetz curve hypothesis\nInstead of splitting the sample, run a nonlinear regression:\n\\[\\frac{ \\text{emissions}_{i t} }{ \\text{population}_{i t} } = \\alpha + \\beta_1*\\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} + \\beta_2*\\left( \\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} \\right)^2 + \\text{other factors}_{i t}\\]\nJustify why \\(\\beta_2&lt;0\\) can be interpreted as a proof of the environmental Kuznets hypothesis. Comment.\n\nimport numpy as np\nnonlinear_regression=smf.ols('Emissions ~ GDP_per_capita + np.power(GDP_per_capita, 2)',df_kuznetz)\nres_nl=nonlinear_regression.fit()\nres_nl.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.465\n\n\nModel:\nOLS\nAdj. R-squared:\n0.465\n\n\nMethod:\nLeast Squares\nF-statistic:\n3019.\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n13:49:50\nLog-Likelihood:\n-19121.\n\n\nNo. Observations:\n6937\nAIC:\n3.825e+04\n\n\nDf Residuals:\n6934\nBIC:\n3.827e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.5088\n0.058\n26.113\n0.000\n1.395\n1.622\n\n\nGDP_per_capita\n0.3719\n0.005\n69.887\n0.000\n0.361\n0.382\n\n\nnp.power(GDP_per_capita, 2)\n-0.0024\n5.76e-05\n-41.345\n0.000\n-0.002\n-0.002\n\n\n\n\n\n\nOmnibus:\n4662.808\nDurbin-Watson:\n1.905\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n103561.456\n\n\nSkew:\n2.902\nProb(JB):\n0.00\n\n\nKurtosis:\n21.017\nCond. No.\n1.95e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.95e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe coefficient on the quadratic term, 𝛽2, is negative, standing at -0,0024, which support the hypothesis that the relationship between emissions and GDP per capita is U-shaped and consistent with the EKH.\n(Bonus) Suggest a way to improve the regression.\nthis result alone does not provide conclusive proof of the EKH, as the coefficient is not statistically significant and as there may be other factors that influence the relationship between emissions and GDP per capita\n\n\n\n\nThe scientific review Data-in-Brief, publishes raw data after a rigorous referee process.\nThe following entry contains booking data for two hotels in Portugal, with many informations about the clients.\nYour goal is to propose a machine learning model to predict whether a given booking will be cancelled.\n(note that the dataset is rather large and that some operations may take some time to complete)\n\n\nImport the dataset. Describe it.\n\nimport pandas\ndf = pandas.read_csv(\"hotel_booking.csv\")\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\nSplit the dataset between a train set and a validation set.\nThe validation set should not be touched until the very end.\n\nimport sklearn\nimport sklearn.model_selection\n\n\ndf_ml, df_validation = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=56)\n# until the very last question, you should use *only* the training set\n\nSplit the df_ml dataframe between a training set and a test set.\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df_ml)\n# the various algorithms can be trained and tested using df_train and df_test\n\n\n\n\nJustify why a machine learning model seems appropriate to predict cancellation. Which one(s) could you use?\nImplement two or more classification models, to predict cancellation.\nCompare their performance on the test set. Which one would you choose?\n\n\n\nUsing your preferred model, use the validation set to compute the confusion matrix. Comment.\n\ns='The rain in Spain stays mainly in the plane'\ns[s.index('m'):s.index('S')+5]\n\n\n'mainl'"
  },
  {
    "objectID": "tutorials/session_8/old_homework.html#co2-emissions-and-the-kusnetz-curve",
    "href": "tutorials/session_8/old_homework.html#co2-emissions-and-the-kusnetz-curve",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "In this exercise, you must use data from the WorldBank to check whether CO2 Emissions can be explained by income per capita.\n\n\nImport the data from file data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv as a dataframe df_wide. (This table was downloaded from the worldbank website)\nHints: check the documentation from pandas.read_csv() to avoid the import error. You can check the first few lines of the file by typing in a cell: !data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv\nHint2: Check that all columns are well defined.\n\nimport pandas as pd\ndf_wide = pd.read_csv('data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv', skiprows=4)\n!data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv\n\n/bin/bash: line 1: data/API_EN.ATM.CO2E.PC_DS2_en_csv_v2_5177406.csv: Permission denied\n\n\nDescribe briefly the data.\nThe data contains information on the level of carbon dioxide (CO2) emissions per capita (in metric tons) for different countries and regions over the years 1960 to 2019. The data is sourced from the World Bank’s World Development Indicators and is reported annually.\nThe dataset contains 264 rows (corresponding to different countries and regions) and 61 columns (corresponding to the years 1960 to 2019 and additional columns with country and region information). The data is in a wide format, with each row representing a country/region and each column representing a year.\nThe data is useful for analyzing trends in CO2 emissions over time and comparing emissions across countries and regions. It can also be used for exploring the relationship between CO2 emissions and various economic and environmental factors.\n\ndf_wide.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n239.000000\n0.0\n0.0\n0.0\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n4.288766\n4.194242\n4.141210\n4.140690\n4.148584\n4.128191\n4.086604\nNaN\nNaN\nNaN\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n5.002821\n4.862196\n4.731198\n4.653372\n4.585664\n4.489473\n4.474616\nNaN\nNaN\nNaN\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.024987\n0.027090\n0.037289\n0.029718\n0.033815\n0.035826\n0.035704\nNaN\nNaN\nNaN\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.720891\n0.785259\n0.771880\n0.769068\n0.802179\n0.797275\n0.790333\nNaN\nNaN\nNaN\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.726281\n2.888558\n2.827949\n2.760181\n2.699348\n2.979403\n2.981762\nNaN\nNaN\nNaN\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n6.264131\n6.028664\n5.792812\n5.828931\n6.021844\n5.950694\n5.890683\nNaN\nNaN\nNaN\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n37.420762\n36.875726\n35.111798\n33.493040\n32.281678\n31.235406\n32.761775\nNaN\nNaN\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\n\n\nConvert the data into the long format using the function pandas.melt(). The columns of the new table df_long should be: [\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\", \"Date\", \"Emissions\"]\n\ndf_long=pd.melt(df_wide,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"Emissions\")\ndf_long\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nEmissions\n\n\n\n\n0\nAruba\nABW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n1\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n2\nAfghanistan\nAFG\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n3\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n4\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1960\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16753\nKosovo\nXKX\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16754\nYemen, Rep.\nYEM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16755\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16756\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n16757\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\nUnnamed: 66\nNaN\n\n\n\n\n16758 rows × 6 columns\n\n\n\nEliminate all rows for which no emission data is available\n\ndf_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nEmissions\n\n\n\n\n7981\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.982136\n\n\n7982\nAfghanistan\nAFG\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.222538\n\n\n7983\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.473669\n\n\n7984\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.554586\n\n\n7985\nAlbania\nALB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n1.819542\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n15954\nSamoa\nWSM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n1.415729\n\n\n15956\nYemen, Rep.\nYEM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.351859\n\n\n15957\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n7.568640\n\n\n15958\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.369958\n\n\n15959\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.765894\n\n\n\n\n7147 rows × 6 columns\n\n\n\nConvert the Date column into a date format.\nHint: look for pandas.to_datetime() (This is not absolutely mandatory but makes graphs nicer)\n\ndf_long['Date'] = pd.to_datetime(df_long['Date'], format='%Y')\nprint(df_long['Date'].dtype)\n\nValueError: time data 'Unnamed: 66' does not match format '%Y' (match)\n\n\n\n\n\nPlot the evolution over time of total recorded carbon emissions (ommiting dates where no information is available).\nHint: use groupby()\n\ntotal_emissions=df_long[df_long['Emissions'].notnull()].groupby(df_long['Date'])['Emissions'].sum()\ntotal_emissions\n\nDate\n1990     992.579902\n1991     971.488470\n1992     971.204768\n1993     955.914952\n1994     949.595751\n1995     948.978410\n1996     964.076804\n1997     970.839153\n1998     970.257143\n1999     964.693322\n2000     967.857097\n2001     984.230281\n2002     993.251894\n2003    1022.691228\n2004    1038.318099\n2005    1048.812274\n2006    1064.012494\n2007    1060.075753\n2008    1053.788436\n2009    1069.598447\n2010    1033.216694\n2011    1037.102602\n2012    1047.851826\n2013    1025.015035\n2014    1002.423899\n2015     989.749130\n2016     989.624913\n2017     991.511483\n2018     986.637647\n2019     976.698404\nName: Emissions, dtype: float64\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(total_emissions)\nplt.xlabel(\"Years\")\nplt.ylabel(\"Emissions as metric tons per capita\")\nplt.title(\"Evolution of total CO2 emissions over time\")\nplt.show\n\n\n\n\n\n\n\n\nPlot cumulative carbon emissions\n\nimport numpy as np\nplt.plot(total_emissions.cumsum())\nplt.xlabel(\"Years\")\nplt.ylabel(\"Emissions as metric tons per capita\")\nplt.title(\"Evolution of cumulated CO2 emissions over time\")\nplt.show\n\n\n\n\n\n\n\n\nPropose some plots to visualize the contribution of the main contributors to yearly carbon emissions and to cumulative carbon emissions. You can get inspiration from ourworldindata.\nWe could use a pie Chart of Global Emissions by Top 10 Emitters. This plot would show us the percentage of global carbon emissions of the top 10 emitters, with each country’s contribution shown as a colored segment of a pie chart in order to provide the relative contributions of each country to global emissions. We could also use an area chart of cumulative emissions by top emitters: This plot shows the cumulative carbon emissions over time for the top emitters, with each country’s contribution shown as a colored area of a stacked area chart. This plot allows for easy comparison of the contributions of different countries to cumulative emissions.\n\n\n\nWe are now interested in the relation between carbon emission and economic development. To this purpose, we would like to run a simple regression\n\\[\\frac{ \\text{emissions}_{i t} }{ \\text{population}_{i t} } = \\alpha + \\beta*\\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} + \\text{other factors}_{i t}\\]\nwhere \\(i\\) is the country index and \\(t\\) the time index. In a first step we will simply assume that the total effect of all other factors is normally distributed.\nWe would then like to consider variants of this regression in order to test the environment Kuznets curve hypothesis.\nBriefly summarize the environmental Kuznets curve hypothesis\n(hint: perform a small websearch. Try to identify your sources)\nThe Environmental Kuznets Curve hypothesis postulates an inverted-U-shaped relationship between different pollutants and per capita income, i.e., environmental pressure increases up to a certain level as income goes up; after that, it decreases. The environmental pollution increases at the beginning of economic growth. However, when it passes a certain level of income, the economic growth allows environmental remediation. from Environmental Kuznets Curve: The Evidence from BSEC Countries* - / EGE ACADEMIC REVIEW and “Environmental Kuznets Curve Hypothesis: A Survey” from Ecological Economics\nPrepare the data\nFrom the world bank website, download data for historical real gdp and population.\nPerform the same steps as in the first part for both series then merge the resulting long databases “on” the Country Code and Date columns.\nYou should obtain one single database data containing the columns [\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\",\"Date\", \"Emissions\", \"GDP\", \"Population\"]\n\ndf_gdp = pd.read_csv('data/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_5358352.csv', skiprows=4)\ndf_gdp.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n1.340000e+02\n1.360000e+02\n1.380000e+02\n1.380000e+02\n1.380000e+02\n1.490000e+02\n1.520000e+02\n1.550000e+02\n1.600000e+02\n1.600000e+02\n...\n2.590000e+02\n2.600000e+02\n2.580000e+02\n2.570000e+02\n2.570000e+02\n2.570000e+02\n2.550000e+02\n2.520000e+02\n2.450000e+02\n0.0\n\n\nmean\n7.103309e+10\n7.186603e+10\n7.549842e+10\n8.144478e+10\n8.941392e+10\n9.084572e+10\n1.010890e+11\n1.048751e+11\n1.100032e+11\n1.216457e+11\n...\n2.477220e+12\n2.538873e+12\n2.400201e+12\n2.444669e+12\n2.619922e+12\n2.783541e+12\n2.845284e+12\n2.790295e+12\n3.276616e+12\nNaN\n\n\nstd\n2.132401e+11\n2.208573e+11\n2.354226e+11\n2.532708e+11\n2.769939e+11\n2.909741e+11\n3.185999e+11\n3.369853e+11\n3.589008e+11\n3.950250e+11\n...\n8.385432e+12\n8.597524e+12\n8.158733e+12\n8.314435e+12\n8.847933e+12\n9.417930e+12\n9.586893e+12\n9.387492e+12\n1.079655e+13\nNaN\n\n\nmin\n1.201201e+07\n1.159201e+07\n9.122751e+06\n1.084010e+07\n1.271247e+07\n1.359393e+07\n1.446908e+07\n1.583518e+07\n1.460000e+07\n1.585000e+07\n...\n3.861749e+07\n3.875969e+07\n3.681166e+07\n4.162950e+07\n4.521766e+07\n4.781829e+07\n5.422315e+07\n5.505471e+07\n6.310096e+07\nNaN\n\n\n25%\n5.151683e+08\n5.215510e+08\n5.354690e+08\n5.336845e+08\n5.526379e+08\n5.929812e+08\n6.427026e+08\n6.264909e+08\n6.454036e+08\n6.912229e+08\n...\n8.488220e+09\n9.029027e+09\n8.589120e+09\n8.595956e+09\n9.252834e+09\n9.880676e+09\n1.088080e+10\n1.016305e+10\n1.226939e+10\nNaN\n\n\n50%\n2.976974e+09\n2.966849e+09\n3.050700e+09\n3.570681e+09\n3.432183e+09\n3.120871e+09\n3.549759e+09\n3.384063e+09\n4.064739e+09\n4.759106e+09\n...\n5.094967e+10\n5.339963e+10\n4.966795e+10\n4.984049e+10\n5.472660e+10\n5.700369e+10\n6.113687e+10\n5.715932e+10\n6.740429e+10\nNaN\n\n\n75%\n2.976519e+10\n2.822553e+10\n2.788859e+10\n3.229580e+10\n2.974366e+10\n2.834471e+10\n2.947510e+10\n3.076382e+10\n3.420277e+10\n3.741948e+10\n...\n5.373933e+11\n5.478617e+11\n5.020845e+11\n5.156547e+11\n5.492678e+11\n5.554554e+11\n5.700678e+11\n6.254289e+11\n8.190352e+11\nNaN\n\n\nmax\n1.392273e+12\n1.448622e+12\n1.550544e+12\n1.671610e+12\n1.830287e+12\n1.993900e+12\n2.163894e+12\n2.302529e+12\n2.485213e+12\n2.741172e+12\n...\n7.760623e+13\n7.973264e+13\n7.518636e+13\n7.646936e+13\n8.140950e+13\n8.646696e+13\n8.765425e+13\n8.511634e+13\n9.652743e+13\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\ndf_pop=pd.read_csv('data/API_SP.POP.TOTL_DS2_en_csv_v2_5358404.csv', skiprows=4)\ndf_pop.describe()\n\n\n\n\n\n\n\n\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n...\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\nUnnamed: 66\n\n\n\n\ncount\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n2.640000e+02\n...\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n2.650000e+02\n0.0\n\n\nmean\n1.172187e+08\n1.188268e+08\n1.209957e+08\n1.236763e+08\n1.263792e+08\n1.291211e+08\n1.319789e+08\n1.348350e+08\n1.377713e+08\n1.408128e+08\n...\n2.925870e+08\n2.964856e+08\n3.003545e+08\n3.042155e+08\n3.080704e+08\n3.118393e+08\n3.155191e+08\n3.190983e+08\n3.223248e+08\nNaN\n\n\nstd\n3.693371e+08\n3.738947e+08\n3.806132e+08\n3.893042e+08\n3.980380e+08\n4.069033e+08\n4.162308e+08\n4.255158e+08\n4.350875e+08\n4.450496e+08\n...\n9.184860e+08\n9.299558e+08\n9.412781e+08\n9.525129e+08\n9.637185e+08\n9.745580e+08\n9.850384e+08\n9.950997e+08\n1.004098e+09\nNaN\n\n\nmin\n2.646000e+03\n2.888000e+03\n3.171000e+03\n3.481000e+03\n3.811000e+03\n4.161000e+03\n4.531000e+03\n4.930000e+03\n5.354000e+03\n5.646000e+03\n...\n1.069400e+04\n1.089900e+04\n1.087700e+04\n1.085200e+04\n1.082800e+04\n1.086500e+04\n1.095600e+04\n1.106900e+04\n1.120400e+04\nNaN\n\n\n25%\n5.132212e+05\n5.231345e+05\n5.337595e+05\n5.449288e+05\n5.566630e+05\n5.651150e+05\n5.691470e+05\n5.773872e+05\n5.832700e+05\n5.875942e+05\n...\n1.697753e+06\n1.743309e+06\n1.788196e+06\n1.777557e+06\n1.791003e+06\n1.797085e+06\n1.788878e+06\n1.790133e+06\n1.786038e+06\nNaN\n\n\n50%\n3.757486e+06\n3.887144e+06\n4.023896e+06\n4.139356e+06\n4.224612e+06\n4.277636e+06\n4.331825e+06\n4.385700e+06\n4.450934e+06\n4.530800e+06\n...\n1.014958e+07\n1.028212e+07\n1.035808e+07\n1.032545e+07\n1.030030e+07\n1.039533e+07\n1.044767e+07\n1.060623e+07\n1.050577e+07\nNaN\n\n\n75%\n2.670606e+07\n2.748694e+07\n2.830289e+07\n2.914708e+07\n3.001684e+07\n3.084892e+07\n3.163010e+07\n3.209247e+07\n3.249927e+07\n3.277149e+07\n...\n6.023395e+07\n6.078914e+07\n6.073058e+07\n6.062750e+07\n6.053671e+07\n6.042176e+07\n5.987258e+07\n6.170452e+07\n6.358833e+07\nNaN\n\n\nmax\n3.031565e+09\n3.072511e+09\n3.126935e+09\n3.193509e+09\n3.260518e+09\n3.328285e+09\n3.398561e+09\n3.468457e+09\n3.540255e+09\n3.614669e+09\n...\n7.229185e+09\n7.317509e+09\n7.404911e+09\n7.491934e+09\n7.578158e+09\n7.661776e+09\n7.742682e+09\n7.820982e+09\n7.888409e+09\nNaN\n\n\n\n\n8 rows × 63 columns\n\n\n\n\ndf2_long=pd.melt(df_gdp,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"GDP\")\ndf2_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nGDP\n\n\n\n\n1\nAfrica Eastern and Southern\nAFE\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n2.129152e+10\n\n\n2\nAfghanistan\nAFG\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n5.377778e+08\n\n\n3\nAfrica Western and Central\nAFW\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n1.040414e+10\n\n\n13\nAustralia\nAUS\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n1.860567e+10\n\n\n14\nAustria\nAUT\nGDP (current US$)\nNY.GDP.MKTP.CD\n1960\n6.592694e+09\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16486\nSamoa\nWSM\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n8.438424e+08\n\n\n16487\nKosovo\nXKX\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n9.412034e+09\n\n\n16489\nSouth Africa\nZAF\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n4.190150e+11\n\n\n16490\nZambia\nZMB\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n2.214763e+10\n\n\n16491\nZimbabwe\nZWE\nGDP (current US$)\nNY.GDP.MKTP.CD\n2021\n2.837124e+10\n\n\n\n\n13156 rows × 6 columns\n\n\n\n\ndf3_long=pd.melt(df_pop,id_vars=[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"],var_name=\"Date\",value_name= \"Population\")\ndf3_long.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nIndicator Code\nDate\nPopulation\n\n\n\n\n0\nAruba\nABW\nPopulation, total\nSP.POP.TOTL\n1960\n54608.0\n\n\n1\nAfrica Eastern and Southern\nAFE\nPopulation, total\nSP.POP.TOTL\n1960\n130692579.0\n\n\n2\nAfghanistan\nAFG\nPopulation, total\nSP.POP.TOTL\n1960\n8622466.0\n\n\n3\nAfrica Western and Central\nAFW\nPopulation, total\nSP.POP.TOTL\n1960\n97256290.0\n\n\n4\nAngola\nAGO\nPopulation, total\nSP.POP.TOTL\n1960\n5357195.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16487\nKosovo\nXKX\nPopulation, total\nSP.POP.TOTL\n2021\n1786038.0\n\n\n16488\nYemen, Rep.\nYEM\nPopulation, total\nSP.POP.TOTL\n2021\n32981641.0\n\n\n16489\nSouth Africa\nZAF\nPopulation, total\nSP.POP.TOTL\n2021\n59392255.0\n\n\n16490\nZambia\nZMB\nPopulation, total\nSP.POP.TOTL\n2021\n19473125.0\n\n\n16491\nZimbabwe\nZWE\nPopulation, total\nSP.POP.TOTL\n2021\n15993524.0\n\n\n\n\n16400 rows × 6 columns\n\n\n\n\ndf_kuznetz= df_long.merge(df2_long,on=[\"Date\",\"Country Name\",\"Country Code\"]).merge(df3_long, on=[\"Date\",\"Country Name\",\"Country Code\"])\ndf_kuznetz.dropna()\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name_x\nIndicator Code_x\nDate\nEmissions\nIndicator Name_y\nIndicator Code_y\nGDP\nIndicator Name\nIndicator Code\nPopulation\n\n\n\n\n7981\nAfrica Eastern and Southern\nAFE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.982136\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.532352e+11\nPopulation, total\nSP.POP.TOTL\n3.098907e+08\n\n\n7983\nAfrica Western and Central\nAFW\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.473669\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.218021e+11\nPopulation, total\nSP.POP.TOTL\n2.067390e+08\n\n\n7984\nAngola\nAGO\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n0.554586\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.122876e+10\nPopulation, total\nSP.POP.TOTL\n1.182864e+07\n\n\n7985\nAlbania\nALB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n1.819542\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.028554e+09\nPopulation, total\nSP.POP.TOTL\n3.286542e+06\n\n\n7986\nAndorra\nAND\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n1990\n7.653680\nGDP (current US$)\nNY.GDP.MKTP.CD\n1.029048e+09\nPopulation, total\nSP.POP.TOTL\n5.356900e+04\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15953\nWorld\nWLD\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n4.435673\nGDP (current US$)\nNY.GDP.MKTP.CD\n8.765425e+13\nPopulation, total\nSP.POP.TOTL\n7.742682e+09\n\n\n15954\nSamoa\nWSM\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n1.415729\nGDP (current US$)\nNY.GDP.MKTP.CD\n9.129445e+08\nPopulation, total\nSP.POP.TOTL\n2.119050e+05\n\n\n15957\nSouth Africa\nZAF\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n7.568640\nGDP (current US$)\nNY.GDP.MKTP.CD\n3.885320e+11\nPopulation, total\nSP.POP.TOTL\n5.808706e+07\n\n\n15958\nZambia\nZMB\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.369958\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.330867e+10\nPopulation, total\nSP.POP.TOTL\n1.838048e+07\n\n\n15959\nZimbabwe\nZWE\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n2019\n0.765894\nGDP (current US$)\nNY.GDP.MKTP.CD\n2.183223e+10\nPopulation, total\nSP.POP.TOTL\n1.535461e+07\n\n\n\n\n6937 rows × 12 columns\n\n\n\nRegress per capita emissions on gdp per capita and comment.\n\nfrom statsmodels.formula import api as smf\ndf_kuznetz['GDP_per_capita']=df_kuznetz['GDP']/1000/df_kuznetz['Population']#on divise le PIB par mille car c'est en million et la population est en milliers\n\n\nmodel=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz)\nres=model.fit()\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.334\n\n\nModel:\nOLS\nAdj. R-squared:\n0.334\n\n\nMethod:\nLeast Squares\nF-statistic:\n3473.\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n12:41:31\nLog-Likelihood:\n-19885.\n\n\nNo. Observations:\n6937\nAIC:\n3.977e+04\n\n\nDf Residuals:\n6935\nBIC:\n3.979e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4500\n0.059\n41.326\n0.000\n2.334\n2.566\n\n\nGDP_per_capita\n0.1852\n0.003\n58.931\n0.000\n0.179\n0.191\n\n\n\n\n\n\nOmnibus:\n3880.879\nDurbin-Watson:\n1.836\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n96298.037\n\n\nSkew:\n2.186\nProb(JB):\n0.00\n\n\nKurtosis:\n20.721\nCond. No.\n21.9\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe estimated regresssion is emissions per capita = 2,45+ 0,1852 gdp per capita. Both the intercept and the coefficient are significant, indicating that higher GDP per capita is associated with higher per capita emissions, holding other factors constant. The R-squared of the model is relatively low at 0.334, suggesting that other factors not included in the model may also be important in explaining per capita emissions.\nSplit the sample into the three quantiles, based on gdp/capita in the last period. Run the same regression in all three subsamples. Comment.\n\ndf_kuznetz['Quantile']= pd.qcut(df_kuznetz['GDP_per_capita'],q = 3, labels=['low', 'medium', 'high'])\nmodel_low=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"low\"])\nres_low=model_low.fit()\nres_low.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.044\n\n\nModel:\nOLS\nAdj. R-squared:\n0.043\n\n\nMethod:\nLeast Squares\nF-statistic:\n68.42\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n2.89e-16\n\n\nTime:\n13:37:32\nLog-Likelihood:\n-2221.9\n\n\nNo. Observations:\n1493\nAIC:\n4448.\n\n\nDf Residuals:\n1491\nBIC:\n4458.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0444\n0.073\n0.605\n0.545\n-0.100\n0.189\n\n\nGDP_per_capita\n1.2173\n0.147\n8.272\n0.000\n0.929\n1.506\n\n\n\n\n\n\nOmnibus:\n1261.416\nDurbin-Watson:\n1.783\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n26143.353\n\n\nSkew:\n4.011\nProb(JB):\n0.00\n\n\nKurtosis:\n21.865\nCond. No.\n6.47\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_medium=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"medium\"])\nres_medium=model_medium.fit()\nres_medium.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.096\n\n\nModel:\nOLS\nAdj. R-squared:\n0.095\n\n\nMethod:\nLeast Squares\nF-statistic:\n274.8\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n9.77e-59\n\n\nTime:\n13:38:08\nLog-Likelihood:\n-5698.0\n\n\nNo. Observations:\n2600\nAIC:\n1.140e+04\n\n\nDf Residuals:\n2598\nBIC:\n1.141e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.7320\n0.099\n7.418\n0.000\n0.539\n0.926\n\n\nGDP_per_capita\n0.6774\n0.041\n16.576\n0.000\n0.597\n0.758\n\n\n\n\n\n\nOmnibus:\n1316.986\nDurbin-Watson:\n1.691\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n7900.449\n\n\nSkew:\n2.395\nProb(JB):\n0.00\n\n\nKurtosis:\n10.071\nCond. No.\n6.42\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmodel_high=smf.ols(\" Emissions ~ GDP_per_capita \", df_kuznetz[df_kuznetz['Quantile'] == \"high\"])\nres_high=model_high.fit()\nres_high.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.131\n\n\nModel:\nOLS\nAdj. R-squared:\n0.130\n\n\nMethod:\nLeast Squares\nF-statistic:\n427.5\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n1.37e-88\n\n\nTime:\n13:38:39\nLog-Likelihood:\n-8941.7\n\n\nNo. Observations:\n2844\nAIC:\n1.789e+04\n\n\nDf Residuals:\n2842\nBIC:\n1.790e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n5.7100\n0.152\n37.652\n0.000\n5.413\n6.007\n\n\nGDP_per_capita\n0.1068\n0.005\n20.676\n0.000\n0.097\n0.117\n\n\n\n\n\n\nOmnibus:\n1447.885\nDurbin-Watson:\n1.902\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12763.970\n\n\nSkew:\n2.245\nProb(JB):\n0.00\n\n\nKurtosis:\n12.357\nCond. No.\n42.3\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThese three linear regressions show us that the higher the GDP per capita of countries, the higher the R-squared. This means that the model is more predictive for countries with a “high” GDP, the higher their GDP per capita, the higher their per capita emissions. This partly confirms the Kuznetz curve hypothesis\nInstead of splitting the sample, run a nonlinear regression:\n\\[\\frac{ \\text{emissions}_{i t} }{ \\text{population}_{i t} } = \\alpha + \\beta_1*\\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} + \\beta_2*\\left( \\frac{ \\text{gdp}_{i t}}{\\text{population}_{i t}} \\right)^2 + \\text{other factors}_{i t}\\]\nJustify why \\(\\beta_2&lt;0\\) can be interpreted as a proof of the environmental Kuznets hypothesis. Comment.\n\nimport numpy as np\nnonlinear_regression=smf.ols('Emissions ~ GDP_per_capita + np.power(GDP_per_capita, 2)',df_kuznetz)\nres_nl=nonlinear_regression.fit()\nres_nl.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nEmissions\nR-squared:\n0.465\n\n\nModel:\nOLS\nAdj. R-squared:\n0.465\n\n\nMethod:\nLeast Squares\nF-statistic:\n3019.\n\n\nDate:\nMon, 03 Apr 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n13:49:50\nLog-Likelihood:\n-19121.\n\n\nNo. Observations:\n6937\nAIC:\n3.825e+04\n\n\nDf Residuals:\n6934\nBIC:\n3.827e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.5088\n0.058\n26.113\n0.000\n1.395\n1.622\n\n\nGDP_per_capita\n0.3719\n0.005\n69.887\n0.000\n0.361\n0.382\n\n\nnp.power(GDP_per_capita, 2)\n-0.0024\n5.76e-05\n-41.345\n0.000\n-0.002\n-0.002\n\n\n\n\n\n\nOmnibus:\n4662.808\nDurbin-Watson:\n1.905\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n103561.456\n\n\nSkew:\n2.902\nProb(JB):\n0.00\n\n\nKurtosis:\n21.017\nCond. No.\n1.95e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.95e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe coefficient on the quadratic term, 𝛽2, is negative, standing at -0,0024, which support the hypothesis that the relationship between emissions and GDP per capita is U-shaped and consistent with the EKH.\n(Bonus) Suggest a way to improve the regression.\nthis result alone does not provide conclusive proof of the EKH, as the coefficient is not statistically significant and as there may be other factors that influence the relationship between emissions and GDP per capita"
  },
  {
    "objectID": "tutorials/session_8/old_homework.html#predicting-booking-cancellations",
    "href": "tutorials/session_8/old_homework.html#predicting-booking-cancellations",
    "title": "Data-Based Economics: Coursework",
    "section": "",
    "text": "The scientific review Data-in-Brief, publishes raw data after a rigorous referee process.\nThe following entry contains booking data for two hotels in Portugal, with many informations about the clients.\nYour goal is to propose a machine learning model to predict whether a given booking will be cancelled.\n(note that the dataset is rather large and that some operations may take some time to complete)\n\n\nImport the dataset. Describe it.\n\nimport pandas\ndf = pandas.read_csv(\"hotel_booking.csv\")\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\nSplit the dataset between a train set and a validation set.\nThe validation set should not be touched until the very end.\n\nimport sklearn\nimport sklearn.model_selection\n\n\ndf_ml, df_validation = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=56)\n# until the very last question, you should use *only* the training set\n\nSplit the df_ml dataframe between a training set and a test set.\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df_ml)\n# the various algorithms can be trained and tested using df_train and df_test\n\n\n\n\nJustify why a machine learning model seems appropriate to predict cancellation. Which one(s) could you use?\nImplement two or more classification models, to predict cancellation.\nCompare their performance on the test set. Which one would you choose?\n\n\n\nUsing your preferred model, use the validation set to compute the confusion matrix. Comment.\n\ns='The rain in Spain stays mainly in the plane'\ns[s.index('m'):s.index('S')+5]\n\n\n'mainl'"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis.html",
    "href": "tutorials/session_8/sentiment_analysis.html",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The following code processes the Lending Club Dataset from kaggle.\nRun and comment the following instructions (fix them if needed). Inspect he dataframe?\n\n# importing libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\nimport numpy as np\n\n/tmp/ipykernel_7230/3363083914.py:2: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\n# import dataset\nloan = pd.read_csv('loans.csv', low_memory=True)\n\n\nloan\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\n0\n3600.0\n1.0\n24.0\n10.0\n55000.0\n0.0\n5.91\n4.0\n29.7\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20000.0\n2.0\n14.0\n10.0\n63000.0\n0.0\n10.78\n10.0\n56.2\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n10400.0\n2.0\n51.0\n3.0\n104433.0\n0.0\n25.37\n1.0\n64.5\n7.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n20000.0\n1.0\n11.0\n10.0\n85000.0\n0.0\n17.61\n8.0\n5.7\n3.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n10000.0\n1.0\n2.0\n6.0\n85000.0\n0.0\n13.07\n1.0\n34.5\n13.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n848449\n22400.0\n2.0\n22.0\n10.0\n119000.0\n1.0\n23.22\n1.0\n51.5\n12.0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n848450\n19400.0\n2.0\n24.0\n0.0\n78000.0\n0.0\n13.02\n2.0\n63.8\n9.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n848451\n11200.0\n2.0\n22.0\n5.0\n86000.0\n1.0\n4.80\n10.0\n54.7\n7.0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n848452\n23800.0\n2.0\n24.0\n10.0\n119000.0\n0.0\n32.73\n0.0\n89.5\n8.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n848453\n24000.0\n2.0\n24.0\n6.0\n110000.0\n1.0\n18.30\n9.0\n68.1\n8.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n848454 rows × 66 columns\n\n\n\nRun and comment the following instructions (fix them if needed). What kind of modeling exercise is performed?\n\nloan['loan_status'].value_counts()\n\nloan_status\n0.0    672377\n1.0    176077\nName: count, dtype: int64\n\n\nThe objective here is to build a model to predict which loans will default (loan_status=1). It is a classification exercise. Since the number of regressors is fairly large it is natural to look for a machine learning approcah.\n\n#  create a list of all regressors (excluding loan_status)\nfeatures = loan.columns.to_list()\nfeatures.remove('loan_status')\nfeatures\n\n['loan_amnt',\n 'term',\n 'sub_grade',\n 'emp_length',\n 'annual_inc',\n 'dti',\n 'mths_since_recent_inq',\n 'revol_util',\n 'num_op_rev_tl',\n 'home_ownership__ANY',\n 'home_ownership__MORTGAGE',\n 'home_ownership__NONE',\n 'home_ownership__OTHER',\n 'home_ownership__OWN',\n 'home_ownership__RENT',\n 'addr_state__AK',\n 'addr_state__AL',\n 'addr_state__AR',\n 'addr_state__AZ',\n 'addr_state__CA',\n 'addr_state__CO',\n 'addr_state__CT',\n 'addr_state__DC',\n 'addr_state__DE',\n 'addr_state__FL',\n 'addr_state__GA',\n 'addr_state__HI',\n 'addr_state__ID',\n 'addr_state__IL',\n 'addr_state__IN',\n 'addr_state__KS',\n 'addr_state__KY',\n 'addr_state__LA',\n 'addr_state__MA',\n 'addr_state__MD',\n 'addr_state__ME',\n 'addr_state__MI',\n 'addr_state__MN',\n 'addr_state__MO',\n 'addr_state__MS',\n 'addr_state__MT',\n 'addr_state__NC',\n 'addr_state__ND',\n 'addr_state__NE',\n 'addr_state__NH',\n 'addr_state__NJ',\n 'addr_state__NM',\n 'addr_state__NV',\n 'addr_state__NY',\n 'addr_state__OH',\n 'addr_state__OK',\n 'addr_state__OR',\n 'addr_state__PA',\n 'addr_state__RI',\n 'addr_state__SC',\n 'addr_state__SD',\n 'addr_state__TN',\n 'addr_state__TX',\n 'addr_state__UT',\n 'addr_state__VA',\n 'addr_state__VT',\n 'addr_state__WA',\n 'addr_state__WI',\n 'addr_state__WV',\n 'addr_state__WY']\n\n\n\n# split dataset into training and test set\n# 25% looks a bit big given the abundance of data\n# use a random state to make results replicable\ndf_train, df_test = train_test_split(loan, test_size=0.25, random_state=42)\n\n\n# we choose a classification model (logistic regression)\nclf = LogisticRegression()\n\nclf.fit(df_train[features], df_train['loan_status'])\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# make prediction on the test set\ny_pred = clf.predict(df_test[features])\n\n# compute the score of the fitted model on the test set\nprint(\n    'Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n        clf.score(df_test[features], df_test['loan_status'])\n    )\n)\n\nAccuracy of logistic regression classifier on test set: 0.79\n\n\n\n# compute the confusion matrix\ncal = sklearn.metrics.confusion_matrix(df_test['loan_status'], y_pred, labels=clf.classes_)\nprint(cal)\n\n[[165146   2918]\n [ 41439   2611]]\n\n\n\n#\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cal, display_labels=clf.classes_)\ndisp.plot()\n\n\n\n\n\n\n\n\nFor the confusion matrix that was just computed compute accuracy, precision, recall and f1 score (lookup the definitions if needed).\n\n# compute the different statistics (by hand or programmatically)\n\nrecall = cal[1,1] / (cal[1,0] + cal[1,1])\nrecall\n\n0.05927355278093076\n\n\nComment on the model validity.\nThe model fails to detect 95% of non-performing loans. It is very inefficient.\n\n\n\nWe use the News Sentiment Dataset from Kaggle.\n\nImport Dataset as a pandas dataframe. Remove rows where selected_text is not available.\n\n\n# the following command checks the current working directory \n# it should end with session_8\n%pwd\n\n'/home/pablo/Teaching/escp/dbe/tutorials/session_8'\n\n\n\nimport pandas\ndf = pandas.read_csv(\"Tweets.csv\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n\n\n\n\n\n\nDescribe Dataset (text and graphs). What is the distribution of the various sentiment values?\n\n\ndf['sentiment'].value_counts() / len(df['sentiment'])\n\nsentiment\nneutral     0.404570\npositive    0.312288\nnegative    0.283141\nName: count, dtype: float64\n\n\nThe sentiments are rather balanced (30% positive, 30% negative)\n\nimport seaborn as sns\n\n\nsns.histplot(df['sentiment'])\n\n\n\n\n\n\n\n\n\nCount the number of tweets mentioning trump.\n\n\n# count number of non-available values\nsum( df['selected_text'].isna() )\n\n1\n\n\n\n# remove na (~ is negation)\ndf = df[~ df['selected_text'].isna()]\n\n\nsum( df['selected_text'].str.count('trump') )\n\n2\n\n\n\nSplit Dataset into training, and test set.\n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, random_state=123)\n\n\ndf_train\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n24126\n52a34843de\nZeb has napped for 6 hours already today, and ...\nhe must be growing fast\npositive\n\n\n8513\nf1049a3c9c\ngood stuff, cant wait for the results\ngood stuff,\npositive\n\n\n10881\n2cc253edc6\nI have the best bestfriend in the whole world...\nbest\npositive\n\n\n10826\ne1a023ed94\nnot good you`re not comin close to where i lo...\nnot good you`re not comin close to where i lov...\nneutral\n\n\n7852\nd442dd8d79\nMaybe until Wednesday?\nMaybe until Wednesday?\nneutral\n\n\n...\n...\n...\n...\n...\n\n\n15378\ncfbec13b5d\n_dam haha, that would be cool. brianna and i s...\nhaha, that would be cool.\npositive\n\n\n21603\ndc582b9f85\nYAY!! that`s so cool aww that woulda been sw...\n! I`m just glad\npositive\n\n\n17731\n1d36d815e9\ni knoww she is da best!\na best\npositive\n\n\n15726\na0d6b5cd94\n**** the day flies when u got 3647 things to d...\n**** the day flies when u got 3647 things to d...\nnegative\n\n\n19967\n2277651477\n_cheryl Lucky, now I want to teleport\n_cheryl Lucky, now I want to teleport\nneutral\n\n\n\n\n20610 rows × 4 columns\n\n\n\n\n\n\nThe goal is now to to build a tweet classifier to predict a tweet sentiment, without any human input.\n\nExtract features from the training dataset. What do you do with non-words / punctuation?\n\n(hint: check the CountVectorizer function and the tutorial on sklearn webpage.)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train = count_vect.fit_transform(df_train['selected_text'])\n\n\nX_train\n\n&lt;20610x14807 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 129423 stored elements in Compressed Sparse Row format&gt;\n\n\n\nConvert occurrencies to frequencies. Make another version with tf-idf.\n\n\n# compute simple frequencies\n# from sklearn.feature_extraction.text import TfidfTransformer\n# tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n# X_train_tf = tf_transformer.transform(X_train)\n# X_train_tf.shape\n\n\n# weight counts by inverse frequencies\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=True).fit(X_train)\nX_train_tf = tf_transformer.transform(X_train)\nX_train_tf.shape\n\n(20610, 14807)\n\n\n\nChoose a classifier to predict the sentiment on the validation set. Compute the confusion matrix.\n\n\n# note that we are using `transform`, not `fit_transform` as we are not recomputing the f\nX_test = count_vect.transform(df_test['selected_text'])\nX_itf_test = tf_transformer.transform(X_test)\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tf,df_train['sentiment'])\n\n\nprediction = clf.predict(X_itf_test)\n\n\n# compare prediction to actual sentiment\n\n\nfrom sklearn.metrics import confusion_matrix\n\nmat = confusion_matrix(prediction, df_test['sentiment'])\nmat\n\narray([[1112,   50,   37],\n       [ 752, 2687,  533],\n       [  71,   48, 1580]])\n\n\n\naccuracy = (mat[0,0] + mat[1,1] +mat[2,2]) / mat.sum()\naccuracy\n\n0.7829694323144105"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis.html#lending-club-dataset",
    "href": "tutorials/session_8/sentiment_analysis.html#lending-club-dataset",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The following code processes the Lending Club Dataset from kaggle.\nRun and comment the following instructions (fix them if needed). Inspect he dataframe?\n\n# importing libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\nimport numpy as np\n\n/tmp/ipykernel_7230/3363083914.py:2: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\n# import dataset\nloan = pd.read_csv('loans.csv', low_memory=True)\n\n\nloan\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nsub_grade\nemp_length\nannual_inc\nloan_status\ndti\nmths_since_recent_inq\nrevol_util\nnum_op_rev_tl\n...\naddr_state__SD\naddr_state__TN\naddr_state__TX\naddr_state__UT\naddr_state__VA\naddr_state__VT\naddr_state__WA\naddr_state__WI\naddr_state__WV\naddr_state__WY\n\n\n\n\n0\n3600.0\n1.0\n24.0\n10.0\n55000.0\n0.0\n5.91\n4.0\n29.7\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20000.0\n2.0\n14.0\n10.0\n63000.0\n0.0\n10.78\n10.0\n56.2\n4.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n10400.0\n2.0\n51.0\n3.0\n104433.0\n0.0\n25.37\n1.0\n64.5\n7.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n20000.0\n1.0\n11.0\n10.0\n85000.0\n0.0\n17.61\n8.0\n5.7\n3.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n10000.0\n1.0\n2.0\n6.0\n85000.0\n0.0\n13.07\n1.0\n34.5\n13.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n848449\n22400.0\n2.0\n22.0\n10.0\n119000.0\n1.0\n23.22\n1.0\n51.5\n12.0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n848450\n19400.0\n2.0\n24.0\n0.0\n78000.0\n0.0\n13.02\n2.0\n63.8\n9.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n848451\n11200.0\n2.0\n22.0\n5.0\n86000.0\n1.0\n4.80\n10.0\n54.7\n7.0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n848452\n23800.0\n2.0\n24.0\n10.0\n119000.0\n0.0\n32.73\n0.0\n89.5\n8.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n848453\n24000.0\n2.0\n24.0\n6.0\n110000.0\n1.0\n18.30\n9.0\n68.1\n8.0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n848454 rows × 66 columns\n\n\n\nRun and comment the following instructions (fix them if needed). What kind of modeling exercise is performed?\n\nloan['loan_status'].value_counts()\n\nloan_status\n0.0    672377\n1.0    176077\nName: count, dtype: int64\n\n\nThe objective here is to build a model to predict which loans will default (loan_status=1). It is a classification exercise. Since the number of regressors is fairly large it is natural to look for a machine learning approcah.\n\n#  create a list of all regressors (excluding loan_status)\nfeatures = loan.columns.to_list()\nfeatures.remove('loan_status')\nfeatures\n\n['loan_amnt',\n 'term',\n 'sub_grade',\n 'emp_length',\n 'annual_inc',\n 'dti',\n 'mths_since_recent_inq',\n 'revol_util',\n 'num_op_rev_tl',\n 'home_ownership__ANY',\n 'home_ownership__MORTGAGE',\n 'home_ownership__NONE',\n 'home_ownership__OTHER',\n 'home_ownership__OWN',\n 'home_ownership__RENT',\n 'addr_state__AK',\n 'addr_state__AL',\n 'addr_state__AR',\n 'addr_state__AZ',\n 'addr_state__CA',\n 'addr_state__CO',\n 'addr_state__CT',\n 'addr_state__DC',\n 'addr_state__DE',\n 'addr_state__FL',\n 'addr_state__GA',\n 'addr_state__HI',\n 'addr_state__ID',\n 'addr_state__IL',\n 'addr_state__IN',\n 'addr_state__KS',\n 'addr_state__KY',\n 'addr_state__LA',\n 'addr_state__MA',\n 'addr_state__MD',\n 'addr_state__ME',\n 'addr_state__MI',\n 'addr_state__MN',\n 'addr_state__MO',\n 'addr_state__MS',\n 'addr_state__MT',\n 'addr_state__NC',\n 'addr_state__ND',\n 'addr_state__NE',\n 'addr_state__NH',\n 'addr_state__NJ',\n 'addr_state__NM',\n 'addr_state__NV',\n 'addr_state__NY',\n 'addr_state__OH',\n 'addr_state__OK',\n 'addr_state__OR',\n 'addr_state__PA',\n 'addr_state__RI',\n 'addr_state__SC',\n 'addr_state__SD',\n 'addr_state__TN',\n 'addr_state__TX',\n 'addr_state__UT',\n 'addr_state__VA',\n 'addr_state__VT',\n 'addr_state__WA',\n 'addr_state__WI',\n 'addr_state__WV',\n 'addr_state__WY']\n\n\n\n# split dataset into training and test set\n# 25% looks a bit big given the abundance of data\n# use a random state to make results replicable\ndf_train, df_test = train_test_split(loan, test_size=0.25, random_state=42)\n\n\n# we choose a classification model (logistic regression)\nclf = LogisticRegression()\n\nclf.fit(df_train[features], df_train['loan_status'])\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# make prediction on the test set\ny_pred = clf.predict(df_test[features])\n\n# compute the score of the fitted model on the test set\nprint(\n    'Accuracy of logistic regression classifier on test set: {:.2f}'.format(\n        clf.score(df_test[features], df_test['loan_status'])\n    )\n)\n\nAccuracy of logistic regression classifier on test set: 0.79\n\n\n\n# compute the confusion matrix\ncal = sklearn.metrics.confusion_matrix(df_test['loan_status'], y_pred, labels=clf.classes_)\nprint(cal)\n\n[[165146   2918]\n [ 41439   2611]]\n\n\n\n#\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cal, display_labels=clf.classes_)\ndisp.plot()\n\n\n\n\n\n\n\n\nFor the confusion matrix that was just computed compute accuracy, precision, recall and f1 score (lookup the definitions if needed).\n\n# compute the different statistics (by hand or programmatically)\n\nrecall = cal[1,1] / (cal[1,0] + cal[1,1])\nrecall\n\n0.05927355278093076\n\n\nComment on the model validity.\nThe model fails to detect 95% of non-performing loans. It is very inefficient."
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis.html#the-dataset",
    "href": "tutorials/session_8/sentiment_analysis.html#the-dataset",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "We use the News Sentiment Dataset from Kaggle.\n\nImport Dataset as a pandas dataframe. Remove rows where selected_text is not available.\n\n\n# the following command checks the current working directory \n# it should end with session_8\n%pwd\n\n'/home/pablo/Teaching/escp/dbe/tutorials/session_8'\n\n\n\nimport pandas\ndf = pandas.read_csv(\"Tweets.csv\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n0\ncb774db0d1\nI`d have responded, if I were going\nI`d have responded, if I were going\nneutral\n\n\n1\n549e992a42\nSooo SAD I will 🦈 miss you here in San Diego!!!\nSooo SAD\nnegative\n\n\n2\n088c60f138\nmy boss is bullying me...\nbullying me\nnegative\n\n\n3\n9642c003ef\nwhat interview! leave me alone\nleave me alone\nnegative\n\n\n4\n358bd9e861\nSons of ****, why couldn`t they put them on t...\nSons of ****,\nnegative\n\n\n\n\n\n\n\n\nDescribe Dataset (text and graphs). What is the distribution of the various sentiment values?\n\n\ndf['sentiment'].value_counts() / len(df['sentiment'])\n\nsentiment\nneutral     0.404570\npositive    0.312288\nnegative    0.283141\nName: count, dtype: float64\n\n\nThe sentiments are rather balanced (30% positive, 30% negative)\n\nimport seaborn as sns\n\n\nsns.histplot(df['sentiment'])\n\n\n\n\n\n\n\n\n\nCount the number of tweets mentioning trump.\n\n\n# count number of non-available values\nsum( df['selected_text'].isna() )\n\n1\n\n\n\n# remove na (~ is negation)\ndf = df[~ df['selected_text'].isna()]\n\n\nsum( df['selected_text'].str.count('trump') )\n\n2\n\n\n\nSplit Dataset into training, and test set.\n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, random_state=123)\n\n\ndf_train\n\n\n\n\n\n\n\n\ntextID\ntext\nselected_text\nsentiment\n\n\n\n\n24126\n52a34843de\nZeb has napped for 6 hours already today, and ...\nhe must be growing fast\npositive\n\n\n8513\nf1049a3c9c\ngood stuff, cant wait for the results\ngood stuff,\npositive\n\n\n10881\n2cc253edc6\nI have the best bestfriend in the whole world...\nbest\npositive\n\n\n10826\ne1a023ed94\nnot good you`re not comin close to where i lo...\nnot good you`re not comin close to where i lov...\nneutral\n\n\n7852\nd442dd8d79\nMaybe until Wednesday?\nMaybe until Wednesday?\nneutral\n\n\n...\n...\n...\n...\n...\n\n\n15378\ncfbec13b5d\n_dam haha, that would be cool. brianna and i s...\nhaha, that would be cool.\npositive\n\n\n21603\ndc582b9f85\nYAY!! that`s so cool aww that woulda been sw...\n! I`m just glad\npositive\n\n\n17731\n1d36d815e9\ni knoww she is da best!\na best\npositive\n\n\n15726\na0d6b5cd94\n**** the day flies when u got 3647 things to d...\n**** the day flies when u got 3647 things to d...\nnegative\n\n\n19967\n2277651477\n_cheryl Lucky, now I want to teleport\n_cheryl Lucky, now I want to teleport\nneutral\n\n\n\n\n20610 rows × 4 columns"
  },
  {
    "objectID": "tutorials/session_8/sentiment_analysis.html#classifying-tweets",
    "href": "tutorials/session_8/sentiment_analysis.html#classifying-tweets",
    "title": "Confusion Matrix and Sentiment Analysis",
    "section": "",
    "text": "The goal is now to to build a tweet classifier to predict a tweet sentiment, without any human input.\n\nExtract features from the training dataset. What do you do with non-words / punctuation?\n\n(hint: check the CountVectorizer function and the tutorial on sklearn webpage.)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train = count_vect.fit_transform(df_train['selected_text'])\n\n\nX_train\n\n&lt;20610x14807 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 129423 stored elements in Compressed Sparse Row format&gt;\n\n\n\nConvert occurrencies to frequencies. Make another version with tf-idf.\n\n\n# compute simple frequencies\n# from sklearn.feature_extraction.text import TfidfTransformer\n# tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n# X_train_tf = tf_transformer.transform(X_train)\n# X_train_tf.shape\n\n\n# weight counts by inverse frequencies\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=True).fit(X_train)\nX_train_tf = tf_transformer.transform(X_train)\nX_train_tf.shape\n\n(20610, 14807)\n\n\n\nChoose a classifier to predict the sentiment on the validation set. Compute the confusion matrix.\n\n\n# note that we are using `transform`, not `fit_transform` as we are not recomputing the f\nX_test = count_vect.transform(df_test['selected_text'])\nX_itf_test = tf_transformer.transform(X_test)\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tf,df_train['sentiment'])\n\n\nprediction = clf.predict(X_itf_test)\n\n\n# compare prediction to actual sentiment\n\n\nfrom sklearn.metrics import confusion_matrix\n\nmat = confusion_matrix(prediction, df_test['sentiment'])\nmat\n\narray([[1112,   50,   37],\n       [ 752, 2687,  533],\n       [  71,   48, 1580]])\n\n\n\naccuracy = (mat[0,0] + mat[1,1] +mat[2,2]) / mat.sum()\naccuracy\n\n0.7829694323144105"
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html",
    "href": "tutorials/session_3/Phillips_curve_correction.html",
    "title": "Visualizing the Philips Curve",
    "section": "",
    "text": "The Philips Curve was initially discovered as a statistical relationship between unemployment and inflation. The original version used historical US data.\nOur goal here is to visually inspect the Philips curve using recent data, for several countries.\nIn the process we will learn to: - import dataframes, inspect them, merge them, clean the resulting data - use matplotlib to create graphs - bonus: experiment with other plotting libraries",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html#importing-the-data",
    "href": "tutorials/session_3/Phillips_curve_correction.html#importing-the-data",
    "title": "Visualizing the Philips Curve",
    "section": "Importing the Data",
    "text": "Importing the Data\nWe start by loading library dbnomics. It is installed on the Nuvolos servers.\n\nimport dbnomics\n\nThe following code imports data for from dbnomics for a few countries.\n\ntable_1 = dbnomics.fetch_series([\n    \"OECD/DP_LIVE/FRA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/GBR.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/USA.CPI.TOT.AGRWTH.Q\",\n    \"OECD/DP_LIVE/DEU.CPI.TOT.AGRWTH.Q\"\n])\n\n\ntable_2 = dbnomics.fetch_series([\n    \"OECD/MEI/DEU.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/FRA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/USA.LRUNTTTT.STSA.Q\",\n    \"OECD/MEI/GBR.LRUNTTTT.STSA.Q\"\n])\n\nDescribe concisely the data that has been imported (periodicity, type of measure, …). You can either check dbnomics website or look at the databases.\n\ntable_1:\n\ngrowth of CPI index. The total of all goods. Quarterly data for France, UK, USA, Germany\n\ntable_2\n\nunemployment rate coming from LO\n\n\nShow the first rows of each database. Make a list of all columns.\n\n# first table\ntable_1.head(3)\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nLOCATION\nINDICATOR\nSUBJECT\nMEASURE\nFREQUENCY\nCountry\nIndicator\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q1\n1956-01-01\n1.746324\n1.746324\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n1\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q2\n1956-04-01\n1.838658\n1.838658\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n2\nquarterly\nOECD\nDP_LIVE\nOECD Data Live dataset\nFRA.CPI.TOT.AGRWTH.Q\nFrance – Inflation (CPI) – Total – Annual grow...\n1956-Q3\n1956-07-01\n2.670692\n2.670692\nFRA\nCPI\nTOT\nAGRWTH\nQ\nFrance\nInflation (CPI)\nTotal\nAnnual growth rate (%)\nQuarterly\n\n\n\n\n\n\n\n\n# to get the list of columns:\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\n# second table\ntable_2.head(3)\n\n\n\n\n\n\n\n\n@frequency\nprovider_code\ndataset_code\ndataset_name\nseries_code\nseries_name\noriginal_period\nperiod\noriginal_value\nvalue\nLOCATION\nSUBJECT\nMEASURE\nFREQUENCY\nCountry\nSubject\nMeasure\nFrequency\n\n\n\n\n0\nquarterly\nOECD\nMEI\nMain Economic Indicators Publication\nDEU.LRUNTTTT.STSA.Q\nGermany – Labour Force Survey - quarterly rate...\n1962-Q1\n1962-01-01\n0.442249\n0.442249\nDEU\nLRUNTTTT\nSTSA\nQ\nGermany\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n1\nquarterly\nOECD\nMEI\nMain Economic Indicators Publication\nDEU.LRUNTTTT.STSA.Q\nGermany – Labour Force Survey - quarterly rate...\n1962-Q2\n1962-04-01\n0.444882\n0.444882\nDEU\nLRUNTTTT\nSTSA\nQ\nGermany\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n2\nquarterly\nOECD\nMEI\nMain Economic Indicators Publication\nDEU.LRUNTTTT.STSA.Q\nGermany – Labour Force Survey - quarterly rate...\n1962-Q3\n1962-07-01\n0.450347\n0.450347\nDEU\nLRUNTTTT\nSTSA\nQ\nGermany\nLabour Force Survey - quarterly rates &gt; Unempl...\nLevel, rate or national currency, s.a.\nQuarterly\n\n\n\n\n\n\n\n\n# to get the list of columns:\ntable_2.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'SUBJECT', 'MEASURE',\n       'FREQUENCY', 'Country', 'Subject', 'Measure', 'Frequency'],\n      dtype='object')\n\n\nCompute averages and standard deviations for unemployment and inflation, per country.\n\n# reminder: to get averages for a whole table\n# we extract the relevant column with table_1['value']\nprint(\"Mean Inflation: \", table_1['value'].mean())\nprint(\"Mean Unemployment: \", table_2['value'].mean())\n\nMean Inflation:  3.8910539919538323\nMean Unemployment:  6.098550447670419\n\n\n\n# reminder: to get averages for a whole s\nprint(\"Std. Deviation Inflation: \", table_1['value'].std())\nprint(\"Std. Deviation Unemployment: \", table_2['value'].std())\n\nStd. Deviation Inflation:  3.5916466190299996\nStd. Deviation Unemployment:  2.550612600349251\n\n\nThere are two strategies to compute statistics per group.\n\n# option 1: by using pandas boolean selection \n\nThe first approach consists in keeping only the observations for a given country:\n\ntable_1['Country'].unique()\n\narray(['France', 'United Kingdom', 'United States', 'Germany'],\n      dtype=object)\n\n\n\ncountry = \"France\"\nis_france = (table_1['Country']==country)\ntable_1_france = table_1[is_france]\ntable_1_france['Country'].value_counts() # check \n\nCountry\nFrance    271\nName: count, dtype: int64\n\n\n\n# the preceding code can be replicated more concisely:\ntable_1_france = table_1[table_1['Country']==\"France\"]\n\n\n# or using a database query:\ntable_1_france = table_1.query(\"Country=='France'\")\n\n\n# we can now compute the statistics:\nprint(\"Mean Inflation: \", table_1_france['value'].mean())\nprint(\"Std. Deviation Inflation: \", table_1_france['value'].std())\n\nMean Inflation:  4.218004559985239\nStd. Deviation Inflation:  3.85319032016491\n\n\n\n# Here is a code to do it for all countries:\n\nfor country in table_1['Country'].unique():\n    print(f'Statistics for: {country}')\n\n    # inflation\n    table_1_country = table_1[table_1['Country']==country]\n    print(\"* Mean Inflation: \", table_1_country['value'].mean())\n    print(\"* Std. Deviation Inflation: \", table_1_country['value'].std())\n\n    # unemployment\n    table_2_country = table_2[table_2['Country']==country]\n    print(\"* Mean Inflation: \", table_2_country['value'].mean())\n    print(\"* Std. Deviation Inflation: \", table_2_country['value'].std())\n    print() # add empty line\n\nStatistics for: France\n* Mean Inflation:  4.218004559985239\n* Std. Deviation Inflation:  3.85319032016491\n* Mean Inflation:  8.680560014887725\n* Std. Deviation Inflation:  0.9840354763739427\n\nStatistics for: United Kingdom\n* Mean Inflation:  5.003996036162361\n* Std. Deviation Inflation:  4.768592756710978\n* Mean Inflation:  6.70511374527728\n* Std. Deviation Inflation:  2.388539226072565\n\nStatistics for: United States\n* Mean Inflation:  3.678550917822878\n* Std. Deviation Inflation:  2.7795046092217035\n* Mean Inflation:  5.851086956521739\n* Std. Deviation Inflation:  1.667886470619543\n\nStatistics for: Germany\n* Mean Inflation:  2.659118566666667\n* Std. Deviation Inflation:  1.8661314171203747\n* Mean Inflation:  4.989272203471847\n* Std. Deviation Inflation:  3.0671989448416817\n\n\n\n\n# option 2: by using groupby\n\nMany statistics can be computed by group.\n\nprint(\"Mean Inflation\")\ntable_1.groupby('Country')['value'].agg('mean')\n\nMean Inflation\n\n\nCountry\nFrance            4.218005\nGermany           2.659119\nUnited Kingdom    5.003996\nUnited States     3.678551\nName: value, dtype: float64\n\n\n\ntable_1.columns\n\nIndex(['@frequency', 'provider_code', 'dataset_code', 'dataset_name',\n       'series_code', 'series_name', 'original_period', 'period',\n       'original_value', 'value', 'LOCATION', 'INDICATOR', 'SUBJECT',\n       'MEASURE', 'FREQUENCY', 'Country', 'Indicator', 'Subject', 'Measure',\n       'Frequency'],\n      dtype='object')\n\n\n\nprint(\"Std Inflation\")\ntable_1.groupby('Country')['value'].agg('std')\n\nStd Inflation\n\n\nCountry\nFrance            3.853190\nGermany           1.866131\nUnited Kingdom    4.768593\nUnited States     2.779505\nName: value, dtype: float64\n\n\n\n# This can be done for several stats at the same time:\n\nprint(\"Statistics for Inflation\")\ntable_1.groupby('Country')['value'].agg(['mean','std'])\n\nStatistics for Inflation\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nCountry\n\n\n\n\n\n\nFrance\n4.218005\n3.853190\n\n\nGermany\n2.659119\n1.866131\n\n\nUnited Kingdom\n5.003996\n4.768593\n\n\nUnited States\n3.678551\n2.779505\n\n\n\n\n\n\n\n\n# Same stats for Unemployment.\n\nprint(\"Statistics for Unemployment\")\ntable_2.groupby('Country')['value'].agg(['mean','std'])\n\nStatistics for Unemployment\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nCountry\n\n\n\n\n\n\nFrance\n8.680560\n0.984035\n\n\nGermany\n4.989272\n3.067199\n\n\nUnited Kingdom\n6.705114\n2.388539\n\n\nUnited States\n5.851087\n1.667886\n\n\n\n\n\n\n\nComment: we observe differences in the level of structural unemployment. Volatility in Germany is rather impressive.\nThe following command merges the two databases together. Explain the role of argument on. What happened to the column names?\n\ntable = table_1.merge(table_2, on=[\"period\", 'Country']) \n\nThe on argument indicate, which columns identify a unique observation. Here the date and the country denote the same observation in both countries. However the column value of the first and the second database have a different meaning (resp unemployment and inflation). To distinguish them, they receive a suffix (_x and _y respectively).\nWe rename the new names for the sake of clarity and normalize everything with lower cases.\n\ntable = table.rename(columns={\n    'period': 'date',         # because it sounds more natural\n    'Country': 'country',\n    'value_x': 'inflation',\n    'value_y': 'unemployment'\n})\n\nOn the merged table, compute at once all the statistics computed before (use groupby and agg).\n\ntable[['country','inflation', 'unemployment']].groupby('country').agg( [ 'mean', 'std'])\n\n\n\n\n\n\n\n\ninflation\nunemployment\n\n\n\nmean\nstd\nmean\nstd\n\n\ncountry\n\n\n\n\n\n\n\n\nFrance\n1.664349\n1.381470\n8.680560\n0.984035\n\n\nGermany\n2.730136\n1.918303\n4.989272\n3.067199\n\n\nUnited Kingdom\n5.404707\n5.253143\n6.705114\n2.388539\n\n\nUnited States\n3.678551\n2.779505\n5.880812\n1.668049\n\n\n\n\n\n\n\nBefore we process further, we should tidy the dataframe by keeping only what we need. - Keep only the columns date, country, inflation and unemployment - Drop all na values - Make a copy of the result\n\ndf = table[['date', 'country', 'inflation', 'unemployment']].dropna()\n\n\ndf = df.copy()\n# note: the copy() function is here to avoid keeping references to the original database\n\n\nprint(df.shape)\n\n(811, 4)\n\n\nWhat is the maximum availability interval for each country? How would you proceed to keep only those dates where all datas are available? In the following we keep the f\n\n# optional: here is a quick way to visualize availability dates\nimport altair as alt\nalt.Chart(df).mark_point().encode(x='date', y='country', color='country')\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\n\ncdf = df.dropna() # remove all lines with non available values\n\n\ncdf.shape # compare with the cell above: there was no line with na.\n\n(811, 4)\n\n\n\n# the following computes all dates where data is available in all 4 countries\nall_available = cdf.groupby(\"date\")['inflation'].count() == 4\nall_available\n\ndate\n1956-01-01    False\n1956-04-01    False\n1956-07-01    False\n1956-10-01    False\n1957-01-01    False\n              ...  \n2022-07-01     True\n2022-10-01     True\n2023-01-01     True\n2023-04-01     True\n2023-07-01     True\nName: inflation, Length: 271, dtype: bool\n\n\n\n# keep only dates\ncommon_dates = all_available[all_available].index\ncommon_dates\n\nDatetimeIndex(['2003-01-01', '2003-04-01', '2003-07-01', '2003-10-01',\n               '2004-01-01', '2004-04-01', '2004-07-01', '2004-10-01',\n               '2005-01-01', '2005-04-01', '2005-07-01', '2005-10-01',\n               '2006-01-01', '2006-04-01', '2006-07-01', '2006-10-01',\n               '2007-01-01', '2007-04-01', '2007-07-01', '2007-10-01',\n               '2008-01-01', '2008-04-01', '2008-07-01', '2008-10-01',\n               '2009-01-01', '2009-04-01', '2009-07-01', '2009-10-01',\n               '2010-01-01', '2010-04-01', '2010-07-01', '2010-10-01',\n               '2011-01-01', '2011-04-01', '2011-07-01', '2011-10-01',\n               '2012-01-01', '2012-04-01', '2012-07-01', '2012-10-01',\n               '2013-01-01', '2013-04-01', '2013-07-01', '2013-10-01',\n               '2014-01-01', '2014-04-01', '2014-07-01', '2014-10-01',\n               '2015-01-01', '2015-04-01', '2015-07-01', '2015-10-01',\n               '2016-01-01', '2016-04-01', '2016-07-01', '2016-10-01',\n               '2017-01-01', '2017-04-01', '2017-07-01', '2017-10-01',\n               '2018-01-01', '2018-04-01', '2018-07-01', '2018-10-01',\n               '2019-01-01', '2019-04-01', '2019-07-01', '2019-10-01',\n               '2020-01-01', '2020-04-01', '2020-07-01', '2020-10-01',\n               '2021-01-01', '2021-04-01', '2021-07-01', '2021-10-01',\n               '2022-01-01', '2022-04-01', '2022-07-01', '2022-10-01',\n               '2023-01-01', '2023-04-01', '2023-07-01'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\n\n# This can be done for several stats at the same time:\ncdf[ cdf['date'].isin(common_dates) ] \n\n\n\n\n\n\n\n\ndate\ncountry\ninflation\nunemployment\n\n\n\n\n0\n2003-01-01\nFrance\n2.366263\n7.922234\n\n\n1\n2003-04-01\nFrance\n1.912854\n8.089598\n\n\n2\n2003-07-01\nFrance\n1.932270\n8.036090\n\n\n3\n2003-10-01\nFrance\n2.184437\n8.349410\n\n\n4\n2004-01-01\nFrance\n1.800087\n8.518631\n\n\n...\n...\n...\n...\n...\n\n\n807\n2022-07-01\nGermany\n7.402639\n3.181081\n\n\n808\n2022-10-01\nGermany\n8.580543\n3.059473\n\n\n809\n2023-01-01\nGermany\n8.236768\n2.961556\n\n\n810\n2023-04-01\nGermany\n6.546894\n2.963810\n\n\n811\n2023-07-01\nGermany\n5.603836\n2.970260\n\n\n\n\n332 rows × 4 columns\n\n\n\nOur DataFrame is now ready for further analysis !\n\n# note: \n# the following code also works when dataframe is not cylindric\n# df = cdf # uncomment if you want to work on cylindric data",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html#plotting-using-matplotlib",
    "href": "tutorials/session_3/Phillips_curve_correction.html#plotting-using-matplotlib",
    "title": "Visualizing the Philips Curve",
    "section": "Plotting using matplotlib",
    "text": "Plotting using matplotlib\nOur goal now consists in plotting inflation against unemployment to see whether a pattern emerges. We will first work on France.\n\nfrom matplotlib import pyplot as plt\n\nCreate a database df_fr which contains only the data for France.\n\ndf_fr = df[ df['country'] == 'France' ].copy() # again, we copy the result, because we plan to modify it\n\nThe following command create a line plot for inflation against unemployment. Can you transform it into a scatterplot ?\n\nplt.plot(df_fr['unemployment'], df_fr['inflation'], 'o')\n\n\n\n\n\n\n\n\nExpand the above command to make the plot nicer (label, title, grid, …)\n\nplt.plot(df_fr['unemployment'], df_fr['inflation'],'.')\nplt.grid()\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation (%)\")\nplt.title(\"Phillips Curve\")\n\nText(0.5, 1.0, 'Phillips Curve')",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html#visualizing-the-regression",
    "href": "tutorials/session_3/Phillips_curve_correction.html#visualizing-the-regression",
    "title": "Visualizing the Philips Curve",
    "section": "Visualizing the regression",
    "text": "Visualizing the regression\nThe following piece of code regresses inflation on unemployment.\n\nfrom statsmodels.formula import api as sm\nmodel = sm.ols(formula='inflation ~ unemployment', data=df_fr)\nresult = model.fit()\n\nWe can use the resulting model to “predict” inflation from unemployment.\n\nresult.predict(df_fr['unemployment'])\n\n0     2.366810\n1     2.211777\n2     2.261342\n3     1.971104\n4     1.814349\n        ...   \n78    3.064908\n79    3.055976\n80    3.141252\n81    2.926298\n82    2.888369\nLength: 83, dtype: float64\n\n\nStore the result in df_fr as reg_unemployment\n\ndf_fr['pred_inflation'] = result.predict(df_fr['unemployment'])\n\nBy expanding again, the command above to make a plot, add the regression line to the scatter plot.\n\nplt.plot(df_fr['unemployment'], df_fr['inflation'],'o', alpha=0.5)\nplt.plot(df_fr['unemployment'], df_fr['pred_inflation'], color='C0')\nplt.title(\"Philips Curve\")\nplt.xlabel(\"Unemployment (%)\")\nplt.ylabel(\"Inflation\")\nplt.grid()\n\n\n\n\n\n\n\n\nNow we would like to compare all countries. Can you find a way to represent the data for all of them (all on one graph, using subplots…) ?\n\n# this solution uses loops and iterators but the same can be done manually\n\n\ncountries = df['country'].unique()\n\n\ndf_countries = []\nfor c in countries:\n    \n    tmp = df[df['country']==c].copy()\n    \n    model = sm.ols(formula='inflation ~ unemployment', data=tmp)\n    result = model.fit()\n    \n    tmp['pred_inflation'] = result.predict(tmp['unemployment'])\n\n    df_countries.append(tmp)\n\n\n# all on one graph\nfor i, (d, c) in enumerate(zip(df_countries, countries)):\n    plt.plot(d['unemployment'], d['inflation'], 'o', alpha=0.2, label=c, color=f\"C{i}\") # missing 'o'\n    plt.plot(d['unemployment'], d['pred_inflation'],   color=f\"C{i}\") # missing 'o'\nplt.legend(loc='upper right')\n\n\n\n\n\n\n\n\n\n# using subplots\n\n\n# all on one graph\nfig = plt.subplots(2,2)\nfor i, (d, c) in enumerate(zip(df_countries, countries)):\n    plt.subplot(2,2,i+1)\n    plt.plot(d['unemployment'], d['inflation'], 'o', alpha=0.2, color=f\"C{i}\") # missing 'o'\n    plt.plot(d['unemployment'], d['pred_inflation'],   color=f\"C{i}\") # missing 'o'\n    plt.xlabel(\"Unemployment\")\n    plt.ylabel(\"Inflation\")\n    plt.title(c)\n    plt.tight_layout()\n\n\n\n\n\n\n\n\nAny comment on these results?\n\nFirst it would be nice to break the period into many subperiods. We know the story about the disappearnce of the Philips curve in the US (and the Lucas Critique).\nSecond, there seems to be a strong contrast between US/UK and Germany/France. If one remembers the mechanisms behind the Philips curve it is not surprising: prices and wages are more rigid in Europe.\n\nIn the case of Germany, the conclusion is certainly not too strong: there has been a lot of volatility.\nIn the case of France, the time span of the time series is much smaller which also weakens the conclusion.",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html#bonus-visualizing-data-using-altair",
    "href": "tutorials/session_3/Phillips_curve_correction.html#bonus-visualizing-data-using-altair",
    "title": "Visualizing the Philips Curve",
    "section": "Bonus: Visualizing data using altair",
    "text": "Bonus: Visualizing data using altair\nAltair is a visualization library (based on Vega-lite) which offers a different syntax to make plots.\nIt is well adapted to the exploration phase, as it can operate on a full database (without splitting it like we did for matplotlib). It also provides some data transformation tools like regressions, and ways to add some interactivity.\n\nimport altair as alt\n\nThe following command makes a basic plot from the dataframe df which contains all the countries. Can you enhance it by providing a title and encoding information to distinguish the various countries?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n).interactive()\nchart\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\n\n# solution: \nchart = alt.Chart(df, title=\"Data for all countries\").mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=\"country\",\n)\nchart\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\nThe following graph plots a regression line, but for all countries, it is rather meaningless. Can you restrict the data to France only?\n\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\n\n# solution (it is also possible to replace df by df_fr...)\nchart = alt.Chart(df).transform_filter('datum.country==\"France\"').mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n)\nchart + chart.transform_regression('unemployment', 'inflation').mark_line()\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\nOne way to visualize data consists in adding some interactivity. Add some title and click on the legend\n\nmulti = alt.selection_multi(fields=[\"country\"])\n\nlegend = alt.Chart(df).mark_point().encode(\n    y=alt.Y('country:N', axis=alt.Axis(orient='right')),\n    color=alt.condition(multi, 'country:N', alt.value('lightgray'), legend=None)\n).add_selection(multi)\n\nchart_2 = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    color=alt.condition(multi, 'country:N', alt.value('lightgray')),\n    # find a way to separate on the graph data from France and US\n)\n\n# Try to click on the legend\nchart_2 | legend\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'selection_multi' is deprecated.  Use 'selection_point'\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n/opt/conda/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)\n\n\n\n\n\n\n\n\nBonus question: in the following graph you can select an interval in the left panel to select some subsample. Can you add the regression line(s) corresponding to the selected data to the last graph?\n\nbrush = alt.selection_interval(encodings=['x'],)\n\nhistorical_chart_1 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='unemployment',\n    color='country'\n).add_selection(\n    brush\n)\nhistorical_chart_2 = alt.Chart(df).mark_line().encode(\n    x='date',\n    y='inflation',\n    color='country'\n)\nchart = alt.Chart(df).mark_point().encode(\n    x='unemployment',\n    y='inflation',\n    # find a way to separate on the graph data from France and US\n    color=alt.condition(brush, 'country:N', alt.value('lightgray'))\n)\nalt.hconcat(historical_chart_1, historical_chart_2, chart,)\n\n/opt/conda/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning: 'add_selection' is deprecated. Use 'add_params' instead.\n  warnings.warn(message, AltairDeprecationWarning, stacklevel=1)\n/opt/conda/lib/python3.10/site-packages/altair/utils/core.py:395: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n  col = df[col_name].apply(to_list_if_array, convert_dtype=False)",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_3/Phillips_curve_correction.html#bonus-2-plotly-express",
    "href": "tutorials/session_3/Phillips_curve_correction.html#bonus-2-plotly-express",
    "title": "Visualizing the Philips Curve",
    "section": "Bonus 2: Plotly Express",
    "text": "Bonus 2: Plotly Express\nAnother popular option is the plotly library for nice-looking interactive plots. Combined with dash or shiny, it can be used to build very powerful interactive interfaces.\n\nimport plotly.express as px\n\n\nfig = px.scatter(df, x='unemployment', y='inflation', color='country', title=\"Philips Curves\")\nfig\n\n/opt/conda/lib/python3.10/site-packages/plotly/express/_core.py:1979: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])",
    "crumbs": [
      "tutorials",
      "Visualizing the Philips Curve"
    ]
  },
  {
    "objectID": "tutorials/session_7/Classification_correction.html",
    "href": "tutorials/session_7/Classification_correction.html",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\nUpdate: the test.csv file now contains the score that should be predicted.\nImport training set and validation sets\nWe follow a three sets approach and define the following sets: - data used for the developing the model (dataset from train.csv) will be split into: - training dataset (variable called train) - test data set (variable called tes - data use\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\nDescribe the dataset.\nHere is a list of columns\n\ndataset.columns\n\nIndex(['Unnamed: 0', 'ID', 'Customer_ID', 'Month', 'Name', 'Age', 'SSN',\n       'Occupation', 'Annual_Income', 'Monthly_Inhand_Salary',\n       'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan',\n       'Type_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment',\n       'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix',\n       'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\nHow is the credit category encoded? Create a new variable representing it with values 0,1,2.\nThe credit score can be found in column Credit_Score. It takes three values.\n\ndataset['Credit_Score'].unique()\n\narray(['Standard', 'Poor', 'Good'], dtype=object)\n\n\n\n# we can use the LabelEncoder function from sklearn\nfrom sklearn.preprocessing import LabelEncoder\ncle = LabelEncoder()\ndataset['Credit_Score'] = cle.fit_transform(dataset['Credit_Score'])\n\n\n\n# check which value corresponds to what\n# this is needed to interpret 0,1,2 below\nscore_categories = cle.inverse_transform([0,1,2])\nscore_categories\n\narray(['Good', 'Poor', 'Standard'], dtype=object)\n\n\nReencode all categorical variables as dummy variables. Remove variables that are not useful for the analysis.\nLet’s reencode all string data as ordinal values.\n\nfrom sklearn.preprocessing import LabelEncoder as le\n\ndataset['Payment_of_Min_Amount'] = le().fit_transform(dataset['Payment_of_Min_Amount'])\ndataset['Payment_Behaviour'] = le().fit_transform(dataset['Payment_Behaviour'])\ndataset['Occupation'] = le().fit_transform(dataset['Occupation'])\ndataset['Type_of_Loan'] = le().fit_transform(dataset['Type_of_Loan'])\ndataset['Credit_Mix'] = le().fit_transform(dataset['Credit_Mix'])\n\n\n\n# we won't use the name variable so that we drop it\ndataset.drop(columns=[\"Name\"], inplace=True)\n\nKeyError: \"['Name'] not found in axis\"\n\n\nMake several plots about the dataset (histograms, correlation plots, …)\nTip: there are cool ideas here\n\nimport seaborn as sns\n\n\nsns.histplot(dataset['Credit_Score'])\n\n\n\n\n\n\n\n\n\n# Let's plot the correlations\nfrom matplotlib import pyplot as plt\nplt.figure(figsize = (14,10))\nsns.heatmap(dataset.corr())\n\n\n\n\n\n\n\n\nWe see from the correlation plot that, as expected, ID, customer_ID, SSN and are not related to the other variables. More surprisingly this is also also the case of “occupation”. This simply means that the numerical variable resulting from the conversion of occupations to numbers, was not ordered. It would be useless in a linear regression, but we can try a nonlinear one.\nA better way to encode the categorical variables would be to create dummy variables for each value.\nSplit the train dataset into a df_train and a df_test dataset.\n\nimport sklearn.model_selection\ndf_train, df_test = sklearn.model_selection.train_test_split(dataset , test_size= 0.25, random_state=243 )\n\nImplement a logistic regression.\n\ndf_train.columns\n\nIndex(['Unnamed: 0', 'ID', 'Customer_ID', 'Month', 'Age', 'SSN', 'Occupation',\n       'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n       'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan',\n       'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n       'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n       'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\n\n# we keep only a subset of all variables:\nvariables = [\n    # 'Credit_Score',\n    'Changed_Credit_Limit',\n    'Payment_of_Min_Amount',\n    'Credit_Mix',\n    'Delay_from_due_date',\n    'Annual_Income',\n    'Monthly_Inhand_Salary',\n    'Age',\n    'Monthly_Balance',\n    'Num_of_Delayed_Payment',\n    'Outstanding_Debt',\n    'Payment_Behaviour',\n    'Credit_History_Age',\n    'Num_Bank_Accounts',\n    'Credit_Utilization_Ratio'\n]\n\n\ndf_test['Credit_Score']\n\n70144    0\n32059    1\n25287    2\n13496    1\n65970    2\n        ..\n59793    0\n33338    2\n51142    0\n29286    0\n58864    2\nName: Credit_Score, Length: 18750, dtype: int64\n\n\n\n# we split the set into features and labels:\nX = df_train.drop(columns=['Credit_Score'])[variables] # all columns except score\nY = df_train['Credit_Score']\n\n# same for the test set:\nX_test = df_test.drop(columns=['Credit_Score'])[variables] # all columns except score\nY_test = df_test['Credit_Score']\n\nWe need a logistic classifier, since we are predicting binary variables\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=1000)\nregression = model.fit(X,Y)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nlr_score_test=model.score(X , Y)\nlr_score_test\n\n0.5916444444444444\n\n\nCompute the confusion matrix using the test set. Comment\nThis is how our model performs on the test set.\n\n# adjust the following code if needed\n\n\nactual = Y_test\npredicted = model.predict(X_test)\n\nFor classification problems a standard way to represent the performance, consists in computing the “confusion matrix”.\n\nfrom sklearn import metrics\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\nconfusion_matrix\n\narray([[ 862,   39, 2462],\n       [ 166, 2299, 3012],\n       [ 703, 1259, 7948]])\n\n\n\ncm_display_0 = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display_0.plot(ax=ax)\n\n\n\n\n\n\n\n\nWe can also normalize by the number of observations for each category (divide each row by the total of this row).\n\nconfusion_matrix_percents = confusion_matrix / confusion_matrix.sum(axis=1)[:,None] *100\nconfusion_matrix_percents # check that each row sums to 1\n\narray([[25.6318763 ,  1.15967886, 73.20844484],\n       [ 3.03085631, 41.97553405, 54.99360964],\n       [ 7.0938446 , 12.70433905, 80.20181635]])\n\n\nThis tells us that 33% of the poor ratings were correctly detected (true positive). Or, equivalently, that 67% of them were not detected (false positive).\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n\n\n\n\n\n\n\nSymmetrically, we can normalize by the number of predicted labels, i.e. the sum of each column.\n\nconfusion_matrix_percents = confusion_matrix / confusion_matrix.sum(axis=0)[None,:] *100\nconfusion_matrix_percents # check that *columns* sum to 1\n\narray([[49.79780474,  1.08423686, 18.34301892],\n       [ 9.58983247, 63.91437309, 22.44076889],\n       [40.6123628 , 35.00139005, 59.21621219]])\n\n\nAs before we can make a graphical representation:\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n\n\n\n\n\n\n\nWe see now that of all the poor ratings that are detected 54% were actually poor. Hence 46% of them were incorrect.\nPerform the same analysis with other classification methods and compare their performance using the test set.\n\n# Clearly we need to use some nonlinear model instead\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nregression = model.fit(X,Y)\n\n\nactual = Y_test\npredicted = model.predict(X_test)\n\n\nfrom sklearn import metrics\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\nconfusion_matrix\n\narray([[2022,  113, 1228],\n       [ 189, 4152, 1136],\n       [1077, 1395, 7438]])\n\n\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nax = plt.subplot(1,2,1)\nax.grid(False)\ncm_display_0.plot(ax=ax)\nplt.title(\"Logistic Regression\")\nax = plt.subplot(1,2,2)\nax.grid(False)\ncm_display.plot(ax=ax)\nplt.title(\"KNeighborsClassifier\")\n\nText(0.5, 1.0, 'KNeighborsClassifier')\n\n\n\n\n\n\n\n\n\nWe see that the new measure performs much better (all the wrong predictions are less frequent exept for a few more good ratings predicted as poor.\nFor poor ratings, the number of false negative is now 24% ((189+1136)/ (189+ 4152+ 1136)) and the number of false positive is 27% ((113+1395)/(113+4152+1395)). This is to be compare to the figures from the logistic regressison (67% and 46% respectively).\nWhich one would you choose? Test its performance on the validation set\nThe KNeighbours classifier clearly performs better on the training set. We still need to check that it generalises properly on the validation set.\nWe need to do the same preprocessing as before. Here we need to pay attention to the fact that the validation set is preprocessed exactly in the same way as the training set.\nA quick search (source) shows that the LabelEncoder attributes numbers in sorting orders: we can then do the same steps as before for the same results.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# encoder = LabelEncoder()\nvalidation['Payment_of_Min_Amount'] = le().fit_transform(validation['Payment_of_Min_Amount'])\nvalidation['Payment_Behaviour'] = le().fit_transform(validation['Payment_Behaviour'])\nvalidation['Occupation'] = le().fit_transform(validation['Occupation'])\nvalidation['Type_of_Loan'] = le().fit_transform(validation['Type_of_Loan'])\nvalidation['Credit_Mix'] = le().fit_transform(validation['Credit_Mix'])\nvalidation['Credit_Score'] = le().fit_transform(validation['Credit_Score'])\n\n\nX_valid = validation.drop(columns=['Credit_Score'])[variables] # all columns except score\nY_valid = validation['Credit_Score']\n\n\nactual = Y_valid\npredicted = model.predict(X_valid)\n\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\ncm_display.plot()\n\n\n\n\n\n\n\n\n\n# normalize by the number of predicted labels (sum of columns)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nax = plt.subplot(1,2,1)\ncm = confusion_matrix/confusion_matrix.sum(axis=0)[None, :]\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = score_categories)\ncm_display.plot(ax=ax)\nplt.title(\"Normalized by column\")\nax = plt.subplot(1,2,2)\ncm = confusion_matrix/confusion_matrix.sum(axis=1)[:,None]\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = score_categories)\ncm_display.plot(ax=ax)\nplt.title(\"Normalized by Rows\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe see that 27% of the poor grades (1-0.73) are misclassified (false negatives) and 25% of the bad scores are actually good or standard (false positive). This is close to the statistics on the training set.\nBonus: the use of the label encoder is not ideal, because it assumes some hierarchy of the undrelying model or requires the use of a nonlinear-model. The following code performs another encoding with one dummy variable per value.\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef create_dummy(category):\n    # this create a new pandas dataframe where each columnn contains a dummy variable for category\n    ohe = OneHotEncoder(sparse_output=False)\n    ft = ohe.fit_transform(dataset[[category]])\n    df = pandas.DataFrame(ft, columns= ohe.categories_ )\n    return df\n\n\n# encoder = LabelEncoder()\npma = create_dummy(\"Payment_of_Min_Amount\")\npb = create_dummy('Payment_Behaviour')\nocc = create_dummy('Occupation')\ntol = create_dummy('Type_of_Loan')\ncm = create_dummy('Credit_Mix')\n\n\ndata = pandas.concat([\n    dataset.drop(columns=[\"Payment_of_Min_Amount\",\"Payment_Behaviour\", \"Occupation\", \"Type_of_Loan\", \"Credit_Mix\"]),\n    pma,\n    pb,\n    occ,\n    tol,\n    cm\n], axis=1)\n\n\n# Unfortunately, this crashes the kernel: we are limited by the memory of online instances",
    "crumbs": [
      "tutorials",
      "Classification and clustering"
    ]
  },
  {
    "objectID": "tutorials/session_7/Classification_correction.html#predicting-the-credit-score",
    "href": "tutorials/session_7/Classification_correction.html#predicting-the-credit-score",
    "title": "Classification and clustering",
    "section": "",
    "text": "The two csv files (origin: kaggle) contain the training set (resp the validation set) about the clients from a “global finance company”.\nYour goal is to use all available information to build a model to accurately predict the probability of default which is coded up as a qualitative variable with three values.\nUpdate: the test.csv file now contains the score that should be predicted.\nImport training set and validation sets\nWe follow a three sets approach and define the following sets: - data used for the developing the model (dataset from train.csv) will be split into: - training dataset (variable called train) - test data set (variable called tes - data use\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\nDescribe the dataset.\nHere is a list of columns\n\ndataset.columns\n\nIndex(['Unnamed: 0', 'ID', 'Customer_ID', 'Month', 'Name', 'Age', 'SSN',\n       'Occupation', 'Annual_Income', 'Monthly_Inhand_Salary',\n       'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan',\n       'Type_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment',\n       'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix',\n       'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\nHow is the credit category encoded? Create a new variable representing it with values 0,1,2.\nThe credit score can be found in column Credit_Score. It takes three values.\n\ndataset['Credit_Score'].unique()\n\narray(['Standard', 'Poor', 'Good'], dtype=object)\n\n\n\n# we can use the LabelEncoder function from sklearn\nfrom sklearn.preprocessing import LabelEncoder\ncle = LabelEncoder()\ndataset['Credit_Score'] = cle.fit_transform(dataset['Credit_Score'])\n\n\n\n# check which value corresponds to what\n# this is needed to interpret 0,1,2 below\nscore_categories = cle.inverse_transform([0,1,2])\nscore_categories\n\narray(['Good', 'Poor', 'Standard'], dtype=object)\n\n\nReencode all categorical variables as dummy variables. Remove variables that are not useful for the analysis.\nLet’s reencode all string data as ordinal values.\n\nfrom sklearn.preprocessing import LabelEncoder as le\n\ndataset['Payment_of_Min_Amount'] = le().fit_transform(dataset['Payment_of_Min_Amount'])\ndataset['Payment_Behaviour'] = le().fit_transform(dataset['Payment_Behaviour'])\ndataset['Occupation'] = le().fit_transform(dataset['Occupation'])\ndataset['Type_of_Loan'] = le().fit_transform(dataset['Type_of_Loan'])\ndataset['Credit_Mix'] = le().fit_transform(dataset['Credit_Mix'])\n\n\n\n# we won't use the name variable so that we drop it\ndataset.drop(columns=[\"Name\"], inplace=True)\n\nKeyError: \"['Name'] not found in axis\"\n\n\nMake several plots about the dataset (histograms, correlation plots, …)\nTip: there are cool ideas here\n\nimport seaborn as sns\n\n\nsns.histplot(dataset['Credit_Score'])\n\n\n\n\n\n\n\n\n\n# Let's plot the correlations\nfrom matplotlib import pyplot as plt\nplt.figure(figsize = (14,10))\nsns.heatmap(dataset.corr())\n\n\n\n\n\n\n\n\nWe see from the correlation plot that, as expected, ID, customer_ID, SSN and are not related to the other variables. More surprisingly this is also also the case of “occupation”. This simply means that the numerical variable resulting from the conversion of occupations to numbers, was not ordered. It would be useless in a linear regression, but we can try a nonlinear one.\nA better way to encode the categorical variables would be to create dummy variables for each value.\nSplit the train dataset into a df_train and a df_test dataset.\n\nimport sklearn.model_selection\ndf_train, df_test = sklearn.model_selection.train_test_split(dataset , test_size= 0.25, random_state=243 )\n\nImplement a logistic regression.\n\ndf_train.columns\n\nIndex(['Unnamed: 0', 'ID', 'Customer_ID', 'Month', 'Age', 'SSN', 'Occupation',\n       'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n       'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan',\n       'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n       'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n       'Credit_Utilization_Ratio', 'Credit_History_Age',\n       'Payment_of_Min_Amount', 'Total_EMI_per_month',\n       'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n       'Credit_Score'],\n      dtype='object')\n\n\n\n# we keep only a subset of all variables:\nvariables = [\n    # 'Credit_Score',\n    'Changed_Credit_Limit',\n    'Payment_of_Min_Amount',\n    'Credit_Mix',\n    'Delay_from_due_date',\n    'Annual_Income',\n    'Monthly_Inhand_Salary',\n    'Age',\n    'Monthly_Balance',\n    'Num_of_Delayed_Payment',\n    'Outstanding_Debt',\n    'Payment_Behaviour',\n    'Credit_History_Age',\n    'Num_Bank_Accounts',\n    'Credit_Utilization_Ratio'\n]\n\n\ndf_test['Credit_Score']\n\n70144    0\n32059    1\n25287    2\n13496    1\n65970    2\n        ..\n59793    0\n33338    2\n51142    0\n29286    0\n58864    2\nName: Credit_Score, Length: 18750, dtype: int64\n\n\n\n# we split the set into features and labels:\nX = df_train.drop(columns=['Credit_Score'])[variables] # all columns except score\nY = df_train['Credit_Score']\n\n# same for the test set:\nX_test = df_test.drop(columns=['Credit_Score'])[variables] # all columns except score\nY_test = df_test['Credit_Score']\n\nWe need a logistic classifier, since we are predicting binary variables\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=1000)\nregression = model.fit(X,Y)\n\n/home/pablo/.local/opt/micromamba/envs/escp/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nlr_score_test=model.score(X , Y)\nlr_score_test\n\n0.5916444444444444\n\n\nCompute the confusion matrix using the test set. Comment\nThis is how our model performs on the test set.\n\n# adjust the following code if needed\n\n\nactual = Y_test\npredicted = model.predict(X_test)\n\nFor classification problems a standard way to represent the performance, consists in computing the “confusion matrix”.\n\nfrom sklearn import metrics\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\nconfusion_matrix\n\narray([[ 862,   39, 2462],\n       [ 166, 2299, 3012],\n       [ 703, 1259, 7948]])\n\n\n\ncm_display_0 = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display_0.plot(ax=ax)\n\n\n\n\n\n\n\n\nWe can also normalize by the number of observations for each category (divide each row by the total of this row).\n\nconfusion_matrix_percents = confusion_matrix / confusion_matrix.sum(axis=1)[:,None] *100\nconfusion_matrix_percents # check that each row sums to 1\n\narray([[25.6318763 ,  1.15967886, 73.20844484],\n       [ 3.03085631, 41.97553405, 54.99360964],\n       [ 7.0938446 , 12.70433905, 80.20181635]])\n\n\nThis tells us that 33% of the poor ratings were correctly detected (true positive). Or, equivalently, that 67% of them were not detected (false positive).\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n\n\n\n\n\n\n\nSymmetrically, we can normalize by the number of predicted labels, i.e. the sum of each column.\n\nconfusion_matrix_percents = confusion_matrix / confusion_matrix.sum(axis=0)[None,:] *100\nconfusion_matrix_percents # check that *columns* sum to 1\n\narray([[49.79780474,  1.08423686, 18.34301892],\n       [ 9.58983247, 63.91437309, 22.44076889],\n       [40.6123628 , 35.00139005, 59.21621219]])\n\n\nAs before we can make a graphical representation:\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.grid(False)\ncm_display.plot(ax=ax)\n\n\n\n\n\n\n\n\nWe see now that of all the poor ratings that are detected 54% were actually poor. Hence 46% of them were incorrect.\nPerform the same analysis with other classification methods and compare their performance using the test set.\n\n# Clearly we need to use some nonlinear model instead\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nregression = model.fit(X,Y)\n\n\nactual = Y_test\npredicted = model.predict(X_test)\n\n\nfrom sklearn import metrics\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\nconfusion_matrix\n\narray([[2022,  113, 1228],\n       [ 189, 4152, 1136],\n       [1077, 1395, 7438]])\n\n\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = score_categories)\n\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nax = plt.subplot(1,2,1)\nax.grid(False)\ncm_display_0.plot(ax=ax)\nplt.title(\"Logistic Regression\")\nax = plt.subplot(1,2,2)\nax.grid(False)\ncm_display.plot(ax=ax)\nplt.title(\"KNeighborsClassifier\")\n\nText(0.5, 1.0, 'KNeighborsClassifier')\n\n\n\n\n\n\n\n\n\nWe see that the new measure performs much better (all the wrong predictions are less frequent exept for a few more good ratings predicted as poor.\nFor poor ratings, the number of false negative is now 24% ((189+1136)/ (189+ 4152+ 1136)) and the number of false positive is 27% ((113+1395)/(113+4152+1395)). This is to be compare to the figures from the logistic regressison (67% and 46% respectively).\nWhich one would you choose? Test its performance on the validation set\nThe KNeighbours classifier clearly performs better on the training set. We still need to check that it generalises properly on the validation set.\nWe need to do the same preprocessing as before. Here we need to pay attention to the fact that the validation set is preprocessed exactly in the same way as the training set.\nA quick search (source) shows that the LabelEncoder attributes numbers in sorting orders: we can then do the same steps as before for the same results.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# encoder = LabelEncoder()\nvalidation['Payment_of_Min_Amount'] = le().fit_transform(validation['Payment_of_Min_Amount'])\nvalidation['Payment_Behaviour'] = le().fit_transform(validation['Payment_Behaviour'])\nvalidation['Occupation'] = le().fit_transform(validation['Occupation'])\nvalidation['Type_of_Loan'] = le().fit_transform(validation['Type_of_Loan'])\nvalidation['Credit_Mix'] = le().fit_transform(validation['Credit_Mix'])\nvalidation['Credit_Score'] = le().fit_transform(validation['Credit_Score'])\n\n\nX_valid = validation.drop(columns=['Credit_Score'])[variables] # all columns except score\nY_valid = validation['Credit_Score']\n\n\nactual = Y_valid\npredicted = model.predict(X_valid)\n\n\nconfusion_matrix = metrics.confusion_matrix(actual, predicted)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_percents, display_labels = score_categories)\ncm_display.plot()\n\n\n\n\n\n\n\n\n\n# normalize by the number of predicted labels (sum of columns)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\nax = plt.subplot(1,2,1)\ncm = confusion_matrix/confusion_matrix.sum(axis=0)[None, :]\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = score_categories)\ncm_display.plot(ax=ax)\nplt.title(\"Normalized by column\")\nax = plt.subplot(1,2,2)\ncm = confusion_matrix/confusion_matrix.sum(axis=1)[:,None]\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = score_categories)\ncm_display.plot(ax=ax)\nplt.title(\"Normalized by Rows\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe see that 27% of the poor grades (1-0.73) are misclassified (false negatives) and 25% of the bad scores are actually good or standard (false positive). This is close to the statistics on the training set.\nBonus: the use of the label encoder is not ideal, because it assumes some hierarchy of the undrelying model or requires the use of a nonlinear-model. The following code performs another encoding with one dummy variable per value.\n\nimport pandas\ndataset = pandas.read_csv(\"train.csv\")\nvalidation = pandas.read_csv(\"test.csv\")\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef create_dummy(category):\n    # this create a new pandas dataframe where each columnn contains a dummy variable for category\n    ohe = OneHotEncoder(sparse_output=False)\n    ft = ohe.fit_transform(dataset[[category]])\n    df = pandas.DataFrame(ft, columns= ohe.categories_ )\n    return df\n\n\n# encoder = LabelEncoder()\npma = create_dummy(\"Payment_of_Min_Amount\")\npb = create_dummy('Payment_Behaviour')\nocc = create_dummy('Occupation')\ntol = create_dummy('Type_of_Loan')\ncm = create_dummy('Credit_Mix')\n\n\ndata = pandas.concat([\n    dataset.drop(columns=[\"Payment_of_Min_Amount\",\"Payment_Behaviour\", \"Occupation\", \"Type_of_Loan\", \"Credit_Mix\"]),\n    pma,\n    pb,\n    occ,\n    tol,\n    cm\n], axis=1)\n\n\n# Unfortunately, this crashes the kernel: we are limited by the memory of online instances",
    "crumbs": [
      "tutorials",
      "Classification and clustering"
    ]
  },
  {
    "objectID": "tutorials/session_7/Classification_correction.html#segmenting-the-bank-clients",
    "href": "tutorials/session_7/Classification_correction.html#segmenting-the-bank-clients",
    "title": "Classification and clustering",
    "section": "Segmenting the bank clients",
    "text": "Segmenting the bank clients\nWith the same database, without using the credit score, implement a k-means clustering algorithm.\n\nfrom sklearn.cluster import KMeans\nkm_model = KMeans(n_clusters=3)\nkm_model.fit(dataset)\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3) \n\n\n\n# we then attribute each element in the training set to a cluster according to the model\ndataset['cluster'] = km_model.predict(dataset)\n\nAre the clusters related to the credit score?\nLet’s compute the proportion of each score in each cluster.\n\ndataset.groupby(\"cluster\")['Credit_Score'].value_counts(normalize =True)\n\ncluster  Credit_Score\n0        2               0.520876\n         1               0.299119\n         0               0.180006\n1        2               0.532145\n         1               0.285075\n         0               0.182780\n2        2               0.540301\n         1               0.283742\n         0               0.175958\nName: proportion, dtype: float64\n\n\nThe proportion of each score does not seem to depend much on the clustering variable.\nIt suggests there are indeed similarity clusters that are uncorrelated to the ability to repay",
    "crumbs": [
      "tutorials",
      "Classification and clustering"
    ]
  },
  {
    "objectID": "tutorials/session_1/qe_collections.html",
    "href": "tutorials/session_1/qe_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Prerequisites\n\nCore data types\n\nOutcomes\n\nOrdered Collections\n\nKnow what a list is and a tuple is\n\nKnow how to tell a list from a tuple\n\nUnderstand the range, zip and enumerate functions\n\nBe able to use common list methods like append, sort, and reverse\n\n\nAssociative Collections\n\nUnderstand what a dict is\n\nKnow the distinction between a dicts keys and values\n\nUnderstand when dicts are useful\n\nBe familiar with common dict methods\n\n\nSets (optional)\n\nKnow what a set is\n\nUnderstand how a set differs from a list and a tuple\n\nKnow when to use a set vs a list or a tuple\n\n\n\n\n\n\nA Python list is an ordered collection of items.\nWe can create lists using the following syntax\n\n[item1, item2, ...,  itemN]\n\nwhere the ... represents any number of additional items.\nEach item can be of any type.\nLet’s create some lists.\n\n# created, but not assigned to a variable\n[2.0, 9.1, \"a rose is a rose is a rose\"]\n\n[2.0, 9.1, 'a rose is a rose is a rose']\n\n\n\n# stored as the variable `x`\nx = [2.0, 9.1, 12.5]\nprint(\"x has type\", type(x))\nx\n\nx has type &lt;class 'list'&gt;\n\n\n[2.0, 9.1, 12.5]\n\n\n\n\nWe can access items in a list called mylist using mylist[N] where N is an integer.\nNote: Anytime that we use the syntax x[i] we are doing what is called indexing – it means that we are selecting a particular element of a collection x.\n\nx[1]\n\n9.1\n\n\nWait? Why did x[1] return 9.1 when the first element in x is actually 2.0?\nThis happened because Python starts counting at zero!\nLets repeat that one more time for emphasis Python starts counting at zero!\nTo access the first element of x we must use x[0]:\n\nx[0]\n\n2.0\n\n\nWe can also determine how many items are in a list using the len function.\n\nlen(x)\n\n3\n\n\nWhat happens if we try to index with a number higher than the number of items in a list?\n\n# uncomment the line below and run\nx[4]\n\nIndexError: list index out of range\n\n\nWe can check if a list contains an element using the in keyword.\n\n2.0 in x\n\nTrue\n\n\n\n1.5 in x\n\nFalse\n\n\nFor our list x, other common operations we might want to do are…\n\nx.reverse()\nx\n\n[12.5, 9.1, 2.0]\n\n\n\nnumber_list = [10, 25, 42, 1.0]\nprint(number_list)\nnumber_list.sort()\nprint(number_list)\n\n[10, 25, 42, 1.0]\n[1.0, 10, 25, 42]\n\n\nNote that in order to sort, we had to have all elements in our list be numbers (int and float), more on this below.\nWe could actually do the same with a list of strings. In this case, sort will put the items in alphabetical order.\n\nstr_list = [\"NY\", \"AZ\", \"TX\"]\nprint(str_list)\nstr_list.sort()\nprint(str_list)\n\n['NY', 'AZ', 'TX']\n['AZ', 'NY', 'TX']\n\n\nThe append method adds an element to the end of existing list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append(10)\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 10]\n\n\nHowever, if you call append with a list, it adds a list to the end, rather than the numbers in that list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, [20, 4]]\n\n\nTo combine the lists instead…\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.extend([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 20, 4]\n\n\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\n\n\nWhile most examples above have all used a list with a single type of variable, this is not required.\nLet’s carefully make a small change to the first example: replace 2.0 with 2\n\nx = [2, 9.1, 12.5]\n\nThis behavior is identical for many operations you might apply to a list.\n\nimport numpy as np\nx = [2, 9.1, 12.5]\nnp.mean(x)\n\n7.866666666666667\n\n\n\n# native python functions\nsum(x) / len(x)\n\n7.866666666666667\n\n\nHere we have also introduced a new module, Numpy, which provides many functions for working with numeric data.\nTaking this further, we can put completely different types of elements inside of a list.\n\n# stored as the variable `x`\nx = [2, \"hello\", 3.0]\nprint(\"x has type\", type(x))\nx\n\nTo see the types of individual elements in the list:\n\nprint(f\"type(x[0]) = {type(x[0])}, type(x[0]) = {type(x[1])}, type(x[2]) = {type(x[2])}\")\n\nWhile no programming limitations prevent this, you should be careful if you write code with different numeric and non-numeric types in the same list.\nFor example, if the types within the list cannot be compared, then how could you sort the elements of the list? (i.e. How do you determine whether the string “hello” is less than the integer 2, “hello” &lt; 2?)\n\nx = [2, \"hello\", 3.0]\n# uncomment the line below and see what happens!\n# x.sort()\n\nA few key exceptions to this general rule are:\n\nLists with both integers and floating points are less error-prone (since mathematical code using the list would work with both types).\n\nWhen working with lists and data, you may want to represent missing values with a different type than the existing values.\n\n\n\n\nOne function you will see often in Python is the range function.\nIt has three versions:\n\nrange(N): goes from 0 to N-1\n\nrange(a, N): goes from a to N-1\n\nrange(a, N, d): goes from a to N-1, counting by d\n\nWhen we call the range function, we get back something that has type range:\n\nrange(1,4) # form 1 to 3 included\n\nrange(1, 4)\n\n\n\nr = range(5)  # from 0 to 4 included\nprint(\"type(r)\", type(r))\n\ntype(r) &lt;class 'range'&gt;\n\n\nTo turn the range into a list:\n\n[*r] # unpack elements\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(r)\n\n[0, 1, 2, 3, 4]\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\nTuples are very similar to lists and hold ordered collections of items.\nHowever, tuples and lists have three main differences:\n\nTuples are created using parenthesis — ( and ) — instead of square brackets — [ and ].\n\nTuples are immutable, which is a fancy computer science word meaning that they can’t be changed or altered after they are created.\n\nTuples and multiple return values from functions are tightly connected, as we will see in functions.\n\n\nt = (1, \"hello\", 3.0)\nprint(\"t is a\", type(t))\nt\n\nt is a &lt;class 'tuple'&gt;\n\n\n(1, 'hello', 3.0)\n\n\n\ntuple(list(t))\n\n(1, 'hello', 3.0)\n\n\nWe can convert a list to a tuple by calling the tuple function on a list.\n\nprint(\"x is a\", type(x))\nprint(\"tuple(x) is a\", type(tuple(x)))\ntuple(x)\n\nx is a &lt;class 'list'&gt;\ntuple(x) is a &lt;class 'tuple'&gt;\n\n\n(2, 9.1, 12.5)\n\n\nWe can also convert a tuple to a list using the list function.\n\nlist(t)\n\n[1, 'hello', 3.0]\n\n\nAs with a list, we access items in a tuple t using t[N] where N is an int.\n\nt[0]  # still start counting at 0\n\n1\n\n\n\nt[2]\n\n3.0\n\n\n\n\n\nSee exercise 3 in the exercise list.\nTuples (and lists) can be unpacked directly into variables.\n\nx, y = (1, \"test\")\nprint(f\"x = {x}, y = {y}\")\n\nThis will be a convenient way to work with functions returning multiple values, as well as within comprehensions and loops.\n\n\n\nShould you use a list or tuple?\nThis depends on what you are storing, whether you might need to reorder the elements, or whether you’d add new elements without a complete reinterpretation of the underlying data.\nFor example, take data representing the GDP (in trillions) and population (in billions) for China in 2015.\n\nchina_data_2015 = (\"China\", 2015, 11.06, 1.371)\n\nprint(china_data_2015)\n\nIn this case, we have used a tuple since: (a) ordering would be meaningless; and (b) adding more data would require a reinterpretation of the whole data structure.\nOn the other hand, consider a list of GDP in China between 2013 and 2015.\n\ngdp_data = [9.607, 10.48, 11.06]\nprint(gdp_data)\n\nIn this case, we have used a list, since adding on a new element to the end of the list for GDP in 2016 would make complete sense.\nAlong these lines, collecting data on China for different years may make sense as a list of tuples (e.g. year, GDP, and population – although we will see better ways to store this sort of data in the Pandas section).\n\nchina_data = [(2015, 11.06, 1.371), (2014, 10.48, 1.364), (2013, 9.607, 1.357)]\nprint(china_data)\n\nIn general, a rule of thumb is to use a list unless you need to use a tuple.\nKey criteria for tuple use are when you want to:\n\nensure the order of elements can’t change\n\nensure the actual values of the elements can’t change\n\nuse the collection as a key in a dict (we will learn what this means soon)\n\n\n\n\nTwo functions that can be extremely useful are zip and enumerate.\nBoth of these functions are best understood by example, so let’s see them in action and then talk about what they do.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nprint(\"type(z)\", type(z))\n\ntype(z) &lt;class 'zip'&gt;\n\n\nTo see what is inside z, let’s convert it to a list.\n\nlist(z)\n\n[(2013, 9.607), (2014, 10.48), (2015, 11.06)]\n\n\nNotice that we now have a list where each item is a tuple.\nWithin each tuple, we have one item from each of the collections we passed to the zip function.\nIn particular, the first item in z contains the first item from [2013, 2014, 2015] and the first item from [9.607, 10.48, 11.06].\nThe second item in z contains the second item from each collection and so on.\nWe can access an element in this and then unpack the resulting tuple directly into variables.\n\nl = list(zip(years, gdp_data))\nx, y = l[0]\nprint(f\"year = {x}, GDP = {y}\")\n\nNow let’s experiment with enumerate.\n\ne = enumerate([\"a\", \"b\", \"c\"])\nprint(\"type(e)\", type(e))\ne\n\ntype(e) &lt;class 'enumerate'&gt;\n\n\n&lt;enumerate at 0x7fef3c8a01c0&gt;\n\n\nAgain, we call list(e) to see what is inside.\n\nlist(e)\n\n[(0, 'a'), (1, 'b'), (2, 'c')]\n\n\nWe again have a list of tuples, but this time, the first element in each tuple is the index of the second tuple element in the initial collection.\nNotice that the third item is (2, 'c') because [\"a\", \"b\", \"c\"][2] is 'c'\n\n\n\nSee exercise 4 in the exercise list.\nAn important quirk of some iterable types that are not lists (such as the above zip) is that you cannot convert the same type to a list twice.\nThis is because zip, enumerate, and range produce what is called a generator.\nA generator will only produce each of its elements a single time, so if you call list on the same generator a second time, it will not have any elements to iterate over anymore.\nFor more information, refer to the Python documentation.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nl = list(z)\nprint(l)\nm = list(z)\nprint(m)\n\n\n\n\n\n\n\n\nA dictionary (or dict) associates keys with values.\nIt will feel similar to a dictionary for words, where the keys are words and the values are the associated definitions.\nThe most common way to create a dict is to use curly braces — { and } — like this:\n\n{\n    \"key1\": value1,\n    \"key2\": value2,\n    \"keyN\": valueN\n}\n\nwhere the ... indicates that we can have any number of additional terms.\nThe crucial part of the syntax is that each key-value pair is written key: value and that these pairs are separated by commas — ,.\nLet’s see an example using our aggregate data on China in 2015.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\nUnlike our above example using a tuple, a dict allows us to associate a name with each field, rather than having to remember the order within the tuple.\nOften, code that makes a dict is easier to read if we put each key: value pair on its own line. (Recall our earlier comment on using whitespace effectively to improve readability!)\nThe code below is equivalent to what we saw above.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\n\nMost often, the keys (e.g. “country”, “year”, “GDP”, and “population”) will be strings, but we could also use numbers (int, or float) or even tuples (or, rarely, a combination of types).\nThe values can be any type and different from each other.\n\n\n\nSee exercise 5 in the exercise list.\nThis next example is meant to emphasize how values can be anything – including another dictionary.\n\ncompanies = {\n    \"AAPL\": {\"bid\": 175.96, \"ask\": 175.98}, \n    \"GE\": {\"bid\": 1047.03, \"ask\": 1048.40},\n    \"TVIX\": {\"bid\": 8.38, \"ask\": 8.40}\n}\nprint(companies)\n\n{'AAPL': {'bid': 175.96, 'ask': 175.98}, 'GE': {'bid': 1047.03, 'ask': 1048.4}, 'TVIX': {'bid': 8.38, 'ask': 8.4}}\n\n\n\n\nWe can now ask Python to tell us the value for a particular key by using the syntax d[k], where d is our dict and k is the key for which we want to find the value.\nFor example,\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nchina_data['year']\n\n2015\n\n\n\n\nprint(\n    f\"country = {china_data['country']}, population = {china_data['population']}\")\n\ncountry = China, population = 1.371\n\n\nNote: when inside of a formatting string, you can use ' instead of \" as above to ensure the formatting still works with the embedded code.\nIf we ask for the value of a key that is not in the dict, we will get an error.\n\n# uncomment the line below to see the error\n# china_data[\"inflation\"]\n\nWe can also add new items to a dict using the syntax d[new_key] = new_value.\nLet’s see some examples.\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nchina_data[\"unemployment\"] = \"4.05%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n\n\nTo update the value, we use assignment in the same way (which will create the key and value as required).\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.051%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.051%'}\n\n\nOr we could change the type.\n\nchina_data[\"unemployment\"] = False or True\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': True}\n\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe can do some common things with dicts.\nWe will demonstrate them with examples below.\n\n# number of key-value pairs in a dict\nlen(china_data)\n\n5\n\n\n\n# get a list of all the keys\nlist(china_data.keys())\n\n['country', 'year', 'GDP', 'population', 'unemployment']\n\n\n\n# get a list of all the values\nlist(china_data.values())\n\n['China', 2015, 11.06, 1.371, True]\n\n\n\nmore_china_data = {\n    \"irrigated_land\": 690_070,\n    \"top_religions\":\n        {\"buddhist\": 18.2, \"christian\" : 5.1, \"muslim\": 1.8}\n}\n\n# Add all key-value pairs in mydict2 to mydict.\n# if the key already appears in mydict, overwrite the\n# value with the value in mydict2\nchina_data.update(more_china_data)\nchina_data\n\n{'country': 'China',\n 'year': 2015,\n 'GDP': 11.06,\n 'population': 1.371,\n 'unemployment': True,\n 'irrigated_land': 690070,\n 'top_religions': {'buddhist': 18.2, 'christian': 5.1, 'muslim': 1.8}}\n\n\n\ntype(china_data.get(\"book\"))\n\nNoneType\n\n\n\n# Get the value associated with a key or return a default value\n# use this to avoid the NameError we saw above if you have a reasonable\n# default value\nchina_data.get(\"irrigated_land\", \"Data Not Available\")\n\n690070\n\n\n\nchina_data.get(\"book\", \"Data Not Available\")\n\n'Data Not Available'\n\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\n\nPython has an additional way to represent collections of items: sets.\nSets come up infrequently, but you should be aware of them.\nIf you are familiar with the mathematical concept of sets, then you will understand the majority of Python sets already.\nIf you don’t know the math behind sets, don’t worry: we’ll cover the basics of Python’s sets here.\nA set is an unordered collection of unique elements.\nThe syntax for creating a set uses curly bracket { and }.\n\n{item1, item2, ..., itemN}\n\nHere is an example.\n\ns = {1, \"hello\", 3.0}\nprint(\"s has type\", type(s))\ns\n\n\n\n\nSee exercise 9 in the exercise list.\nAs with lists and tuples, we can check if something is in the set and check the set’s length:\n\nprint(\"len(s) =\", len(s))\n\"hello\" in s\n\nUnlike lists and tuples, we can’t extract elements of a set s using s[N] where N is a number.\n\n# Uncomment the line below to see what happens\n# s[1]\n\nThis is because sets are not ordered, so the notion of getting the second element (s[1]) is not well defined.\nWe add elements to a set s using s.add.\n\ns.add(100)\ns\n\n\ns.add(\"hello\") # nothing happens, why?\ns\n\nWe can also do set operations.\nConsider the set s from above and the set s2 = {\"hello\", \"world\"}.\n\ns.union(s2): returns a set with all elements in either s or s2\n\ns.intersection(s2): returns a set with all elements in both s and s2\n\ns.difference(s2): returns a set with all elements in s that aren’t in s2\n\ns.symmetric_difference(s2): returns a set with all elements in only one of s and s2\n\n\n\n\nSee exercise 10 in the exercise list.\nAs with tuples and lists, a set function can convert other collections to sets.\n\nx = [1, 2, 3, 1]\nset(x)\n\n\nt = (1, 2, 3, 1)\nset(t)\n\nLikewise, we can convert sets to lists and tuples.\n\nlist(s)\n\n\ntuple(s)\n\n\n\n\n\n\n\n\nIn the first cell, try y.append(z).\nIn the second cell try y.extend(z).\nExplain the behavior.\nWhen you are trying to explain use y.append? and y.extend? to see a description of what these methods are supposed to do.\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\n# \n\n['a', 'b', 'c', [1, 2, 3]]\n\n\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\n# \n\n['a', 'b', 'c', 1, 2, 3]\n\n\n(back to text)\n\n\n\nExperiment with the other two versions of the range function.\n\na = 2\nN = 5\n\n# (range(a, N))\nlist(range(a, N))\n\n[2, 3, 4]\n\n\n\nrange?\n\n\nInit signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     \n\n\n\n\nd = 7\nlist(range(a, N, d)) \n\n[2]\n\n\n\nlist(range(0, 99, 2)) ;\n\n(back to text)\n\n[*range(0, 21, 5)]\n\n[0, 5, 10, 15, 20]\n\n\n\n\n\nVerify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\nt=(100,2,3,4)\n\n\nt+('!',)\n\n(100, 2, 3, 4, '!')\n\n\n\n# sorting t\nsorted(t)\n\n[2, 3, 4, 100]\n\n\n\n# reversing t\nt[::-1]  # from stackoverflow https://stackoverflow.com/questions/10201977/how-to-reverse-tuples-in-python\n\n(4, 3, 2, 100)\n\n\n(back to text)\n\n\n\nChallenging For the tuple foo below, use a combination of zip, range, and len to mimic enumerate(foo).\nVerify that your proposed solution is correct by converting each to a list and checking equality with ==.\nYou can see what the answer should look like by starting with list(enumerate(foo)).\n\nfoo = (\"good\", \"luck!\")\n\n(back to text)\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n\n(back to text)\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\n\n(back to text)\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\n#china_data.pop()\n\n(back to text)\n\n\n\nExplain what happens to the value you popped.\nExperiment with calling pop twice.\n\n# your code here\n\n(back to text)\n\n\n\nTry creating a set with repeated elements (e.g. {1, 2, 1, 2, 1, 2}).\nWhat happens?\nWhy?\n\n# your code here\n\n(back to text)\n\n\n\nTest out two of the operations described above using the original set we created, s, and the set created below s2.\n\ns2 = {\"hello\", \"world\"}\n\n\n# Operation 1\n\n\n# Operation 2\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections.html#ordered-collections",
    "href": "tutorials/session_1/qe_collections.html#ordered-collections",
    "title": "Collections",
    "section": "",
    "text": "A Python list is an ordered collection of items.\nWe can create lists using the following syntax\n\n[item1, item2, ...,  itemN]\n\nwhere the ... represents any number of additional items.\nEach item can be of any type.\nLet’s create some lists.\n\n# created, but not assigned to a variable\n[2.0, 9.1, \"a rose is a rose is a rose\"]\n\n[2.0, 9.1, 'a rose is a rose is a rose']\n\n\n\n# stored as the variable `x`\nx = [2.0, 9.1, 12.5]\nprint(\"x has type\", type(x))\nx\n\nx has type &lt;class 'list'&gt;\n\n\n[2.0, 9.1, 12.5]\n\n\n\n\nWe can access items in a list called mylist using mylist[N] where N is an integer.\nNote: Anytime that we use the syntax x[i] we are doing what is called indexing – it means that we are selecting a particular element of a collection x.\n\nx[1]\n\n9.1\n\n\nWait? Why did x[1] return 9.1 when the first element in x is actually 2.0?\nThis happened because Python starts counting at zero!\nLets repeat that one more time for emphasis Python starts counting at zero!\nTo access the first element of x we must use x[0]:\n\nx[0]\n\n2.0\n\n\nWe can also determine how many items are in a list using the len function.\n\nlen(x)\n\n3\n\n\nWhat happens if we try to index with a number higher than the number of items in a list?\n\n# uncomment the line below and run\nx[4]\n\nIndexError: list index out of range\n\n\nWe can check if a list contains an element using the in keyword.\n\n2.0 in x\n\nTrue\n\n\n\n1.5 in x\n\nFalse\n\n\nFor our list x, other common operations we might want to do are…\n\nx.reverse()\nx\n\n[12.5, 9.1, 2.0]\n\n\n\nnumber_list = [10, 25, 42, 1.0]\nprint(number_list)\nnumber_list.sort()\nprint(number_list)\n\n[10, 25, 42, 1.0]\n[1.0, 10, 25, 42]\n\n\nNote that in order to sort, we had to have all elements in our list be numbers (int and float), more on this below.\nWe could actually do the same with a list of strings. In this case, sort will put the items in alphabetical order.\n\nstr_list = [\"NY\", \"AZ\", \"TX\"]\nprint(str_list)\nstr_list.sort()\nprint(str_list)\n\n['NY', 'AZ', 'TX']\n['AZ', 'NY', 'TX']\n\n\nThe append method adds an element to the end of existing list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append(10)\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 10]\n\n\nHowever, if you call append with a list, it adds a list to the end, rather than the numbers in that list.\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.append([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, [20, 4]]\n\n\nTo combine the lists instead…\n\nnum_list = [10, 25, 42, 8]\nprint(num_list)\nnum_list.extend([20, 4])\nprint(num_list)\n\n[10, 25, 42, 8]\n[10, 25, 42, 8, 20, 4]\n\n\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\n\n\nWhile most examples above have all used a list with a single type of variable, this is not required.\nLet’s carefully make a small change to the first example: replace 2.0 with 2\n\nx = [2, 9.1, 12.5]\n\nThis behavior is identical for many operations you might apply to a list.\n\nimport numpy as np\nx = [2, 9.1, 12.5]\nnp.mean(x)\n\n7.866666666666667\n\n\n\n# native python functions\nsum(x) / len(x)\n\n7.866666666666667\n\n\nHere we have also introduced a new module, Numpy, which provides many functions for working with numeric data.\nTaking this further, we can put completely different types of elements inside of a list.\n\n# stored as the variable `x`\nx = [2, \"hello\", 3.0]\nprint(\"x has type\", type(x))\nx\n\nTo see the types of individual elements in the list:\n\nprint(f\"type(x[0]) = {type(x[0])}, type(x[0]) = {type(x[1])}, type(x[2]) = {type(x[2])}\")\n\nWhile no programming limitations prevent this, you should be careful if you write code with different numeric and non-numeric types in the same list.\nFor example, if the types within the list cannot be compared, then how could you sort the elements of the list? (i.e. How do you determine whether the string “hello” is less than the integer 2, “hello” &lt; 2?)\n\nx = [2, \"hello\", 3.0]\n# uncomment the line below and see what happens!\n# x.sort()\n\nA few key exceptions to this general rule are:\n\nLists with both integers and floating points are less error-prone (since mathematical code using the list would work with both types).\n\nWhen working with lists and data, you may want to represent missing values with a different type than the existing values.\n\n\n\n\nOne function you will see often in Python is the range function.\nIt has three versions:\n\nrange(N): goes from 0 to N-1\n\nrange(a, N): goes from a to N-1\n\nrange(a, N, d): goes from a to N-1, counting by d\n\nWhen we call the range function, we get back something that has type range:\n\nrange(1,4) # form 1 to 3 included\n\nrange(1, 4)\n\n\n\nr = range(5)  # from 0 to 4 included\nprint(\"type(r)\", type(r))\n\ntype(r) &lt;class 'range'&gt;\n\n\nTo turn the range into a list:\n\n[*r] # unpack elements\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(r)\n\n[0, 1, 2, 3, 4]\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\nTuples are very similar to lists and hold ordered collections of items.\nHowever, tuples and lists have three main differences:\n\nTuples are created using parenthesis — ( and ) — instead of square brackets — [ and ].\n\nTuples are immutable, which is a fancy computer science word meaning that they can’t be changed or altered after they are created.\n\nTuples and multiple return values from functions are tightly connected, as we will see in functions.\n\n\nt = (1, \"hello\", 3.0)\nprint(\"t is a\", type(t))\nt\n\nt is a &lt;class 'tuple'&gt;\n\n\n(1, 'hello', 3.0)\n\n\n\ntuple(list(t))\n\n(1, 'hello', 3.0)\n\n\nWe can convert a list to a tuple by calling the tuple function on a list.\n\nprint(\"x is a\", type(x))\nprint(\"tuple(x) is a\", type(tuple(x)))\ntuple(x)\n\nx is a &lt;class 'list'&gt;\ntuple(x) is a &lt;class 'tuple'&gt;\n\n\n(2, 9.1, 12.5)\n\n\nWe can also convert a tuple to a list using the list function.\n\nlist(t)\n\n[1, 'hello', 3.0]\n\n\nAs with a list, we access items in a tuple t using t[N] where N is an int.\n\nt[0]  # still start counting at 0\n\n1\n\n\n\nt[2]\n\n3.0\n\n\n\n\n\nSee exercise 3 in the exercise list.\nTuples (and lists) can be unpacked directly into variables.\n\nx, y = (1, \"test\")\nprint(f\"x = {x}, y = {y}\")\n\nThis will be a convenient way to work with functions returning multiple values, as well as within comprehensions and loops.\n\n\n\nShould you use a list or tuple?\nThis depends on what you are storing, whether you might need to reorder the elements, or whether you’d add new elements without a complete reinterpretation of the underlying data.\nFor example, take data representing the GDP (in trillions) and population (in billions) for China in 2015.\n\nchina_data_2015 = (\"China\", 2015, 11.06, 1.371)\n\nprint(china_data_2015)\n\nIn this case, we have used a tuple since: (a) ordering would be meaningless; and (b) adding more data would require a reinterpretation of the whole data structure.\nOn the other hand, consider a list of GDP in China between 2013 and 2015.\n\ngdp_data = [9.607, 10.48, 11.06]\nprint(gdp_data)\n\nIn this case, we have used a list, since adding on a new element to the end of the list for GDP in 2016 would make complete sense.\nAlong these lines, collecting data on China for different years may make sense as a list of tuples (e.g. year, GDP, and population – although we will see better ways to store this sort of data in the Pandas section).\n\nchina_data = [(2015, 11.06, 1.371), (2014, 10.48, 1.364), (2013, 9.607, 1.357)]\nprint(china_data)\n\nIn general, a rule of thumb is to use a list unless you need to use a tuple.\nKey criteria for tuple use are when you want to:\n\nensure the order of elements can’t change\n\nensure the actual values of the elements can’t change\n\nuse the collection as a key in a dict (we will learn what this means soon)\n\n\n\n\nTwo functions that can be extremely useful are zip and enumerate.\nBoth of these functions are best understood by example, so let’s see them in action and then talk about what they do.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nprint(\"type(z)\", type(z))\n\ntype(z) &lt;class 'zip'&gt;\n\n\nTo see what is inside z, let’s convert it to a list.\n\nlist(z)\n\n[(2013, 9.607), (2014, 10.48), (2015, 11.06)]\n\n\nNotice that we now have a list where each item is a tuple.\nWithin each tuple, we have one item from each of the collections we passed to the zip function.\nIn particular, the first item in z contains the first item from [2013, 2014, 2015] and the first item from [9.607, 10.48, 11.06].\nThe second item in z contains the second item from each collection and so on.\nWe can access an element in this and then unpack the resulting tuple directly into variables.\n\nl = list(zip(years, gdp_data))\nx, y = l[0]\nprint(f\"year = {x}, GDP = {y}\")\n\nNow let’s experiment with enumerate.\n\ne = enumerate([\"a\", \"b\", \"c\"])\nprint(\"type(e)\", type(e))\ne\n\ntype(e) &lt;class 'enumerate'&gt;\n\n\n&lt;enumerate at 0x7fef3c8a01c0&gt;\n\n\nAgain, we call list(e) to see what is inside.\n\nlist(e)\n\n[(0, 'a'), (1, 'b'), (2, 'c')]\n\n\nWe again have a list of tuples, but this time, the first element in each tuple is the index of the second tuple element in the initial collection.\nNotice that the third item is (2, 'c') because [\"a\", \"b\", \"c\"][2] is 'c'\n\n\n\nSee exercise 4 in the exercise list.\nAn important quirk of some iterable types that are not lists (such as the above zip) is that you cannot convert the same type to a list twice.\nThis is because zip, enumerate, and range produce what is called a generator.\nA generator will only produce each of its elements a single time, so if you call list on the same generator a second time, it will not have any elements to iterate over anymore.\nFor more information, refer to the Python documentation.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nz = zip(years, gdp_data)\nl = list(z)\nprint(l)\nm = list(z)\nprint(m)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections.html#associative-collections",
    "href": "tutorials/session_1/qe_collections.html#associative-collections",
    "title": "Collections",
    "section": "",
    "text": "A dictionary (or dict) associates keys with values.\nIt will feel similar to a dictionary for words, where the keys are words and the values are the associated definitions.\nThe most common way to create a dict is to use curly braces — { and } — like this:\n\n{\n    \"key1\": value1,\n    \"key2\": value2,\n    \"keyN\": valueN\n}\n\nwhere the ... indicates that we can have any number of additional terms.\nThe crucial part of the syntax is that each key-value pair is written key: value and that these pairs are separated by commas — ,.\nLet’s see an example using our aggregate data on China in 2015.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\nUnlike our above example using a tuple, a dict allows us to associate a name with each field, rather than having to remember the order within the tuple.\nOften, code that makes a dict is easier to read if we put each key: value pair on its own line. (Recall our earlier comment on using whitespace effectively to improve readability!)\nThe code below is equivalent to what we saw above.\n\nchina_data = {\n    \"country\": \"China\",\n    \"year\": 2015,\n    \"GDP\" : 11.06,\n    \"population\": 1.371\n}\n\nMost often, the keys (e.g. “country”, “year”, “GDP”, and “population”) will be strings, but we could also use numbers (int, or float) or even tuples (or, rarely, a combination of types).\nThe values can be any type and different from each other.\n\n\n\nSee exercise 5 in the exercise list.\nThis next example is meant to emphasize how values can be anything – including another dictionary.\n\ncompanies = {\n    \"AAPL\": {\"bid\": 175.96, \"ask\": 175.98}, \n    \"GE\": {\"bid\": 1047.03, \"ask\": 1048.40},\n    \"TVIX\": {\"bid\": 8.38, \"ask\": 8.40}\n}\nprint(companies)\n\n{'AAPL': {'bid': 175.96, 'ask': 175.98}, 'GE': {'bid': 1047.03, 'ask': 1048.4}, 'TVIX': {'bid': 8.38, 'ask': 8.4}}\n\n\n\n\nWe can now ask Python to tell us the value for a particular key by using the syntax d[k], where d is our dict and k is the key for which we want to find the value.\nFor example,\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nchina_data['year']\n\n2015\n\n\n\n\nprint(\n    f\"country = {china_data['country']}, population = {china_data['population']}\")\n\ncountry = China, population = 1.371\n\n\nNote: when inside of a formatting string, you can use ' instead of \" as above to ensure the formatting still works with the embedded code.\nIf we ask for the value of a key that is not in the dict, we will get an error.\n\n# uncomment the line below to see the error\n# china_data[\"inflation\"]\n\nWe can also add new items to a dict using the syntax d[new_key] = new_value.\nLet’s see some examples.\n\nchina_data\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371}\n\n\n\nchina_data[\"unemployment\"] = \"4.05%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n\n\nTo update the value, we use assignment in the same way (which will create the key and value as required).\n\nprint(china_data)\nchina_data[\"unemployment\"] = \"4.051%\"\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.05%'}\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': '4.051%'}\n\n\nOr we could change the type.\n\nchina_data[\"unemployment\"] = False or True\nprint(china_data)\n\n{'country': 'China', 'year': 2015, 'GDP': 11.06, 'population': 1.371, 'unemployment': True}\n\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nWe can do some common things with dicts.\nWe will demonstrate them with examples below.\n\n# number of key-value pairs in a dict\nlen(china_data)\n\n5\n\n\n\n# get a list of all the keys\nlist(china_data.keys())\n\n['country', 'year', 'GDP', 'population', 'unemployment']\n\n\n\n# get a list of all the values\nlist(china_data.values())\n\n['China', 2015, 11.06, 1.371, True]\n\n\n\nmore_china_data = {\n    \"irrigated_land\": 690_070,\n    \"top_religions\":\n        {\"buddhist\": 18.2, \"christian\" : 5.1, \"muslim\": 1.8}\n}\n\n# Add all key-value pairs in mydict2 to mydict.\n# if the key already appears in mydict, overwrite the\n# value with the value in mydict2\nchina_data.update(more_china_data)\nchina_data\n\n{'country': 'China',\n 'year': 2015,\n 'GDP': 11.06,\n 'population': 1.371,\n 'unemployment': True,\n 'irrigated_land': 690070,\n 'top_religions': {'buddhist': 18.2, 'christian': 5.1, 'muslim': 1.8}}\n\n\n\ntype(china_data.get(\"book\"))\n\nNoneType\n\n\n\n# Get the value associated with a key or return a default value\n# use this to avoid the NameError we saw above if you have a reasonable\n# default value\nchina_data.get(\"irrigated_land\", \"Data Not Available\")\n\n690070\n\n\n\nchina_data.get(\"book\", \"Data Not Available\")\n\n'Data Not Available'\n\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\n\nPython has an additional way to represent collections of items: sets.\nSets come up infrequently, but you should be aware of them.\nIf you are familiar with the mathematical concept of sets, then you will understand the majority of Python sets already.\nIf you don’t know the math behind sets, don’t worry: we’ll cover the basics of Python’s sets here.\nA set is an unordered collection of unique elements.\nThe syntax for creating a set uses curly bracket { and }.\n\n{item1, item2, ..., itemN}\n\nHere is an example.\n\ns = {1, \"hello\", 3.0}\nprint(\"s has type\", type(s))\ns\n\n\n\n\nSee exercise 9 in the exercise list.\nAs with lists and tuples, we can check if something is in the set and check the set’s length:\n\nprint(\"len(s) =\", len(s))\n\"hello\" in s\n\nUnlike lists and tuples, we can’t extract elements of a set s using s[N] where N is a number.\n\n# Uncomment the line below to see what happens\n# s[1]\n\nThis is because sets are not ordered, so the notion of getting the second element (s[1]) is not well defined.\nWe add elements to a set s using s.add.\n\ns.add(100)\ns\n\n\ns.add(\"hello\") # nothing happens, why?\ns\n\nWe can also do set operations.\nConsider the set s from above and the set s2 = {\"hello\", \"world\"}.\n\ns.union(s2): returns a set with all elements in either s or s2\n\ns.intersection(s2): returns a set with all elements in both s and s2\n\ns.difference(s2): returns a set with all elements in s that aren’t in s2\n\ns.symmetric_difference(s2): returns a set with all elements in only one of s and s2\n\n\n\n\nSee exercise 10 in the exercise list.\nAs with tuples and lists, a set function can convert other collections to sets.\n\nx = [1, 2, 3, 1]\nset(x)\n\n\nt = (1, 2, 3, 1)\nset(t)\n\nLikewise, we can convert sets to lists and tuples.\n\nlist(s)\n\n\ntuple(s)"
  },
  {
    "objectID": "tutorials/session_1/qe_collections.html#exercises",
    "href": "tutorials/session_1/qe_collections.html#exercises",
    "title": "Collections",
    "section": "",
    "text": "In the first cell, try y.append(z).\nIn the second cell try y.extend(z).\nExplain the behavior.\nWhen you are trying to explain use y.append? and y.extend? to see a description of what these methods are supposed to do.\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\n# \n\n['a', 'b', 'c', [1, 2, 3]]\n\n\n\ny = [\"a\", \"b\", \"c\"]\nz = [1, 2, 3]\n# \n\n['a', 'b', 'c', 1, 2, 3]\n\n\n(back to text)\n\n\n\nExperiment with the other two versions of the range function.\n\na = 2\nN = 5\n\n# (range(a, N))\nlist(range(a, N))\n\n[2, 3, 4]\n\n\n\nrange?\n\n\nInit signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     \n\n\n\n\nd = 7\nlist(range(a, N, d)) \n\n[2]\n\n\n\nlist(range(0, 99, 2)) ;\n\n(back to text)\n\n[*range(0, 21, 5)]\n\n[0, 5, 10, 15, 20]\n\n\n\n\n\nVerify that tuples are indeed immutable by attempting the following:\n\nChanging the first element of t to be 100\n\nAppending a new element \"!!\" to the end of t (remember with a list x we would use x.append(\"!!\") to do this\n\nSorting t\n\nReversing t\n\n\nt=(100,2,3,4)\n\n\nt+('!',)\n\n(100, 2, 3, 4, '!')\n\n\n\n# sorting t\nsorted(t)\n\n[2, 3, 4, 100]\n\n\n\n# reversing t\nt[::-1]  # from stackoverflow https://stackoverflow.com/questions/10201977/how-to-reverse-tuples-in-python\n\n(4, 3, 2, 100)\n\n\n(back to text)\n\n\n\nChallenging For the tuple foo below, use a combination of zip, range, and len to mimic enumerate(foo).\nVerify that your proposed solution is correct by converting each to a list and checking equality with ==.\nYou can see what the answer should look like by starting with list(enumerate(foo)).\n\nfoo = (\"good\", \"luck!\")\n\n(back to text)\n\n\n\nCreate a new dict which associates stock tickers with its stock price.\nHere are some tickers and a price.\n\nAAPL: 175.96\n\nGOOGL: 1047.43\n\nTVIX: 8.38\n\n\n# your code here\n\n(back to text)\n\n\n\nLook at the World Factbook for Australia and create a dictionary with data containing the following types: float, string, integer, list, and dict. Choose any data you wish.\nTo confirm, you should have a dictionary that you identified via a key.\n\n# your code here\n\n(back to text)\n\n\n\nUse Jupyter’s help facilities to learn how to use the pop method to remove the key \"irrigated_land\" (and its value) from the dict.\n\n# uncomment and use the Inspector or ?\n#china_data.pop()\n\n(back to text)\n\n\n\nExplain what happens to the value you popped.\nExperiment with calling pop twice.\n\n# your code here\n\n(back to text)\n\n\n\nTry creating a set with repeated elements (e.g. {1, 2, 1, 2, 1, 2}).\nWhat happens?\nWhy?\n\n# your code here\n\n(back to text)\n\n\n\nTest out two of the operations described above using the original set we created, s, and the set created below s2.\n\ns2 = {\"hello\", \"world\"}\n\n\n# Operation 1\n\n\n# Operation 2\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html",
    "href": "tutorials/session_1/qe_functions_answers.html",
    "title": "Functions",
    "section": "",
    "text": "Prerequisites\n\nBasics\n\nCollections\n\nControl Flow\n\nOutcomes\n\nEconomic Production Functions\n\nUnderstand the basics of production functions in economics\n\n\nFunctions\n\nKnow how to define your own function\n\nKnow how to find and write your own function documentation\n\nKnow why we use functions\n\nUnderstand scoping rules and blocks\n\n\n\n\n\nProduction functions are useful when modeling the economics of firms producing goods or the aggregate output in an economy.\nThough the term “function” is used in a mathematical sense here, we will be making tight connections between the programming of mathematical functions and Python functions.\n\n\nThe factors of production are the inputs used in the production of some sort of output.\nSome example factors of production include\n\nPhysical capital, e.g. machines, buildings, computers, and power stations.\n\nLabor, e.g. all of the hours of work from different types of employees of a firm.\n\nHuman Capital, e.g. the knowledge of employees within a firm.\n\nA production function maps a set of inputs to the output, e.g. the amount of wheat produced by a farm, or widgets produced in a factory.\nAs an example of the notation, we denote the total units of labor and physical capital used in a factory as $ L $ and $ K $ respectively.\nIf we denote the physical output of the factory as $ Y $, then a production function $ F $ that transforms labor and capital into output might have the form:\n\\[\nY = F(K, L)\n\\]\n\n\n\n\nThroughout this lecture, we will use the Cobb-Douglas production function to help us understand how to create Python functions and why they are useful.\nThe Cobb-Douglas production function has appealing statistical properties when brought to data.\nThis function is displayed below.\n\\[\nY = z K^{\\alpha} L^{1-\\alpha}\n\\]\nThe function is parameterized by:\n\nA parameter $ $, called the “output elasticity of capital”.\n\nA value $ z $ called the Total Factor Productivity (TFP).\n\n\n\n\n\nIn this class, we will often talk about functions.\nSo what is a function?\nWe like to think of a function as a production line in a manufacturing plant: we pass zero or more things to it, operations take place in a set linear sequence, and zero or more things come out.\nWe use functions for the following purposes:\n\nRe-usability: Writing code to do a specific task just once, and reuse the code by calling the function.\n\nOrganization: Keep the code for distinct operations separated and organized.\n\nSharing/collaboration: Sharing code across multiple projects or sharing pieces of code with collaborators.\n\n\n\n\nThe basic syntax to create our own function is as follows:\n\ndef function_name(inputs):\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nHere we see two new keywords: def and return.\n\ndef is used to tell Python we would like to define a new function.\n\nreturn is used to tell Python what we would like to return from a function.\n\nLet’s look at an example and then discuss each part:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n\n    return answer\n\nHere we defined a function mean that has one input (numbers), does three steps, and has one output (answer).\nLet’s see what happens when we call this function on the list of numbers [1, 2, 3, 4].\n\nx = [1, 2, 3, 4]\nthe_mean = mean(x)\nthe_mean\n\nAdditionally, as we saw in the control flow lecture, indentation controls blocks of code (along with the scope rules).\nTo see this, compare a function with no inputs or return values.\n\ndef f():\n    print(\"1\")\n    print(\"2\")\nf()\n\nWith the following change of indentation…\n\ndef f():\n    print(\"1\")\nprint(\"2\")\nf()\n\n\n\n\nNotice that we named the input to the function x and we called the output the_mean.\nWhen we defined the function, the input was called numbers and the output answer… what gives?\nThis is an example of a programming concept called variable scope.\nIn Python, functions define their own scope for variables.\nIn English, this means that regardless of what name we give an input variable (x in this example), the input will always be referred to as numbers inside the body of the mean function.\nIt also means that although we called the output answer inside of the function mean, that this variable name was only valid inside of our function.\nTo use the output of the function, we had to give it our own name (the_mean in this example).\nAnother point to make here is that the intermediate variables we defined inside mean (total and N) are only defined inside of the mean function – we can’t access them from outside. We can verify this by trying to see what the value of total is:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n    return answer # or directly return total / N\n\n# uncomment the line below and execute to see the error\n# total\n\nThis point can be taken even further: the same name can be bound to variables inside of blocks of code and in the outer “scope”.\n\nx = 4\nprint(f\"x = {x}\")\ndef f():\n    x = 5 # a different \"x\"\n    print(f\"x = {x}\")\nf() # calls function\nprint(f\"x = {x}\")\n\nThe final point we want to make about scope is that function inputs and output don’t have to be given a name outside the function.\n\nmean([10, 20, 30])\n\nNotice that we didn’t name the input or the output, but the function was called successfully.\nNow, we’ll use our new knowledge to define a function which computes the output from a Cobb-Douglas production function with parameters $ z = 1 $ and $ = 0.33 $ and takes inputs $ K $ and $ L $.\n\ndef cobb_douglas(K, L):\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K**alpha * L**(1 - alpha)\n\nWe can use this function as we did the mean function.\n\ncobb_douglas(1.0, 0.5)\n\n\n\n\n\nEconomists are often interested in this question: how much does output change if we modify our inputs?\nFor example, take a production function $ Y_1 = F(K_1,L_1) $ which produces $ Y_1 $ units of the goods.\nIf we then multiply the inputs each by $ \\(, so that\\) K_2 = K_1 $ and $ L_2 = L_1 $, then the output is\n\\[\nY_2 = F(K_2, L_2) = F(\\gamma K_1, \\gamma L_1)\n\\]\nHow does $ Y_1 $ compare to $ Y_2 $?\nAnswering this question involves something called returns to scale.\nReturns to scale tells us whether our inputs are more or less productive as we have more of them.\nFor example, imagine that you run a restaurant. How would you expect the amount of food you could produce would change if you could build an exact replica of your restaurant and kitchen and hire the same number of cooks and waiters? You would probably expect it to double.\nIf, for any $ K, L $, we multiply $ K, L $ by a value $ $ then\n\nIf $ &lt; $ then we say the production function has decreasing returns to scale.\n\nIf $ = $ then we say the production function has constant returns to scale.\n\nIf $ &gt; $ then we say the production function has increasing returns to scale.\n\nLet’s try it and see what our function is!\n\ny1 = cobb_douglas(1.0, 0.5)\nprint(y1)\ny2 = cobb_douglas(2*1.0, 2*0.5)\nprint(y2)\n\nHow did $ Y_1 $ and $ Y_2 $ relate?\n\ny2 / y1\n\n$ Y_2 $ was exactly double $ Y_1 $!\nLet’s write a function that will compute the returns to scale for different values of $ K $ and $ L $.\nThis is an example of how writing functions can allow us to re-use code in ways we might not originally anticipate. (You didn’t know we’d be writing a returns_to_scale function when we wrote cobb_douglas.)\n\ndef returns_to_scale(K, L, gamma):\n    y1 = cobb_douglas(K, L)\n    y2 = cobb_douglas(gamma*K, gamma*L)\n    y_ratio = y2 / y1\n    return y_ratio / gamma\n\n\nreturns_to_scale(1.0, 0.5, 2.0)\n\n\n\n\nSee exercise 1 in the exercise list.\nIt turns out that with a little bit of algebra, we can check that this will always hold for our Cobb-Douglas example above.\nTo show this, take an arbitrary $ K, L $ and multiply the inputs by an arbitrary $ $.\n\\[\n\\begin{aligned}\n    F(\\gamma K, \\gamma L) &= z (\\gamma K)^{\\alpha} (\\gamma L)^{1-\\alpha}\\\\\n    &=  z \\gamma^{\\alpha}\\gamma^{1-\\alpha} K^{\\alpha} L^{1-\\alpha}\\\\\n    &= \\gamma z K^{\\alpha} L^{1-\\alpha} = \\gamma F(K, L)\n\\end{aligned}\n\\]\nFor an example of a production function that is not CRS, look at a generalization of the Cobb-Douglas production function that has different “output elasticities” for the 2 inputs.\n\\[\nY = z K^{\\alpha_1} L^{\\alpha_2}\n\\]\nNote that if $ _2 = 1 - _1 $, this is our Cobb-Douglas production function.\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\n\nAnother valuable element to analyze on production functions is how output changes as we change only one of the inputs. We will call this the marginal product.\nFor example, compare the output using $ K, L $ units of inputs to that with an $ $ units of labor.\nThen the marginal product of labor (MPL) is defined as\n\\[\n\\frac{F(K, L + \\varepsilon) - F(K, L)}{\\varepsilon}\n\\]\nThis tells us how much additional output is created relative to the additional input. (Spoiler alert: This should look like the definition for a partial derivative!)\nIf the input can be divided into small units, then we can use calculus to take this limit, using the partial derivative of the production function relative to that input.\nIn this case, we define the marginal product of labor (MPL) and marginal product of capital (MPK) as\n\\[\n\\begin{aligned}\nMPL(K, L) &= \\frac{\\partial F(K, L)}{\\partial L}\\\\\nMPK(K, L) &= \\frac{\\partial F(K, L)}{\\partial K}\n\\end{aligned}\n\\]\nIn the Cobb-Douglas example above, this becomes\n\\[\n\\begin{aligned}\nMPK(K, L) &= z  \\alpha \\left(\\frac{K}{L} \\right)^{\\alpha - 1}\\\\\nMPL(K, L) &= (1-\\alpha) z \\left(\\frac{K}{L} \\right)^{\\alpha}\\\\\n\\end{aligned}\n\\]\nLet’s test it out with Python! We’ll also see that we can actually return multiple things in a Python function.\nThe syntax for a return statement with multiple items is return item1, item2, ….\nIn this case, we’ll compute both the MPL and the MPK and then return both.\n\ndef marginal_products(K, L, epsilon):\n\n    mpl = (cobb_douglas(K, L + epsilon) - cobb_douglas(K, L)) / epsilon\n    mpk = (cobb_douglas(K + epsilon, L) - cobb_douglas(K, L)) / epsilon\n\n    return mpl, mpk\n\n\ntup = marginal_products(1.0, 0.5,  1e-4)\nprint(tup)\n\nInstead of using the tuple, these can be directly unpacked to variables.\n\nmpl, mpk = marginal_products(1.0, 0.5,  1e-4)\nprint(f\"mpl = {mpl}, mpk = {mpk}\")\n\nWe can use this to calculate the marginal products for different K, fixing L using a comprehension.\n\nKs = [1.0, 2.0, 3.0]\n[marginal_products(K, 0.5, 1e-4) for K in Ks] # create a tuple for each K\n\n\n\n\nIn a previous exercise, we asked you to find help for the cobb_douglas and returns_to_scale functions using ?.\nIt didn’t provide any useful information.\nTo provide this type of help information, we need to add what Python programmers call a “docstring” to our functions.\nThis is done by putting a string (not assigned to any variable name) as the first line of the body of the function (after the line with def).\nBelow is a new version of the template we used to define functions.\n\ndef function_name(inputs):\n    \"\"\"\n    Docstring\n    \"\"\"\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nLet’s re-define our cobb_douglas function to include a docstring.\n\ndef cobb_douglas(K, L):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n\n    We restrict z = 1 and alpha = 0.33\n    \"\"\"\n    return 1.0 * K**(0.33) * L**(1.0 - 0.33)\n\nNow when we have Jupyter evaluate cobb_douglas?, our message is displayed (or use the Contextual Help window with Jupyterlab and Ctrl-I or Cmd-I).\n\ncobb_douglas?\n\nWe recommend that you always include at least a very simple docstring for nontrivial functions.\nThis is in the same spirit as adding comments to your code — it makes it easier for future readers/users (including yourself) to understand what the code does.\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nFunctions can have optional arguments.\nTo accomplish this, we must these arguments a default value by saying name=default_value instead of just name as we list the arguments.\nTo demonstrate this functionality, let’s now make $ z $ and $ $ arguments to our cobb_douglas function!\n\ndef cobb_douglas(K, L, alpha=0.33, z=1):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n    \"\"\"\n    return z * K**(alpha) * L**(1.0 - alpha)\n\nWe can now call this function by passing in just K and L. Notice that it will produce same result as earlier because alpha and z are the same as earlier.\n\ncobb_douglas(1.0, 0.5)\n\nHowever, we can also set the other arguments of the function by passing more than just K/L.\n\ncobb_douglas(1.0, 0.5, 0.35, 1.6)\n\nIn the example above, we used alpha = 0.35, z = 1.6.\nWe can also refer to function arguments by their name, instead of only their position (order).\nTo do this, we would write func_name(arg=value) for as many of the arguments as we want.\nHere’s how to do that with our cobb_douglas example.\n\ncobb_douglas(1.0, 0.5, z = 1.5)\n\n\n\n\nSee exercise 4 in the exercise list.\nIn terms of variable scope, the z name within the function is different from any other z in the outer scope.\nTo be clear,\n\nx = 5\ndef f(x):\n    return x\nf(x) # \"coincidence\" that it has the same name\n\nThis is also true with named function arguments, above.\n\nz = 1.5\ncobb_douglas(1.0, 0.5, z = z) # no problem!\n\nIn that example, the z on the left hand side of z = z refers to the local variable name in the function whereas the z on the right hand side refers to the z in the outer scope.\n\n\n\nAs we learned earlier, all variables in Python have a type associated with them.\nDifferent types of variables have different functions or operations defined for them.\nFor example, I can divide one number by another or make a string uppercase.\nIt wouldn’t make sense to divide one string by another or make a number uppercase.\nWhen certain functionality is closely tied to the type of an object, it is often implemented as a special kind of function known as a method.\nFor now, you only need to know two things about methods:\n\nWe call them by doing variable.method_name(other_arguments) instead of function_name(variable, other_arguments).\n\nA method is a function, even though we call it using a different notation.\n\nWhen we introduced the core data types, we saw many methods defined on these types.\nLet’s revisit them for the str, or string type.\nNotice that we call each of these functions using the dot syntax described above.\n\ns = \"This is my handy string!\"\n\n\ns.upper()\n\n\ns.title()\n\n\n\n\n\nKeep in mind that with mathematical functions, the arguments are just dummy names that can be interchanged.\nThat is, the following are identical.\n\\[\n\\begin{eqnarray}\n    f(K, L) &= z\\, K^{\\alpha} L^{1-\\alpha}\\\\\n    f(K_2, L_2) &= z\\, K_2^{\\alpha} L_2^{1-\\alpha}\n\\end{eqnarray}\n\\]\nThe same concept applies to Python functions, where the arguments are just placeholder names, and our cobb_douglas function is identical to\n\ndef cobb_douglas2(K2, L2): # changed dummy variable names\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K2**alpha * L2**(1 - alpha)\n\ncobb_douglas2(1.0, 0.5)\n\nThis is an appealing feature of functions for avoiding coding errors: names of variables within the function are localized and won’t clash with those on the outside (with more examples in scope).\nImportantly, when Python looks for variables matching a particular name, it begins in the most local scope.\nThat is, note that having an alpha in the outer scope does not impact the local one.\n\ndef cobb_douglas3(K, L, alpha): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    return z * K**alpha * L**(1 - alpha) # sees local argument alpha\n\nprint(cobb_douglas3(1.0, 0.5, 0.2))\nprint(\"Setting alpha, does the result change?\")\nalpha = 0.5 # in the outer scope\nprint(cobb_douglas3(1.0, 0.5, 0.2))\n\nA crucial element of the above function is that the alpha variable was available in the local scope of the function.\nConsider the alternative where it is not. We have removed the alpha function parameter as well as the local definition of alpha.\n\ndef cobb_douglas4(K, L): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    # there are no local alpha in scope!\n    return z * K**alpha * L**(1 - alpha)\n\nalpha = 0.2 # in the outer scope\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\nalpha = 0.3\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\n\nThe intuition of scoping does not apply only for the “global” vs. “function” naming of variables, but also for nesting.\nFor example, we can define a version of cobb_douglas which is also missing a z in its inner-most scope, then put the function inside of another function.\n\nz = 1\ndef output_given_alpha(alpha):\n    # Scoping logic:\n    # 1. local function name doesn't clash with global one\n    # 2. alpha comes from the function parameter\n    # 3. z comes from the outer global scope\n    def cobb_douglas(K, L):\n        return z * K**alpha * L**(1 - alpha)\n\n    # using this function\n    return cobb_douglas(1.0, 0.5)\n\nalpha = 100 # ignored\nalphas = [0.2, 0.3, 0.5]\n# comprehension variables also have local scope\n# and don't clash with the alpha = 100\n[output_given_alpha(alpha) for alpha in alphas]\n\n\n\n\n\n\n\nWhat happens if we try different inputs in our Cobb-Douglas production function?\n\n# Compute returns to scale with different values of `K` and `L` and `gamma`\n\n(back to text)\n\n\n\nDefine a function named var that takes a list (call it x) and computes the variance. This function should use the mean function that we defined earlier.\n$ = _i (x_i - (x))^2 $\n\n# Your code here.\n\n(back to text)\n\n\n\nRedefine the returns_to_scale function and add a docstring.\nConfirm that it works by running the cell containing returns_to_scale? below.\nNote: You do not need to change the actual code in the function — just copy/paste and add a docstring in the correct line.\n\n# re-define the `returns_to_scale` function here\n\n\n# test it here\n\nreturns_to_scale?\n\n(back to text)\n\n\n\nExperiment with the sep and end arguments to the print function.\nThese can only be set by name.\n\n# Your code here.\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html#application-production-functions",
    "href": "tutorials/session_1/qe_functions_answers.html#application-production-functions",
    "title": "Functions",
    "section": "",
    "text": "Production functions are useful when modeling the economics of firms producing goods or the aggregate output in an economy.\nThough the term “function” is used in a mathematical sense here, we will be making tight connections between the programming of mathematical functions and Python functions.\n\n\nThe factors of production are the inputs used in the production of some sort of output.\nSome example factors of production include\n\nPhysical capital, e.g. machines, buildings, computers, and power stations.\n\nLabor, e.g. all of the hours of work from different types of employees of a firm.\n\nHuman Capital, e.g. the knowledge of employees within a firm.\n\nA production function maps a set of inputs to the output, e.g. the amount of wheat produced by a farm, or widgets produced in a factory.\nAs an example of the notation, we denote the total units of labor and physical capital used in a factory as $ L $ and $ K $ respectively.\nIf we denote the physical output of the factory as $ Y $, then a production function $ F $ that transforms labor and capital into output might have the form:\n\\[\nY = F(K, L)\n\\]\n\n\n\n\nThroughout this lecture, we will use the Cobb-Douglas production function to help us understand how to create Python functions and why they are useful.\nThe Cobb-Douglas production function has appealing statistical properties when brought to data.\nThis function is displayed below.\n\\[\nY = z K^{\\alpha} L^{1-\\alpha}\n\\]\nThe function is parameterized by:\n\nA parameter $ $, called the “output elasticity of capital”.\n\nA value $ z $ called the Total Factor Productivity (TFP)."
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html#what-are-python-functions",
    "href": "tutorials/session_1/qe_functions_answers.html#what-are-python-functions",
    "title": "Functions",
    "section": "",
    "text": "In this class, we will often talk about functions.\nSo what is a function?\nWe like to think of a function as a production line in a manufacturing plant: we pass zero or more things to it, operations take place in a set linear sequence, and zero or more things come out.\nWe use functions for the following purposes:\n\nRe-usability: Writing code to do a specific task just once, and reuse the code by calling the function.\n\nOrganization: Keep the code for distinct operations separated and organized.\n\nSharing/collaboration: Sharing code across multiple projects or sharing pieces of code with collaborators."
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html#how-to-define-python-functions",
    "href": "tutorials/session_1/qe_functions_answers.html#how-to-define-python-functions",
    "title": "Functions",
    "section": "",
    "text": "The basic syntax to create our own function is as follows:\n\ndef function_name(inputs):\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nHere we see two new keywords: def and return.\n\ndef is used to tell Python we would like to define a new function.\n\nreturn is used to tell Python what we would like to return from a function.\n\nLet’s look at an example and then discuss each part:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n\n    return answer\n\nHere we defined a function mean that has one input (numbers), does three steps, and has one output (answer).\nLet’s see what happens when we call this function on the list of numbers [1, 2, 3, 4].\n\nx = [1, 2, 3, 4]\nthe_mean = mean(x)\nthe_mean\n\nAdditionally, as we saw in the control flow lecture, indentation controls blocks of code (along with the scope rules).\nTo see this, compare a function with no inputs or return values.\n\ndef f():\n    print(\"1\")\n    print(\"2\")\nf()\n\nWith the following change of indentation…\n\ndef f():\n    print(\"1\")\nprint(\"2\")\nf()\n\n\n\n\nNotice that we named the input to the function x and we called the output the_mean.\nWhen we defined the function, the input was called numbers and the output answer… what gives?\nThis is an example of a programming concept called variable scope.\nIn Python, functions define their own scope for variables.\nIn English, this means that regardless of what name we give an input variable (x in this example), the input will always be referred to as numbers inside the body of the mean function.\nIt also means that although we called the output answer inside of the function mean, that this variable name was only valid inside of our function.\nTo use the output of the function, we had to give it our own name (the_mean in this example).\nAnother point to make here is that the intermediate variables we defined inside mean (total and N) are only defined inside of the mean function – we can’t access them from outside. We can verify this by trying to see what the value of total is:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n    return answer # or directly return total / N\n\n# uncomment the line below and execute to see the error\n# total\n\nThis point can be taken even further: the same name can be bound to variables inside of blocks of code and in the outer “scope”.\n\nx = 4\nprint(f\"x = {x}\")\ndef f():\n    x = 5 # a different \"x\"\n    print(f\"x = {x}\")\nf() # calls function\nprint(f\"x = {x}\")\n\nThe final point we want to make about scope is that function inputs and output don’t have to be given a name outside the function.\n\nmean([10, 20, 30])\n\nNotice that we didn’t name the input or the output, but the function was called successfully.\nNow, we’ll use our new knowledge to define a function which computes the output from a Cobb-Douglas production function with parameters $ z = 1 $ and $ = 0.33 $ and takes inputs $ K $ and $ L $.\n\ndef cobb_douglas(K, L):\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K**alpha * L**(1 - alpha)\n\nWe can use this function as we did the mean function.\n\ncobb_douglas(1.0, 0.5)\n\n\n\n\n\nEconomists are often interested in this question: how much does output change if we modify our inputs?\nFor example, take a production function $ Y_1 = F(K_1,L_1) $ which produces $ Y_1 $ units of the goods.\nIf we then multiply the inputs each by $ \\(, so that\\) K_2 = K_1 $ and $ L_2 = L_1 $, then the output is\n\\[\nY_2 = F(K_2, L_2) = F(\\gamma K_1, \\gamma L_1)\n\\]\nHow does $ Y_1 $ compare to $ Y_2 $?\nAnswering this question involves something called returns to scale.\nReturns to scale tells us whether our inputs are more or less productive as we have more of them.\nFor example, imagine that you run a restaurant. How would you expect the amount of food you could produce would change if you could build an exact replica of your restaurant and kitchen and hire the same number of cooks and waiters? You would probably expect it to double.\nIf, for any $ K, L $, we multiply $ K, L $ by a value $ $ then\n\nIf $ &lt; $ then we say the production function has decreasing returns to scale.\n\nIf $ = $ then we say the production function has constant returns to scale.\n\nIf $ &gt; $ then we say the production function has increasing returns to scale.\n\nLet’s try it and see what our function is!\n\ny1 = cobb_douglas(1.0, 0.5)\nprint(y1)\ny2 = cobb_douglas(2*1.0, 2*0.5)\nprint(y2)\n\nHow did $ Y_1 $ and $ Y_2 $ relate?\n\ny2 / y1\n\n$ Y_2 $ was exactly double $ Y_1 $!\nLet’s write a function that will compute the returns to scale for different values of $ K $ and $ L $.\nThis is an example of how writing functions can allow us to re-use code in ways we might not originally anticipate. (You didn’t know we’d be writing a returns_to_scale function when we wrote cobb_douglas.)\n\ndef returns_to_scale(K, L, gamma):\n    y1 = cobb_douglas(K, L)\n    y2 = cobb_douglas(gamma*K, gamma*L)\n    y_ratio = y2 / y1\n    return y_ratio / gamma\n\n\nreturns_to_scale(1.0, 0.5, 2.0)\n\n\n\n\nSee exercise 1 in the exercise list.\nIt turns out that with a little bit of algebra, we can check that this will always hold for our Cobb-Douglas example above.\nTo show this, take an arbitrary $ K, L $ and multiply the inputs by an arbitrary $ $.\n\\[\n\\begin{aligned}\n    F(\\gamma K, \\gamma L) &= z (\\gamma K)^{\\alpha} (\\gamma L)^{1-\\alpha}\\\\\n    &=  z \\gamma^{\\alpha}\\gamma^{1-\\alpha} K^{\\alpha} L^{1-\\alpha}\\\\\n    &= \\gamma z K^{\\alpha} L^{1-\\alpha} = \\gamma F(K, L)\n\\end{aligned}\n\\]\nFor an example of a production function that is not CRS, look at a generalization of the Cobb-Douglas production function that has different “output elasticities” for the 2 inputs.\n\\[\nY = z K^{\\alpha_1} L^{\\alpha_2}\n\\]\nNote that if $ _2 = 1 - _1 $, this is our Cobb-Douglas production function.\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\n\nAnother valuable element to analyze on production functions is how output changes as we change only one of the inputs. We will call this the marginal product.\nFor example, compare the output using $ K, L $ units of inputs to that with an $ $ units of labor.\nThen the marginal product of labor (MPL) is defined as\n\\[\n\\frac{F(K, L + \\varepsilon) - F(K, L)}{\\varepsilon}\n\\]\nThis tells us how much additional output is created relative to the additional input. (Spoiler alert: This should look like the definition for a partial derivative!)\nIf the input can be divided into small units, then we can use calculus to take this limit, using the partial derivative of the production function relative to that input.\nIn this case, we define the marginal product of labor (MPL) and marginal product of capital (MPK) as\n\\[\n\\begin{aligned}\nMPL(K, L) &= \\frac{\\partial F(K, L)}{\\partial L}\\\\\nMPK(K, L) &= \\frac{\\partial F(K, L)}{\\partial K}\n\\end{aligned}\n\\]\nIn the Cobb-Douglas example above, this becomes\n\\[\n\\begin{aligned}\nMPK(K, L) &= z  \\alpha \\left(\\frac{K}{L} \\right)^{\\alpha - 1}\\\\\nMPL(K, L) &= (1-\\alpha) z \\left(\\frac{K}{L} \\right)^{\\alpha}\\\\\n\\end{aligned}\n\\]\nLet’s test it out with Python! We’ll also see that we can actually return multiple things in a Python function.\nThe syntax for a return statement with multiple items is return item1, item2, ….\nIn this case, we’ll compute both the MPL and the MPK and then return both.\n\ndef marginal_products(K, L, epsilon):\n\n    mpl = (cobb_douglas(K, L + epsilon) - cobb_douglas(K, L)) / epsilon\n    mpk = (cobb_douglas(K + epsilon, L) - cobb_douglas(K, L)) / epsilon\n\n    return mpl, mpk\n\n\ntup = marginal_products(1.0, 0.5,  1e-4)\nprint(tup)\n\nInstead of using the tuple, these can be directly unpacked to variables.\n\nmpl, mpk = marginal_products(1.0, 0.5,  1e-4)\nprint(f\"mpl = {mpl}, mpk = {mpk}\")\n\nWe can use this to calculate the marginal products for different K, fixing L using a comprehension.\n\nKs = [1.0, 2.0, 3.0]\n[marginal_products(K, 0.5, 1e-4) for K in Ks] # create a tuple for each K\n\n\n\n\nIn a previous exercise, we asked you to find help for the cobb_douglas and returns_to_scale functions using ?.\nIt didn’t provide any useful information.\nTo provide this type of help information, we need to add what Python programmers call a “docstring” to our functions.\nThis is done by putting a string (not assigned to any variable name) as the first line of the body of the function (after the line with def).\nBelow is a new version of the template we used to define functions.\n\ndef function_name(inputs):\n    \"\"\"\n    Docstring\n    \"\"\"\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nLet’s re-define our cobb_douglas function to include a docstring.\n\ndef cobb_douglas(K, L):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n\n    We restrict z = 1 and alpha = 0.33\n    \"\"\"\n    return 1.0 * K**(0.33) * L**(1.0 - 0.33)\n\nNow when we have Jupyter evaluate cobb_douglas?, our message is displayed (or use the Contextual Help window with Jupyterlab and Ctrl-I or Cmd-I).\n\ncobb_douglas?\n\nWe recommend that you always include at least a very simple docstring for nontrivial functions.\nThis is in the same spirit as adding comments to your code — it makes it easier for future readers/users (including yourself) to understand what the code does.\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nFunctions can have optional arguments.\nTo accomplish this, we must these arguments a default value by saying name=default_value instead of just name as we list the arguments.\nTo demonstrate this functionality, let’s now make $ z $ and $ $ arguments to our cobb_douglas function!\n\ndef cobb_douglas(K, L, alpha=0.33, z=1):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n    \"\"\"\n    return z * K**(alpha) * L**(1.0 - alpha)\n\nWe can now call this function by passing in just K and L. Notice that it will produce same result as earlier because alpha and z are the same as earlier.\n\ncobb_douglas(1.0, 0.5)\n\nHowever, we can also set the other arguments of the function by passing more than just K/L.\n\ncobb_douglas(1.0, 0.5, 0.35, 1.6)\n\nIn the example above, we used alpha = 0.35, z = 1.6.\nWe can also refer to function arguments by their name, instead of only their position (order).\nTo do this, we would write func_name(arg=value) for as many of the arguments as we want.\nHere’s how to do that with our cobb_douglas example.\n\ncobb_douglas(1.0, 0.5, z = 1.5)\n\n\n\n\nSee exercise 4 in the exercise list.\nIn terms of variable scope, the z name within the function is different from any other z in the outer scope.\nTo be clear,\n\nx = 5\ndef f(x):\n    return x\nf(x) # \"coincidence\" that it has the same name\n\nThis is also true with named function arguments, above.\n\nz = 1.5\ncobb_douglas(1.0, 0.5, z = z) # no problem!\n\nIn that example, the z on the left hand side of z = z refers to the local variable name in the function whereas the z on the right hand side refers to the z in the outer scope.\n\n\n\nAs we learned earlier, all variables in Python have a type associated with them.\nDifferent types of variables have different functions or operations defined for them.\nFor example, I can divide one number by another or make a string uppercase.\nIt wouldn’t make sense to divide one string by another or make a number uppercase.\nWhen certain functionality is closely tied to the type of an object, it is often implemented as a special kind of function known as a method.\nFor now, you only need to know two things about methods:\n\nWe call them by doing variable.method_name(other_arguments) instead of function_name(variable, other_arguments).\n\nA method is a function, even though we call it using a different notation.\n\nWhen we introduced the core data types, we saw many methods defined on these types.\nLet’s revisit them for the str, or string type.\nNotice that we call each of these functions using the dot syntax described above.\n\ns = \"This is my handy string!\"\n\n\ns.upper()\n\n\ns.title()"
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html#more-on-scope-optional",
    "href": "tutorials/session_1/qe_functions_answers.html#more-on-scope-optional",
    "title": "Functions",
    "section": "",
    "text": "Keep in mind that with mathematical functions, the arguments are just dummy names that can be interchanged.\nThat is, the following are identical.\n\\[\n\\begin{eqnarray}\n    f(K, L) &= z\\, K^{\\alpha} L^{1-\\alpha}\\\\\n    f(K_2, L_2) &= z\\, K_2^{\\alpha} L_2^{1-\\alpha}\n\\end{eqnarray}\n\\]\nThe same concept applies to Python functions, where the arguments are just placeholder names, and our cobb_douglas function is identical to\n\ndef cobb_douglas2(K2, L2): # changed dummy variable names\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K2**alpha * L2**(1 - alpha)\n\ncobb_douglas2(1.0, 0.5)\n\nThis is an appealing feature of functions for avoiding coding errors: names of variables within the function are localized and won’t clash with those on the outside (with more examples in scope).\nImportantly, when Python looks for variables matching a particular name, it begins in the most local scope.\nThat is, note that having an alpha in the outer scope does not impact the local one.\n\ndef cobb_douglas3(K, L, alpha): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    return z * K**alpha * L**(1 - alpha) # sees local argument alpha\n\nprint(cobb_douglas3(1.0, 0.5, 0.2))\nprint(\"Setting alpha, does the result change?\")\nalpha = 0.5 # in the outer scope\nprint(cobb_douglas3(1.0, 0.5, 0.2))\n\nA crucial element of the above function is that the alpha variable was available in the local scope of the function.\nConsider the alternative where it is not. We have removed the alpha function parameter as well as the local definition of alpha.\n\ndef cobb_douglas4(K, L): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    # there are no local alpha in scope!\n    return z * K**alpha * L**(1 - alpha)\n\nalpha = 0.2 # in the outer scope\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\nalpha = 0.3\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\n\nThe intuition of scoping does not apply only for the “global” vs. “function” naming of variables, but also for nesting.\nFor example, we can define a version of cobb_douglas which is also missing a z in its inner-most scope, then put the function inside of another function.\n\nz = 1\ndef output_given_alpha(alpha):\n    # Scoping logic:\n    # 1. local function name doesn't clash with global one\n    # 2. alpha comes from the function parameter\n    # 3. z comes from the outer global scope\n    def cobb_douglas(K, L):\n        return z * K**alpha * L**(1 - alpha)\n\n    # using this function\n    return cobb_douglas(1.0, 0.5)\n\nalpha = 100 # ignored\nalphas = [0.2, 0.3, 0.5]\n# comprehension variables also have local scope\n# and don't clash with the alpha = 100\n[output_given_alpha(alpha) for alpha in alphas]"
  },
  {
    "objectID": "tutorials/session_1/qe_functions_answers.html#exercises",
    "href": "tutorials/session_1/qe_functions_answers.html#exercises",
    "title": "Functions",
    "section": "",
    "text": "What happens if we try different inputs in our Cobb-Douglas production function?\n\n# Compute returns to scale with different values of `K` and `L` and `gamma`\n\n(back to text)\n\n\n\nDefine a function named var that takes a list (call it x) and computes the variance. This function should use the mean function that we defined earlier.\n$ = _i (x_i - (x))^2 $\n\n# Your code here.\n\n(back to text)\n\n\n\nRedefine the returns_to_scale function and add a docstring.\nConfirm that it works by running the cell containing returns_to_scale? below.\nNote: You do not need to change the actual code in the function — just copy/paste and add a docstring in the correct line.\n\n# re-define the `returns_to_scale` function here\n\n\n# test it here\n\nreturns_to_scale?\n\n(back to text)\n\n\n\nExperiment with the sep and end arguments to the print function.\nThese can only be set by name.\n\n# Your code here.\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html",
    "href": "tutorials/session_1/qe_control.html",
    "title": "Control Flow",
    "section": "",
    "text": "Prerequisites\n\nBooleans section in Basics\n\nCollections\n\nOutcomes\n\nAsset pricing and NPV\n\nUnderstand basic principles of pricing assets with deterministic payoffs\n\nApply programming with iteration and conditionals to asset pricing examples\n\n\nConditionals\n\nUnderstand what a conditional is\n\nBe able to construct if/elif/else conditional blocks\n\nUnderstand how conditionals can be used to selectively execute blocks of code\n\n\nIteration\n\nUnderstand what an iterable is\n\nBe able to write for and while loops\n\nUnderstand the keywords break and continue\n\n\n\n\n\nIn this lecture, we’ll introduce two related topics from economics:\n\nNet present valuations\n\nAsset pricing\n\nThese topics will motivate some of the programming we do in this course.\nIn economics and finance, “assets” provide a stream of payoffs.\nThese “assets” can be concrete or abstract: a stock pays dividends over time, a bond pays interest, an apple tree provides apples, a job pays wages, and an education provides possible jobs (which, in turn, pay wages).\nWhen deciding the price to pay for an asset or how to choose between different alternatives, we need to take into account that most people would prefer to receive 1 today vs. 1 next year.\nThis reflection on consumer preferences leads to the notion of a discount rate. If you are indifferent between receiving 1.00 today and 1.10 next year, then the discount rate over the next year is $ r = 0.10 $.\nIf we assume that an individuals preferences are consistent over time, then we can apply that same discount rate to valuing assets further into the future.\nFor example, we would expect that the consumer would be indifferent between consuming 1.00 today and $ (1+r)(1+r) = 1.21 $ dollars two years from now (i.e. discount twice).\nInverting this formula, 1 delivered two years from now is equivalent to $ $ today.\n\n\n\nSee exercise 1 in the exercise list.\n\n\n\nIf an asset pays a stream of payoffs over multiple time periods, then we can use a discount rate to calculate the value to the consumer of a entire sequence of payoffs.\nMost generally, we enumerate each discrete time period (e.g. year, month, day) by the index $ t $ where today is $ t=0 $ and the asset lives for $ T $ periods.\nList the payoff at each time period as $ y_t $, which we will assume, for now, is known in advance.\nThen if the discount factor is $ r $, the consumer “values” the payoff $ y_t $ delivered at time $ t $ as $ y_t $ where we note that if $ t=0 \\(, the value is just the current payoff\\) y_0 $.\nUsing this logic, we can write an expression for the value of the entire sequence of payoffs with a sum.\n \\[\nP_0 = \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t y_t \\tag{1}\n\\]\nIf $ y_t $ is a constant, then we can compute this sum with a simple formula!\nBelow, we present some useful formulas that come from infinite series that we will use to get our net present value formula.\nFor any constant $ 0 &lt; &lt; 1 $ and integer value $ &gt; 0 $,\n \\[\n\\begin{aligned}\n\\sum_{t=0}^{\\infty} \\beta^t & = \\frac{1}{1-\\beta}\\\\\n\\sum_{t=0}^{\\tau} \\beta^t &= \\frac{1- \\beta^{\\tau+1}}{1-\\beta}\\\\\n\\sum_{t=\\tau}^{\\infty} \\beta^t &=  \\frac{\\beta^{\\tau}}{1-\\beta}\n\\end{aligned} \\tag{2}\n\\]\nIn the case of an asset which pays one dollar until time $ T $, we can use these formulas, taking $ = $ and $ T = $, to find\n\\[\n\\begin{aligned}\nP_0 &= \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t = \\frac{1- (\\frac{1}{1+r})^{\\tau+1}}{1-\\frac{1}{1+r}}\\\\\n&= \\frac{1 + r}{r} - \\frac{1}{r}\\left(\\frac{1}{1+r} \\right)^\\tau\n\\end{aligned}\n\\]\nNote that we can also consider an asset that lives and pays forever if $ T= $, and from (2), the value of an asset which pays 1 forever is $ $.\n\n\n\n\nSometimes, we will only want to execute some piece of code if a certain condition is met.\nThese conditions can be anything.\nFor example, we might add to total sales if the transaction value is positive, but add to total returns if the value is negative.\nOr, we might want to add up all incurred costs, only if the transaction happened before a certain date.\nWe use conditionals to run particular pieces of code when certain criterion are met.\nConditionals are closely tied to booleans, so if you don’t remember what those are, go back to the basics lecture for a refresher.\nThe basic syntax for conditionals is\n\nif condition:\n        \n    # code to run when condition is True\nelse:\n    # code to run if no conditions above are True\n\nNote that immediately following the condition, there is a colon and that the next line begins with blank spaces.\nUsing 4 spaces is a very strong convention, so that is what we do — we recommend that you do the same.\nAlso note that the else clause is optional.\nLet’s see some simple examples.\n\nif True:\n    print(\"This is where `True` code is run\")\n\nThis is where `True` code is run\n\n\nAlternatively, you could have a test which returns a booleans\n\nif 1 &lt; 2:\n     print(\"This is where `True` code is run\")\n\nThis is where `True` code is run\n\n\nThis example is equivalent to just typing the print statement, but the example below isn’t…\n\nif False:\n    print(\"This is where `True` code is run\")\n\nOr\n\nif 1 &gt; 2:\n     print(\"This is where `True` code is run\")\n\nNotice that when you run the cells above nothing is printed.\nThat is because the condition for the if statement was not true, so the code inside the indented block was never run.\nThis also allows us to demonstrate the role of indentation in determining the “block” of code.\n\nval = False\n\nif val is True: # check an expression\n    print(\"This is where `True` code is run\")\n    print(\"More code in the if block\")\n    print(\"Code runs after 'if' block, regardless of val\")\n\n\n\n\nSee exercise 2 in the exercise list.\nThe next example shows us how else works.\n\nval = (2 == 4)  # returns False\nif val is True:\n    print(\"This is where `True` code is run\")\nelse:\n    print(\"This is where `False` code is run\")\n    print(\"More else code\")\nprint(\"Code runs after 'if' block, regardless of val\")\n\nThe if False: ... part of this example is the same as the example before, but now, we added an else: clause.\nIn this case, because the conditional for the if statement was not True, the if code block was not executed, but the else block was.\nFinally, the Condition is True is assumed in the if statement, and is often left out. For example, the following are identical\n\nif (1 &lt; 2) is True:\n    print(\"1 &lt; 2\")\n\nif 1 &lt; 2:\n    print(\"1 &lt; 2\")\n\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nSee exercise 4 in the exercise list.\n\n\nSometimes, you have more than one condition you want to check.\nFor example, you might want to run a different set of code based on which quarter a particular transaction took place in.\nIn this case you could check whether the date is in Q1, or in Q2, or in Q3, or if not any of these it must be in Q4.\nThe way to express this type of conditional is to use one or more elif clause in addition to the if and the else.\nThe syntax is\n\nif condition1:\n    # code to run when condition1 is True\nelif condition2:\n    # code to run when condition2 is True\nelif condition3:\n    # code to run when condition3 is True\nelse:\n    # code to run when none of the above are true\n\nYou can include as many elif clauses as you want.\nAs before, the else part is optional.\nHere’s how we might express the quarter example referred to above.\n\nimport datetime\nhalloween = datetime.date(2017, 10, 31)\n\nif halloween.month &gt; 9:\n    print(\"Halloween is in Q4\")\nelif halloween.month &gt; 6:\n    print(\"Halloween is in Q3\")\nelif halloween.month &gt; 3:\n    print(\"Halloween is in Q2\")\nelse:\n    print(\"Halloween is in Q1\")\n\nNote that when there are multiple if or elif conditions, only the code corresponding to the first true clause is run.\nWe saw this in action above.\nWe know that when halloween.month &gt; 9 is true, then halloween.month &gt; 6 and halloween.month &gt; 3 must also be true, but only the code block associated with halloween.month &gt; 9 was printed.\n\n\n\n\nWhen doing computations or analyzing data, we often need to repeat certain operations a finite number of times or until some condition is met.\nExamples include processing all data files in a directory (folder), aggregating revenues and costs for every period in a year, or computing the net present value of certain assets. (In fact, later in this section, we will verify the equations that we wrote down above.)\nThese are all examples of a programming concept called iteration.\nWe feel the concept is best understood through example, so we will present a contrived example and then discuss the details behind doing iteration in Python.\n\n\nSuppose we wanted to print out the first 10 integers and their squares.\nWe could do something like this.\n\nprint(f\"1**2  is: {1**2}\")\nprint(f\"2**2  is: {2**2}\")\nprint(f\"3**2  is: {3**2}\")\nprint(f\"4**2  is: {4**2}\")\n# .. and so on until 10\n\n1**2  is: 1\n2**2  is: 4\n3**2  is: 9\n4**2  is: 16\n\n\nAs you can see, the code above is repetitive.\nFor each integer, the code is exactly the same except for the two places where the “current” integer appears.\nSuppose that I asked you to write the same print statement for an int stored in a variable named i.\nYou might write the following code:\n\nprint(f\"{i}**2 = {i**2}\")\n\nThis more general version of the operation suggests a strategy for achieving our goal with less repetition: have a variable i take on the values 1 through 10 (Quiz: How can we use range to create the numbers 1 to 10?) and run the line of code above for each new value of i.\nThis can be accomplished with a for loop!\n\nfor i in range(1, 10):\n    print(f\"{i}**2 = {i**2}\")\n\n1**2 = 1\n2**2 = 4\n3**2 = 9\n4**2 = 16\n5**2 = 25\n6**2 = 36\n7**2 = 49\n8**2 = 64\n9**2 = 81\n\n\nWhoa, what just happened?\nThe integer i took on the values in range(1, 11) one by one and for each new value it did the operations in the indented block (here just one line that called the print function).\n\n\n\nThe general structure of a standard for loop is as follows.\n\nfor item in iterable:\n   # operation 1 with item\n   # operation 2 with item\n   # ...\n   # operation N with item\n\nwhere iterable is anything capable of producing one item at a time (see here for official definition from the Python team).\nWe’ve actually already seen some of the most common iterables!\nLists, tuples, dicts, and range/zip/enumerate objects are all iterables.\nNote that we can have as many operations as we want inside the indented block.\nWe will refer to the indented block as the “body” of the loop.\nWhen the for loop is executed, item will take on one value from iterable at a time and execute the loop body for each value.\n\n\n\n\nSee exercise 5 in the exercise list.\nWhen iterating, each item in iterable might actually contain more than one value.\nRecall that tuples (and lists) can be unpacked directly into variables.\n\ntup = (4, \"test\")\ni, x = tup\nprint(f\"i = {i}, x = {x}, tup = {tup}\")\n\nAlso, recall that the value of a enumerate(iterable) is a tuple of the form (i, x) where iterable[i] == x.\nWhen we use enumerate in a for loop, we can “unpack” both values at the same time as follows:\n\n# revenue by quarter\ncompany_revenue = [5.12, 5.20, 5.50, 6.50]\n\nfor index, value in enumerate(company_revenue):\n    print(f\"quarter {index} revenue is ${value} million\")\n\nSimilarly, the index can be used to access another vector.\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\nfor index, city in enumerate(cities):\n    state = states[index]\n    print(f\"{city} is in {state}\")\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nA related but slightly different form of iteration is to repeat something until some condition is met.\nThis is typically achieved using a while loop.\nThe structure of a while loop is\n\nwhile True_condition:\n    # repeat these steps\n\nwhere True_condition is some conditional statement that should evaluate to True when iterations should continue and False when Python should stop iterating.\nFor example, suppose we wanted to know the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nWe figure this out using a while loop as follows.\n\ntotal = 0\ni = 0\nwhile total &lt;= 1000:\n    i = i + 1\n    total = total + i\n\nprint(\"The answer is\", i)\n\nLet’s check our work.\n\n# Should be just less than 1000 because range(45) goes from 0 to 44\nsum(range(45))\n\n\n# should be between 990 + 45 = 1035\nsum(range(46))\n\nA warning: one common programming error with while loops is to forget to set the variable you use in the condition prior to executing. For example, take the following code which correctly sets a counter\n\ni = 0\n\nAnd then executes a while loop\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\nNo problems. But if you were to execute the above cell again, or another cell, the i=3 remains, and code is never executed (since i &lt; 3 begins as False).\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\n\n\nSometimes we want to stop a loop early if some condition is met.\nLet’s revisit the example of finding the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nClearly N must be less than 1000, so we know we will find the answer if we start with a for loop over all items in range(1001).\nThen, we can keep a running total as we proceed and tell Python to stop iterating through our range once total goes above 1000.\n\ntotal = 0\nfor i in range(1001):\n    total = total + i\n    if total &gt; 1000:\n        break\n\nprint(\"The answer is\", i)\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\nSometimes we might want to stop the body of a loop early if a condition is met.\nTo do this we can use the continue keyword.\nThe basic syntax for doing this is:\n\nfor item in iterable:\n    # always do these operations\n    if condition:\n        continue\n\n    # only do these operations if condition is False\n\nInside the loop body, Python will stop that loop iteration of the loop and continue directly to the next iteration when it encounters the continue statement.\nFor example, suppose I ask you to loop over the numbers 1 to 10 and print out the message “{i} An odd number!” whenever the number i is odd, and do nothing otherwise.\nYou can use continue to do this as follows:\n\nfor i in range(1, 11):\n    if i % 2 == 0:  # an even number... This is modulus division\n        continue\n\n    print(i, \"is an odd number!\")\n\n\n\n\nSee exercise 9 in the exercise list.\n\n\n\n\n\nOften, we will want to perform a very simple operation for every element of some iterable and create a new iterable with these values.\nThis could be done by writing a for loop and saving each value, but often using what is called a comprehension is more readable.\nLike many Python concepts, a comprehension is easiest to understand through example.\nImagine that we have a list x with a list of numbers. We would like to create a list x2 which has the squared values of x.\n\nx = list(range(4))\n\n# Create squared values with a loop\nx2_loop = []\nfor x_val in x:\n    x2_loop.append(x_val**2)\n\n# Create squared values with a comprehension\nx2_comp = [x_val**2 for x_val in x]\n\nprint(x2_loop)\nprint(x2_comp)\n\nNotice that much of the same text appears when we do the operation in the loop and when we do the operation with the comprehension.\n\nWe need to specify what we are iterating over – in both cases, this is for x_val in x.\n\nWe need to square each element x_val**2.\n\nIt needs to be stored somewhere – in x2_loop, this is done by appending each element to a list, and in x2_comp, this is done automatically because the operation is enclosed in a list.\n\nWe can do comprehension with many different types of iterables, so we demonstrate a few more below.\n\n# Create a dictionary from lists\ntickers = [\"AAPL\", \"GOOGL\", \"TVIX\"]\nprices = [175.96, 1047.43, 8.38]\nd = {key: value for key, value in zip(tickers, prices)}\nd\n\n\n# Create a list from a dictionary\nd = {\"AMZN\": \"Seattle\", \"TVIX\": \"Zurich\", \"AAPL\": \"Cupertino\"}\n\nhq_cities = [d[ticker] for ticker in d.keys()]\nhq_cities\n\n\nimport math\n\n# List from list\nx = range(10)\n\nsin_x = [math.sin(x_val) for x_val in x]\nsin_x\n\n\n\n\nSee exercise 10 in the exercise list.\nFinally, we can use this approach to build complicated nested dictionaries.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nexports = [ {\"manufacturing\": 2.4, \"agriculture\": 1.5, \"services\": 0.5},\n            {\"manufacturing\": 2.5, \"agriculture\": 1.4, \"services\": 0.9},\n            {\"manufacturing\": 2.7, \"agriculture\": 1.4, \"services\": 1.5}]\ndata = zip(years, gdp_data,exports)\ndata_dict = {year : {\"gdp\" : gdp, \"exports\": exports} for year, gdp, exports in data}\nprint(data_dict)\n\n# total exports by year\n[data_dict[year][\"exports\"][\"services\"] for year in data_dict.keys()]\n\n\n\n\n\n\n\nGovernment bonds are often issued as zero-coupon bonds meaning that they make no payments throughout the entire time that they are held, but, rather make a single payment at the time of maturity.\nHow much should you be willing to pay for a zero-coupon bond that paid 100 in 10 years with an interest rate of 5%?\n\n# your code here\n\n(back to text)\n\n\n\nRun the following two variations on the code with only a single change in the indentation.\nAfter, modify the x to print 3 and then 2, 3 instead.\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\n    print(\"2\")\nprint(\"3\")\n\n1\n2\n3\n\n\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\nprint(\"2\") # changed the indentation\nprint(\"3\")\n\n1\n2\n3\n\n\n(back to text)\n\n\n\nUsing the code cell below as a start, print \"Good afternoon\" if the current_time is past noon.\nOtherwise, do nothing.\nWrite some conditional based on current_time.hour.\n\nimport datetime\ncurrent_time = datetime.datetime.now()\n\n## your code here\n\nmore text after\n(back to text)\n\n\n\nIn this example, you will generate a random number between 0 and 1 and then display “x &gt; 0.5” or “x &lt; 0.5” depending on the value of the number.\nThis also introduces a new package numpy.random for drawing random numbers (more in the randomness lecture).\n\nimport numpy as np\nx = np.random.random()\nprint(f\"x = {x}\")\n\n## your code here\n\n(back to text)\n\n\n\nIn economics, when an individual has some knowledge, skills, or education which provides them with a source of future income, we call it human capital.\nWhen a student graduating from high school is considering whether to continue with post-secondary education, they may consider that it gives them higher paying jobs in the future, but requires that they don’t begin working until after graduation.\nConsider the simplified example where a student has perfectly forecastable employment and is given two choices:\n\nBegin working immediately and make 40,000 a year until they retire 40 years later.\n\nPay 5,000 a year for the next 4 years to attend university, then get a job paying 50,000 a year until they retire 40 years after making the college attendance decision.\n\nShould the student enroll in school if the discount rate is r = 0.05?\n\n# Discount rate\nr = 0.05\n\n# High school wage\nw_hs = 40_000\n\n# College wage and cost of college\nc_college = 5_000\nw_college = 50_000\n\n# Compute npv of being a hs worker\n\n# Compute npv of attending college\n\n# Compute npv of being a college worker\n\n# Is npv_collegeworker - npv_collegecost &gt; npv_hsworker\n\n(back to text)\n\n\n\nInstead of the above, write a for loop that uses the lists of cities and states below to print the same “{city} is in {state}” using a zip instead of an enumerate.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# Your code here\n\n(back to text)\n\n\n\nCompanies often invest in training their employees to raise their productivity. Economists sometimes wonder why companies spend this money when this incentivizes other companies to hire their employees away with higher salaries since employees gain human capital from training?\nLet’s say that it costs a company 25,000 dollars to teach their employees Python, but it raises their output by 2,500 per month. How many months would an employee need to stay for the company to find it profitable to pay for their employees to learn Python if their discount rate is r = 0.01?\n\n# Define cost of teaching python\ncost = 25_000\nr = 0.01\n\n# Per month value\nadded_value = 2500\n\nn_months = 0\ntotal_npv = 0.0\n\n# Put condition below here\nwhile False: # (replace False with your condition here)\n    n_months = n_months + 1  # Increment how many months they've worked\n\n    # Increase total_npv\n\n(back to text)\n\n\n\nTry to find the index of the first value in x that is greater than 0.999 using a for loop and break.\ntry iterating over range(len(x)).\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nWrite a for loop that adds up all values in x that are greater than or equal to 0.5.\nUse the continue word to end the body of the loop early for all values of x that are less than 0.5.\nTry starting your loop with for value in x: instead of iterating over the indices of x.\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nReturning to our previous example: print “{city} is in {state}” for each combination using a zip and a comprehension.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# your code here\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#net-present-values-and-asset-pricing",
    "href": "tutorials/session_1/qe_control.html#net-present-values-and-asset-pricing",
    "title": "Control Flow",
    "section": "",
    "text": "In this lecture, we’ll introduce two related topics from economics:\n\nNet present valuations\n\nAsset pricing\n\nThese topics will motivate some of the programming we do in this course.\nIn economics and finance, “assets” provide a stream of payoffs.\nThese “assets” can be concrete or abstract: a stock pays dividends over time, a bond pays interest, an apple tree provides apples, a job pays wages, and an education provides possible jobs (which, in turn, pay wages).\nWhen deciding the price to pay for an asset or how to choose between different alternatives, we need to take into account that most people would prefer to receive 1 today vs. 1 next year.\nThis reflection on consumer preferences leads to the notion of a discount rate. If you are indifferent between receiving 1.00 today and 1.10 next year, then the discount rate over the next year is $ r = 0.10 $.\nIf we assume that an individuals preferences are consistent over time, then we can apply that same discount rate to valuing assets further into the future.\nFor example, we would expect that the consumer would be indifferent between consuming 1.00 today and $ (1+r)(1+r) = 1.21 $ dollars two years from now (i.e. discount twice).\nInverting this formula, 1 delivered two years from now is equivalent to $ $ today."
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercise",
    "href": "tutorials/session_1/qe_control.html#exercise",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 1 in the exercise list.\n\n\n\nIf an asset pays a stream of payoffs over multiple time periods, then we can use a discount rate to calculate the value to the consumer of a entire sequence of payoffs.\nMost generally, we enumerate each discrete time period (e.g. year, month, day) by the index $ t $ where today is $ t=0 $ and the asset lives for $ T $ periods.\nList the payoff at each time period as $ y_t $, which we will assume, for now, is known in advance.\nThen if the discount factor is $ r $, the consumer “values” the payoff $ y_t $ delivered at time $ t $ as $ y_t $ where we note that if $ t=0 \\(, the value is just the current payoff\\) y_0 $.\nUsing this logic, we can write an expression for the value of the entire sequence of payoffs with a sum.\n \\[\nP_0 = \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t y_t \\tag{1}\n\\]\nIf $ y_t $ is a constant, then we can compute this sum with a simple formula!\nBelow, we present some useful formulas that come from infinite series that we will use to get our net present value formula.\nFor any constant $ 0 &lt; &lt; 1 $ and integer value $ &gt; 0 $,\n \\[\n\\begin{aligned}\n\\sum_{t=0}^{\\infty} \\beta^t & = \\frac{1}{1-\\beta}\\\\\n\\sum_{t=0}^{\\tau} \\beta^t &= \\frac{1- \\beta^{\\tau+1}}{1-\\beta}\\\\\n\\sum_{t=\\tau}^{\\infty} \\beta^t &=  \\frac{\\beta^{\\tau}}{1-\\beta}\n\\end{aligned} \\tag{2}\n\\]\nIn the case of an asset which pays one dollar until time $ T $, we can use these formulas, taking $ = $ and $ T = $, to find\n\\[\n\\begin{aligned}\nP_0 &= \\sum_{t=0}^T \\left(\\frac{1}{1 + r}\\right)^t = \\frac{1- (\\frac{1}{1+r})^{\\tau+1}}{1-\\frac{1}{1+r}}\\\\\n&= \\frac{1 + r}{r} - \\frac{1}{r}\\left(\\frac{1}{1+r} \\right)^\\tau\n\\end{aligned}\n\\]\nNote that we can also consider an asset that lives and pays forever if $ T= $, and from (2), the value of an asset which pays 1 forever is $ $."
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#conditional-statements-and-blocks",
    "href": "tutorials/session_1/qe_control.html#conditional-statements-and-blocks",
    "title": "Control Flow",
    "section": "",
    "text": "Sometimes, we will only want to execute some piece of code if a certain condition is met.\nThese conditions can be anything.\nFor example, we might add to total sales if the transaction value is positive, but add to total returns if the value is negative.\nOr, we might want to add up all incurred costs, only if the transaction happened before a certain date.\nWe use conditionals to run particular pieces of code when certain criterion are met.\nConditionals are closely tied to booleans, so if you don’t remember what those are, go back to the basics lecture for a refresher.\nThe basic syntax for conditionals is\n\nif condition:\n        \n    # code to run when condition is True\nelse:\n    # code to run if no conditions above are True\n\nNote that immediately following the condition, there is a colon and that the next line begins with blank spaces.\nUsing 4 spaces is a very strong convention, so that is what we do — we recommend that you do the same.\nAlso note that the else clause is optional.\nLet’s see some simple examples.\n\nif True:\n    print(\"This is where `True` code is run\")\n\nThis is where `True` code is run\n\n\nAlternatively, you could have a test which returns a booleans\n\nif 1 &lt; 2:\n     print(\"This is where `True` code is run\")\n\nThis is where `True` code is run\n\n\nThis example is equivalent to just typing the print statement, but the example below isn’t…\n\nif False:\n    print(\"This is where `True` code is run\")\n\nOr\n\nif 1 &gt; 2:\n     print(\"This is where `True` code is run\")\n\nNotice that when you run the cells above nothing is printed.\nThat is because the condition for the if statement was not true, so the code inside the indented block was never run.\nThis also allows us to demonstrate the role of indentation in determining the “block” of code.\n\nval = False\n\nif val is True: # check an expression\n    print(\"This is where `True` code is run\")\n    print(\"More code in the if block\")\n    print(\"Code runs after 'if' block, regardless of val\")"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercise-1",
    "href": "tutorials/session_1/qe_control.html#exercise-1",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 2 in the exercise list.\nThe next example shows us how else works.\n\nval = (2 == 4)  # returns False\nif val is True:\n    print(\"This is where `True` code is run\")\nelse:\n    print(\"This is where `False` code is run\")\n    print(\"More else code\")\nprint(\"Code runs after 'if' block, regardless of val\")\n\nThe if False: ... part of this example is the same as the example before, but now, we added an else: clause.\nIn this case, because the conditional for the if statement was not True, the if code block was not executed, but the else block was.\nFinally, the Condition is True is assumed in the if statement, and is often left out. For example, the following are identical\n\nif (1 &lt; 2) is True:\n    print(\"1 &lt; 2\")\n\nif 1 &lt; 2:\n    print(\"1 &lt; 2\")"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercise-2",
    "href": "tutorials/session_1/qe_control.html#exercise-2",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 3 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercise-3",
    "href": "tutorials/session_1/qe_control.html#exercise-3",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 4 in the exercise list.\n\n\nSometimes, you have more than one condition you want to check.\nFor example, you might want to run a different set of code based on which quarter a particular transaction took place in.\nIn this case you could check whether the date is in Q1, or in Q2, or in Q3, or if not any of these it must be in Q4.\nThe way to express this type of conditional is to use one or more elif clause in addition to the if and the else.\nThe syntax is\n\nif condition1:\n    # code to run when condition1 is True\nelif condition2:\n    # code to run when condition2 is True\nelif condition3:\n    # code to run when condition3 is True\nelse:\n    # code to run when none of the above are true\n\nYou can include as many elif clauses as you want.\nAs before, the else part is optional.\nHere’s how we might express the quarter example referred to above.\n\nimport datetime\nhalloween = datetime.date(2017, 10, 31)\n\nif halloween.month &gt; 9:\n    print(\"Halloween is in Q4\")\nelif halloween.month &gt; 6:\n    print(\"Halloween is in Q3\")\nelif halloween.month &gt; 3:\n    print(\"Halloween is in Q2\")\nelse:\n    print(\"Halloween is in Q1\")\n\nNote that when there are multiple if or elif conditions, only the code corresponding to the first true clause is run.\nWe saw this in action above.\nWe know that when halloween.month &gt; 9 is true, then halloween.month &gt; 6 and halloween.month &gt; 3 must also be true, but only the code block associated with halloween.month &gt; 9 was printed."
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#iteration",
    "href": "tutorials/session_1/qe_control.html#iteration",
    "title": "Control Flow",
    "section": "",
    "text": "When doing computations or analyzing data, we often need to repeat certain operations a finite number of times or until some condition is met.\nExamples include processing all data files in a directory (folder), aggregating revenues and costs for every period in a year, or computing the net present value of certain assets. (In fact, later in this section, we will verify the equations that we wrote down above.)\nThese are all examples of a programming concept called iteration.\nWe feel the concept is best understood through example, so we will present a contrived example and then discuss the details behind doing iteration in Python.\n\n\nSuppose we wanted to print out the first 10 integers and their squares.\nWe could do something like this.\n\nprint(f\"1**2  is: {1**2}\")\nprint(f\"2**2  is: {2**2}\")\nprint(f\"3**2  is: {3**2}\")\nprint(f\"4**2  is: {4**2}\")\n# .. and so on until 10\n\n1**2  is: 1\n2**2  is: 4\n3**2  is: 9\n4**2  is: 16\n\n\nAs you can see, the code above is repetitive.\nFor each integer, the code is exactly the same except for the two places where the “current” integer appears.\nSuppose that I asked you to write the same print statement for an int stored in a variable named i.\nYou might write the following code:\n\nprint(f\"{i}**2 = {i**2}\")\n\nThis more general version of the operation suggests a strategy for achieving our goal with less repetition: have a variable i take on the values 1 through 10 (Quiz: How can we use range to create the numbers 1 to 10?) and run the line of code above for each new value of i.\nThis can be accomplished with a for loop!\n\nfor i in range(1, 10):\n    print(f\"{i}**2 = {i**2}\")\n\n1**2 = 1\n2**2 = 4\n3**2 = 9\n4**2 = 16\n5**2 = 25\n6**2 = 36\n7**2 = 49\n8**2 = 64\n9**2 = 81\n\n\nWhoa, what just happened?\nThe integer i took on the values in range(1, 11) one by one and for each new value it did the operations in the indented block (here just one line that called the print function).\n\n\n\nThe general structure of a standard for loop is as follows.\n\nfor item in iterable:\n   # operation 1 with item\n   # operation 2 with item\n   # ...\n   # operation N with item\n\nwhere iterable is anything capable of producing one item at a time (see here for official definition from the Python team).\nWe’ve actually already seen some of the most common iterables!\nLists, tuples, dicts, and range/zip/enumerate objects are all iterables.\nNote that we can have as many operations as we want inside the indented block.\nWe will refer to the indented block as the “body” of the loop.\nWhen the for loop is executed, item will take on one value from iterable at a time and execute the loop body for each value.\n\n\n\n\nSee exercise 5 in the exercise list.\nWhen iterating, each item in iterable might actually contain more than one value.\nRecall that tuples (and lists) can be unpacked directly into variables.\n\ntup = (4, \"test\")\ni, x = tup\nprint(f\"i = {i}, x = {x}, tup = {tup}\")\n\nAlso, recall that the value of a enumerate(iterable) is a tuple of the form (i, x) where iterable[i] == x.\nWhen we use enumerate in a for loop, we can “unpack” both values at the same time as follows:\n\n# revenue by quarter\ncompany_revenue = [5.12, 5.20, 5.50, 6.50]\n\nfor index, value in enumerate(company_revenue):\n    print(f\"quarter {index} revenue is ${value} million\")\n\nSimilarly, the index can be used to access another vector.\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\nfor index, city in enumerate(cities):\n    state = states[index]\n    print(f\"{city} is in {state}\")\n\n\n\n\nSee exercise 6 in the exercise list.\n\n\n\nA related but slightly different form of iteration is to repeat something until some condition is met.\nThis is typically achieved using a while loop.\nThe structure of a while loop is\n\nwhile True_condition:\n    # repeat these steps\n\nwhere True_condition is some conditional statement that should evaluate to True when iterations should continue and False when Python should stop iterating.\nFor example, suppose we wanted to know the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nWe figure this out using a while loop as follows.\n\ntotal = 0\ni = 0\nwhile total &lt;= 1000:\n    i = i + 1\n    total = total + i\n\nprint(\"The answer is\", i)\n\nLet’s check our work.\n\n# Should be just less than 1000 because range(45) goes from 0 to 44\nsum(range(45))\n\n\n# should be between 990 + 45 = 1035\nsum(range(46))\n\nA warning: one common programming error with while loops is to forget to set the variable you use in the condition prior to executing. For example, take the following code which correctly sets a counter\n\ni = 0\n\nAnd then executes a while loop\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\nNo problems. But if you were to execute the above cell again, or another cell, the i=3 remains, and code is never executed (since i &lt; 3 begins as False).\n\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\nprint(\"done\")\n\n\n\n\nSee exercise 7 in the exercise list.\n\n\n\n\n\nSometimes we want to stop a loop early if some condition is met.\nLet’s revisit the example of finding the smallest N such that $ _{i=0}^N i &gt; 1000 $.\nClearly N must be less than 1000, so we know we will find the answer if we start with a for loop over all items in range(1001).\nThen, we can keep a running total as we proceed and tell Python to stop iterating through our range once total goes above 1000.\n\ntotal = 0\nfor i in range(1001):\n    total = total + i\n    if total &gt; 1000:\n        break\n\nprint(\"The answer is\", i)\n\n\n\n\nSee exercise 8 in the exercise list.\n\n\n\nSometimes we might want to stop the body of a loop early if a condition is met.\nTo do this we can use the continue keyword.\nThe basic syntax for doing this is:\n\nfor item in iterable:\n    # always do these operations\n    if condition:\n        continue\n\n    # only do these operations if condition is False\n\nInside the loop body, Python will stop that loop iteration of the loop and continue directly to the next iteration when it encounters the continue statement.\nFor example, suppose I ask you to loop over the numbers 1 to 10 and print out the message “{i} An odd number!” whenever the number i is odd, and do nothing otherwise.\nYou can use continue to do this as follows:\n\nfor i in range(1, 11):\n    if i % 2 == 0:  # an even number... This is modulus division\n        continue\n\n    print(i, \"is an odd number!\")\n\n\n\n\nSee exercise 9 in the exercise list."
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#comprehension",
    "href": "tutorials/session_1/qe_control.html#comprehension",
    "title": "Control Flow",
    "section": "",
    "text": "Often, we will want to perform a very simple operation for every element of some iterable and create a new iterable with these values.\nThis could be done by writing a for loop and saving each value, but often using what is called a comprehension is more readable.\nLike many Python concepts, a comprehension is easiest to understand through example.\nImagine that we have a list x with a list of numbers. We would like to create a list x2 which has the squared values of x.\n\nx = list(range(4))\n\n# Create squared values with a loop\nx2_loop = []\nfor x_val in x:\n    x2_loop.append(x_val**2)\n\n# Create squared values with a comprehension\nx2_comp = [x_val**2 for x_val in x]\n\nprint(x2_loop)\nprint(x2_comp)\n\nNotice that much of the same text appears when we do the operation in the loop and when we do the operation with the comprehension.\n\nWe need to specify what we are iterating over – in both cases, this is for x_val in x.\n\nWe need to square each element x_val**2.\n\nIt needs to be stored somewhere – in x2_loop, this is done by appending each element to a list, and in x2_comp, this is done automatically because the operation is enclosed in a list.\n\nWe can do comprehension with many different types of iterables, so we demonstrate a few more below.\n\n# Create a dictionary from lists\ntickers = [\"AAPL\", \"GOOGL\", \"TVIX\"]\nprices = [175.96, 1047.43, 8.38]\nd = {key: value for key, value in zip(tickers, prices)}\nd\n\n\n# Create a list from a dictionary\nd = {\"AMZN\": \"Seattle\", \"TVIX\": \"Zurich\", \"AAPL\": \"Cupertino\"}\n\nhq_cities = [d[ticker] for ticker in d.keys()]\nhq_cities\n\n\nimport math\n\n# List from list\nx = range(10)\n\nsin_x = [math.sin(x_val) for x_val in x]\nsin_x"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercise-9",
    "href": "tutorials/session_1/qe_control.html#exercise-9",
    "title": "Control Flow",
    "section": "",
    "text": "See exercise 10 in the exercise list.\nFinally, we can use this approach to build complicated nested dictionaries.\n\ngdp_data = [9.607, 10.48, 11.06]\nyears = [2013, 2014, 2015]\nexports = [ {\"manufacturing\": 2.4, \"agriculture\": 1.5, \"services\": 0.5},\n            {\"manufacturing\": 2.5, \"agriculture\": 1.4, \"services\": 0.9},\n            {\"manufacturing\": 2.7, \"agriculture\": 1.4, \"services\": 1.5}]\ndata = zip(years, gdp_data,exports)\ndata_dict = {year : {\"gdp\" : gdp, \"exports\": exports} for year, gdp, exports in data}\nprint(data_dict)\n\n# total exports by year\n[data_dict[year][\"exports\"][\"services\"] for year in data_dict.keys()]"
  },
  {
    "objectID": "tutorials/session_1/qe_control.html#exercises",
    "href": "tutorials/session_1/qe_control.html#exercises",
    "title": "Control Flow",
    "section": "",
    "text": "Government bonds are often issued as zero-coupon bonds meaning that they make no payments throughout the entire time that they are held, but, rather make a single payment at the time of maturity.\nHow much should you be willing to pay for a zero-coupon bond that paid 100 in 10 years with an interest rate of 5%?\n\n# your code here\n\n(back to text)\n\n\n\nRun the following two variations on the code with only a single change in the indentation.\nAfter, modify the x to print 3 and then 2, 3 instead.\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\n    print(\"2\")\nprint(\"3\")\n\n1\n2\n3\n\n\n\nx = 1\n\nif x &gt; 0:\n    print(\"1\")\nprint(\"2\") # changed the indentation\nprint(\"3\")\n\n1\n2\n3\n\n\n(back to text)\n\n\n\nUsing the code cell below as a start, print \"Good afternoon\" if the current_time is past noon.\nOtherwise, do nothing.\nWrite some conditional based on current_time.hour.\n\nimport datetime\ncurrent_time = datetime.datetime.now()\n\n## your code here\n\nmore text after\n(back to text)\n\n\n\nIn this example, you will generate a random number between 0 and 1 and then display “x &gt; 0.5” or “x &lt; 0.5” depending on the value of the number.\nThis also introduces a new package numpy.random for drawing random numbers (more in the randomness lecture).\n\nimport numpy as np\nx = np.random.random()\nprint(f\"x = {x}\")\n\n## your code here\n\n(back to text)\n\n\n\nIn economics, when an individual has some knowledge, skills, or education which provides them with a source of future income, we call it human capital.\nWhen a student graduating from high school is considering whether to continue with post-secondary education, they may consider that it gives them higher paying jobs in the future, but requires that they don’t begin working until after graduation.\nConsider the simplified example where a student has perfectly forecastable employment and is given two choices:\n\nBegin working immediately and make 40,000 a year until they retire 40 years later.\n\nPay 5,000 a year for the next 4 years to attend university, then get a job paying 50,000 a year until they retire 40 years after making the college attendance decision.\n\nShould the student enroll in school if the discount rate is r = 0.05?\n\n# Discount rate\nr = 0.05\n\n# High school wage\nw_hs = 40_000\n\n# College wage and cost of college\nc_college = 5_000\nw_college = 50_000\n\n# Compute npv of being a hs worker\n\n# Compute npv of attending college\n\n# Compute npv of being a college worker\n\n# Is npv_collegeworker - npv_collegecost &gt; npv_hsworker\n\n(back to text)\n\n\n\nInstead of the above, write a for loop that uses the lists of cities and states below to print the same “{city} is in {state}” using a zip instead of an enumerate.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# Your code here\n\n(back to text)\n\n\n\nCompanies often invest in training their employees to raise their productivity. Economists sometimes wonder why companies spend this money when this incentivizes other companies to hire their employees away with higher salaries since employees gain human capital from training?\nLet’s say that it costs a company 25,000 dollars to teach their employees Python, but it raises their output by 2,500 per month. How many months would an employee need to stay for the company to find it profitable to pay for their employees to learn Python if their discount rate is r = 0.01?\n\n# Define cost of teaching python\ncost = 25_000\nr = 0.01\n\n# Per month value\nadded_value = 2500\n\nn_months = 0\ntotal_npv = 0.0\n\n# Put condition below here\nwhile False: # (replace False with your condition here)\n    n_months = n_months + 1  # Increment how many months they've worked\n\n    # Increase total_npv\n\n(back to text)\n\n\n\nTry to find the index of the first value in x that is greater than 0.999 using a for loop and break.\ntry iterating over range(len(x)).\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nWrite a for loop that adds up all values in x that are greater than or equal to 0.5.\nUse the continue word to end the body of the loop early for all values of x that are less than 0.5.\nTry starting your loop with for value in x: instead of iterating over the indices of x.\n\nx = np.random.rand(10_000)\n# Your code here\n\n(back to text)\n\n\n\nReturning to our previous example: print “{city} is in {state}” for each combination using a zip and a comprehension.\nTry using zip\n\ncities = [\"Phoenix\", \"Austin\", \"San Diego\", \"New York\"]\nstates = [\"Arizona\", \"Texas\", \"California\", \"New York\"]\n\n# your code here\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html",
    "href": "tutorials/session_1/qe_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Prerequisites\n\nBasics\n\nCollections\n\nControl Flow\n\nOutcomes\n\nEconomic Production Functions\n\nUnderstand the basics of production functions in economics\n\n\nFunctions\n\nKnow how to define your own function\n\nKnow how to find and write your own function documentation\n\nKnow why we use functions\n\nUnderstand scoping rules and blocks\n\n\n\n\n\nProduction functions are useful when modeling the economics of firms producing goods or the aggregate output in an economy.\nThough the term “function” is used in a mathematical sense here, we will be making tight connections between the programming of mathematical functions and Python functions.\n\n\nThe factors of production are the inputs used in the production of some sort of output.\nSome example factors of production include\n\nPhysical capital, e.g. machines, buildings, computers, and power stations.\n\nLabor, e.g. all of the hours of work from different types of employees of a firm.\n\nHuman Capital, e.g. the knowledge of employees within a firm.\n\nA production function maps a set of inputs to the output, e.g. the amount of wheat produced by a farm, or widgets produced in a factory.\nAs an example of the notation, we denote the total units of labor and physical capital used in a factory as $ L $ and $ K $ respectively.\nIf we denote the physical output of the factory as $ Y $, then a production function $ F $ that transforms labor and capital into output might have the form:\n\\[\nY = F(K, L)\n\\]\n\n\n\n\nThroughout this lecture, we will use the Cobb-Douglas production function to help us understand how to create Python functions and why they are useful.\nThe Cobb-Douglas production function has appealing statistical properties when brought to data.\nThis function is displayed below.\n\\[\nY = z K^{\\alpha} L^{1-\\alpha}\n\\]\nThe function is parameterized by:\n\nA parameter $ $, called the “output elasticity of capital”.\n\nA value $ z $ called the Total Factor Productivity (TFP).\n\n\n\n\n\nIn this class, we will often talk about functions.\nSo what is a function?\nWe like to think of a function as a production line in a manufacturing plant: we pass zero or more things to it, operations take place in a set linear sequence, and zero or more things come out.\nWe use functions for the following purposes:\n\nRe-usability: Writing code to do a specific task just once, and reuse the code by calling the function.\n\nOrganization: Keep the code for distinct operations separated and organized.\n\nSharing/collaboration: Sharing code across multiple projects or sharing pieces of code with collaborators.\n\n\n\n\nThe basic syntax to create our own function is as follows:\n\ndef function_name(inputs):\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nHere we see two new keywords: def and return.\n\ndef is used to tell Python we would like to define a new function.\n\nreturn is used to tell Python what we would like to return from a function.\n\nLet’s look at an example and then discuss each part:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n\n    return answer\n\nHere we defined a function mean that has one input (numbers), does three steps, and has one output (answer).\nLet’s see what happens when we call this function on the list of numbers [1, 2, 3, 4].\n\nx = [1, 2, 3, 4]\nthe_mean = mean(x)\nthe_mean\n\nAdditionally, as we saw in the control flow lecture, indentation controls blocks of code (along with the scope rules).\nTo see this, compare a function with no inputs or return values.\n\ndef f():\n    print(\"1\")\n    print(\"2\")\nf()\n\nWith the following change of indentation…\n\ndef f():\n    print(\"1\")\nprint(\"2\")\nf()\n\n\n\n\nNotice that we named the input to the function x and we called the output the_mean.\nWhen we defined the function, the input was called numbers and the output answer… what gives?\nThis is an example of a programming concept called variable scope.\nIn Python, functions define their own scope for variables.\nIn English, this means that regardless of what name we give an input variable (x in this example), the input will always be referred to as numbers inside the body of the mean function.\nIt also means that although we called the output answer inside of the function mean, that this variable name was only valid inside of our function.\nTo use the output of the function, we had to give it our own name (the_mean in this example).\nAnother point to make here is that the intermediate variables we defined inside mean (total and N) are only defined inside of the mean function – we can’t access them from outside. We can verify this by trying to see what the value of total is:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n    return answer # or directly return total / N\n\n# uncomment the line below and execute to see the error\n# total\n\nThis point can be taken even further: the same name can be bound to variables inside of blocks of code and in the outer “scope”.\n\nx = 4\nprint(f\"x = {x}\")\ndef f():\n    x = 5 # a different \"x\"\n    print(f\"x = {x}\")\nf() # calls function\nprint(f\"x = {x}\")\n\nThe final point we want to make about scope is that function inputs and output don’t have to be given a name outside the function.\n\nmean([10, 20, 30])\n\nNotice that we didn’t name the input or the output, but the function was called successfully.\nNow, we’ll use our new knowledge to define a function which computes the output from a Cobb-Douglas production function with parameters $ z = 1 $ and $ = 0.33 $ and takes inputs $ K $ and $ L $.\n\ndef cobb_douglas(K, L):\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K**alpha * L**(1 - alpha)\n\nWe can use this function as we did the mean function.\n\ncobb_douglas(1.0, 0.5)\n\n\n\n\n\nEconomists are often interested in this question: how much does output change if we modify our inputs?\nFor example, take a production function $ Y_1 = F(K_1,L_1) $ which produces $ Y_1 $ units of the goods.\nIf we then multiply the inputs each by $ \\(, so that\\) K_2 = K_1 $ and $ L_2 = L_1 $, then the output is\n\\[\nY_2 = F(K_2, L_2) = F(\\gamma K_1, \\gamma L_1)\n\\]\nHow does $ Y_1 $ compare to $ Y_2 $?\nAnswering this question involves something called returns to scale.\nReturns to scale tells us whether our inputs are more or less productive as we have more of them.\nFor example, imagine that you run a restaurant. How would you expect the amount of food you could produce would change if you could build an exact replica of your restaurant and kitchen and hire the same number of cooks and waiters? You would probably expect it to double.\nIf, for any $ K, L $, we multiply $ K, L $ by a value $ $ then\n\nIf $ &lt; $ then we say the production function has decreasing returns to scale.\n\nIf $ = $ then we say the production function has constant returns to scale.\n\nIf $ &gt; $ then we say the production function has increasing returns to scale.\n\nLet’s try it and see what our function is!\n\ny1 = cobb_douglas(1.0, 0.5)\nprint(y1)\ny2 = cobb_douglas(2*1.0, 2*0.5)\nprint(y2)\n\nHow did $ Y_1 $ and $ Y_2 $ relate?\n\ny2 / y1\n\n$ Y_2 $ was exactly double $ Y_1 $!\nLet’s write a function that will compute the returns to scale for different values of $ K $ and $ L $.\nThis is an example of how writing functions can allow us to re-use code in ways we might not originally anticipate. (You didn’t know we’d be writing a returns_to_scale function when we wrote cobb_douglas.)\n\ndef returns_to_scale(K, L, gamma):\n    y1 = cobb_douglas(K, L)\n    y2 = cobb_douglas(gamma*K, gamma*L)\n    y_ratio = y2 / y1\n    return y_ratio / gamma\n\n\nreturns_to_scale(1.0, 0.5, 2.0)\n\n\n\n\nSee exercise 1 in the exercise list.\nIt turns out that with a little bit of algebra, we can check that this will always hold for our Cobb-Douglas example above.\nTo show this, take an arbitrary $ K, L $ and multiply the inputs by an arbitrary $ $.\n\\[\n\\begin{aligned}\n    F(\\gamma K, \\gamma L) &= z (\\gamma K)^{\\alpha} (\\gamma L)^{1-\\alpha}\\\\\n    &=  z \\gamma^{\\alpha}\\gamma^{1-\\alpha} K^{\\alpha} L^{1-\\alpha}\\\\\n    &= \\gamma z K^{\\alpha} L^{1-\\alpha} = \\gamma F(K, L)\n\\end{aligned}\n\\]\nFor an example of a production function that is not CRS, look at a generalization of the Cobb-Douglas production function that has different “output elasticities” for the 2 inputs.\n\\[\nY = z K^{\\alpha_1} L^{\\alpha_2}\n\\]\nNote that if $ _2 = 1 - _1 $, this is our Cobb-Douglas production function.\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\n\nAnother valuable element to analyze on production functions is how output changes as we change only one of the inputs. We will call this the marginal product.\nFor example, compare the output using $ K, L $ units of inputs to that with an $ $ units of labor.\nThen the marginal product of labor (MPL) is defined as\n\\[\n\\frac{F(K, L + \\varepsilon) - F(K, L)}{\\varepsilon}\n\\]\nThis tells us how much additional output is created relative to the additional input. (Spoiler alert: This should look like the definition for a partial derivative!)\nIf the input can be divided into small units, then we can use calculus to take this limit, using the partial derivative of the production function relative to that input.\nIn this case, we define the marginal product of labor (MPL) and marginal product of capital (MPK) as\n\\[\n\\begin{aligned}\nMPL(K, L) &= \\frac{\\partial F(K, L)}{\\partial L}\\\\\nMPK(K, L) &= \\frac{\\partial F(K, L)}{\\partial K}\n\\end{aligned}\n\\]\nIn the Cobb-Douglas example above, this becomes\n\\[\n\\begin{aligned}\nMPK(K, L) &= z  \\alpha \\left(\\frac{K}{L} \\right)^{\\alpha - 1}\\\\\nMPL(K, L) &= (1-\\alpha) z \\left(\\frac{K}{L} \\right)^{\\alpha}\\\\\n\\end{aligned}\n\\]\nLet’s test it out with Python! We’ll also see that we can actually return multiple things in a Python function.\nThe syntax for a return statement with multiple items is return item1, item2, ….\nIn this case, we’ll compute both the MPL and the MPK and then return both.\n\ndef marginal_products(K, L, epsilon):\n\n    mpl = (cobb_douglas(K, L + epsilon) - cobb_douglas(K, L)) / epsilon\n    mpk = (cobb_douglas(K + epsilon, L) - cobb_douglas(K, L)) / epsilon\n\n    return mpl, mpk\n\n\ntup = marginal_products(1.0, 0.5,  1e-4)\nprint(tup)\n\nInstead of using the tuple, these can be directly unpacked to variables.\n\nmpl, mpk = marginal_products(1.0, 0.5,  1e-4)\nprint(f\"mpl = {mpl}, mpk = {mpk}\")\n\nWe can use this to calculate the marginal products for different K, fixing L using a comprehension.\n\nKs = [1.0, 2.0, 3.0]\n[marginal_products(K, 0.5, 1e-4) for K in Ks] # create a tuple for each K\n\n\n\n\nIn a previous exercise, we asked you to find help for the cobb_douglas and returns_to_scale functions using ?.\nIt didn’t provide any useful information.\nTo provide this type of help information, we need to add what Python programmers call a “docstring” to our functions.\nThis is done by putting a string (not assigned to any variable name) as the first line of the body of the function (after the line with def).\nBelow is a new version of the template we used to define functions.\n\ndef function_name(inputs):\n    \"\"\"\n    Docstring\n    \"\"\"\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nLet’s re-define our cobb_douglas function to include a docstring.\n\ndef cobb_douglas(K, L):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n\n    We restrict z = 1 and alpha = 0.33\n    \"\"\"\n    return 1.0 * K**(0.33) * L**(1.0 - 0.33)\n\nNow when we have Jupyter evaluate cobb_douglas?, our message is displayed (or use the Contextual Help window with Jupyterlab and Ctrl-I or Cmd-I).\n\ncobb_douglas?\n\nWe recommend that you always include at least a very simple docstring for nontrivial functions.\nThis is in the same spirit as adding comments to your code — it makes it easier for future readers/users (including yourself) to understand what the code does.\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nFunctions can have optional arguments.\nTo accomplish this, we must these arguments a default value by saying name=default_value instead of just name as we list the arguments.\nTo demonstrate this functionality, let’s now make $ z $ and $ $ arguments to our cobb_douglas function!\n\ndef cobb_douglas(K, L, alpha=0.33, z=1):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n    \"\"\"\n    return z * K**(alpha) * L**(1.0 - alpha)\n\nWe can now call this function by passing in just K and L. Notice that it will produce same result as earlier because alpha and z are the same as earlier.\n\ncobb_douglas(1.0, 0.5)\n\nHowever, we can also set the other arguments of the function by passing more than just K/L.\n\ncobb_douglas(1.0, 0.5, 0.35, 1.6)\n\nIn the example above, we used alpha = 0.35, z = 1.6.\nWe can also refer to function arguments by their name, instead of only their position (order).\nTo do this, we would write func_name(arg=value) for as many of the arguments as we want.\nHere’s how to do that with our cobb_douglas example.\n\ncobb_douglas(1.0, 0.5, z = 1.5)\n\n\n\n\nSee exercise 4 in the exercise list.\nIn terms of variable scope, the z name within the function is different from any other z in the outer scope.\nTo be clear,\n\nx = 5\ndef f(x):\n    return x\nf(x) # \"coincidence\" that it has the same name\n\nThis is also true with named function arguments, above.\n\nz = 1.5\ncobb_douglas(1.0, 0.5, z = z) # no problem!\n\nIn that example, the z on the left hand side of z = z refers to the local variable name in the function whereas the z on the right hand side refers to the z in the outer scope.\n\n\n\nAs we learned earlier, all variables in Python have a type associated with them.\nDifferent types of variables have different functions or operations defined for them.\nFor example, I can divide one number by another or make a string uppercase.\nIt wouldn’t make sense to divide one string by another or make a number uppercase.\nWhen certain functionality is closely tied to the type of an object, it is often implemented as a special kind of function known as a method.\nFor now, you only need to know two things about methods:\n\nWe call them by doing variable.method_name(other_arguments) instead of function_name(variable, other_arguments).\n\nA method is a function, even though we call it using a different notation.\n\nWhen we introduced the core data types, we saw many methods defined on these types.\nLet’s revisit them for the str, or string type.\nNotice that we call each of these functions using the dot syntax described above.\n\ns = \"This is my handy string!\"\n\n\ns.upper()\n\n\ns.title()\n\n\n\n\n\nKeep in mind that with mathematical functions, the arguments are just dummy names that can be interchanged.\nThat is, the following are identical.\n\\[\n\\begin{eqnarray}\n    f(K, L) &= z\\, K^{\\alpha} L^{1-\\alpha}\\\\\n    f(K_2, L_2) &= z\\, K_2^{\\alpha} L_2^{1-\\alpha}\n\\end{eqnarray}\n\\]\nThe same concept applies to Python functions, where the arguments are just placeholder names, and our cobb_douglas function is identical to\n\ndef cobb_douglas2(K2, L2): # changed dummy variable names\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K2**alpha * L2**(1 - alpha)\n\ncobb_douglas2(1.0, 0.5)\n\nThis is an appealing feature of functions for avoiding coding errors: names of variables within the function are localized and won’t clash with those on the outside (with more examples in scope).\nImportantly, when Python looks for variables matching a particular name, it begins in the most local scope.\nThat is, note that having an alpha in the outer scope does not impact the local one.\n\ndef cobb_douglas3(K, L, alpha): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    return z * K**alpha * L**(1 - alpha) # sees local argument alpha\n\nprint(cobb_douglas3(1.0, 0.5, 0.2))\nprint(\"Setting alpha, does the result change?\")\nalpha = 0.5 # in the outer scope\nprint(cobb_douglas3(1.0, 0.5, 0.2))\n\nA crucial element of the above function is that the alpha variable was available in the local scope of the function.\nConsider the alternative where it is not. We have removed the alpha function parameter as well as the local definition of alpha.\n\ndef cobb_douglas4(K, L): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    # there are no local alpha in scope!\n    return z * K**alpha * L**(1 - alpha)\n\nalpha = 0.2 # in the outer scope\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\nalpha = 0.3\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\n\nThe intuition of scoping does not apply only for the “global” vs. “function” naming of variables, but also for nesting.\nFor example, we can define a version of cobb_douglas which is also missing a z in its inner-most scope, then put the function inside of another function.\n\nz = 1\ndef output_given_alpha(alpha):\n    # Scoping logic:\n    # 1. local function name doesn't clash with global one\n    # 2. alpha comes from the function parameter\n    # 3. z comes from the outer global scope\n    def cobb_douglas(K, L):\n        return z * K**alpha * L**(1 - alpha)\n\n    # using this function\n    return cobb_douglas(1.0, 0.5)\n\nalpha = 100 # ignored\nalphas = [0.2, 0.3, 0.5]\n# comprehension variables also have local scope\n# and don't clash with the alpha = 100\n[output_given_alpha(alpha) for alpha in alphas]\n\n\n\n\n\n\n\nWhat happens if we try different inputs in our Cobb-Douglas production function?\n\n# Compute returns to scale with different values of `K` and `L` and `gamma`\n\n(back to text)\n\n\n\nDefine a function named var that takes a list (call it x) and computes the variance. This function should use the mean function that we defined earlier.\n$ = _i (x_i - (x))^2 $\n\n# Your code here.\n\n(back to text)\n\n\n\nRedefine the returns_to_scale function and add a docstring.\nConfirm that it works by running the cell containing returns_to_scale? below.\nNote: You do not need to change the actual code in the function — just copy/paste and add a docstring in the correct line.\n\n# re-define the `returns_to_scale` function here\n\n\n# test it here\n\nreturns_to_scale?\n\n(back to text)\n\n\n\nExperiment with the sep and end arguments to the print function.\nThese can only be set by name.\n\n# Your code here.\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html#application-production-functions",
    "href": "tutorials/session_1/qe_functions.html#application-production-functions",
    "title": "Functions",
    "section": "",
    "text": "Production functions are useful when modeling the economics of firms producing goods or the aggregate output in an economy.\nThough the term “function” is used in a mathematical sense here, we will be making tight connections between the programming of mathematical functions and Python functions.\n\n\nThe factors of production are the inputs used in the production of some sort of output.\nSome example factors of production include\n\nPhysical capital, e.g. machines, buildings, computers, and power stations.\n\nLabor, e.g. all of the hours of work from different types of employees of a firm.\n\nHuman Capital, e.g. the knowledge of employees within a firm.\n\nA production function maps a set of inputs to the output, e.g. the amount of wheat produced by a farm, or widgets produced in a factory.\nAs an example of the notation, we denote the total units of labor and physical capital used in a factory as $ L $ and $ K $ respectively.\nIf we denote the physical output of the factory as $ Y $, then a production function $ F $ that transforms labor and capital into output might have the form:\n\\[\nY = F(K, L)\n\\]\n\n\n\n\nThroughout this lecture, we will use the Cobb-Douglas production function to help us understand how to create Python functions and why they are useful.\nThe Cobb-Douglas production function has appealing statistical properties when brought to data.\nThis function is displayed below.\n\\[\nY = z K^{\\alpha} L^{1-\\alpha}\n\\]\nThe function is parameterized by:\n\nA parameter $ $, called the “output elasticity of capital”.\n\nA value $ z $ called the Total Factor Productivity (TFP)."
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html#what-are-python-functions",
    "href": "tutorials/session_1/qe_functions.html#what-are-python-functions",
    "title": "Functions",
    "section": "",
    "text": "In this class, we will often talk about functions.\nSo what is a function?\nWe like to think of a function as a production line in a manufacturing plant: we pass zero or more things to it, operations take place in a set linear sequence, and zero or more things come out.\nWe use functions for the following purposes:\n\nRe-usability: Writing code to do a specific task just once, and reuse the code by calling the function.\n\nOrganization: Keep the code for distinct operations separated and organized.\n\nSharing/collaboration: Sharing code across multiple projects or sharing pieces of code with collaborators."
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html#how-to-define-python-functions",
    "href": "tutorials/session_1/qe_functions.html#how-to-define-python-functions",
    "title": "Functions",
    "section": "",
    "text": "The basic syntax to create our own function is as follows:\n\ndef function_name(inputs):\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nHere we see two new keywords: def and return.\n\ndef is used to tell Python we would like to define a new function.\n\nreturn is used to tell Python what we would like to return from a function.\n\nLet’s look at an example and then discuss each part:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n\n    return answer\n\nHere we defined a function mean that has one input (numbers), does three steps, and has one output (answer).\nLet’s see what happens when we call this function on the list of numbers [1, 2, 3, 4].\n\nx = [1, 2, 3, 4]\nthe_mean = mean(x)\nthe_mean\n\nAdditionally, as we saw in the control flow lecture, indentation controls blocks of code (along with the scope rules).\nTo see this, compare a function with no inputs or return values.\n\ndef f():\n    print(\"1\")\n    print(\"2\")\nf()\n\nWith the following change of indentation…\n\ndef f():\n    print(\"1\")\nprint(\"2\")\nf()\n\n\n\n\nNotice that we named the input to the function x and we called the output the_mean.\nWhen we defined the function, the input was called numbers and the output answer… what gives?\nThis is an example of a programming concept called variable scope.\nIn Python, functions define their own scope for variables.\nIn English, this means that regardless of what name we give an input variable (x in this example), the input will always be referred to as numbers inside the body of the mean function.\nIt also means that although we called the output answer inside of the function mean, that this variable name was only valid inside of our function.\nTo use the output of the function, we had to give it our own name (the_mean in this example).\nAnother point to make here is that the intermediate variables we defined inside mean (total and N) are only defined inside of the mean function – we can’t access them from outside. We can verify this by trying to see what the value of total is:\n\ndef mean(numbers):\n    total = sum(numbers)\n    N = len(numbers)\n    answer = total / N\n    return answer # or directly return total / N\n\n# uncomment the line below and execute to see the error\n# total\n\nThis point can be taken even further: the same name can be bound to variables inside of blocks of code and in the outer “scope”.\n\nx = 4\nprint(f\"x = {x}\")\ndef f():\n    x = 5 # a different \"x\"\n    print(f\"x = {x}\")\nf() # calls function\nprint(f\"x = {x}\")\n\nThe final point we want to make about scope is that function inputs and output don’t have to be given a name outside the function.\n\nmean([10, 20, 30])\n\nNotice that we didn’t name the input or the output, but the function was called successfully.\nNow, we’ll use our new knowledge to define a function which computes the output from a Cobb-Douglas production function with parameters $ z = 1 $ and $ = 0.33 $ and takes inputs $ K $ and $ L $.\n\ndef cobb_douglas(K, L):\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K**alpha * L**(1 - alpha)\n\nWe can use this function as we did the mean function.\n\ncobb_douglas(1.0, 0.5)\n\n\n\n\n\nEconomists are often interested in this question: how much does output change if we modify our inputs?\nFor example, take a production function $ Y_1 = F(K_1,L_1) $ which produces $ Y_1 $ units of the goods.\nIf we then multiply the inputs each by $ \\(, so that\\) K_2 = K_1 $ and $ L_2 = L_1 $, then the output is\n\\[\nY_2 = F(K_2, L_2) = F(\\gamma K_1, \\gamma L_1)\n\\]\nHow does $ Y_1 $ compare to $ Y_2 $?\nAnswering this question involves something called returns to scale.\nReturns to scale tells us whether our inputs are more or less productive as we have more of them.\nFor example, imagine that you run a restaurant. How would you expect the amount of food you could produce would change if you could build an exact replica of your restaurant and kitchen and hire the same number of cooks and waiters? You would probably expect it to double.\nIf, for any $ K, L $, we multiply $ K, L $ by a value $ $ then\n\nIf $ &lt; $ then we say the production function has decreasing returns to scale.\n\nIf $ = $ then we say the production function has constant returns to scale.\n\nIf $ &gt; $ then we say the production function has increasing returns to scale.\n\nLet’s try it and see what our function is!\n\ny1 = cobb_douglas(1.0, 0.5)\nprint(y1)\ny2 = cobb_douglas(2*1.0, 2*0.5)\nprint(y2)\n\nHow did $ Y_1 $ and $ Y_2 $ relate?\n\ny2 / y1\n\n$ Y_2 $ was exactly double $ Y_1 $!\nLet’s write a function that will compute the returns to scale for different values of $ K $ and $ L $.\nThis is an example of how writing functions can allow us to re-use code in ways we might not originally anticipate. (You didn’t know we’d be writing a returns_to_scale function when we wrote cobb_douglas.)\n\ndef returns_to_scale(K, L, gamma):\n    y1 = cobb_douglas(K, L)\n    y2 = cobb_douglas(gamma*K, gamma*L)\n    y_ratio = y2 / y1\n    return y_ratio / gamma\n\n\nreturns_to_scale(1.0, 0.5, 2.0)\n\n\n\n\nSee exercise 1 in the exercise list.\nIt turns out that with a little bit of algebra, we can check that this will always hold for our Cobb-Douglas example above.\nTo show this, take an arbitrary $ K, L $ and multiply the inputs by an arbitrary $ $.\n\\[\n\\begin{aligned}\n    F(\\gamma K, \\gamma L) &= z (\\gamma K)^{\\alpha} (\\gamma L)^{1-\\alpha}\\\\\n    &=  z \\gamma^{\\alpha}\\gamma^{1-\\alpha} K^{\\alpha} L^{1-\\alpha}\\\\\n    &= \\gamma z K^{\\alpha} L^{1-\\alpha} = \\gamma F(K, L)\n\\end{aligned}\n\\]\nFor an example of a production function that is not CRS, look at a generalization of the Cobb-Douglas production function that has different “output elasticities” for the 2 inputs.\n\\[\nY = z K^{\\alpha_1} L^{\\alpha_2}\n\\]\nNote that if $ _2 = 1 - _1 $, this is our Cobb-Douglas production function.\n\n\n\nSee exercise 2 in the exercise list.\n\n\n\n\nAnother valuable element to analyze on production functions is how output changes as we change only one of the inputs. We will call this the marginal product.\nFor example, compare the output using $ K, L $ units of inputs to that with an $ $ units of labor.\nThen the marginal product of labor (MPL) is defined as\n\\[\n\\frac{F(K, L + \\varepsilon) - F(K, L)}{\\varepsilon}\n\\]\nThis tells us how much additional output is created relative to the additional input. (Spoiler alert: This should look like the definition for a partial derivative!)\nIf the input can be divided into small units, then we can use calculus to take this limit, using the partial derivative of the production function relative to that input.\nIn this case, we define the marginal product of labor (MPL) and marginal product of capital (MPK) as\n\\[\n\\begin{aligned}\nMPL(K, L) &= \\frac{\\partial F(K, L)}{\\partial L}\\\\\nMPK(K, L) &= \\frac{\\partial F(K, L)}{\\partial K}\n\\end{aligned}\n\\]\nIn the Cobb-Douglas example above, this becomes\n\\[\n\\begin{aligned}\nMPK(K, L) &= z  \\alpha \\left(\\frac{K}{L} \\right)^{\\alpha - 1}\\\\\nMPL(K, L) &= (1-\\alpha) z \\left(\\frac{K}{L} \\right)^{\\alpha}\\\\\n\\end{aligned}\n\\]\nLet’s test it out with Python! We’ll also see that we can actually return multiple things in a Python function.\nThe syntax for a return statement with multiple items is return item1, item2, ….\nIn this case, we’ll compute both the MPL and the MPK and then return both.\n\ndef marginal_products(K, L, epsilon):\n\n    mpl = (cobb_douglas(K, L + epsilon) - cobb_douglas(K, L)) / epsilon\n    mpk = (cobb_douglas(K + epsilon, L) - cobb_douglas(K, L)) / epsilon\n\n    return mpl, mpk\n\n\ntup = marginal_products(1.0, 0.5,  1e-4)\nprint(tup)\n\nInstead of using the tuple, these can be directly unpacked to variables.\n\nmpl, mpk = marginal_products(1.0, 0.5,  1e-4)\nprint(f\"mpl = {mpl}, mpk = {mpk}\")\n\nWe can use this to calculate the marginal products for different K, fixing L using a comprehension.\n\nKs = [1.0, 2.0, 3.0]\n[marginal_products(K, 0.5, 1e-4) for K in Ks] # create a tuple for each K\n\n\n\n\nIn a previous exercise, we asked you to find help for the cobb_douglas and returns_to_scale functions using ?.\nIt didn’t provide any useful information.\nTo provide this type of help information, we need to add what Python programmers call a “docstring” to our functions.\nThis is done by putting a string (not assigned to any variable name) as the first line of the body of the function (after the line with def).\nBelow is a new version of the template we used to define functions.\n\ndef function_name(inputs):\n    \"\"\"\n    Docstring\n    \"\"\"\n    # step 1\n    # step 2\n    # ...\n    return outputs\n\nLet’s re-define our cobb_douglas function to include a docstring.\n\ndef cobb_douglas(K, L):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n\n    We restrict z = 1 and alpha = 0.33\n    \"\"\"\n    return 1.0 * K**(0.33) * L**(1.0 - 0.33)\n\nNow when we have Jupyter evaluate cobb_douglas?, our message is displayed (or use the Contextual Help window with Jupyterlab and Ctrl-I or Cmd-I).\n\ncobb_douglas?\n\nWe recommend that you always include at least a very simple docstring for nontrivial functions.\nThis is in the same spirit as adding comments to your code — it makes it easier for future readers/users (including yourself) to understand what the code does.\n\n\n\nSee exercise 3 in the exercise list.\n\n\n\nFunctions can have optional arguments.\nTo accomplish this, we must these arguments a default value by saying name=default_value instead of just name as we list the arguments.\nTo demonstrate this functionality, let’s now make $ z $ and $ $ arguments to our cobb_douglas function!\n\ndef cobb_douglas(K, L, alpha=0.33, z=1):\n    \"\"\"\n    Computes the production F(K, L) for a Cobb-Douglas production function\n\n    Takes the form F(K, L) = z K^{\\alpha} L^{1 - \\alpha}\n    \"\"\"\n    return z * K**(alpha) * L**(1.0 - alpha)\n\nWe can now call this function by passing in just K and L. Notice that it will produce same result as earlier because alpha and z are the same as earlier.\n\ncobb_douglas(1.0, 0.5)\n\nHowever, we can also set the other arguments of the function by passing more than just K/L.\n\ncobb_douglas(1.0, 0.5, 0.35, 1.6)\n\nIn the example above, we used alpha = 0.35, z = 1.6.\nWe can also refer to function arguments by their name, instead of only their position (order).\nTo do this, we would write func_name(arg=value) for as many of the arguments as we want.\nHere’s how to do that with our cobb_douglas example.\n\ncobb_douglas(1.0, 0.5, z = 1.5)\n\n\n\n\nSee exercise 4 in the exercise list.\nIn terms of variable scope, the z name within the function is different from any other z in the outer scope.\nTo be clear,\n\nx = 5\ndef f(x):\n    return x\nf(x) # \"coincidence\" that it has the same name\n\nThis is also true with named function arguments, above.\n\nz = 1.5\ncobb_douglas(1.0, 0.5, z = z) # no problem!\n\nIn that example, the z on the left hand side of z = z refers to the local variable name in the function whereas the z on the right hand side refers to the z in the outer scope.\n\n\n\nAs we learned earlier, all variables in Python have a type associated with them.\nDifferent types of variables have different functions or operations defined for them.\nFor example, I can divide one number by another or make a string uppercase.\nIt wouldn’t make sense to divide one string by another or make a number uppercase.\nWhen certain functionality is closely tied to the type of an object, it is often implemented as a special kind of function known as a method.\nFor now, you only need to know two things about methods:\n\nWe call them by doing variable.method_name(other_arguments) instead of function_name(variable, other_arguments).\n\nA method is a function, even though we call it using a different notation.\n\nWhen we introduced the core data types, we saw many methods defined on these types.\nLet’s revisit them for the str, or string type.\nNotice that we call each of these functions using the dot syntax described above.\n\ns = \"This is my handy string!\"\n\n\ns.upper()\n\n\ns.title()"
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html#more-on-scope-optional",
    "href": "tutorials/session_1/qe_functions.html#more-on-scope-optional",
    "title": "Functions",
    "section": "",
    "text": "Keep in mind that with mathematical functions, the arguments are just dummy names that can be interchanged.\nThat is, the following are identical.\n\\[\n\\begin{eqnarray}\n    f(K, L) &= z\\, K^{\\alpha} L^{1-\\alpha}\\\\\n    f(K_2, L_2) &= z\\, K_2^{\\alpha} L_2^{1-\\alpha}\n\\end{eqnarray}\n\\]\nThe same concept applies to Python functions, where the arguments are just placeholder names, and our cobb_douglas function is identical to\n\ndef cobb_douglas2(K2, L2): # changed dummy variable names\n\n    # Create alpha and z\n    z = 1\n    alpha = 0.33\n\n    return z * K2**alpha * L2**(1 - alpha)\n\ncobb_douglas2(1.0, 0.5)\n\nThis is an appealing feature of functions for avoiding coding errors: names of variables within the function are localized and won’t clash with those on the outside (with more examples in scope).\nImportantly, when Python looks for variables matching a particular name, it begins in the most local scope.\nThat is, note that having an alpha in the outer scope does not impact the local one.\n\ndef cobb_douglas3(K, L, alpha): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    return z * K**alpha * L**(1 - alpha) # sees local argument alpha\n\nprint(cobb_douglas3(1.0, 0.5, 0.2))\nprint(\"Setting alpha, does the result change?\")\nalpha = 0.5 # in the outer scope\nprint(cobb_douglas3(1.0, 0.5, 0.2))\n\nA crucial element of the above function is that the alpha variable was available in the local scope of the function.\nConsider the alternative where it is not. We have removed the alpha function parameter as well as the local definition of alpha.\n\ndef cobb_douglas4(K, L): # added new argument\n\n    # Create alpha and z\n    z = 1\n\n    # there are no local alpha in scope!\n    return z * K**alpha * L**(1 - alpha)\n\nalpha = 0.2 # in the outer scope\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\nalpha = 0.3\nprint(f\"alpha = {alpha} gives {cobb_douglas4(1.0, 0.5)}\")\n\nThe intuition of scoping does not apply only for the “global” vs. “function” naming of variables, but also for nesting.\nFor example, we can define a version of cobb_douglas which is also missing a z in its inner-most scope, then put the function inside of another function.\n\nz = 1\ndef output_given_alpha(alpha):\n    # Scoping logic:\n    # 1. local function name doesn't clash with global one\n    # 2. alpha comes from the function parameter\n    # 3. z comes from the outer global scope\n    def cobb_douglas(K, L):\n        return z * K**alpha * L**(1 - alpha)\n\n    # using this function\n    return cobb_douglas(1.0, 0.5)\n\nalpha = 100 # ignored\nalphas = [0.2, 0.3, 0.5]\n# comprehension variables also have local scope\n# and don't clash with the alpha = 100\n[output_given_alpha(alpha) for alpha in alphas]"
  },
  {
    "objectID": "tutorials/session_1/qe_functions.html#exercises",
    "href": "tutorials/session_1/qe_functions.html#exercises",
    "title": "Functions",
    "section": "",
    "text": "What happens if we try different inputs in our Cobb-Douglas production function?\n\n# Compute returns to scale with different values of `K` and `L` and `gamma`\n\n(back to text)\n\n\n\nDefine a function named var that takes a list (call it x) and computes the variance. This function should use the mean function that we defined earlier.\n$ = _i (x_i - (x))^2 $\n\n# Your code here.\n\n(back to text)\n\n\n\nRedefine the returns_to_scale function and add a docstring.\nConfirm that it works by running the cell containing returns_to_scale? below.\nNote: You do not need to change the actual code in the function — just copy/paste and add a docstring in the correct line.\n\n# re-define the `returns_to_scale` function here\n\n\n# test it here\n\nreturns_to_scale?\n\n(back to text)\n\n\n\nExperiment with the sep and end arguments to the print function.\nThese can only be set by name.\n\n# Your code here.\n\n(back to text)"
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions_correction.html",
    "href": "tutorials/session_6/machine_learning_regressions_correction.html",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed",
    "crumbs": [
      "tutorials",
      "Machine learning: regressions"
    ]
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "href": "tutorials/session_6/machine_learning_regressions_correction.html#diabetes-dataset-basic-regression",
    "title": "Machine learning: regressions",
    "section": "",
    "text": "Import the diabetes dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n\ndataset = sklearn.datasets.load_diabetes()\n# the result is a dictionary:\n# 'data': features\n# 'target' labels\n# 'feature_names': names of the features\n# `DESCR`: description\n\n\nprint( dataset['DESCR'] )\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\n\n\n# create a dataframe\nimport pandas\n\ndf = pandas.DataFrame(dataset['data'], columns=dataset['feature_names'])\n\ndf['disease_progression'] = dataset['target']\n\n\ndf.describe()\n# we observe that mean of varaibles  is zero\n# standard deviations are the same for all variables\n# model has been normalized already:\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ndisease_progression\n\n\n\n\ncount\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n4.420000e+02\n442.000000\n\n\nmean\n-2.511817e-19\n1.230790e-17\n-2.245564e-16\n-4.797570e-17\n-1.381499e-17\n3.918434e-17\n-5.777179e-18\n-9.042540e-18\n9.293722e-17\n1.130318e-17\n152.133484\n\n\nstd\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n4.761905e-02\n77.093005\n\n\nmin\n-1.072256e-01\n-4.464164e-02\n-9.027530e-02\n-1.123988e-01\n-1.267807e-01\n-1.156131e-01\n-1.023071e-01\n-7.639450e-02\n-1.260971e-01\n-1.377672e-01\n25.000000\n\n\n25%\n-3.729927e-02\n-4.464164e-02\n-3.422907e-02\n-3.665608e-02\n-3.424784e-02\n-3.035840e-02\n-3.511716e-02\n-3.949338e-02\n-3.324559e-02\n-3.317903e-02\n87.000000\n\n\n50%\n5.383060e-03\n-4.464164e-02\n-7.283766e-03\n-5.670422e-03\n-4.320866e-03\n-3.819065e-03\n-6.584468e-03\n-2.592262e-03\n-1.947171e-03\n-1.077698e-03\n140.500000\n\n\n75%\n3.807591e-02\n5.068012e-02\n3.124802e-02\n3.564379e-02\n2.835801e-02\n2.984439e-02\n2.931150e-02\n3.430886e-02\n3.243232e-02\n2.791705e-02\n211.500000\n\n\nmax\n1.107267e-01\n5.068012e-02\n1.705552e-01\n1.320436e-01\n1.539137e-01\n1.987880e-01\n1.811791e-01\n1.852344e-01\n1.335973e-01\n1.356118e-01\n346.000000\n\n\n\n\n\n\n\n\nimport seaborn\n\n\nseaborn.pairplot(df)\n\n\n\n\n\n\n\n\nSplit the dataset into a training set (70%) and a test set (30%)\n\nfrom sklearn.model_selection import train_test_split\n\n\n# features: dataset['data']\n# dataset['data'].shape # one line per observation, one column per feature (variable)\n\n\n# labels: dataset['target'] what we are trying to predict\ndataset['target'].shape\n\n(442,)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3, random_state=56)\n# the choice of a random_state initializes a random seed so that every time it is run the notebook\n# returns exactly the same results\n\nTrain a linear model (with intercept) on the training set\n\n# since the model is already normalized, we can create the model directly\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# visualize model predictions:\n\n# from matplotlib import pyplot as plt\n\n# plt.plot(  )\n# plt.plot( model.predict(X_train) )\n\n\nmodel.intercept_ # a\n\n152.82810842206453\n\n\n\nmodel.coef_ # b_1, b_2, .... b_10|\n\narray([   3.04174075, -209.76813682,  501.77871853,  286.88207011,\n       -991.92731799,  603.10838272,  228.80501285,  226.30296964,\n        905.67772303,   92.55739263])\n\n\nCompute the fitting score on the test set. (Bonus: compare with your own computation of \\(R^2\\))\n\nmodel.score(X_test, y_test)\n\n0.43965636272283437\n\n\n\n# compare with the training set:\nmodel.score(X_train, y_train)\n\n0.541861476456197\n\n\nShould we adjust the size of the test set? What would be the problem?\n\n#### WARNING\n####\n#### very bad approach\n\n\n# let's try different sizes\n\nsizes = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nscores = []\nfor s in sizes:\n    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\n\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(sizes, scores)\n\n\n\n\n\n\n\n\nImplement \\(k\\)-fold model with \\(k=3\\).\n\nX = dataset['data']\ny = dataset['target']\n\n\n# to keep the scores\nscores = []\n\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ## train a model in X_train, y_train\n    ## test it on X_test, y_test\n    model = LinearRegression()   # don't forget the round bracket to get a model object\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test) # score with x% test set\n\n    scores.append(score)\n\n\nscores\n\n[0.46930417754348197, 0.4872526062543143, 0.5095496056127979]\n\n\n\n# it gives us a sense of the predictive power of the regression\n\n\nregresults.summary() # econometric estimation of R^2 is 0.51\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndisease_progression\nR-squared:\n0.518\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.27\n\n\nDate:\nMon, 27 Mar 2023\nProb (F-statistic):\n3.83e-62\n\n\nTime:\n21:46:43\nLog-Likelihood:\n-2386.0\n\n\nNo. Observations:\n442\nAIC:\n4794.\n\n\nDf Residuals:\n431\nBIC:\n4839.\n\n\nDf Model:\n10\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n152.1335\n2.576\n59.061\n0.000\n147.071\n157.196\n\n\nage\n-10.0099\n59.749\n-0.168\n0.867\n-127.446\n107.426\n\n\nsex\n-239.8156\n61.222\n-3.917\n0.000\n-360.147\n-119.484\n\n\nbmi\n519.8459\n66.533\n7.813\n0.000\n389.076\n650.616\n\n\nbp\n324.3846\n65.422\n4.958\n0.000\n195.799\n452.970\n\n\ns1\n-792.1756\n416.680\n-1.901\n0.058\n-1611.153\n26.802\n\n\ns2\n476.7390\n339.030\n1.406\n0.160\n-189.620\n1143.098\n\n\ns3\n101.0433\n212.531\n0.475\n0.635\n-316.684\n518.770\n\n\ns4\n177.0632\n161.476\n1.097\n0.273\n-140.315\n494.441\n\n\ns5\n751.2737\n171.900\n4.370\n0.000\n413.407\n1089.140\n\n\ns6\n67.6267\n65.984\n1.025\n0.306\n-62.064\n197.318\n\n\n\n\n\n\nOmnibus:\n1.506\nDurbin-Watson:\n2.029\n\n\nProb(Omnibus):\n0.471\nJarque-Bera (JB):\n1.404\n\n\nSkew:\n0.017\nProb(JB):\n0.496\n\n\nKurtosis:\n2.726\nCond. No.\n227.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo use lasso regression:\n\nfrom sklearn.linear_model import Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size=0.3)\nmodel = Lasso()   # don't forget the round bracket to get a model object\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test) # s\n\n\n# on the test set, the fit of the lasso regression is worse than regular regression\n# the regularization parameter should be changed",
    "crumbs": [
      "tutorials",
      "Machine learning: regressions"
    ]
  },
  {
    "objectID": "tutorials/session_6/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "href": "tutorials/session_6/machine_learning_regressions_correction.html#sparse-regressions-on-the-boston-house-price-dataset",
    "title": "Machine learning: regressions",
    "section": "Sparse regressions on the Boston House Price Dataset",
    "text": "Sparse regressions on the Boston House Price Dataset\nImport the Boston House Price Dataset from sklearn. Explore the data (description, correlations, histograms…)\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\n\nprint(dataset[\"DESCR\"])\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nAn household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surpinsingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n# dataset = sklearn.datasets.load_boston()\nfrom sklearn.datasets import fetch_california_housing \ndataset = fetch_california_housing()\n\nSplit the dataset into a training set (70%) and a test set (30%).\n\nX = dataset['data']\ny = dataset['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=58)\n\nTrain a lasso model to predict house prices. Compute the score on the test set.\n\n# we should check that the data is normalized, or normalize it ourselves\n\n\nfrom sklearn.linear_model import Lasso\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\nmodel_lasso.score(X_test, y_test)\n\n0.28204855993177635\n\n\nTrain a ridge model to predict house prices. Which one is better?\n\nfrom sklearn.linear_model import Ridge\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train, y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nmodel_ridge.score(X_test, y_test)\n\n0.6060031802405054\n\n\nIt looks like the ridge model has a better fit (score). However, we should have left a test set appart and not used it at all during training phase. Here it has influenced the choice of the model (between ridge and lasso).",
    "crumbs": [
      "tutorials",
      "Machine learning: regressions"
    ]
  },
  {
    "objectID": "pushups/pushups_3.html",
    "href": "pushups/pushups_3.html",
    "title": "Sklearn: sparse regression",
    "section": "",
    "text": "Sklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\nProperly train a linear logistic regression to predict cancer morbidity.\nUse k-fold validation to validate the model\nTry with other classifiers. Which one is best?"
  },
  {
    "objectID": "pushups/pushups_3.html#predicting-breast-cancer",
    "href": "pushups/pushups_3.html#predicting-breast-cancer",
    "title": "Sklearn: sparse regression",
    "section": "",
    "text": "Sklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\nProperly train a linear logistic regression to predict cancer morbidity.\nUse k-fold validation to validate the model\nTry with other classifiers. Which one is best?"
  },
  {
    "objectID": "pushups/pushups_3_correction.html",
    "href": "pushups/pushups_3_correction.html",
    "title": "Sparse regression with sklearn",
    "section": "",
    "text": "Sklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n# the as_frame option makes the function return a dataframe\ndataset = sklearn.datasets.load_breast_cancer(as_frame=True)\n\n\ndata = dataset['data']\ntarget = dataset['target']\n\nProperly train a linear logistic regression to predict cancer morbidity.\n\n# separate the training set and the testset\nimport sklearn.model_selection\ndata_train, data_test, target_train, target_test = sklearn.model_selection.train_test_split(data, target)\n\n\n# quickly check thes size of th samples, correspond to  what we want:\n[e.shape for e in [data_train, data_test, target_train, target_test]]\n\n[(426, 30), (143, 30), (426,), (143,)]\n\n\n\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LogisticRegression()\n\n\nmodel.fit(data_train, target_train)\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# We can check the performance out of sample:\n\n\nmodel.score(data_test, target_test)\n\n0.8951048951048951\n\n\n\n# to know what the scores represent, we can read the doc\n# it shows that score is measured by mean accuracy\n# i.e. number of correct predictions divided by total number of predictions\nmodel.score?\n\n\nSignature: model.score(X, y, sample_weight=None)\nDocstring:\nReturn the mean accuracy on the given test data and labels.\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for `X`.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\nReturns\n-------\nscore : float\n    Mean accuracy of ``self.predict(X)`` wrt. `y`.\nFile:      /opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/base.py\nType:      method\n\n\n\nBonus: the warning message suggests to scale the data. Let’s redo the last few steps accordingly\n\nimport sklearn.preprocessing\nscaler = sklearn.preprocessing.StandardScaler()\nscaler.fit(data)\nscaled_data = scaler.transform(data)\n\n\n# let's repackage in a dataframe\nimport pandas\nscaled_data = pandas.DataFrame(scaled_data, columns=data.columns)\n# and check the result has zero mean and constant standard deviation\nscaled_data.describe()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\ncount\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n...\n5.690000e+02\n5.690000e+02\n5.690000e+02\n569.000000\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n\n\nmean\n-1.373633e-16\n6.868164e-17\n-1.248757e-16\n-2.185325e-16\n-8.366672e-16\n1.873136e-16\n4.995028e-17\n-4.995028e-17\n1.748260e-16\n4.745277e-16\n...\n-8.241796e-16\n1.248757e-17\n-3.746271e-16\n0.000000\n-2.372638e-16\n-3.371644e-16\n7.492542e-17\n2.247763e-16\n2.622390e-16\n-5.744282e-16\n\n\nstd\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n...\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n\n\nmin\n-2.029648e+00\n-2.229249e+00\n-1.984504e+00\n-1.454443e+00\n-3.112085e+00\n-1.610136e+00\n-1.114873e+00\n-1.261820e+00\n-2.744117e+00\n-1.819865e+00\n...\n-1.726901e+00\n-2.223994e+00\n-1.693361e+00\n-1.222423\n-2.682695e+00\n-1.443878e+00\n-1.305831e+00\n-1.745063e+00\n-2.160960e+00\n-1.601839e+00\n\n\n25%\n-6.893853e-01\n-7.259631e-01\n-6.919555e-01\n-6.671955e-01\n-7.109628e-01\n-7.470860e-01\n-7.437479e-01\n-7.379438e-01\n-7.032397e-01\n-7.226392e-01\n...\n-6.749213e-01\n-7.486293e-01\n-6.895783e-01\n-0.642136\n-6.912304e-01\n-6.810833e-01\n-7.565142e-01\n-7.563999e-01\n-6.418637e-01\n-6.919118e-01\n\n\n50%\n-2.150816e-01\n-1.046362e-01\n-2.359800e-01\n-2.951869e-01\n-3.489108e-02\n-2.219405e-01\n-3.422399e-01\n-3.977212e-01\n-7.162650e-02\n-1.782793e-01\n...\n-2.690395e-01\n-4.351564e-02\n-2.859802e-01\n-0.341181\n-4.684277e-02\n-2.695009e-01\n-2.182321e-01\n-2.234689e-01\n-1.274095e-01\n-2.164441e-01\n\n\n75%\n4.693926e-01\n5.841756e-01\n4.996769e-01\n3.635073e-01\n6.361990e-01\n4.938569e-01\n5.260619e-01\n6.469351e-01\n5.307792e-01\n4.709834e-01\n...\n5.220158e-01\n6.583411e-01\n5.402790e-01\n0.357589\n5.975448e-01\n5.396688e-01\n5.311411e-01\n7.125100e-01\n4.501382e-01\n4.507624e-01\n\n\nmax\n3.971288e+00\n4.651889e+00\n3.976130e+00\n5.250529e+00\n4.770911e+00\n4.568425e+00\n4.243589e+00\n3.927930e+00\n4.484751e+00\n4.910919e+00\n...\n4.094189e+00\n3.885905e+00\n4.287337e+00\n5.930172\n3.955374e+00\n5.112877e+00\n4.700669e+00\n2.685877e+00\n6.046041e+00\n6.846856e+00\n\n\n\n\n8 rows × 30 columns\n\n\n\n\n# for compatibility purpose we save the scaled dataframe as data\ndata = scaled_data\n\n\n# and redo the same training\n\n\n# separate the training set and the testset\ndata_train, data_test, target_train, target_test = sklearn.model_selection.train_test_split(data, target)\n\n\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LogisticRegression()\n\n\nmodel.fit(data_train, target_train) # this time, we don't get any error message\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# and actually improve the prediction (which might just be chance)\n\n\nmodel.score(data_test, target_test)\n\n0.972027972027972\n\n\nUse k-fold validation to validate the model\n\n# because the dataset is relatively small we didn't set aside a validation set\n# instead we rely on cross-validation\n\n# we split the dataset in 5\n# this provides 5 different testsets (with 20% of observation) to test the training on the remaining set (80%)\n\n\nkf = sklearn.model_selection.KFold(n_splits=5)\n\n\nscores = []\n\nfor i_train, i_test in kf.split(data):\n    \n    # i_train and i_test are indices of observations belonging to one of the two datasets\n    kf_data_train = data.iloc[i_train,:]\n    kf_target_train = target.iloc[i_train]\n    \n    kf_data_test = data.iloc[i_test,:]\n    kf_target_test = target.iloc[i_test]\n    \n    model_kf = sklearn.linear_model.LogisticRegression()\n    \n    # we train the model\n    model_kf.fit(kf_data_train, kf_target_train)\n    \n    # and test it\n    sc = model_kf.score(kf_data_test, kf_target_test)\n    \n    scores.append(sc)\n    \n    print(f\"Score: {sc}\")\n\nScore: 0.9736842105263158\nScore: 0.956140350877193\nScore: 0.9824561403508771\nScore: 0.9824561403508771\nScore: 0.9911504424778761\n\n\nThere is some volatility in the scores, but it stays reliably over 95% accuracy.\n\n# to get an estimate of accuracy we can compute the mean:\nprint(f\"KFold validation: mean accuracy {sum(scores)/5}\")\n\nKFold validation: mean accuracy 0.9771774569166279\n\n\nTry with other classifiers. Which one is best?\nThe dataset being relatively small we can try Support Vector Machines, which are known to generalize well (see discussion here).\nWe perform a kfold selection exactly as above.\n\nkf = sklearn.model_selection.KFold(n_splits=5)\n\n\nscores_svc = []\n\nfor i_train, i_test in kf.split(data):\n    \n    # i_train and i_test are indices of observations belonging to one of the two datasets\n    kf_data_train = data.iloc[i_train,:]\n    kf_target_train = target.iloc[i_train]\n    \n    kf_data_test = data.iloc[i_test,:]\n    kf_target_test = target.iloc[i_test]\n    \n    # we just change the following line\n    model_kf = sklearn.svm.SVC()\n    \n    # we train the model\n    model_kf.fit(kf_data_train, kf_target_train)\n    \n    # and test it\n    sc = model_kf.score(kf_data_test, kf_target_test)\n    \n    scores_svc.append(sc)\n    \n    print(f\"Score: {sc}\")\n\nScore: 0.9473684210526315\nScore: 0.9649122807017544\nScore: 0.9736842105263158\nScore: 0.9912280701754386\nScore: 0.9734513274336283\n\n\n\n# to get an estimate of accuracy we can compute the mean:\nprint(f\"KFold validation: mean accuracy {sum(scores_svc)/5}\")\n\nKFold validation: mean accuracy 0.9701288619779538\n\n\nComment: performance of support vector machine is similar to logistic regression. To assess the gains, we can compare the difference to both estimate (0.007) to the standard deviation of either of two models. Both are geater than 0.01, meaning that the difference between the two models is probably not significant.\n\n# we can compute the standard deviation as follows (googld standard deviation python)\n\nimport numpy \nprint( numpy.std(scores) )\nprint( numpy.std(scores_svc) )\n\n0.01188053806820839\n0.0142415326274357",
    "crumbs": [
      "pushups",
      "Sparse regression with sklearn"
    ]
  },
  {
    "objectID": "pushups/pushups_3_correction.html#predicting-breast-cancer",
    "href": "pushups/pushups_3_correction.html#predicting-breast-cancer",
    "title": "Sparse regression with sklearn",
    "section": "",
    "text": "Sklearn includes the Winsconsin breast cancer database. It associates medical outcomes for tumor observation, with several characteristics. Can a machine learn how to predict whether a cancer is benign or malignant ?\nImport the Breast Cancer Dataset from sklearn. Describe it.\n\nimport sklearn\nimport sklearn.datasets\n# the as_frame option makes the function return a dataframe\ndataset = sklearn.datasets.load_breast_cancer(as_frame=True)\n\n\ndata = dataset['data']\ntarget = dataset['target']\n\nProperly train a linear logistic regression to predict cancer morbidity.\n\n# separate the training set and the testset\nimport sklearn.model_selection\ndata_train, data_test, target_train, target_test = sklearn.model_selection.train_test_split(data, target)\n\n\n# quickly check thes size of th samples, correspond to  what we want:\n[e.shape for e in [data_train, data_test, target_train, target_test]]\n\n[(426, 30), (143, 30), (426,), (143,)]\n\n\n\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LogisticRegression()\n\n\nmodel.fit(data_train, target_train)\n\n/opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# We can check the performance out of sample:\n\n\nmodel.score(data_test, target_test)\n\n0.8951048951048951\n\n\n\n# to know what the scores represent, we can read the doc\n# it shows that score is measured by mean accuracy\n# i.e. number of correct predictions divided by total number of predictions\nmodel.score?\n\n\nSignature: model.score(X, y, sample_weight=None)\nDocstring:\nReturn the mean accuracy on the given test data and labels.\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for `X`.\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\nReturns\n-------\nscore : float\n    Mean accuracy of ``self.predict(X)`` wrt. `y`.\nFile:      /opt/conda/envs/escpython/lib/python3.10/site-packages/sklearn/base.py\nType:      method\n\n\n\nBonus: the warning message suggests to scale the data. Let’s redo the last few steps accordingly\n\nimport sklearn.preprocessing\nscaler = sklearn.preprocessing.StandardScaler()\nscaler.fit(data)\nscaled_data = scaler.transform(data)\n\n\n# let's repackage in a dataframe\nimport pandas\nscaled_data = pandas.DataFrame(scaled_data, columns=data.columns)\n# and check the result has zero mean and constant standard deviation\nscaled_data.describe()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\ncount\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n...\n5.690000e+02\n5.690000e+02\n5.690000e+02\n569.000000\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n5.690000e+02\n\n\nmean\n-1.373633e-16\n6.868164e-17\n-1.248757e-16\n-2.185325e-16\n-8.366672e-16\n1.873136e-16\n4.995028e-17\n-4.995028e-17\n1.748260e-16\n4.745277e-16\n...\n-8.241796e-16\n1.248757e-17\n-3.746271e-16\n0.000000\n-2.372638e-16\n-3.371644e-16\n7.492542e-17\n2.247763e-16\n2.622390e-16\n-5.744282e-16\n\n\nstd\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n...\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n1.000880e+00\n\n\nmin\n-2.029648e+00\n-2.229249e+00\n-1.984504e+00\n-1.454443e+00\n-3.112085e+00\n-1.610136e+00\n-1.114873e+00\n-1.261820e+00\n-2.744117e+00\n-1.819865e+00\n...\n-1.726901e+00\n-2.223994e+00\n-1.693361e+00\n-1.222423\n-2.682695e+00\n-1.443878e+00\n-1.305831e+00\n-1.745063e+00\n-2.160960e+00\n-1.601839e+00\n\n\n25%\n-6.893853e-01\n-7.259631e-01\n-6.919555e-01\n-6.671955e-01\n-7.109628e-01\n-7.470860e-01\n-7.437479e-01\n-7.379438e-01\n-7.032397e-01\n-7.226392e-01\n...\n-6.749213e-01\n-7.486293e-01\n-6.895783e-01\n-0.642136\n-6.912304e-01\n-6.810833e-01\n-7.565142e-01\n-7.563999e-01\n-6.418637e-01\n-6.919118e-01\n\n\n50%\n-2.150816e-01\n-1.046362e-01\n-2.359800e-01\n-2.951869e-01\n-3.489108e-02\n-2.219405e-01\n-3.422399e-01\n-3.977212e-01\n-7.162650e-02\n-1.782793e-01\n...\n-2.690395e-01\n-4.351564e-02\n-2.859802e-01\n-0.341181\n-4.684277e-02\n-2.695009e-01\n-2.182321e-01\n-2.234689e-01\n-1.274095e-01\n-2.164441e-01\n\n\n75%\n4.693926e-01\n5.841756e-01\n4.996769e-01\n3.635073e-01\n6.361990e-01\n4.938569e-01\n5.260619e-01\n6.469351e-01\n5.307792e-01\n4.709834e-01\n...\n5.220158e-01\n6.583411e-01\n5.402790e-01\n0.357589\n5.975448e-01\n5.396688e-01\n5.311411e-01\n7.125100e-01\n4.501382e-01\n4.507624e-01\n\n\nmax\n3.971288e+00\n4.651889e+00\n3.976130e+00\n5.250529e+00\n4.770911e+00\n4.568425e+00\n4.243589e+00\n3.927930e+00\n4.484751e+00\n4.910919e+00\n...\n4.094189e+00\n3.885905e+00\n4.287337e+00\n5.930172\n3.955374e+00\n5.112877e+00\n4.700669e+00\n2.685877e+00\n6.046041e+00\n6.846856e+00\n\n\n\n\n8 rows × 30 columns\n\n\n\n\n# for compatibility purpose we save the scaled dataframe as data\ndata = scaled_data\n\n\n# and redo the same training\n\n\n# separate the training set and the testset\ndata_train, data_test, target_train, target_test = sklearn.model_selection.train_test_split(data, target)\n\n\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LogisticRegression()\n\n\nmodel.fit(data_train, target_train) # this time, we don't get any error message\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# and actually improve the prediction (which might just be chance)\n\n\nmodel.score(data_test, target_test)\n\n0.972027972027972\n\n\nUse k-fold validation to validate the model\n\n# because the dataset is relatively small we didn't set aside a validation set\n# instead we rely on cross-validation\n\n# we split the dataset in 5\n# this provides 5 different testsets (with 20% of observation) to test the training on the remaining set (80%)\n\n\nkf = sklearn.model_selection.KFold(n_splits=5)\n\n\nscores = []\n\nfor i_train, i_test in kf.split(data):\n    \n    # i_train and i_test are indices of observations belonging to one of the two datasets\n    kf_data_train = data.iloc[i_train,:]\n    kf_target_train = target.iloc[i_train]\n    \n    kf_data_test = data.iloc[i_test,:]\n    kf_target_test = target.iloc[i_test]\n    \n    model_kf = sklearn.linear_model.LogisticRegression()\n    \n    # we train the model\n    model_kf.fit(kf_data_train, kf_target_train)\n    \n    # and test it\n    sc = model_kf.score(kf_data_test, kf_target_test)\n    \n    scores.append(sc)\n    \n    print(f\"Score: {sc}\")\n\nScore: 0.9736842105263158\nScore: 0.956140350877193\nScore: 0.9824561403508771\nScore: 0.9824561403508771\nScore: 0.9911504424778761\n\n\nThere is some volatility in the scores, but it stays reliably over 95% accuracy.\n\n# to get an estimate of accuracy we can compute the mean:\nprint(f\"KFold validation: mean accuracy {sum(scores)/5}\")\n\nKFold validation: mean accuracy 0.9771774569166279\n\n\nTry with other classifiers. Which one is best?\nThe dataset being relatively small we can try Support Vector Machines, which are known to generalize well (see discussion here).\nWe perform a kfold selection exactly as above.\n\nkf = sklearn.model_selection.KFold(n_splits=5)\n\n\nscores_svc = []\n\nfor i_train, i_test in kf.split(data):\n    \n    # i_train and i_test are indices of observations belonging to one of the two datasets\n    kf_data_train = data.iloc[i_train,:]\n    kf_target_train = target.iloc[i_train]\n    \n    kf_data_test = data.iloc[i_test,:]\n    kf_target_test = target.iloc[i_test]\n    \n    # we just change the following line\n    model_kf = sklearn.svm.SVC()\n    \n    # we train the model\n    model_kf.fit(kf_data_train, kf_target_train)\n    \n    # and test it\n    sc = model_kf.score(kf_data_test, kf_target_test)\n    \n    scores_svc.append(sc)\n    \n    print(f\"Score: {sc}\")\n\nScore: 0.9473684210526315\nScore: 0.9649122807017544\nScore: 0.9736842105263158\nScore: 0.9912280701754386\nScore: 0.9734513274336283\n\n\n\n# to get an estimate of accuracy we can compute the mean:\nprint(f\"KFold validation: mean accuracy {sum(scores_svc)/5}\")\n\nKFold validation: mean accuracy 0.9701288619779538\n\n\nComment: performance of support vector machine is similar to logistic regression. To assess the gains, we can compare the difference to both estimate (0.007) to the standard deviation of either of two models. Both are geater than 0.01, meaning that the difference between the two models is probably not significant.\n\n# we can compute the standard deviation as follows (googld standard deviation python)\n\nimport numpy \nprint( numpy.std(scores) )\nprint( numpy.std(scores_svc) )\n\n0.01188053806820839\n0.0142415326274357",
    "crumbs": [
      "pushups",
      "Sparse regression with sklearn"
    ]
  },
  {
    "objectID": "pushups/pushups_2.html",
    "href": "pushups/pushups_2.html",
    "title": "Graph Replication Exercise",
    "section": "",
    "text": "Our World in Data features tons of very interesting graphs like the following:\n\nFor each graph it is possible to download the corresponding data file as csv:\n\n\n\nChoose one graph that you find particularly interesting or beautiful. Download it from the website and upload it on nuvolos in the current folder\n\nImport the csv file using function pandas.read_csv and print some rows of the database\nTry to replicate the graph using matplotlib or altair (it doesn’t have to be identical, but the main message must be conveyed)."
  },
  {
    "objectID": "pushups/pushups_2.html#import-data",
    "href": "pushups/pushups_2.html#import-data",
    "title": "Graph Replication Exercise",
    "section": "",
    "text": "Choose one graph that you find particularly interesting or beautiful. Download it from the website and upload it on nuvolos in the current folder\n\nImport the csv file using function pandas.read_csv and print some rows of the database\nTry to replicate the graph using matplotlib or altair (it doesn’t have to be identical, but the main message must be conveyed)."
  }
]